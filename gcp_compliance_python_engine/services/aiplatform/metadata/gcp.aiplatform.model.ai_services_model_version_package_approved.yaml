rule_id: gcp.aiplatform.model.ai_services_model_version_package_approved
service: aiplatform
resource: model
requirement: Ai Services Model Version Package Approved
scope: aiplatform.model.security
domain: compliance_and_governance
subcategory: security_monitoring
severity: low
title: Approve AI Model Versions in GCP AI Platform
rationale: Approving AI model versions ensures that only vetted and secure model packages
  are deployed, reducing the risk of introducing vulnerabilities or unauthorized changes.
  This supports compliance with regulatory standards and protects sensitive data processed
  by AI models from potential exploitation and breaches.
description: This check verifies that all AI model versions deployed in the Google
  Cloud AI Platform have been approved through a defined security review process.
  The configuration should include a mechanism for model version approval before deployment,
  ensuring that each model version package meets security and compliance standards.
  Administrators can remediate by implementing an approval workflow and ensuring that
  only approved versions are deployed. Verification involves reviewing audit logs
  for model deployments to confirm compliance with approval processes.
references:
- https://cloud.google.com/ai-platform/docs/model-management
- https://cloud.google.com/security/compliance
- https://cloud.google.com/security/compliance/frameworks/cis#cis-benchmarks
- https://cloud.google.com/security/compliance/frameworks/iso-27001
- https://cloud.google.com/security/best-practices/identity
- https://cloud.google.com/security/overview
