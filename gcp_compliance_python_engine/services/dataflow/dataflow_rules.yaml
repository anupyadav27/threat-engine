# ============================================================================
# CURSOR AI: GCP SERVICE VALIDATION INSTRUCTIONS
# ============================================================================
# 
# üéØ YOUR MISSION: Validate and fix this GCP service YAML file
#
# WORKFLOW:
# 1. Read this entire YAML file structure
# 2. Run: `export GCP_ENGINE_FILTER_SERVICES="<SERVICE>" && python engine/gcp_engine.py > output/test_<service>.json 2>&1`
# 3. Analyze output - check for inventories, main_checks, errors
# 4. Fix issues in discovery and checks sections below
# 5. Re-run engine and verify output
# 6. Iterate until all checks work (PASS/FAIL based on resources)
# 7. Update validation status at bottom of file
#
# COMMON ISSUES & FIXES:
#
# ‚ùå Empty inventories ‚Üí Fix discovery action (must be: list_<resource>, aggregatedList_<resource>)
# ‚ùå Zero checks executed ‚Üí Fix for_each to match discovery_id
# ‚ùå All checks fail ‚Üí Fix field paths to match GCP API response
# ‚ùå Skipped checks ‚Üí Verify discovery_id exists and completed
# ‚ùå Python errors ‚Üí Check YAML syntax, action names
#
# TESTING:
# - Run engine: `export GCP_ENGINE_FILTER_SERVICES="<service>" && python engine/gcp_engine.py > output/test.json 2>&1`
# - Check output: `cat output/test.json | python3 -m json.tool`
# - Verify: inventories array has resources, main_checks array has results
#
# SUCCESS CRITERIA:
# ‚úÖ Engine runs without Python exceptions
# ‚úÖ Inventories populated (if resources exist in GCP)
# ‚úÖ All checks execute (PASS/FAIL based on actual resource state)
# ‚úÖ No skipped checks (unless expected)
# ‚úÖ Clean JSON output
#
# DOCUMENTATION:
# After fixing, update validation section at bottom:
# # VALIDATION STATUS:
# # - Issues Found: <what was wrong>
# # - Fixes Applied: <what you fixed>
# # - Tested: <date>
# # - Inventories: <count>
# # - Checks: <count executed>
# # - Status: ‚úÖ VALIDATED / ‚è≥ IN PROGRESS / ‚ùå NEEDS WORK
#
# ============================================================================

service_name:  # e.g., compute, gcs, pubsub

dataflow:
  version: '1.0'
  provider: gcp
  service: dataflow
  scope: regional
  discovery:
  - discovery_id: list_dataflow_jobs
    calls:
    - action: list
      fields:
      - path: name
        var: job_name
      - path: id
        var: job_id
      - path: location
        var: job_location
  - discovery_id: list_dataflow_templates
    calls:
    - action: list
      fields:
      - path: name
        var: template_name
      - path: metadata
        var: template_metadata
  - discovery_id: list_dataflow_pipelines
    calls:
    - action: list
      fields:
      - path: name
        var: pipeline_name
      - path: pipelineDefinition
        var: pipeline_definition
  checks:
  - check_id: gcp.dataflow.job.data_pipeline_allowed_values_defined_where_applicable
    title: Ensure Dataflow Pipeline Uses Allowed Parameter Values
    severity: medium
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.userAgent
        operator: exists
      - path: runtimeParameters
        operator: exists
    - action: eval
      fields:
      - path: runtimeParameters
        operator: contains
        expected: allowedValues
  - check_id: gcp.dataflow.job.data_pipeline_attributes_no_plaintext_secrets
    title: Prevent Plaintext Secrets in Dataflow Job Attributes
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels
        operator: not_contains
        expected: password
    - action: eval
      fields:
      - path: environment.additionalUserLabels
        operator: not_contains
        expected: secret
    - action: eval
      fields:
      - path: environment.additionalUserLabels
        operator: not_contains
        expected: key
  - check_id: gcp.dataflow.job.data_pipeline_binding_only_expected_params_bound
    title: Ensure Dataflow Job Parameters Are Properly Bound
    severity: medium
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: runtimeParameters
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalExperiments
        operator: exists
  - check_id: gcp.dataflow.job.data_pipeline_binding_secret_refs_resolved_at_runtime
    title: Ensure Dataflow Secret Refs Resolved at Runtime for Security
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.secret-ref
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.secret-ref
        operator: contains
        expected: projects/
  - check_id: gcp.dataflow.job.data_pipeline_binding_sensitive_params_not_logged
    title: Prevent Logging of Sensitive Params in Dataflow Jobs
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.log-sensitive
        operator: not_exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.debug-mode
        operator: not_equals
        expected: 'true'
  - check_id: gcp.dataflow.job.data_pipeline_constraint_allowlist_defined_when_applicable
    title: Ensure Dataflow Job Pipeline Constraints Are Allowlisted
    severity: medium
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.constraint-allowlist
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.constraint-allowlist
        operator: not_equals
        expected: ''
  - check_id: gcp.dataflow.job.data_pipeline_constraint_max_length_defined_when_applicable
    title: Ensure Dataflow Job Pipeline Max Length is Defined
    severity: low
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.max-length
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.max-length
        operator: not_equals
        expected: '0'
  - check_id: gcp.dataflow.job.data_pipeline_constraint_min_length_defined_when_applicable
    title: Ensure Dataflow Pipeline Min Length Constraint Is Defined
    severity: low
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.min-length
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.min-length
        operator: not_equals
        expected: '0'
  - check_id: gcp.dataflow.job.data_pipeline_constraint_pattern_defined_when_applicable
    title: Ensure Dataflow Job Constraint Patterns for Data Pipelines
    severity: medium
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.pattern-constraint
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.pattern-constraint
        operator: not_equals
        expected: ''
  - check_id: gcp.dataflow.job.data_pipeline_default_not_sensitive
    title: Ensure Dataflow Job Pipelines Are Not Using Default Encryption
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.tempLocation
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.encryption
        operator: not_equals
        expected: default
  - check_id: gcp.dataflow.job.data_pipeline_definition_signed_and_verified
    title: Ensure Dataflow Pipeline Definitions are Signed and Verified
    severity: high
    for_each: list_dataflow_pipelines
    logic: AND
    calls:
    - action: eval
      fields:
      - path: pipelineDefinition.signature
        operator: exists
    - action: eval
      fields:
      - path: pipelineDefinition.verified
        operator: equals
        expected: true
  - check_id: gcp.dataflow.job.data_pipeline_definition_storage_encrypted
    title: Ensure Dataflow Job Pipeline Definitions Are Encrypted at Rest
    severity: high
    for_each: list_dataflow_pipelines
    logic: AND
    calls:
    - action: eval
      fields:
      - path: pipelineDefinition.storageEncryption
        operator: exists
    - action: eval
      fields:
      - path: pipelineDefinition.storageEncryption.enabled
        operator: equals
        expected: true
  - check_id: gcp.dataflow.job.data_pipeline_definition_storage_private
    title: Ensure Dataflow Pipeline Definitions are Stored Privately
    severity: high
    for_each: list_dataflow_pipelines
    logic: AND
    calls:
    - action: eval
      fields:
      - path: pipelineDefinition.access
        operator: equals
        expected: private
    - action: eval
      fields:
      - path: pipelineDefinition.publicAccess
        operator: not_exists
  - check_id: gcp.dataflow.job.data_pipeline_definition_version_immutability_enforced
    title: Ensure Dataflow Pipeline Definition Version Immutability
    severity: medium
    for_each: list_dataflow_pipelines
    logic: AND
    calls:
    - action: eval
      fields:
      - path: pipelineDefinition.versionImmutable
        operator: equals
        expected: true
    - action: eval
      fields:
      - path: pipelineDefinition.version
        operator: exists
  - check_id: gcp.dataflow.job.data_pipeline_max_length_defined_where_applicable
    title: Ensure Dataflow Job Pipeline Max Length is Defined
    severity: low
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.pipeline-max-length
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.pipeline-max-length
        operator: not_equals
        expected: ''
  - check_id: gcp.dataflow.job.data_pipeline_metadata_disallowed_attribute_keys_blocked
    title: Block Disallowed Metadata Keys in Dataflow Jobs
    severity: medium
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.admin
        operator: not_exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.root
        operator: not_exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.system
        operator: not_exists
  - check_id: gcp.dataflow.job.data_pipeline_metadata_sensitive_keys_require_secret_type
    title: Ensure Secret Type for Sensitive Keys in Dataflow Metadata
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.secret-type
        operator: exists
  - check_id: gcp.dataflow.job.data_pipeline_metadata_values_not_plaintext_secret
    title: Ensure Dataflow Metadata Values Are Not Plaintext Secrets
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels
        operator: not_contains
        expected: BEGIN PRIVATE KEY
    - action: eval
      fields:
      - path: environment.additionalUserLabels
        operator: not_contains
        expected: BEGIN RSA PRIVATE KEY
  - check_id: gcp.dataflow.job.data_pipeline_min_length_defined_where_applicable
    title: Ensure Data Pipeline Min Length is Defined for Dataflow Jobs
    severity: low
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.pipeline-min-length
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.pipeline-min-length
        operator: not_equals
        expected: ''
  - check_id: gcp.dataflow.job.data_pipeline_object_environment_no_plaintext_secrets
    title: Avoid Plaintext Secrets in Dataflow Pipeline Environments
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalExperiments
        operator: not_contains
        expected: password=
    - action: eval
      fields:
      - path: environment.additionalExperiments
        operator: not_contains
        expected: secret=
    - action: eval
      fields:
      - path: environment.additionalExperiments
        operator: not_contains
        expected: key=
  - check_id: gcp.dataflow.job.data_pipeline_object_metadata_no_plaintext_secrets
    title: Ensure No Plaintext Secrets in Dataflow Pipeline Object Metadata
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: labels
        operator: not_contains
        expected: password
    - action: eval
      fields:
      - path: labels
        operator: not_contains
        expected: secret
    - action: eval
      fields:
      - path: labels
        operator: not_contains
        expected: private-key
  - check_id: gcp.dataflow.job.data_pipeline_object_no_plaintext_secrets
    title: Ensure Dataflow Pipeline Objects Contain No Plaintext Secrets
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: __self__
        operator: not_contains
        expected: BEGIN PRIVATE KEY
    - action: eval
      fields:
      - path: __self__
        operator: not_contains
        expected: 'password:'
    - action: eval
      fields:
      - path: __self__
        operator: not_contains
        expected: 'secret:'
  - check_id: gcp.dataflow.job.data_pipeline_object_parameter_no_plaintext_secrets
    title: Ensure Dataflow Pipeline Parameters Contain No Plaintext Secrets
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: runtimeParameters
        operator: not_contains
        expected: password
    - action: eval
      fields:
      - path: runtimeParameters
        operator: not_contains
        expected: secret
    - action: eval
      fields:
      - path: runtimeParameters
        operator: not_contains
        expected: private_key
  - check_id: gcp.dataflow.job.data_pipeline_object_parameter_values_no_plaintext_secrets
    title: Ensure Dataflow Pipeline Parameter Values Contain No Plaintext Secrets
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: runtimeParameters
        operator: exists
    - action: eval
      fields:
      - path: runtimeParameters
        operator: not_contains
        expected: BEGIN CERTIFICATE
    - action: eval
      fields:
      - path: runtimeParameters
        operator: not_contains
        expected: BEGIN PRIVATE KEY
  - check_id: gcp.dataflow.job.data_pipeline_parameter_allowed_values_defined_where_applicable
    title: Ensure Dataflow Pipeline Parameter Allowed Values Are Defined
    severity: medium
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: runtimeParameters
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.allowed-values
        operator: exists
  - check_id: gcp.dataflow.job.data_pipeline_parameter_constraint_allowlist_defined_when_applicable
    title: Ensure Dataflow Pipeline Parameter Constraint Allowlist is Defined
    severity: medium
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.parameter-allowlist
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.parameter-allowlist
        operator: not_equals
        expected: ''
  - check_id: gcp.dataflow.job.data_pipeline_parameter_constraint_max_length_defined_when_applicable
    title: Ensure Dataflow Pipeline Parameter Max Length Constraint is Defined
    severity: low
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.parameter-max-length
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.parameter-max-length
        operator: not_equals
        expected: '0'
  - check_id: gcp.dataflow.job.data_pipeline_parameter_constraint_min_length_defined_when_applicable
    title: Ensure Dataflow Pipeline Parameter Min Length Constraint is Defined
    severity: low
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.parameter-min-length
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.parameter-min-length
        operator: not_equals
        expected: '0'
  - check_id: gcp.dataflow.job.data_pipeline_parameter_constraint_pattern_defined_when_applicable
    title: Ensure Dataflow Pipeline Parameter Pattern Constraint is Defined
    severity: medium
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.parameter-pattern
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.parameter-pattern
        operator: not_equals
        expected: ''
  - check_id: gcp.dataflow.job.data_pipeline_parameter_default_not_sensitive
    title: Ensure Dataflow Pipeline Parameter Defaults Are Not Sensitive
    severity: high
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: runtimeParameters
        operator: not_contains
        expected: default_password
    - action: eval
      fields:
      - path: runtimeParameters
        operator: not_contains
        expected: default_secret
    - action: eval
      fields:
      - path: runtimeParameters
        operator: not_contains
        expected: default_key
  - check_id: gcp.dataflow.job.data_pipeline_parameter_max_length_defined_where_applicable
    title: Ensure Dataflow Pipeline Parameter Max Length is Defined Where Applicable
    severity: low
    for_each: list_dataflow_jobs
    logic: AND
    calls:
    - action: eval
      fields:
      - path: environment.additionalUserLabels.param-max-length
        operator: exists
    - action: eval
      fields:
      - path: environment.additionalUserLabels.param-max-length
        operator: not_equals
        expected: ''
  api_name: dataflow
  api_version: v1b3
  project_param_format: projects/{{project_id}}
