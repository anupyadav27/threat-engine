rule_id: gcp.dataflow.job.data_pipeline_constraint_max_length_defined_when_applicable
service: dataflow
resource: job
requirement: Data Pipeline Constraint Max Length Defined When Applicable
scope: dataflow.job.security
domain: data_protection_and_privacy
subcategory: encryption_at_rest
severity: medium
title: Ensure Dataflow Job Pipeline Max Length is Defined
rationale: Defining a maximum length for data pipelines in Dataflow jobs helps prevent
  resource exhaustion and potential service disruptions. This is crucial for maintaining
  the availability and reliability of services, as well as safeguarding sensitive
  data from unintentional exposure due to overly complex or long-running processes.
  Compliance with data protection regulations often requires such constraints to manage
  and mitigate operational risks.
description: This rule checks if the maximum length for data pipelines in Dataflow
  jobs is specified where applicable. Configuring this setting involves defining constraints
  in the pipeline's configuration files or using the Dataflow API. To verify, review
  the pipeline configuration in the Dataflow console or via the GCP CLI, ensuring
  a max length is set. Remediation involves updating the pipeline configuration to
  include a max length constraint, thereby optimizing resource usage and enhancing
  security posture.
references:
- https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
- https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
- https://www.iso.org/iso-27001-information-security.html
- https://cloud.google.com/security/deployment-best-practices
