rule_id: gcp.dataflow.job.data_pipeline_max_length_defined_where_applicable
service: dataflow
resource: job
requirement: Data Pipeline Max Length Defined Where Applicable
scope: dataflow.job.security
domain: data_protection_and_privacy
subcategory: encryption_at_rest
severity: medium
title: Ensure Dataflow Job Pipeline Max Length is Defined
rationale: Defining a maximum length for Dataflow job pipelines helps prevent excessive
  data processing durations which can lead to increased costs, potential data exposure,
  and compliance violations. By setting clear limits, organizations can mitigate risks
  associated with long-running data processes that may inadvertently handle sensitive
  data longer than necessary, aligning with data protection regulations.
description: This rule checks whether a maximum length is configured for Dataflow
  job pipelines to ensure they do not run indefinitely. Without these limits, pipelines
  might process data longer than intended, increasing the likelihood of data retention
  beyond policy stipulations. To verify, review Dataflow job configurations to ensure
  a max length is defined and adjust the settings via the GCP Console or gcloud command-line
  tool. This helps maintain control over data processing durations and aligns with
  best practices for data lifecycle management.
references:
- https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
- https://cloud.google.com/security/compliance/cis#gcp
- https://www.iso.org/iso-27001-information-security.html
- https://cloud.google.com/security/compliance
