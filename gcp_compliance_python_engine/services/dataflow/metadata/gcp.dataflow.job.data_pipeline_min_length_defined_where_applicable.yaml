rule_id: gcp.dataflow.job.data_pipeline_min_length_defined_where_applicable
service: dataflow
resource: job
requirement: Data Pipeline Min Length Defined Where Applicable
scope: dataflow.job.security
domain: data_protection_and_privacy
subcategory: encryption_at_rest
severity: medium
title: Ensure Data Pipeline Min Length is Defined for Dataflow Jobs
rationale: Defining a minimum data pipeline length in Dataflow jobs is crucial for
  maintaining data integrity and consistency, especially in complex data processing
  environments. This requirement helps mitigate risks of data loss or incomplete data
  processing, which can lead to inaccurate analytics and business decisions. Additionally,
  compliance with data protection standards such as GDPR and HIPAA may mandate specific
  data handling processes, which this rule supports by ensuring pipelines are properly
  configured.
description: This rule checks that Dataflow jobs have a minimum pipeline length defined
  where applicable, ensuring that data processing tasks are not prematurely terminated.
  Verification involves reviewing job configurations to ensure that pipeline length
  parameters are set appropriately. Remediation requires configuring the Dataflow
  job settings through the GCP Console or using IaC tools like Terraform to specify
  a minimum pipeline length that aligns with business and compliance requirements.
references:
- https://cloud.google.com/dataflow/docs/guides/specifying-exec-params
- https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- https://www.iso.org/iso-iec-27001-information-security.html
- https://cloud.google.com/security/compliance
- https://cloud.google.com/dataflow/docs/resources/faq
- https://cloud.google.com/dataflow/docs/guides/setting-pipeline-options
