metadata:
  csp: GCP
  description: Enterprise-grade GCP rules - AI Enhanced
  version: 2.0.0
  enhancement_date: '2025-12-02'
  total_rules: 1576
  quality_grade: A+ (GPT-4o Enhanced)
  ai_engine: gpt-4o
statistics:
  total_rules: 1576
  enhanced: 1107
  failed: 8
  connection_errors: 0
  api_calls: 1115
rules:
- rule_id: gcp.accessapproval.approval_request.enrollment_configured
  service: accessapproval
  resource: approval_request
  requirement: Enrollment Configured
  scope: accessapproval.approval_request.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Access Approval Enrollment is Configured
  rationale: Configuring Access Approval enrollment is critical for maintaining control over sensitive operations and preventing unauthorized access. This rule helps mitigate the risk of unauthorized changes by requiring explicit approval for sensitive actions, aligning with compliance standards such as SOC2 and PCI-DSS. Without proper enrollment, organizations risk non-compliance and potential data breaches.
  description: This rule checks whether Access Approval enrollment is configured for the approval_request resource in GCP. Proper configuration ensures that sensitive actions require explicit approvals before execution, enhancing security oversight. To verify, check that the Access Approval API is enabled and policies are set for relevant projects or folders. Remediation involves enabling the Access Approval API and configuring approval requests for critical operations.
  references:
  - https://cloud.google.com/access-approval/docs
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/access-approval/docs/configure-access-approval
- rule_id: gcp.aiplatform.batch_prediction_job.data_governance_ai_transform_job_input_output_encrypted
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Governance Ai Transform Job Input Output Encrypted
  scope: aiplatform.batch_prediction_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Batch Prediction Job Data Encryption
  rationale: Encrypting input and output data for AI Platform batch prediction jobs is critical to protect sensitive information from unauthorized access and potential data breaches. This is particularly important to mitigate risks associated with data leaks and to comply with data protection regulations such as GDPR, HIPAA, and CCPA. Failure to encrypt data at rest can result in significant legal penalties and damage to an organization's reputation.
  description: This rule checks that all input and output data associated with AI Platform batch prediction jobs are encrypted using Customer-Managed Encryption Keys (CMEK). Verify that the encryption configuration for batch prediction jobs specifies CMEK to ensure data is protected at rest. Remediation involves configuring the batch prediction job to use CMEK by specifying the key in the job's 'encryptionSpec' field. This can be done via the GCP Console, gcloud command-line tool, or the AI Platform client libraries.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/predictions/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.batch_prediction_job.data_governance_ai_transform_job_logs_enabled
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Governance Ai Transform Job Logs Enabled
  scope: aiplatform.batch_prediction_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logs for AI Platform Batch Prediction Job
  rationale: Enabling logs for AI Platform batch prediction jobs is crucial for maintaining data governance and ensuring the traceability of data transformations. This is essential for understanding how data is processed, which aids in identifying potential security incidents and supporting compliance with regulations like GDPR and CCPA. Without these logs, organizations risk undetected data breaches and non-compliance with legal requirements.
  description: This rule checks whether logging is enabled for AI Platform batch prediction jobs. Logging should be configured to capture details about data transformations within AI jobs, aiding in auditing and forensic analysis. To verify, ensure that the logging configuration in AI Platform includes all necessary events. Remediation involves setting the 'logConfig' parameter in the batch prediction job configuration to enable logging.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.batch_prediction_job.data_governance_ai_transform_job_private_networking_enforced
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Governance Ai Transform Job Private Networking Enforced
  scope: aiplatform.batch_prediction_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Platform Batch Prediction Jobs
  rationale: Ensuring that AI Platform batch prediction jobs use private networking is crucial to limit exposure to the public internet, reducing the risk of data breaches and unauthorized access. This measure helps protect sensitive data processed during AI transformations, aligning with compliance mandates such as GDPR and HIPAA that require stringent data protection controls.
  description: This rule checks if AI Platform batch prediction jobs are configured to use private networking, which restricts network access to internal IP addresses only. Verify the configuration by ensuring that 'network' attribute specifies a VPC with private IPs. Remediation involves configuring batch jobs to run within a VPC network, using private IP settings, to ensure secure, internal data flow and connectivity.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/using-private-ip
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.batch_prediction_job.data_governance_ai_transform_job_roles_least_privilege
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Governance Ai Transform Job Roles Least Privilege
  scope: aiplatform.batch_prediction_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Transform Job Roles
  rationale: Implementing least privilege for AI Transform Job roles minimizes the risk of unauthorized access and data leaks, which can lead to financial loss, reputational damage, and regulatory non-compliance. Ensuring roles have only necessary permissions helps prevent malicious or accidental misuse of sensitive data and resources.
  description: This rule checks if AI Platform Batch Prediction Jobs are configured with roles that adhere to the principle of least privilege. Specifically, it verifies that roles attached to these jobs have only the permissions essential for their function, reducing the attack surface. To remediate, review and adjust IAM policies to ensure roles do not exceed required permissions, using predefined roles or custom roles tailored to the job's specific needs.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - NIST SP 800-53 - Access Control
  - ISO 27001 Annex A.9 - Access Control
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.aiplatform.batch_prediction_job.data_privacy_ai_transform_job_input_output_encrypted
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Privacy Ai Transform Job Input Output Encrypted
  scope: aiplatform.batch_prediction_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Transform Job Data is Encrypted at Rest
  rationale: Encrypting AI Transform job input and output data mitigates the risk of unauthorized data access and helps maintain confidentiality and integrity. This is crucial in preventing data breaches and aligns with compliance requirements such as GDPR and HIPAA, which mandate data protection measures. It also safeguards against potential insider threats and reduces the risk of exposure in case of storage system compromise.
  description: This check ensures that batch prediction job input and output data in GCP's AI Platform is encrypted at rest using customer-managed keys (CMKs). Verify that Cloud Storage buckets used for storing job data are configured with a CMK for encryption. To remediate, configure your Cloud Storage bucket to use a CMK by updating the bucket's settings through the GCP Console or by using gcloud CLI commands to set a specific KMS key for the bucket.
  references:
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://cloud.google.com/ai-platform/prediction/docs/custom-service-account
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.aiplatform.batch_prediction_job.data_privacy_ai_transform_job_logs_enabled
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Privacy Ai Transform Job Logs Enabled
  scope: aiplatform.batch_prediction_job.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Logs for AI Transform Jobs to Protect Data Privacy
  rationale: Enabling logs for AI Transform jobs in GCP is crucial to monitor and audit data access and transformations, ensuring data privacy is maintained. Without logging, unauthorized data manipulation might go undetected, potentially leading to data breaches and non-compliance with regulations such as GDPR and HIPAA. Effective logging supports incident response and forensic analysis, reducing the risk of data leaks and enhancing trust in AI operations.
  description: This rule checks if logging is enabled for AI Transform jobs within batch prediction jobs on GCP's AI Platform. To verify, ensure that the 'log_ttl_days' configuration is set for the batch prediction job, indicating that logs are retained for auditing. Remediation involves adjusting the AI Platform job settings to enable and configure logging, ensuring logs are stored in a secure GCP Logging bucket with appropriate access controls.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/logging
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/logging/docs/best-practices
- rule_id: gcp.aiplatform.batch_prediction_job.data_privacy_ai_transform_job_private_networking_enforced
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Privacy Ai Transform Job Private Networking Enforced
  scope: aiplatform.batch_prediction_job.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Private Networking for AI Transform Jobs
  rationale: Enforcing private networking for AI Transform Jobs in GCP minimizes exposure to public networks, reducing the risk of data breaches and unauthorized access. This configuration supports compliance with data protection regulations such as GDPR and HIPAA by ensuring data remains within controlled network boundaries. Without private networking, sensitive data processed in AI jobs could be vulnerable to interception or exfiltration.
  description: This rule checks if AI Transform Jobs in GCP AI Platform are configured to use private networking, which routes job traffic through internal IPs instead of the public internet. To verify, ensure that the 'network' field in the BatchPredictionJob resource is set to a valid VPC network. Remediation involves updating existing jobs to specify a VPC network and configuring new jobs with private networking by default. This can typically be done through the GCP Console, gcloud command-line tool, or API.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/batch-predictions
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/architecture/encryption-at-rest
  - https://cloud.google.com/security/privacy/
- rule_id: gcp.aiplatform.batch_prediction_job.data_privacy_ai_transform_job_roles_least_privilege
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Privacy Ai Transform Job Roles Least Privilege
  scope: aiplatform.batch_prediction_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Batch Prediction Jobs
  rationale: Ensuring least privilege for AI Platform batch prediction jobs minimizes potential exposure of sensitive data by restricting access to only necessary roles. This reduces the risk of data breaches and unauthorized access, which is crucial for maintaining data privacy and meeting compliance with regulations such as GDPR and HIPAA.
  description: This rule checks that roles assigned to AI Platform batch prediction jobs adhere to the principle of least privilege, ensuring no excessive permissions are granted. Verify IAM policies to ensure only necessary roles are assigned, such as roles/aiplatform.user, while avoiding overly permissive roles like roles/owner. To remediate, review and adjust IAM roles using the Google Cloud Console or gcloud CLI to align with least privilege best practices.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/aiplatform/docs/batch-predictions
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, 1.6 Ensure that IAM users are assigned only the roles they need
- rule_id: gcp.aiplatform.batch_prediction_job.machine_learning_transform_job_input_output_encrypted
  service: aiplatform
  resource: batch_prediction_job
  requirement: Machine Learning Transform Job Input Output Encrypted
  scope: aiplatform.batch_prediction_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt Input and Output for AI Platform Batch Jobs
  rationale: Ensuring that input and output data of AI Platform batch prediction jobs are encrypted is crucial to protect sensitive machine learning data from unauthorized access and breaches. Unencrypted data can lead to exposure of proprietary algorithms or customer information, violating compliance requirements such as GDPR or HIPAA, and resulting in financial and reputational damage.
  description: This rule checks if the input and output files for AI Platform batch prediction jobs are encrypted using Customer-Managed Encryption Keys (CMEK). To verify, ensure that the 'encryptionSpec' field is configured with a valid KMS key in the job settings. Remediation involves modifying the job configuration to specify a CMEK, thereby securing data at rest and aligning with best practices for data protection.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.batch_prediction_job.machine_learning_transform_job_logs_enabled
  service: aiplatform
  resource: batch_prediction_job
  requirement: Machine Learning Transform Job Logs Enabled
  scope: aiplatform.batch_prediction_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for AI Platform Batch Prediction Jobs
  rationale: Enabling logs for AI Platform Batch Prediction Jobs is crucial for monitoring, troubleshooting, and auditing activities within the machine learning environment. Without logging, it becomes challenging to identify and respond to security incidents, compliance breaches, or performance issues, potentially leading to financial losses or regulatory penalties.
  description: This rule checks whether logging is enabled for AI Platform Batch Prediction Jobs. To ensure compliance and security, configure your jobs to send logs to Cloud Logging. Verify the setting by inspecting the job configurations in the Google Cloud Console or via the gcloud CLI. To enable logging, set the 'logging' parameter to 'true' in the job configuration. This allows capturing of job activity, facilitating monitoring and troubleshooting.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/logging
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.aiplatform.batch_prediction_job.machine_learning_transform_job_private_networking_enforced
  service: aiplatform
  resource: batch_prediction_job
  requirement: Machine Learning Transform Job Private Networking Enforced
  scope: aiplatform.batch_prediction_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for ML Transform Jobs
  rationale: Ensuring that machine learning transform jobs utilize private networking significantly reduces the attack surface by preventing exposure to the public internet. This mitigates the risk of data breaches and unauthorized access, supporting compliance with standards like HIPAA and ISO 27001 which require robust network security controls.
  description: This rule checks if batch prediction jobs in the AI Platform are configured to use private networking, ensuring they do not have external IP addresses. Verify that the 'network' field in the job configuration is set to a private network. To enforce this, configure a VPC Network to enable private connectivity, and apply it to your batch prediction jobs. This reduces potential attack vectors by restricting access to internal GCP networks only.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/deploying-models
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.aiplatform.batch_prediction_job.machine_learning_transform_job_roles_least_privilege
  service: aiplatform
  resource: batch_prediction_job
  requirement: Machine Learning Transform Job Roles Least Privilege
  scope: aiplatform.batch_prediction_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Batch Prediction Jobs
  rationale: Implementing least privilege for AI Platform batch prediction jobs minimizes the attack surface by restricting access to only necessary roles and permissions, thus reducing the risk of unauthorized data access or manipulation. This is crucial for protecting sensitive data and maintaining compliance with regulations such as GDPR, HIPAA, and PCI-DSS, which mandate strict access controls.
  description: This rule checks that roles assigned to batch prediction jobs in the AI Platform adhere to the principle of least privilege. Ensure that service accounts associated with these jobs have only the permissions necessary to perform their functions. Review and update IAM policies to eliminate excessive permissions, and regularly audit roles to maintain minimal access. Remediation involves adjusting IAM settings to align with GCP's best practices for access control.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/ai-platform/prediction/docs/access-control
  - CIS GCP Benchmark 1.1.0 - 4.1 Identity and Access Management
  - NIST SP 800-53 Rev. 5 - AC-6 Least Privilege
  - 'PCI DSS 3.2.1 - Requirement 7: Restrict access to cardholder data by business need to know'
  - ISO/IEC 27001:2013 - A.9 Access Control
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_inter_container_traffic_enc_enabled
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job Inter Container Traffic Enc Enabled
  scope: aiplatform.custom_job.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure AI Platform Custom Job Inter-Container Traffic Encryption
  rationale: Encrypting inter-container traffic in AI Platform Custom Jobs mitigates the risk of data interception and unauthorized access during data exchanges between containers. This is crucial for protecting sensitive training data and models from potential man-in-the-middle attacks, ensuring compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule verifies that inter-container traffic for AI Platform Custom Jobs is encrypted. Check the configuration settings to ensure that the 'enable_inter_container_traffic_encryption' flag is set to true for custom jobs. If not enabled, update the job configuration to enforce encryption. This can be done via the Google Cloud Console or using gcloud CLI by specifying the appropriate flag in your custom job configuration file.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-containers-training#configure_encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_network_isolation_enabled
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job Network Isolation Enabled
  scope: aiplatform.custom_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Isolation for AI Training Jobs
  rationale: Enabling network isolation for AI training jobs protects sensitive data from unauthorized network access and mitigates risks such as data leakage and unauthorized data manipulation. This control is crucial for maintaining the confidentiality and integrity of AI models, especially in industries with stringent data protection regulations like healthcare and finance. It supports compliance with standards such as NIST SP 800-53, which emphasizes secure network configurations.
  description: This rule verifies that network isolation is enabled for AI Platform custom training jobs by checking if a valid VPC is configured. Network isolation prevents jobs from accessing the public internet, reducing exposure to external threats. To enable it, specify a VPC network when creating or updating an AI custom job. Validate configuration through the GCP Console or gcloud CLI by ensuring the 'network' field is set in the job configuration.
  references:
  - https://cloud.google.com/ai-platform/training/docs/using-vpc
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/virtual-private-cloud
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_output_encryption_enabled
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job Output Encryption Enabled
  scope: aiplatform.custom_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure AI Job Output Encryption in Custom Jobs
  rationale: Enabling encryption for AI job outputs in GCP mitigates the risk of unauthorized data access and exposure, which is crucial for maintaining data confidentiality and integrity. It addresses potential threat scenarios where sensitive AI model outputs might be intercepted or stored in an unsecured manner. This practice is vital for compliance with data protection regulations such as GDPR, HIPAA, and ensures alignment with organizational security policies.
  description: This rule checks whether AI Platform custom job outputs have encryption enabled at rest using customer-managed encryption keys (CMEK). To verify, ensure that the custom job configurations specify an encryption key. Remediation involves updating the AI job configuration to include a valid CMEK, which can be done via the GCP Console or CLI by specifying the `kmsKeyName` parameter. This ensures that all data written by the AI job is encrypted with the specified key, enhancing data protection.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-jobs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_volume_encryption_enabled
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job Volume Encryption Enabled
  scope: aiplatform.custom_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure AI Training Job Volumes Are Encrypted at Rest
  rationale: Encrypting AI training job volumes at rest is crucial to protect sensitive data from unauthorized access and potential breaches. This practice helps mitigate risks such as data theft or exposure, especially in scenarios where data is stored in shared or multi-tenant environments. Meeting this requirement aligns with compliance mandates like GDPR, HIPAA, and PCI-DSS, which demand robust data protection and privacy measures.
  description: This rule checks if encryption is enabled for volumes used in Google Cloud AI Platform custom training jobs. Specifically, it verifies that Customer-Managed Encryption Keys (CMEK) are configured for storage resources to ensure data is encrypted with keys managed by the customer. To verify, audit the AI Platform custom job configurations for CMEK settings. Remediation involves configuring AI training jobs to specify CMEK for volume encryption, ensuring data is secured using keys within your control.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-jobs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_vpc_configured
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job VPC Configured
  scope: aiplatform.custom_job.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure VPC Configuration for AI Platform Custom Jobs
  rationale: Configuring a VPC for AI Platform custom jobs is vital to control network access and protect data from unauthorized exposure. Without a VPC, data could traverse the public internet, increasing the risk of interception or unauthorized access. Proper VPC setup also helps meet compliance requirements by ensuring that data processing occurs within a controlled network environment.
  description: This rule verifies that AI Platform custom jobs are configured to use a Virtual Private Cloud (VPC) network, which ensures network isolation and secure data transit. To check compliance, confirm that each custom job specifies a VPC network in its configuration. Remediation involves updating the custom job settings to include a VPC network specification, which can be done via the Google Cloud Console or using the gcloud command-line tool.
  references:
  - https://cloud.google.com/ai-platform/docs/custom-jobs
  - https://cloud.google.com/vpc/docs/using-vpc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.aiplatform.custom_job.ai_services_training_pipeline_private_networking_enforced
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Pipeline Private Networking Enforced
  scope: aiplatform.custom_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Private Networking for AI Training Pipelines
  rationale: Enforcing private networking for AI training pipelines reduces the attack surface by preventing unauthorized or unintended access from the public internet. This configuration mitigates risks of data breaches and ensures that sensitive machine learning data remains within secure network boundaries, aligning with compliance mandates like GDPR and HIPAA.
  description: This rule checks whether AI Services Training Pipelines are configured to use private networking. It ensures that custom jobs in AI Platform are executed within a VPC, preventing exposure to the public internet. To verify, check the network settings of the custom job for a private IP configuration. If not compliant, configure the custom job to utilize a VPC network with appropriate firewall rules to restrict access.
  references:
  - https://cloud.google.com/ai-platform/training/docs/use-vpc
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 7.1
  - 'NIST SP 800-53 Rev. 5: AC-4, SC-7'
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.custom_job.ai_services_training_pipeline_secrets_from_vault_only
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Pipeline Secrets From Vault Only
  scope: aiplatform.custom_job.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Use Vault for AI Platform Training Pipeline Secrets
  rationale: Storing secrets in Google Cloud Vault ensures they are encrypted and access-controlled, reducing the risk of unauthorized access or data breaches. This practice aligns with compliance requirements for data protection and supports the integrity of AI models by safeguarding sensitive information. It mitigates the risk of exposure during AI pipeline execution, which could lead to intellectual property theft or data misuse.
  description: This rule checks that AI Platform Training Pipelines only use secrets stored in Google Cloud Vault. By ensuring secrets are managed by Vault, you leverage its robust access control and encryption capabilities. To verify, inspect the AI Platform custom job configurations and confirm secrets are sourced from Vault. Remediation involves updating your pipeline configurations to reference secrets stored in Vault, ensuring consistent security practices.
  references:
  - https://cloud.google.com/vertex-ai/docs/pipelines/configure-gcp-resources
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.custom_job.data_governance_ai_training_job_input_output_encrypted
  service: aiplatform
  resource: custom_job
  requirement: Data Governance Ai Training Job Input Output Encrypted
  scope: aiplatform.custom_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of AI Platform Custom Job Data at Rest
  rationale: Encrypting input and output data for AI training jobs in Google's AI Platform Custom Jobs is critical to protect sensitive information from unauthorized access and potential data breaches. This practice mitigates risks related to data leakage and ensures compliance with regulatory standards such as GDPR and CCPA, which mandate strong data protection measures.
  description: This rule checks whether encryption is enabled for input and output data of AI Platform Custom Jobs. To verify, ensure that the AI Platform Custom Job configuration specifies a valid customer-managed encryption key (CMEK) for data at rest. If encryption is not configured, update the job settings to include a CMEK to enhance data protection. This can be done via the Google Cloud Console or using the gcloud CLI by specifying the --kms-key-name flag when creating or updating jobs.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-service-encryption
  - https://cloud.google.com/security/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.aiplatform.custom_job.data_governance_ai_training_job_logs_enabled
  service: aiplatform
  resource: custom_job
  requirement: Data Governance Ai Training Job Logs Enabled
  scope: aiplatform.custom_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for AI Training Custom Jobs
  rationale: Enabling logging for AI training custom jobs is crucial for maintaining transparency and accountability in data operations. It helps in tracking the flow of data, identifying anomalies, and ensuring that data handling complies with organizational policies and regulatory requirements. Without logging, it becomes challenging to audit data processing activities, increasing the risk of data breaches and compliance violations.
  description: This rule checks whether audit logging is enabled for custom jobs in the AI Platform. Audit logs should be configured to capture all data processing activities, including data input, output, and transformation operations. To verify, ensure that Stackdriver Logging is set up for the AI Platform and that custom job logs are actively monitored. Remediation involves configuring audit logs through the Google Cloud Console or using gcloud commands to enable and manage Stackdriver Logging for the AI Platform.
  references:
  - https://cloud.google.com/ai-platform/docs/training/logging
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.custom_job.data_governance_ai_training_job_private_networking_enforced
  service: aiplatform
  resource: custom_job
  requirement: Data Governance Ai Training Job Private Networking Enforced
  scope: aiplatform.custom_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Training Jobs
  rationale: Private networking for AI training jobs is crucial to prevent unauthorized access and mitigate potential data exfiltration risks. This enhances data governance by ensuring that sensitive data processed during AI training remains within a controlled network environment, aligning with compliance requirements such as GDPR and CCPA.
  description: This rule checks if AI Platform Custom Jobs are configured to use private networking, which restricts traffic to private IP addresses and prevents exposure to the public internet. To verify, ensure that the 'network' field in the custom job configuration specifies a VPC network. Remediation involves updating the custom job configurations to include a VPC network that aligns with organizational security policies.
  references:
  - https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs#CustomJob
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.custom_job.data_governance_ai_training_job_roles_least_privilege
  service: aiplatform
  resource: custom_job
  requirement: Data Governance Ai Training Job Roles Least Privilege
  scope: aiplatform.custom_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Custom Job Roles
  rationale: Implementing least privilege for AI Platform custom jobs is crucial to minimizing the potential attack surface and preventing unauthorized access to sensitive data and resources. Over-privileged roles can lead to data breaches, financial loss, and non-compliance with regulations such as GDPR and HIPAA. Ensuring roles have only the necessary permissions helps mitigate insider threats and external attacks.
  description: This rule checks that IAM roles assigned to AI Platform custom jobs adhere to the principle of least privilege, meaning they only have permissions necessary for their intended functions. Verify role assignments by reviewing IAM policies and removing any excess permissions or roles not required for job execution. Remediate by adjusting policies to align with job requirements, ensuring sensitive resources are accessible only to authorized jobs.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/custom-training-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.aiplatform.custom_job.data_privacy_ai_training_job_input_output_encrypted
  service: aiplatform
  resource: custom_job
  requirement: Data Privacy Ai Training Job Input Output Encrypted
  scope: aiplatform.custom_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of AI Training Job Input/Output in GCP
  rationale: Encrypting the input and output data of AI training jobs is crucial for protecting sensitive information from unauthorized access and ensuring compliance with data privacy regulations such as GDPR and CCPA. This reduces the risk of data breaches and supports the organizationâ€™s commitment to maintaining customer trust and safeguarding intellectual property.
  description: This rule checks whether input and output data for AI Platform Custom Jobs are encrypted using Customer-Managed Encryption Keys (CMEK). It involves verifying that the encryption configuration is set in the AI Platform job settings. To remediate, ensure that the AI Platform job configuration specifies a CMEK for data encryption, thereby enhancing data protection by using keys managed by the organization.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-jobs-overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-5/final
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.aiplatform.custom_job.data_privacy_ai_training_job_logs_enabled
  service: aiplatform
  resource: custom_job
  requirement: Data Privacy Ai Training Job Logs Enabled
  scope: aiplatform.custom_job.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Training Job Logs are Enabled for Data Privacy
  rationale: Enabling AI training job logs is crucial for maintaining data privacy and ensuring compliance with regulatory standards such as GDPR and HIPAA. Logs provide a detailed record of access and processing, which helps in identifying unauthorized access or data misuse. This control mitigates risks associated with data breaches and unauthorized data usage, thus protecting sensitive information and maintaining customer trust.
  description: This rule checks whether logging is enabled for custom AI training jobs on GCP. Logging should be configured to capture detailed information about data access and processing activities. To verify, ensure that the 'enable_logging' parameter is set to true in the AI Platform custom job configuration. Remediation involves updating the job configuration to enable logging, thereby ensuring that all operations are tracked and auditable.
  references:
  - https://cloud.google.com/ai-platform/training/docs/logging
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.custom_job.data_privacy_ai_training_job_private_networking_enforced
  service: aiplatform
  resource: custom_job
  requirement: Data Privacy Ai Training Job Private Networking Enforced
  scope: aiplatform.custom_job.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Private Networking for AI Training Jobs
  rationale: Ensuring AI training jobs use private networking is crucial to prevent unauthorized access to sensitive data. Without this configuration, training data can be exposed to public networks, increasing the risk of data breaches and non-compliance with regulations such as GDPR and CCPA. This measure helps maintain data confidentiality and integrity, crucial for trust and compliance in AI operations.
  description: This rule checks that AI Platform custom jobs are configured to use private networking, restricting access to internal networks only. Verify that the 'network' field in AI Platform job configurations specifies a private VPC. Remediation involves setting up a VPC with appropriate firewall rules and ensuring AI Platform jobs reference this network. Confirm this setting via the GCP Console or gcloud CLI to protect data in transit.
  references:
  - https://cloud.google.com/ai-platform/docs/training-overview
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/ai-platform/docs/security
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.custom_job.data_privacy_ai_training_job_roles_least_privilege
  service: aiplatform
  resource: custom_job
  requirement: Data Privacy Ai Training Job Roles Least Privilege
  scope: aiplatform.custom_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Training Job Roles Have Least Privilege
  rationale: Assigning excessive permissions to AI training job roles can expose sensitive data and increase the risk of data breaches. By adhering to the principle of least privilege, organizations can minimize potential attack vectors and protect sensitive datasets used during AI model training. This practice supports compliance with data protection regulations like GDPR and CCPA, which mandate stringent access control to personal data.
  description: This rule checks that roles assigned to AI Platform Custom Jobs are limited to the minimum necessary permissions. It verifies that the roles are not overly permissive and do not include access to resources unrelated to the job's function. To comply, review the roles assigned to each custom job and ensure they align strictly with the job's operational requirements, removing any unnecessary permissions. Use GCP IAM policy analysis tools to identify and rectify any discrepancies.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/ai-platform/training/docs/custom-service-auth
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.aiplatform.dataset.ai_services_dataset_access_logging_enabled
  service: aiplatform
  resource: dataset
  requirement: Ai Services Dataset Access Logging Enabled
  scope: aiplatform.dataset.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure AI Platform Dataset Access Logs are Enabled
  rationale: Enabling access logging for AI Platform datasets is crucial for detecting unauthorized access and ensuring accountability. Without logging, organizations risk undetected data breaches and non-compliance with regulatory standards such as GDPR and HIPAA, which mandate detailed access records for sensitive data. Accurate logs are also essential for forensic investigations and maintaining a strong security posture.
  description: This rule checks if access logging is enabled for datasets in Google Cloud's AI Platform. To verify, ensure that Cloud Audit Logs are configured to capture 'DATA_READ' and 'DATA_WRITE' for 'aiplatform.googleapis.com'. Remediation involves enabling audit logging in the IAM & admin settings of the Google Cloud Console. This will help in tracking access and modifications to datasets, providing an audit trail for compliance and security purposes.
  references:
  - https://cloud.google.com/aiplatform/docs/audit-logs
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.dataset.ai_services_dataset_cloud_storage_block_public_access
  service: aiplatform
  resource: dataset
  requirement: Ai Services Dataset Cloud Storage Block Public Access
  scope: aiplatform.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Block Public Access to AI Platform Datasets in Cloud Storage
  rationale: Allowing public access to AI Platform datasets can expose sensitive data and intellectual property to unauthorized users, leading to data breaches and compliance violations. This exposure can also result in financial and reputational damage, especially when handling regulated data under standards like GDPR or HIPAA.
  description: This rule verifies that public access is blocked for datasets stored in Google Cloud Storage used by AI Platform services. To ensure compliance, check the bucket's IAM policy to confirm that no public access permissions are granted. Remediation involves removing 'allUsers' and 'allAuthenticatedUsers' roles from the bucket's permissions, and configuring appropriate IAM roles for authorized users only.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/storage/docs/public-datasets
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.dataset.ai_services_dataset_cloud_storage_encrypted_at_rest
  service: aiplatform
  resource: dataset
  requirement: Ai Services Dataset Cloud Storage Encrypted At Rest
  scope: aiplatform.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Datasets are Encrypted at Rest
  rationale: Encrypting AI Platform datasets at rest protects sensitive data from unauthorized access and breaches, mitigating risks such as data leaks and financial loss. It aligns with compliance standards like PCI-DSS and HIPAA, which mandate encryption of sensitive data to safeguard customer information and maintain trust.
  description: This rule checks that AI Platform datasets stored in Cloud Storage are encrypted at rest using Google-managed or customer-managed encryption keys. Verify encryption settings by inspecting the datasetâ€™s storage configuration in the Google Cloud Console or using gcloud CLI. To remediate, enable encryption in the Cloud Storage settings for the dataset, ensuring that default or custom encryption keys are applied.
  references:
  - https://cloud.google.com/aiplatform/docs/storage-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/docs/security/encryption/default-encryption
- rule_id: gcp.aiplatform.dataset.ai_services_dataset_lifecycle_policy_configured
  service: aiplatform
  resource: dataset
  requirement: Ai Services Dataset Lifecycle Policy Configured
  scope: aiplatform.dataset.lifecycle_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure AI Dataset Lifecycle Policies are Configured
  rationale: Configuring lifecycle policies for AI datasets helps reduce the risk of unauthorized access and data leakage by ensuring datasets are properly expired, archived, or deleted according to compliance and business requirements. This is crucial for maintaining data integrity and adhering to regulations such as GDPR and CCPA, which mandate data minimization and timely deletion of personal data.
  description: This rule checks if lifecycle management policies are set for AI datasets in Google Cloud's AI Platform. Lifecycle policies define automatic actions on datasets, like deletion or archival, based on conditions such as age or last access time. To verify compliance, inspect the dataset configurations in the AI Platform and ensure policies are active and align with organizational data retention standards. Remediation involves configuring these policies via the Google Cloud Console or using the AI Platform's API to enforce dataset management best practices.
  references:
  - https://cloud.google.com/ai-platform/docs/datasets/lifecycle-management
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/storage/docs/lifecycle
- rule_id: gcp.aiplatform.dataset.ai_services_encrypted_at_rest_cmek
  service: aiplatform
  resource: dataset
  requirement: Ai Services Encrypted At Rest Cmek
  scope: aiplatform.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Datasets Use CMEK for Encryption at Rest
  rationale: Utilizing Customer-Managed Encryption Keys (CMEK) for encrypting AI Platform datasets at rest significantly enhances data security by giving organizations control over their encryption keys. This mitigates the risk of unauthorized data access and aligns with compliance requirements such as GDPR and HIPAA, which demand strict data protection measures. Failure to implement CMEK can result in data breaches, legal penalties, and reputational damage.
  description: This rule checks if AI Platform datasets are encrypted at rest using Customer-Managed Encryption Keys (CMEK). To verify, ensure that dataset configurations specify a CMEK key from Cloud Key Management Service (KMS). Remediation involves updating dataset settings to include a CMEK key, which requires appropriate permissions to access and manage keys within Cloud KMS. This enhances control over data encryption processes and supports compliance with stringent data protection standards.
  references:
  - https://cloud.google.com/ai-platform/docs/datasets
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.dataset.ai_services_public_access_blocked
  service: aiplatform
  resource: dataset
  requirement: Ai Services Public Access Blocked
  scope: aiplatform.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Block Public Access to AI Platform Datasets
  rationale: Blocking public access to AI Platform datasets is critical to prevent unauthorized access to sensitive data, which could lead to data breaches or intellectual property theft. Exposing datasets publicly increases the risk of data leakage and non-compliance with regulations such as GDPR and CCPA, which mandate strict data protection measures.
  description: This rule checks that AI Platform datasets are not publicly accessible by ensuring that IAM policies deny public access. To verify, inspect the dataset's IAM policy and ensure that 'allUsers' or 'allAuthenticatedUsers' are not granted roles that provide read access. Remediation involves updating IAM policies to restrict access to only authorized users and groups, reducing the risk of unauthorized data exposure.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.aiplatform.endpoint.ai_services_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Authn Required
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce Authn for GCP AI Platform Endpoints
  rationale: Requiring authentication for AI Platform endpoints mitigates unauthorized access risks, ensuring that only legitimate users and services can interact with your AI models. This is crucial for protecting sensitive data processed by AI services and maintaining compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks that all AI Platform endpoints are configured to require authentication, preventing public and anonymous access. Verify that IAM roles are correctly assigned to users and service accounts with appropriate permissions. To remediate, configure endpoints to enforce authentication by updating IAM policies to restrict access only to authenticated entities.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.ai_services_authz_policies_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Authz Policies Enforced
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Services Authz Policies Are Enforced on Endpoints
  rationale: Enforcing authorization policies on AI services endpoints is crucial to prevent unauthorized access and data breaches. Without proper controls, sensitive AI models and data can be exposed to malicious actors, leading to potential loss of intellectual property and compliance violations. This is particularly important for organizations handling sensitive information that must adhere to regulatory standards such as GDPR or HIPAA.
  description: This rule checks that authorization policies are enforced on Google Cloud AI Platform endpoints. By ensuring that AI services have proper IAM policies applied, access can be restricted to only authorized users and applications. To verify, review the IAM policies associated with each AI Platform endpoint to confirm they align with organizational security requirements. Remediation involves configuring the IAM settings to restrict access based on the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/ai-platform/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.endpoint.ai_services_inference_endpoint_data_capture_gcs_encrypted
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Inference Endpoint Data Capture Gcs Encrypted
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure GCS Encryption for AI Endpoint Data Capture
  rationale: Encrypting data captured by AI inference endpoints in Google Cloud Storage (GCS) is critical to protect sensitive information from unauthorized access and comply with data protection regulations such as GDPR and CCPA. Unencrypted data at rest increases the risk of data breaches, potentially leading to financial loss and reputational damage.
  description: This rule checks if the data captured by AI Platform inference endpoints is stored in GCS with encryption enabled. Verify that the GCS bucket used for data capture is configured with either Google-managed encryption keys (default) or customer-managed encryption keys (CMEK) for enhanced security. To remediate, configure the GCS bucket to enforce encryption at rest using the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/ai-platform/prediction/docs/data-capture
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.endpoint.ai_services_inference_endpoint_group_restrictive
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Inference Endpoint Group Restrictive
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Access to AI Platform Inference Endpoints
  rationale: Restricting access to AI Platform inference endpoints minimizes exposure to unauthorized data access and reduces the risk of malicious activities such as data exfiltration or inference tampering. Ensuring endpoints are only accessible from trusted sources can help meet compliance requirements for data protection regulations like GDPR and HIPAA, while also supporting business continuity by safeguarding AI models against potential security threats.
  description: This rule checks if the AI Platform inference endpoints have been configured with restrictive access controls, such as using VPC Service Controls or IAM roles to limit access. Verify that only necessary and authorized entities can reach these endpoints by reviewing IAM policies and network settings. To remediate, implement network security policies that restrict access to trusted IPs and configure IAM roles with the principle of least privilege to manage endpoint access.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/deploying-models
  - https://cloud.google.com/security-command-center/docs/how-to-use
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - 'NIST SP 800-53: AC-3 Access Enforcement'
  - ISO/IEC 27001:2013 - A.9.1.2 Access to networks and network services
- rule_id: gcp.aiplatform.endpoint.ai_services_inference_endpoint_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Inference Endpoint KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption for AI Platform Inference Endpoints
  rationale: Enabling KMS encryption for AI Platform inference endpoints ensures that sensitive data processed by AI models is protected at rest. This minimizes the risk of unauthorized data access and exposure, which is critical for maintaining customer trust, complying with data protection regulations like GDPR and HIPAA, and mitigating potential financial and reputational damage.
  description: This rule checks if AI Platform inference endpoints are configured to use Customer-Managed Encryption Keys (CMEK) with Cloud Key Management Service (KMS) for data encryption at rest. Verify that the `encryptionSpec` field is set with the correct KMS key resource ID in the endpoint configuration. Remediate by specifying a KMS key when creating or updating an endpoint using the AI Platform console, gcloud command-line tool, or API, ensuring that all sensitive data processed by the endpoints is encrypted with customer-managed keys.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/predictions/deploy-model-endpoints#specifying_customer_managed_encryption_keys
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.aiplatform.endpoint.ai_services_inference_endpoint_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Inference Endpoint Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Inference Endpoints
  rationale: Enforcing private networking for AI inference endpoints mitigates the risk of unauthorized access and data exfiltration by restricting network access to internal resources. This is crucial for maintaining the confidentiality and integrity of AI services, ensuring that sensitive data is not exposed to the public internet, which could lead to data breaches or compliance violations with standards like ISO 27001 and GDPR.
  description: This rule checks if AI Platform inference endpoints are configured to use private networking, which means they can only be accessed from within the organization's VPC network. To verify, ensure that the `network` field is specified in the endpoint configuration. Remediation involves updating the endpoint to include a private network setting, ensuring it aligns with internal security policies and access controls.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects.locations.endpoints
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/network-connectivity/docs/vpc
- rule_id: gcp.aiplatform.endpoint.ai_services_model_deployment_endpoint_in_vpc
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Model Deployment Endpoint In VPC
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Model Endpoints are Deployed within VPC Network
  rationale: Deploying AI model endpoints within a VPC enhances security by restricting exposure to the public internet, reducing the risk of unauthorized access and potential data breaches. This configuration helps comply with industry standards and regulations that require strict network access control, thereby protecting sensitive data processed by AI models.
  description: This rule checks whether AI model endpoints on Google Cloud are deployed within a Virtual Private Cloud (VPC) network. To verify, ensure that the endpoint's network configuration specifies a VPC. Remediation involves configuring the AI Platform endpoint to use a VPC network, which can be done via the Cloud Console or gcloud CLI, ensuring all communications occur within a controlled network environment.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/predictions/using-private-endpoints
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.aiplatform.endpoint.ai_services_model_deployment_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Model Deployment KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Model Deployments in Endpoints
  rationale: Encrypting AI model deployments using Customer-Managed Encryption Keys (CMEK) mitigates the risk of unauthorized data access and aligns with regulatory requirements for data protection. It ensures sensitive model data is protected against potential breaches, especially in industries with strict data privacy laws like healthcare and finance.
  description: This rule checks if AI model deployments in AI Platform Endpoints use CMEK for encryption at rest. To verify, ensure that the endpoint configuration specifies a valid KMS key. Remediation involves updating the endpoint settings to include a KMS key, which can be done via the Google Cloud Console or gcloud command-line tool. This ensures that the model data is encrypted with a key managed by your organization, enhancing control over data access.
  references:
  - https://cloud.google.com/ai-platform/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.endpoint.ai_services_model_deployment_logging_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Model Deployment Logging Enabled
  scope: aiplatform.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Model Deployment Logging for AI Platform Endpoints
  rationale: Logging for AI model deployments on GCP AI Platform is crucial for tracking changes, diagnosing issues, and ensuring compliance with internal and external regulations. Without proper logging, organizations may be unable to detect unauthorized access, troubleshoot deployment failures, or meet audit requirements, potentially leading to data breaches or regulatory fines.
  description: This rule checks whether logging is enabled for AI model deployments on AI Platform endpoints. It verifies that all interactions and changes to model deployments are captured in Google Cloud Logging. To enable logging, ensure that the AI Platform's endpoint logging settings are correctly configured in the GCP Console or via the gcloud command line tool. Remediation involves setting up and verifying the logging configuration to align with organizational policies and compliance mandates.
  references:
  - https://cloud.google.com/ai-platform/docs/reference/rest/v1/projects.locations.endpoints
  - https://cloud.google.com/audit-logs/docs/configure-data-access
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.aiplatform.endpoint.ai_services_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce AI Services Private Networking for AI Platform Endpoints
  rationale: Enforcing private networking for AI services minimizes exposure to the public internet, reducing the risk of unauthorized access and data breaches. This is crucial for protecting sensitive machine learning models and data, ensuring compliance with regulations such as GDPR and HIPAA, and maintaining customer trust by safeguarding intellectual property.
  description: This rule checks if AI Platform endpoints are configured to use private networking, ensuring that traffic is restricted to internal networks. To verify compliance, review the AI Platform endpoint configuration to confirm that private IP addresses are used. Remediation involves updating endpoint settings to enable private networking, which can be done through the GCP Console or gcloud CLI by specifying a private network during endpoint creation.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/private-ip
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/nist
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.ai_services_prompt_gateway_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Prompt Gateway Authn Required
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Services Prompt Gateway Authentication is Enabled
  rationale: Requiring authentication for AI services prompt gateway access helps prevent unauthorized usage and data exposure, safeguarding sensitive AI model interactions. Lack of authentication can lead to security breaches, compromise of AI models, and non-compliance with regulations such as GDPR and CCPA, impacting business reputation and financial standing.
  description: This rule checks that all AI Platform endpoints have authentication enabled for their prompt gateways, ensuring only authorized users can access AI services. To verify, check the endpoint configuration for the presence of authentication settings. Remediation involves configuring OAuth 2.0 for API access and ensuring IAM roles are correctly assigned. This reduces the risk of unauthorized access and data breaches.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/architecture/framework/security
- rule_id: gcp.aiplatform.endpoint.ai_services_prompt_gateway_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Prompt Gateway Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Services Prompt Gateway
  rationale: Ensuring AI Services Prompt Gateway endpoints use private networking reduces exposure to public internet vulnerabilities, mitigating risks of unauthorized access or data breaches. This control supports compliance with data protection regulations and enhances overall cloud security posture by limiting network attack surfaces.
  description: This rule checks if AI Services Prompt Gateway endpoints are configured to use private networking by verifying the absence of public IP addresses and the presence of VPC Service Controls. To enforce private networking, configure endpoints within a Virtual Private Cloud (VPC) and restrict access using firewall rules and private service access. Remediation involves updating endpoint settings to disable public IPs and ensuring they are only accessible within the private network.
  references:
  - https://cloud.google.com/ai-platform/docs/networking
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc-service-controls/docs/overview
- rule_id: gcp.aiplatform.endpoint.ai_services_prompt_gateway_token_scopes_least_privilege
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Prompt Gateway Token Scopes Least Privilege
  scope: aiplatform.endpoint.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Prompt Gateway Token Scopes
  rationale: Applying the principle of least privilege to AI Platform Prompt Gateway token scopes minimizes the risk of unauthorized access and potential data exposure. This practice reduces attack surface, ensuring that tokens have only the permissions necessary for their function, thereby mitigating the impact of compromised credentials and aligning with compliance mandates like NIST and ISO 27001.
  description: This rule verifies that AI Platform endpoints are configured with token scopes that adhere to the principle of least privilege, granting the minimal permissions required for operation. To ensure compliance, review the IAM policies associated with the AI Platform endpoints and adjust scopes to eliminate unnecessary permissions. Remediation involves using the GCP Console or CLI to edit IAM roles and scopes for the AI Platform endpoints, ensuring they are appropriately restrictive.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/aiplatform/docs/security-overview
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.aiplatform.endpoint.ai_services_rate_limiting_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Rate Limiting Enabled
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enable Rate Limiting for AI Platform Endpoint Services
  rationale: Enabling rate limiting on AI Platform endpoints helps prevent abuse and denial of service attacks, which can lead to service disruption and increased costs. Proper rate limiting ensures that AI services are available to legitimate users while mitigating risks of resource exhaustion and maintaining compliance with organizational security policies.
  description: This rule verifies that rate limiting is configured for AI Platform endpoint services, ensuring that the number of requests per user or IP is controlled. To enable rate limiting, configure API quotas and request limits in the Google Cloud Console under AI Platform settings. Regularly review and adjust these limits based on usage patterns to ensure optimal performance and security.
  references:
  - https://cloud.google.com/ai-platform/docs/resource-quotas
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.aiplatform.endpoint.ai_services_waf_attached
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Waf Attached
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure WAF is Attached to AI Platform Endpoints
  rationale: Attaching a Web Application Firewall (WAF) to AI Platform endpoints helps protect against common web exploits and vulnerabilities that could compromise sensitive data processed by AI models. It mitigates the risk of unauthorized access and data breaches, essential for compliance with regulations like GDPR or CCPA. Without a WAF, endpoints are vulnerable to attacks that could lead to service disruptions or data loss.
  description: This rule checks if a Web Application Firewall (WAF) is attached to Google Cloud AI Platform endpoints. The WAF should be configured to monitor and filter HTTP traffic to the AI services, providing a layer of security against common threats such as SQL injection and XSS attacks. To verify compliance, ensure that the AI Platform endpoints are integrated with Cloud Armor or another WAF solution. Remediation involves configuring a WAF policy and associating it with the respective AI Platform endpoint.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/architecture/security-privacy-compliance
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.aiplatform.endpoint.data_governance_ai_endpoint_config_content_type_whi_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Ai Endpoint Config Content Type Whi Enforced
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Endpoint Config Content Type Whitelisting Enforced
  rationale: Enforcing content type whitelisting for AI endpoints mitigates the risk of data breaches and unauthorized data processing. This practice is essential for maintaining data integrity and privacy, especially in environments dealing with sensitive or regulated data. Failure to enforce such controls can lead to non-compliance with data protection regulations and potential exposure to legal and financial penalties.
  description: This rule checks if AI Platform endpoints have content type whitelisting enforced, ensuring that only approved data types are processed. To verify, inspect the endpoint configuration settings in the Google Cloud Console or via the gcloud command-line tool to ensure the 'contentTypes' field specifies allowed content types. Remediation involves updating the endpoint configurations to include a whitelist of authorized content types, preventing unauthorized data formats from being processed.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/using-resources#endpoint-configuration
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.data_governance_ai_endpoint_config_data_capture_buck_private
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Ai Endpoint Config Data Capture Buck Private
  scope: aiplatform.endpoint.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Endpoint Data Capture Buckets are Private
  rationale: Private data capture buckets help mitigate unauthorized access risks, protecting sensitive data processed by AI endpoints. An open bucket can expose sensitive information to unintended parties, leading to data breaches and violating compliance with standards such as GDPR and HIPAA.
  description: This rule checks that AI Platform endpoint data capture buckets are configured with private access settings. It verifies that these buckets do not allow public access, ensuring data is only accessible to authorized users. To remediate, review bucket permissions in Google Cloud Storage and ensure that no public grants are configured, restricting access to a minimum set of known identities.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/access-control
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.endpoint.data_governance_ai_endpoint_config_kms_keys_restricted
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Ai Endpoint Config KMS Keys Restricted
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Restrict AI Endpoint Config to Specific KMS Keys
  rationale: Limiting AI endpoint configurations to specific KMS keys reduces the risk of unauthorized data access and breaches by ensuring that only approved encryption keys are used. This enhances data protection against threats such as data exfiltration and unauthorized decryption while maintaining compliance with regulations like GDPR and HIPAA.
  description: This rule checks that only designated Cloud KMS keys are used for encrypting data at rest for AI Platform endpoints. To verify, inspect the AI Platform endpoint configurations for compliance with approved KMS key policies. Remediation involves updating endpoint configurations to ensure only specific KMS key versions are used, which can be done via the Google Cloud Console or CLI by setting the 'kmsKeyName' parameter correctly.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/overview
- rule_id: gcp.aiplatform.endpoint.data_governance_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Authn Required
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Endpoints Require Authentication for Data Access
  rationale: Requiring authentication for accessing AI Platform endpoints ensures that only authorized users can access sensitive data, which mitigates the risk of unauthorized data exposure. This is crucial for maintaining data integrity and confidentiality, especially when handling proprietary or regulated data, and helps organizations meet compliance requirements such as GDPR, HIPAA, and PCI-DSS.
  description: This rule checks whether authentication is enforced on AI Platform endpoints to control access to data. To verify compliance, ensure that all AI Platform endpoint configurations require authentication tokens or credentials before granting data access. Remediation involves configuring your AI Platform endpoints to enforce authentication by setting appropriate IAM policies and using service accounts. Regular audits and updates to access controls are recommended to maintain security posture.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/security
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.endpoint.data_governance_authz_policies_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Authz Policies Enforced
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Governance Policies on AI Platform Endpoints
  rationale: Enforcing data governance authorization policies on AI Platform endpoints is crucial for safeguarding sensitive data and adhering to regulatory requirements. Without proper authorization controls, unauthorized access could lead to data breaches, financial losses, and non-compliance with standards like GDPR and HIPAA. Implementing these controls helps maintain data integrity, confidentiality, and ensures that only authorized personnel have access to critical data assets.
  description: This rule checks whether data governance authorization policies are enforced on AI Platform endpoints, ensuring that appropriate identity and access management (IAM) policies are in place. To verify compliance, review the IAM policies associated with AI Platform endpoints and confirm that access is restricted to authorized users and service accounts. Remediation involves applying least privilege principles, updating IAM policies to enforce strict access controls, and regularly auditing access logs to detect unauthorized access attempts.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.endpoint.data_governance_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Data Governance KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption Enabled for AI Platform Endpoints
  rationale: Enabling KMS encryption for AI Platform endpoints protects sensitive data and models from unauthorized access and potential data breaches. This feature helps organizations comply with regulatory standards such as GDPR and HIPAA, which require robust data protection measures. Failure to encrypt data at rest increases the risk of exposure to cyber threats and can lead to significant financial and reputational damage.
  description: This rule checks if AI Platform endpoints are configured to use Customer-Managed Encryption Keys (CMEK) for data encryption at rest. To verify, ensure the endpoint configuration includes a valid KMS key resource identifier. Remediation involves updating the endpoint settings to specify a KMS key, which can be done through the Google Cloud Console or gcloud CLI. It is crucial to regularly audit and manage encryption keys to maintain data security.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/use-cases/data-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0 - 9.1
  - NIST SP 800-57 Part 1 - General Information on Key Management
  - 'PCI DSS v3.2.1 - Requirement 3: Protect Stored Cardholder Data'
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.endpoint.data_governance_logging_enabled
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Logging Enabled
  scope: aiplatform.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Data Governance Logging for AI Platform Endpoints
  rationale: Enabling data governance logging for AI Platform endpoints is essential to ensure full visibility into data access and usage patterns. This logging helps detect unauthorized access attempts and provides a trail for forensic investigations. Additionally, it supports compliance with regulatory requirements like GDPR and CCPA, which mandate detailed data access records.
  description: This rule checks whether data governance logs are enabled for AI Platform endpoints, ensuring audit trails are maintained. Verify that logging is active by inspecting the endpoint's configuration settings in the GCP Console under 'Logging and Monitoring'. To enable logging, navigate to the AI Platform, select the endpoint, and ensure that logging is turned on in the 'Logging' section. Regularly review logs to monitor for suspicious activity.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/security/compliance/cis#cis-gcp-v1.1.0
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/73906.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.aiplatform.endpoint.data_governance_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Private Networking for AI Platform Endpoints
  rationale: Enforcing private networking for AI Platform endpoints mitigates the risk of unauthorized access and data breaches by restricting network exposure to only trusted internal networks. This is crucial for protecting sensitive data processed by AI models and complying with data protection regulations such as GDPR and CCPA, which mandate stringent data handling and privacy controls.
  description: This rule checks if AI Platform endpoints are configured to use private networking, ensuring they are not exposed to the public internet. To verify, ensure that endpoints are accessible only via internal IPs within a Google Cloud VPC. Remediation involves configuring endpoints to use the 'privateEndpoints' setting, allowing only VPC-based access and ensuring the appropriate VPC service controls are applied.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/using-private-endpoints
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/security/privacy/
- rule_id: gcp.aiplatform.endpoint.data_privacy_ai_endpoint_config_capture_store_encryp_private
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Ai Endpoint Config Capture Store Encryp Private
  scope: aiplatform.endpoint.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Endpoint Config Capture Data is Encrypted Privately
  rationale: Encrypting data captured by AI endpoints ensures the confidentiality and integrity of sensitive information, mitigating risks such as unauthorized access and data breaches. This is crucial for maintaining trust with stakeholders and complying with data protection regulations like GDPR and CCPA, which mandate strict controls over personal data handling and storage.
  description: This rule checks if the AI Platform endpoint is configured to encrypt captured data using private keys. Ensuring that data is encrypted with private keys enhances security by preventing exposure to unauthorized entities. Verify that the endpoint's data capture configuration specifies encryption settings using customer-managed keys (CMKs) within Cloud Key Management Service (KMS). Remediate by updating the endpoint configuration to use CMKs, thereby applying robust encryption standards.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/privacy/data-encryption
- rule_id: gcp.aiplatform.endpoint.data_privacy_ai_endpoint_config_content_type_whitel_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Ai Endpoint Config Content Type Whitel Enforced
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Content Type Whitelisting on AI Endpoint Configurations
  rationale: Enforcing content type whitelisting on AI endpoint configurations is crucial to prevent data leakage and unauthorized data processing, which can lead to compliance violations and reputational damage. This control helps mitigate risks of improper data handling and exposure to potential security vulnerabilities by ensuring only approved data formats are processed.
  description: This rule checks that all AI Platform endpoint configurations have content type whitelisting enforced, which restricts data input to predefined formats. To verify, review the endpoint configuration settings in the Google Cloud Console for content type specifications. Remediation involves updating the endpoint configurations to include valid and approved content types only, ensuring alignment with organizational data policies.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.aiplatform.endpoint.data_privacy_ai_endpoint_config_kms_keys_restricted
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Ai Endpoint Config KMS Keys Restricted
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Restrict KMS Keys for AI Platform Endpoint Configurations
  rationale: Restricting KMS keys for AI Platform endpoints is crucial to prevent unauthorized data access and ensure that sensitive data is encrypted at rest. Limiting key usage only to specific services and environments mitigates risk of data breaches and supports compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks that AI Platform endpoint configurations use customer-managed encryption keys (CMEK) from Cloud Key Management Service (KMS). Ensure that only authorized personnel and services have access to these keys. Verify key restrictions by reviewing IAM policies associated with each key and limit the access scope to necessary roles. Remediation involves updating IAM policies to enforce principle of least privilege and rotating keys regularly.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/using-cmek
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/key-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.aiplatform.endpoint.data_privacy_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Authn Required
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Endpoint Authentication for Data Privacy
  rationale: Enforcing authentication for AI Platform endpoints is crucial to prevent unauthorized access to sensitive data, which could lead to data breaches and non-compliance with regulations such as GDPR and HIPAA. Inadequate authentication can expose endpoints to malicious actors, increasing the risk of data theft and reputational damage.
  description: This rule checks that AI Platform endpoints require authentication to access data, ensuring that only authorized users can interact with the endpoint. To verify, ensure that IAM roles are correctly assigned and that authentication mechanisms, such as OAuth 2.0, are in place for accessing endpoints. Remediation involves setting up IAM policies that enforce authentication and regularly reviewing access logs to detect unauthorized attempts.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.aiplatform.endpoint.data_privacy_authz_policies_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Authz Policies Enforced
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Data Privacy Policies on AI Platform Endpoints
  rationale: Enforcing data privacy authorization policies on AI Platform endpoints is crucial to protect sensitive data from unauthorized access, reducing the risk of data breaches and maintaining customer trust. It also ensures compliance with regulatory standards such as GDPR and HIPAA, which mandate strict access controls and privacy measures for data handling.
  description: This rule checks that all AI Platform endpoints have data privacy authorization policies enforced, ensuring that only authorized users and services can access sensitive data. Verification involves reviewing IAM roles and policies associated with each endpoint to confirm that they align with organizational data privacy requirements. Remediation includes defining and applying appropriate IAM roles that restrict access to authorized identities, and leveraging VPC Service Controls for additional security boundaries.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - 'CIS GCP Benchmark: 4.1 Ensure that Cloud IAM policies do not allow broad access'
  - 'NIST SP 800-53 Rev. 5: AC-3 Access Enforcement'
- rule_id: gcp.aiplatform.endpoint.data_privacy_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption is Enabled for AI Platform Endpoints
  rationale: Enabling Key Management Service (KMS) encryption for AI Platform endpoints protects sensitive data by encrypting it with customer-managed keys, thereby mitigating the risk of unauthorized data access. This practice is crucial for maintaining data confidentiality, especially in environments handling personal or sensitive information, and it supports compliance with regulatory frameworks such as GDPR and HIPAA.
  description: This rule checks if AI Platform endpoints are configured to use customer-managed KMS keys for data encryption at rest. To verify, ensure that each endpoint within your AI Platform is associated with a KMS key. Remediation involves configuring AI Platform endpoints to use an existing KMS key by updating the endpoint configuration to include the 'encryptionSpec.kmsKeyName' property with the appropriate key identifier.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/encrypt-resources
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/how-tos
  - https://cloud.google.com/architecture/devops/using-kms-to-secure-cloud-resources
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.aiplatform.endpoint.data_privacy_logging_enabled
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Logging Enabled
  scope: aiplatform.endpoint.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Privacy Logging is Enabled for AI Platform Endpoints
  rationale: Enabling data privacy logging for AI Platform endpoints helps ensure that access to sensitive data is monitored and recorded, reducing the risk of unauthorized access and data breaches. This capability supports compliance with data protection regulations such as GDPR and CCPA by providing an audit trail of data access and usage, which is essential for maintaining trust and transparency with clients and stakeholders.
  description: This rule checks that data privacy logging is enabled for AI Platform endpoints, which involves configuring logging settings to capture detailed access logs. To verify, ensure that Cloud Audit Logs are set up to include DATA_READ and DATA_WRITE activities for AI Platform endpoints. If not configured, enable logging via the Google Cloud Console or gcloud CLI by setting the appropriate IAM roles and permissions. This action helps maintain a comprehensive audit log of data access for security and compliance purposes.
  references:
  - https://cloud.google.com/aiplatform/docs/logging
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.aiplatform.endpoint.data_privacy_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Platform Endpoints
  rationale: Enforcing private networking for AI Platform endpoints ensures that data is transmitted over secure, controlled networks, reducing exposure to public internet threats such as man-in-the-middle attacks and unauthorized access. This is crucial for safeguarding sensitive data, maintaining customer trust, and meeting regulatory compliance requirements such as GDPR and HIPAA.
  description: This rule checks whether AI Platform endpoints are configured to use private networking, which restricts access to the internal network and avoids exposure to the public internet. To verify, ensure that the endpoint's network configuration specifies a VPC network and that 'enablePrivateServiceConnect' is set to true. Remediation involves updating the endpoint settings to utilize a VPC network with private service access enabled, following GCPâ€™s best practices for private connectivity.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects.locations.endpoints
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/privacy
- rule_id: gcp.aiplatform.endpoint.encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption is Enabled for AI Platform Endpoints
  rationale: Enabling encryption for AI Platform endpoints protects sensitive data from unauthorized access and data breaches, which can lead to significant financial losses and reputational damage. Encryption at rest is critical for meeting compliance with regulations such as GDPR, HIPAA, and CCPA that mandate the protection of personal data stored in cloud environments.
  description: This rule checks if encryption is enabled for AI Platform endpoints to safeguard data at rest. Verify that each AI Platform endpoint has Customer-Managed Encryption Keys (CMEK) configured, which allows for greater control over cryptographic keys and data protection. Remediation involves setting up CMEK in Google Cloud Key Management Service (KMS) and applying it to your AI Platform endpoints through the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/encryption
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.aiplatform.endpoint.machine_learning_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Authn Required
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce Authentication for AI Platform Endpoints
  rationale: Requiring authentication for AI Platform endpoints ensures that only authorized users and applications can access machine learning models and services. This prevents unauthorized access, which could lead to data breaches or misuse of machine learning resources. Ensuring proper authentication aligns with compliance requirements such as GDPR and helps safeguard sensitive data processed by AI services.
  description: This rule checks that all AI Platform endpoints have authentication mechanisms enabled to restrict access to authorized entities. To verify, ensure that endpoint configurations require authentication tokens or service account credentials for access. Remediation involves updating endpoint settings to enforce authentication protocols such as OAuth 2.0. This enhances security by validating the identity of users and applications before granting access to AI resources.
  references:
  - https://cloud.google.com/ai-platform/docs/authentication
  - https://cloud.google.com/security/compliance/gdpr
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.aiplatform.endpoint.machine_learning_authz_policies_enforced
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Authz Policies Enforced
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce ML Endpoint Authz Policies in AI Platform
  rationale: Ensuring that machine learning endpoints are protected with robust authorization policies prevents unauthorized access, which could lead to data breaches, model theft, or service disruptions. Proper enforcement of these policies is crucial for maintaining the integrity of machine learning operations and complying with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks if authorization policies are enforced on AI Platform endpoints to control access. It verifies that Identity and Access Management (IAM) roles and policies are correctly configured to only allow permitted entities to interact with endpoints. Remediation involves reviewing IAM configurations, ensuring that least privilege principles are applied, and updating policies to restrict access appropriately.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - 'CIS GCP Benchmark: https://www.cisecurity.org/benchmark/google_cloud_computing_platform'
  - 'NIST SP 800-53: Access Control'
  - 'PCI-DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - ISO 27001:2013 A.9 Access Control
- rule_id: gcp.aiplatform.endpoint.machine_learning_config_content_type_whitelist_enforced
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Config Content Type Whitelist Enforced
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce AI Platform Endpoint Content Type Whitelisting
  rationale: Enforcing a content type whitelist on AI Platform endpoints minimizes the risk of malicious data being processed, which could lead to unauthorized access or data breaches. This control helps protect sensitive data and ensures compliance with regulatory standards that mandate data integrity and confidentiality.
  description: This rule checks if AI Platform endpoints have a content type whitelist configured, restricting data formats to only those explicitly allowed. To verify, review the endpoint's configuration in the Google Cloud Console or via the gcloud CLI to ensure a whitelist is defined. If not configured, update the endpoint settings to specify allowed content types, enhancing security by preventing the processing of potentially harmful data formats.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/architecture/framework/security
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.machine_learning_config_data_capture_bucket_encrypte_private
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Config Data Capture Bucket Encrypte Private
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Data Capture Buckets Use Customer-Managed Encryption Keys
  rationale: Encrypting machine learning data capture buckets using customer-managed encryption keys (CMEK) ensures that sensitive data is protected from unauthorized access and potential breaches. This practice supports regulatory compliance with standards like GDPR and HIPAA, which mandate robust data protection measures. It also mitigates risks associated with data leaks or unauthorized data manipulation, ultimately safeguarding intellectual property and building customer trust.
  description: This rule checks if the machine learning data capture bucket associated with an AI Platform endpoint is encrypted using customer-managed encryption keys. To verify, ensure that the bucket's encryption configuration specifies a valid CMEK. Remediation involves creating a CMEK in Cloud KMS and configuring the bucket to use this key for encryption. This ensures that you maintain control over the encryption keys and comply with organizational policies on data security.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/data-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.endpoint.machine_learning_config_kms_keys_restricted
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Config KMS Keys Restricted
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Restrict KMS Keys for AI Platform Endpoint Config
  rationale: Restricting KMS keys for AI Platform endpoints ensures that only authorized keys are used for encrypting machine learning configurations, protecting sensitive data from unauthorized access. This minimizes the risk of data breaches and supports compliance with regulatory standards such as GDPR and CCPA, which mandate strong data protection measures.
  description: This rule checks that only whitelisted KMS keys are used for encrypting AI Platform endpoint configurations. Ensure that endpoints are configured with specific KMS keys that adhere to your organization's encryption policies. To verify, review the endpoint's configuration in the GCP console or use the gcloud CLI to list the KMS keys in use and compare them with the approved list. Remediate by updating the endpoint configuration to use the correct KMS key and restrict access to unauthorized keys.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/security/encryption
  - https://cloud.google.com/kms/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-security-and-privacy-controls
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.endpoint.machine_learning_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Platform Endpoints
  rationale: Enabling KMS encryption for AI Platform endpoints is crucial to protect sensitive machine learning models and data from unauthorized access and potential breaches. This practice mitigates the risk of data exposure and aligns with regulatory requirements for data protection, such as GDPR and CCPA, by ensuring that data at rest is encrypted using Cloud KMS keys.
  description: This rule checks whether AI Platform endpoints are configured to use customer-managed encryption keys (CMEK) provided by Google Cloud Key Management Service (KMS). To verify compliance, ensure that each endpoint's encryption settings explicitly reference a valid KMS key. Remediate non-compliant endpoints by configuring them to use a KMS key via the AI Platform's endpoint configuration settings in the GCP Console or by using the appropriate gcloud CLI command.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/encryption
  - https://cloud.google.com/security/compliance/cis#cis_google_cloud_platform_foundations_benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/architecture/security-foundations/encryption
- rule_id: gcp.aiplatform.endpoint.machine_learning_logging_enabled
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Logging Enabled
  scope: aiplatform.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for AI Platform Endpoints
  rationale: Enabling logging for AI Platform endpoints provides visibility into operations and potential misuse by capturing audit trails of requests and responses. This is essential for detecting anomalies, mitigating risks, and ensuring compliance with regulations such as GDPR and HIPAA, which mandate data protection and the ability to audit data access and processing.
  description: This rule checks if logging is enabled for endpoints within the AI Platform, ensuring that all interactions with machine learning models are recorded. To verify and enable logging, ensure that audit logging is configured in the Google Cloud Console under the AI Platform section. Remediation involves enabling 'Admin Activity' and 'Data Access' logs to capture detailed operational insights.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.machine_learning_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Platform Endpoints
  rationale: Enforcing private networking for AI Platform endpoints reduces exposure to the public internet, minimizing the risk of unauthorized access and data breaches. This improves data confidentiality and integrity, supports compliance with regulatory frameworks such as GDPR and HIPAA, and aligns with best practices for secure machine learning operations.
  description: This rule checks if AI Platform endpoints are configured to use private IPs, ensuring they are accessible only through private networking within a Virtual Private Cloud (VPC). To verify, inspect the endpoint's network configuration for private IP addresses. Remediation involves configuring the endpoint to use private networking through the GCP Console or gcloud CLI, ensuring it is part of a peered VPC network.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/networking-overview
  - https://cloud.google.com/vpc/docs/private-access-options
  - CIS Google Cloud Platform Foundation Benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.endpoint.ml_ops_monitoring_logs_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ml Ops Monitoring Logs Enabled
  scope: aiplatform.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable ML Ops Monitoring Logs for AI Platform Endpoints
  rationale: Enabling ML Ops monitoring logs for AI Platform endpoints is crucial for maintaining visibility into model performance and operational metrics. This logging helps detect anomalies, troubleshoot issues, and ensure compliance with data handling policies. It mitigates risks associated with undetected model drift and performance degradation, which could lead to inaccurate predictions and business losses.
  description: This rule checks whether ML Ops monitoring logs are enabled for AI Platform endpoints, ensuring that model operations are being logged for analysis. To verify, navigate to the Google Cloud Console under AI Platform > Endpoints, and confirm that logging is enabled in the settings. If not enabled, configure logging by setting up a logging sink that routes logs to Cloud Logging. This allows for continuous monitoring and early detection of issues in ML models.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/logging
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/setup
  - 'NIST SP 800-53: AU-2 Audit Events'
  - ISO 27001:2013 A12.4 Logging and Monitoring
- rule_id: gcp.aiplatform.endpoint.ml_ops_monitoring_outputs_encrypted_and_private
  service: aiplatform
  resource: endpoint
  requirement: Ml Ops Monitoring Outputs Encrypted And Private
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure MLOps Monitoring Outputs Are Encrypted and Private
  rationale: Encryption of MLOps monitoring outputs is crucial to protect sensitive data from unauthorized access, which can lead to data breaches and non-compliance with regulations such as GDPR and HIPAA. Ensuring privacy and encryption helps mitigate risks associated with data leaks and enhances trust in GCP's AI Platform services.
  description: This rule checks that MLOps monitoring outputs in AI Platform endpoints are encrypted at rest using customer-managed encryption keys (CMEK). Verify that the outputs are stored in a Google Cloud Storage bucket with encryption enabled and access restricted to authorized users only. To remediate, configure the endpoint to use CMEK for encryption and set appropriate IAM policies to limit access.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.endpoint.ml_ops_monitoring_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ml Ops Monitoring Private Networking Enforced
  scope: aiplatform.endpoint.monitoring
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for MLOps Monitoring on AI Platform
  rationale: Enforcing private networking for AI Platform MLOps monitoring reduces exposure to public internet threats, minimizing risks of unauthorized access or data leaks. This ensures sensitive machine learning operations and data are protected, aligning with security best practices and compliance standards like HIPAA and ISO 27001.
  description: This rule checks if AI Platform endpoints for MLOps monitoring are configured to use private networking, preventing exposure to public networks. Verify that endpoints are accessible only through VPCs, enhancing security by ensuring data and operations remain within a controlled network environment. To remediate, configure AI Platform with private IPs in the VPC settings, ensuring all monitoring endpoints are accessible only through secure, internal networks.
  references:
  - https://cloud.google.com/ai-platform/docs/security#private_network_access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/solutions/security-overview
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/architecture/security-best-practices
- rule_id: gcp.aiplatform.endpoint.ml_ops_monitoring_roles_least_privilege
  service: aiplatform
  resource: endpoint
  requirement: Ml Ops Monitoring Roles Least Privilege
  scope: aiplatform.endpoint.monitoring
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Endpoint Monitoring Roles
  rationale: Implementing least privilege for AI Platform endpoint monitoring roles minimizes the risk of unauthorized access and potential data breaches. It ensures that users have only the permissions necessary to perform their job functions, which is crucial for maintaining compliance with security standards like GDPR and HIPAA, as well as reducing the attack surface.
  description: This rule checks that roles associated with AI Platform endpoint monitoring are configured to follow the principle of least privilege. It involves reviewing IAM policies to ensure that only essential permissions are granted. Remediation involves auditing and modifying IAM roles and policies to remove excessive permissions. Verification can be done by examining IAM role bindings and comparing them with operational requirements.
  references:
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.aiplatform.experiment.data_governance_logs_enabled
  service: aiplatform
  resource: experiment
  requirement: Data Governance Logs Enabled
  scope: aiplatform.experiment.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Governance Logs for AI Platform Experiments
  rationale: Enabling data governance logs for AI Platform experiments is crucial for tracking data access and changes, which helps in identifying unauthorized access and ensuring data integrity. This capability allows organizations to meet compliance requirements such as GDPR and CCPA by providing a clear audit trail of data usage. Additionally, it aids in forensic investigations by offering detailed logs that help mitigate the risk of data breaches and insider threats.
  description: This rule checks if data governance logging is enabled for AI Platform experiments. To verify, ensure that audit logs are configured to capture 'DATA_READ', 'DATA_WRITE', and 'ADMIN_READ' activities for the relevant AI Platform resources. Remediation involves enabling audit logging in the Google Cloud Console under the 'Logging' section for AI Platform resources. This will ensure that any access or modifications to experiment data are logged appropriately, enhancing the security posture of the AI Platform.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/ai-platform/docs/security
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 4
  - ISO/IEC 27001:2013
- rule_id: gcp.aiplatform.experiment.data_governance_metadata_store_encrypted
  service: aiplatform
  resource: experiment
  requirement: Data Governance Metadata Store Encrypted
  scope: aiplatform.experiment.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Metadata Store Encryption at Rest
  rationale: Encrypting the Metadata Store for AI Platform Experiments protects sensitive data from unauthorized access and potential breaches. It safeguards intellectual property and research data, ensuring compliance with data protection regulations such as GDPR and CCPA. Failure to encrypt can lead to data leaks that harm business reputation and result in financial penalties.
  description: This rule checks if the Metadata Store in GCP AI Platform Experiments is encrypted using customer-managed encryption keys (CMEK). Verify that the metadata store is configured with CMEK by reviewing the AI Platform settings in the Google Cloud Console. To remediate, enable encryption at rest using CMEK, which provides greater control over encryption keys and meets compliance requirements.
  references:
  - https://cloud.google.com/ai-platform/docs/experiments/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.experiment.data_governance_rbac_least_privilege
  service: aiplatform
  resource: experiment
  requirement: Data Governance RBAC Least Privilege
  scope: aiplatform.experiment.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Experiment Access
  rationale: Applying the principle of least privilege in AI Platform experiments minimizes risk by limiting data access to only those who need it for their roles. This reduces the potential for data breaches and unauthorized access, which can lead to data leaks and non-compliance with regulations like GDPR and HIPAA. Failing to implement this can result in significant financial and reputational damage.
  description: This rule checks that roles assigned to users for accessing AI Platform experiments adhere to the principle of least privilege. It requires that permissions granted are strictly necessary for job functions, reducing excess access. To verify compliance, review IAM policies on AI Platform resources, ensuring that users have the minimum required roles. Remediation involves adjusting IAM roles and permissions to remove unnecessary access rights, potentially using predefined roles instead of overly permissive custom roles.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.aiplatform.experiment.data_privacy_logs_enabled
  service: aiplatform
  resource: experiment
  requirement: Data Privacy Logs Enabled
  scope: aiplatform.experiment.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Data Privacy Logging for AI Platform Experiments
  rationale: Enabling data privacy logs for AI Platform experiments is crucial to monitor and audit data interactions, ensuring data integrity and compliance with privacy regulations such as GDPR and CCPA. Without logging, organizations face increased risk of unauthorized data access, potential data breaches, and non-compliance penalties.
  description: This rule checks that data privacy logs are enabled for AI Platform experiments. To verify, ensure that logging is configured in the AI Platform settings to capture data access and usage details. Remediation involves updating the experiment configurations to enable comprehensive logging, which can be done via the Google Cloud Console or CLI by setting the appropriate logging parameters.
  references:
  - https://cloud.google.com/ai-platform/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/privacy
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/auditing
- rule_id: gcp.aiplatform.experiment.data_privacy_metadata_store_encrypted
  service: aiplatform
  resource: experiment
  requirement: Data Privacy Metadata Store Encrypted
  scope: aiplatform.experiment.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Experiment Metadata Store Encryption
  rationale: Encrypting the metadata store in AI Platform experiments is crucial for protecting sensitive data against unauthorized access and potential data breaches. This practice helps mitigate risks of data exposure in case of security incidents and ensures compliance with regulatory standards such as GDPR and HIPAA, which mandate encryption of sensitive information at rest.
  description: This rule checks whether the metadata store associated with AI Platform experiments is encrypted using customer-managed encryption keys (CMEK). To verify, ensure that the encryption configuration specifies a valid CMEK. Remediation involves configuring AI Platform to use CMEK for the metadata store by updating the experimentâ€™s encryption settings in the GCP Console or via gcloud CLI commands. This enhances control over encryption keys and aligns with best practices for data protection.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.experiment.data_privacy_rbac_least_privilege
  service: aiplatform
  resource: experiment
  requirement: Data Privacy RBAC Least Privilege
  scope: aiplatform.experiment.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Platform Experiments Use Least Privilege Access
  rationale: Implementing least privilege for AI Platform experiments is critical to minimize the risk of unauthorized data access, which could lead to data breaches, financial loss, and non-compliance with regulations like GDPR and HIPAA. Proper RBAC configurations help prevent privilege escalation attacks and ensure that users have only the necessary permissions to perform their tasks, reducing the attack surface.
  description: This rule checks for the correct configuration of Role-Based Access Control (RBAC) on AI Platform experiment resources to enforce the principle of least privilege. It ensures that users have the minimal set of permissions required to manage experiments without unnecessary access to sensitive data. Verify by auditing IAM roles assigned to users and service accounts associated with AI Platform experiments, ensuring they align with their job functions. Remediate by adjusting roles to more restrictive ones, removing excessive permissions, and regularly reviewing permissions for ongoing appropriateness.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.experiment.machine_learning_logs_enabled
  service: aiplatform
  resource: experiment
  requirement: Machine Learning Logs Enabled
  scope: aiplatform.experiment.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Machine Learning Logs are Enabled for AI Platform Experiments
  rationale: Enabling machine learning logs is crucial for monitoring and auditing the activity within AI Platform experiments. This provides visibility into model training processes and data handling, which is essential for detecting anomalies, ensuring data integrity, and meeting compliance requirements such as GDPR and SOC2. Without logging, organizations risk undetected data breaches and non-compliance with regulatory standards, potentially leading to financial and reputational damage.
  description: This rule checks if logging is enabled for machine learning experiments in Google Cloud's AI Platform. To verify, ensure that the AI Platform's logging configurations are correctly set to capture detailed logs of model training and data processing activities. Remediation involves configuring the logging settings within the AI Platform to ensure comprehensive log collection, which can be done via the Google Cloud Console or by using the gcloud command-line tool. This facilitates effective monitoring and auditing of AI workloads.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.aiplatform.experiment.machine_learning_metadata_store_encrypted
  service: aiplatform
  resource: experiment
  requirement: Machine Learning Metadata Store Encrypted
  scope: aiplatform.experiment.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Metadata Store is Encrypted at Rest
  rationale: Encrypting the Machine Learning Metadata Store ensures that sensitive data, such as model parameters and training data lineage, is protected from unauthorized access and potential data breaches. Unencrypted data can lead to compliance violations and exposure of intellectual property, increasing the risk of data theft and business disruption.
  description: This rule checks whether the Machine Learning Metadata Store in GCP's AI Platform is encrypted at rest using customer-managed encryption keys (CMEK). To verify, review the metadata store's configuration in the GCP Cloud Console or use the gcloud CLI to ensure CMEK is enabled. If not configured, enable CMEK by updating the metadata store settings to use a Cloud Key Management Service (KMS) key. This enhances the security and compliance posture by ensuring data encryption aligns with organizational policies.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/architecture/encryption-and-key-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.aiplatform.experiment.machine_learning_rbac_least_privilege
  service: aiplatform
  resource: experiment
  requirement: Machine Learning RBAC Least Privilege
  scope: aiplatform.experiment.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Platform Experiment RBAC
  rationale: Limiting permissions to the least privilege necessary for AI Platform experiments reduces the risk of unauthorized access and potential data breaches. This approach mitigates insider threats and ensures compliance with data protection regulations by minimizing the exposure of sensitive machine learning models and datasets.
  description: This rule verifies that role-based access control (RBAC) for AI Platform experiments is configured to follow the principle of least privilege. Ensure that users and service accounts have only the permissions necessary to perform their specific tasks. Regularly audit and adjust permissions, removing unnecessary or excessive roles. Remediation involves using IAM policies to assign roles at the appropriate resource level and reviewing permissions periodically.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_offline_store_block_public_access
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store Offline Store Block Public Access
  scope: aiplatform.featurestore.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to AI Feature Store Offline Data
  rationale: Public access to AI Feature Store Offline Store can expose sensitive ML data, leading to data breaches, intellectual property theft, and non-compliance with regulations such as GDPR and CCPA. Ensuring data is only accessible to authorized users mitigates risks of unauthorized data manipulation and leakage.
  description: This rule checks that the AI Feature Store Offline Store is configured to block public access. Verify that access policies are set to deny access from 'allUsers' and 'allAuthenticatedUsers'. To remediate, update IAM policies to restrict access to specific service accounts or user groups. Use Google Cloud Console or gcloud CLI to modify IAM settings, ensuring that only authorized entities can interact with the offline data.
  references:
  - https://cloud.google.com/ai-platform/docs/feature-store/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_offline_store_gcs_encrypted
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store Offline Store Gcs Encrypted
  scope: aiplatform.featurestore.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Offline Store GCS Buckets are Encrypted in Feature Store
  rationale: Encrypting data at rest in GCS ensures that sensitive information is protected against unauthorized access and data breaches. This is critical for meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strong data protection measures. Failure to encrypt can lead to financial penalties and loss of customer trust.
  description: This rule checks if the Google Cloud Storage (GCS) buckets used for the Offline Store in AI Platform's Feature Store are encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). To verify, ensure that the `encryption_spec` field is configured for the Feature Store with an appropriate key name. If not configured, update the Feature Store settings to include encryption specifications using the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/featurestore/feature-stores
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis#section1.0
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_online_store_kms_encryptio_enabled
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store Online Store KMS Encryptio Enabled
  scope: aiplatform.featurestore.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure KMS Encryption Enabled for Feature Store Online Store
  rationale: Enabling KMS encryption for AI Feature Store Online Store protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, especially when handling personally identifiable information (PII) or proprietary machine learning features. Compliance with standards like GDPR or CCPA may require encryption to safeguard user data and prevent legal penalties.
  description: This rule checks if Key Management Service (KMS) encryption is enabled for the AI Platform Feature Store Online Store. Without KMS encryption, data at rest may be vulnerable to unauthorized access. To verify, ensure that the Feature Store's online store is configured to use a KMS key. Remediation involves setting up a KMS key in Google Cloud and configuring the Feature Store to utilize this key for encryption, providing an additional layer of security over default encryption.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/featurestore/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_private_networking_enforced
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store Private Networking Enforced
  scope: aiplatform.featurestore.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Feature Store
  rationale: Enforcing private networking for AI Feature Store minimizes exposure to public internet, thereby reducing the risk of unauthorized access and data breaches. This is critical for maintaining data privacy and meeting regulatory requirements such as GDPR and CCPA, which mandate stringent data protection measures. Private networking ensures that sensitive AI data is accessed only through secure, internal networks, thereby mitigating potential security threats.
  description: This rule checks whether AI Feature Store is configured to use private networking, preventing public IP access. Verify that the feature store is set to use Virtual Private Cloud (VPC) networks by checking the network configurations in the Google Cloud Console under AI Platform. To enforce private networking, configure VPC Service Controls and use private endpoints for communication. This ensures that all data traffic remains within Google's secure infrastructure.
  references:
  - https://cloud.google.com/ai-platform/documentation/feature-store/networking
  - https://cloud.google.com/vpc/docs/private-service-connect
  - 'CIS GCP Benchmark: Ensure that VPC Service Controls are used to restrict access'
  - 'NIST SP 800-53: AC-17 Remote Access'
  - 'PCI-DSS Requirement 1: Install and maintain a firewall configuration to protect cardholder data'
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_rbac_least_privilege
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store RBAC Least Privilege
  scope: aiplatform.featurestore.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Platform Feature Store Roles
  rationale: Implementing least privilege for AI Platform Feature Store ensures that users have only the necessary permissions to perform their job functions, reducing the risk of unauthorized access and potential data breaches. This control is crucial for maintaining data integrity and confidentiality, especially in environments handling sensitive or proprietary data. Compliance with least privilege principles is often a mandate under frameworks like PCI-DSS and ISO 27001, which require strict access controls.
  description: This rule checks if the AI Platform Feature Store roles are configured to adhere to the principle of least privilege. It involves auditing permissions granted to users and service accounts, ensuring they align strictly with required operational tasks. Remediation includes reviewing IAM policies on Feature Store resources and removing or adjusting permissions that exceed the necessary scope of access. Verification can be done through the IAM section of the GCP Console or using gcloud CLI commands to list and review roles.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://cloud.google.com/ai-platform/docs/feature-store
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.aiplatform.featurestore.ai_services_vector_index_authn_required
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Vector Index Authn Required
  scope: aiplatform.featurestore.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Authn for AI Services Vector Index in Featurestore
  rationale: Requiring authentication for accessing AI Services Vector Index prevents unauthorized access to sensitive data and models, mitigating risks such as data leakage and unauthorized data manipulation. This is crucial for maintaining data integrity and compliance with regulations like GDPR, which mandates strict access controls on personal data. Ensuring authentication helps organizations protect intellectual property and maintain trust with stakeholders.
  description: This rule checks that authentication is required when accessing the AI Services Vector Index within a Featurestore in GCP's AI Platform. It verifies whether proper authentication mechanisms, such as OAuth 2.0 or service account tokens, are enforced. To remediate, configure the Featurestore to require authentication by setting appropriate IAM policies and ensuring all accessing services and users have the necessary credentials. Regularly audit access logs to ensure compliance and adjust access controls as necessary.
  references:
  - https://cloud.google.com/ai-platform
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.aiplatform.featurestore.ai_services_vector_index_encrypted_at_rest_cmek
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Vector Index Encrypted At Rest Cmek
  scope: aiplatform.featurestore.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Vector Index Encryption at Rest with CMEK
  rationale: Encrypting the AI Services Vector Index at rest using Customer-Managed Encryption Keys (CMEK) provides enhanced control over the encryption process. This practice mitigates the risk of unauthorized data access, aligns with regulatory compliance requirements, and ensures that sensitive data remains secure even if the underlying storage is compromised.
  description: This rule checks whether AI Platform Featurestore's Vector Index is encrypted at rest using CMEK. To verify, ensure that the Featurestore resource is configured with a customer-managed key rather than the default Google-managed keys. If not configured, update the Featurestore settings to specify a CMEK for encryption. This can be done through the GCP Console or CLI by setting the 'encryptionSpec.kmsKeyName' field to the desired Cloud KMS key.
  references:
  - https://cloud.google.com/vertex-ai/docs/featurestore/concepts
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/customer-managed-encryption-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library
- rule_id: gcp.aiplatform.featurestore.ai_services_vector_index_private_networking_enforced
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Vector Index Private Networking Enforced
  scope: aiplatform.featurestore.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Services Vector Index
  rationale: Enforcing private networking for AI Services Vector Index in GCP's AI Platform Feature Store minimizes exposure to the public internet, reducing the risk of unauthorized access and potential data breaches. This is critical for protecting sensitive data and maintaining compliance with regulations such as GDPR and HIPAA, which mandate stringent data protection measures.
  description: This rule checks if AI Services Vector Index in the AI Platform Feature Store is configured to use private networking, which restricts access to internal IPs only. To verify, ensure that the AI Platform Feature Store's network settings are configured to use VPC Service Controls or private Google access. Remediation involves updating the network configuration to disable public access and enable access only through a private network, ensuring compliance with best practices for network security.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/networking
  - https://cloud.google.com/vpc-service-controls/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.aiplatform.featurestore.ai_services_vector_index_rbac_tenant_isolation_enforced
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Vector Index RBAC Tenant Isolation Enforced
  scope: aiplatform.featurestore.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Tenant Isolation in Feature Store RBAC for AI Services
  rationale: Enforcing tenant isolation in AI services vector index RBAC is crucial to prevent unauthorized access and data leakage between tenants. Without proper isolation, a compromised account or misconfiguration could expose sensitive data to unauthorized parties, increasing the risk of data breaches and violating data protection regulations such as GDPR and CCPA.
  description: This rule checks if Role-Based Access Control (RBAC) is configured to enforce tenant isolation in Google's AI Platform Feature Store. It ensures that each tenant has access only to their vector indexes, preventing unauthorized access to other tenants' data. Verify that roles and permissions are correctly assigned to maintain isolation. Remediation involves reviewing and updating IAM policies to enforce strict tenant boundaries.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/featurestore/overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.aiplatform.model.ai_services_model_artifact_encrypted_at_rest
  service: aiplatform
  resource: model
  requirement: Ai Services Model Artifact Encrypted At Rest
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Service Model Artifacts Are Encrypted at Rest
  rationale: Encrypting AI Service model artifacts at rest protects sensitive data from unauthorized access and breaches, crucial for maintaining data confidentiality and integrity. This is especially important in scenarios where models contain proprietary algorithms or sensitive training data. Compliance with regulations such as GDPR, HIPAA, and PCI-DSS often mandates encryption of sensitive data at rest, reducing the risk of financial and reputational damage.
  description: This rule checks that all AI Platform model artifacts are encrypted at rest using customer-managed encryption keys (CMEK) or Google-managed encryption keys. To verify, ensure that the encryption configuration is set in the AI Platform settings. Remediation involves configuring the AI Platform to use CMEK by specifying the Cloud Key Management Service (KMS) key during model creation or updating existing models to include encryption settings.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-111.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.ai_services_model_artifact_encrypted_at_rest_cmek
  service: aiplatform
  resource: model
  requirement: Ai Services Model Artifact Encrypted At Rest Cmek
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt AI Model Artifacts with CMEK in GCP AI Platform
  rationale: Encrypting AI model artifacts using Customer-Managed Encryption Keys (CMEK) ensures that organizations maintain control over cryptographic keys, reducing the risk of unauthorized access. This measure is crucial for protecting intellectual property and sensitive data, aligning with regulatory mandates like GDPR and HIPAA that require robust data protection practices.
  description: This rule checks if AI model artifacts in Google Cloud AI Platform are encrypted using Customer-Managed Encryption Keys (CMEK). To verify, ensure that model resources are configured with a user-specified KMS key. Remediation involves setting the AI Platform model to use a CMEK by updating the model's encryption settings via the Google Cloud Console or gcloud command-line tool. This enhances data protection by giving you full control over encryption keys.
  references:
  - https://cloud.google.com/ai-platform/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.model.ai_services_model_execution_role_least_privilege
  service: aiplatform
  resource: model
  requirement: Ai Services Model Execution Role Least Privilege
  scope: aiplatform.model.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Model Execution Roles
  rationale: Implementing least privilege for AI model execution roles minimizes the attack surface and reduces the risk of unauthorized access to sensitive resources. Over-privileged roles can lead to data breaches, service disruptions, and non-compliance with regulations such as GDPR and PCI-DSS, which mandate strict access controls to protect sensitive data.
  description: This rule checks whether AI model execution roles in Google Cloud's AI Platform are configured with the minimum necessary permissions. It verifies the role assignments to ensure they do not exceed the required permissions for executing AI models. Remediation involves auditing current role permissions, removing unnecessary access, and adhering strictly to the least privilege principle. To verify, review the IAM policy bindings for AI models and adjust roles using the GCP Console or gcloud CLI to ensure they are limited to execution-related permissions only.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/best-practices-for-using-iam
- rule_id: gcp.aiplatform.model.ai_services_model_image_scan_passed
  service: aiplatform
  resource: model
  requirement: Ai Services Model Image Scan Passed
  scope: aiplatform.model.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure AI Model Images Pass Security Scans
  rationale: Unscanned AI model images can harbor vulnerabilities that may be exploited by attackers, leading to unauthorized access or data breaches. Regular scanning of model images mitigates these risks by identifying and addressing vulnerabilities before deployment, ensuring compliance with security standards and protecting organizational assets.
  description: This rule verifies that AI model images in the Google Cloud AI Platform have passed security scans for vulnerabilities. It involves configuring automatic image scanning using Google Cloud's Container Analysis API and monitoring scan results. If vulnerabilities are detected, remediate them by updating the image or applying necessary patches before deploying the model. Regularly review and maintain scanning configurations to ensure new images are also scanned.
  references:
  - https://cloud.google.com/container-analysis/docs/overview
  - https://cloud.google.com/security-command-center/docs/posture-management-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/ai-platform/training/docs/using-containers
- rule_id: gcp.aiplatform.model.ai_services_model_kms_cmk_used
  service: aiplatform
  resource: model
  requirement: Ai Services Model KMS CMK Used
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Models Use Customer-Managed Encryption Keys
  rationale: Utilizing Customer-Managed Encryption Keys (CMKs) for AI models ensures that sensitive data used in AI processing is protected and under the control of your organization. This reduces the risk of unauthorized access or data breaches by providing granular control over encryption keys and meeting regulatory requirements for data protection, such as GDPR and HIPAA.
  description: This rule verifies that AI models in Google Cloud AI Platform are encrypted using Customer-Managed Encryption Keys (CMKs) instead of Google-managed keys. To ensure compliance, check that all AI models are configured to use a KMS CMK by specifying the 'kmsKeyName' during model deployment. Remediate by updating model configurations to use a CMK, which can be done via the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/encryption-at-rest
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/itl/nist-special-publication-800-57
- rule_id: gcp.aiplatform.model.ai_services_model_registry_access_rbac_least_privilege
  service: aiplatform
  resource: model
  requirement: Ai Services Model Registry Access RBAC Least Privilege
  scope: aiplatform.model.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Model Registry Access
  rationale: Implementing least privilege for AI Model Registry access minimizes the risk of unauthorized access, which can lead to data breaches, intellectual property theft, and compliance violations. Limiting access strictly to necessary roles reduces the attack surface and aligns with regulatory requirements for data protection and privacy, such as GDPR and PCI-DSS.
  description: This rule verifies that AI Model Registry access in Google Cloud Platform is configured according to the principle of least privilege. It checks that only specific roles with necessary permissions are granted access to the model registry, preventing over-privileged identities from making unauthorized modifications. Remediation involves auditing current access roles, revoking unnecessary permissions, and ensuring that roles like 'Viewer' or 'Editor' are only assigned to users with a clear business need.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/ai-platform/training/docs/using-gpus
- rule_id: gcp.aiplatform.model.ai_services_model_version_image_scan_passed
  service: aiplatform
  resource: model
  requirement: Ai Services Model Version Image Scan Passed
  scope: aiplatform.model.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure AI Model Image Scans Pass Security Checks
  rationale: Unscanned or vulnerable images in AI models can introduce security risks, such as unauthorized code execution or data breaches, potentially leading to financial loss or reputational damage. Ensuring images pass security scans helps mitigate these risks by identifying vulnerabilities before deployment, aligning with compliance standards like PCI-DSS and ISO 27001, and supporting secure AI service operations.
  description: This rule checks if AI Platform model versions have their container images scanned for vulnerabilities, ensuring any detected issues are addressed before deployment. To verify, ensure that the image scanning feature is enabled and configured to block deployments of images with high-severity vulnerabilities. Remediation involves configuring automatic scans and reviewing scan results regularly, addressing any critical or high vulnerabilities immediately.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/container-analysis/docs/container-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-207/final
- rule_id: gcp.aiplatform.model.ai_services_model_version_kms_encryption_enabled
  service: aiplatform
  resource: model
  requirement: Ai Services Model Version KMS Encryption Enabled
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Model Versions
  rationale: Encrypting AI model versions with Customer-Managed Encryption Keys (CMEK) protects sensitive AI data from unauthorized access and ensures compliance with regulatory frameworks like GDPR and HIPAA. Without proper encryption, AI models may be vulnerable to data breaches, risking intellectual property theft and regulatory fines.
  description: This rule checks if AI Platform model versions are encrypted using Google Cloud Key Management Service (KMS). To verify, ensure that the 'kmsKeyName' attribute is specified in the model's configuration. If not set, update the model configuration to include a KMS key. This enforces encryption at rest using customer-managed keys, enhancing data security and control.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/custom-service-account
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.model.ai_services_model_version_package_approved
  service: aiplatform
  resource: model
  requirement: Ai Services Model Version Package Approved
  scope: aiplatform.model.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Approve AI Model Versions in GCP AI Platform
  rationale: Approving AI model versions ensures that only vetted and secure model packages are deployed, reducing the risk of introducing vulnerabilities or unauthorized changes. This supports compliance with regulatory standards and protects sensitive data processed by AI models from potential exploitation and breaches.
  description: This check verifies that all AI model versions deployed in the Google Cloud AI Platform have been approved through a defined security review process. The configuration should include a mechanism for model version approval before deployment, ensuring that each model version package meets security and compliance standards. Administrators can remediate by implementing an approval workflow and ensuring that only approved versions are deployed. Verification involves reviewing audit logs for model deployments to confirm compliance with approval processes.
  references:
  - https://cloud.google.com/ai-platform/docs/model-management
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/frameworks/cis#cis-benchmarks
  - https://cloud.google.com/security/compliance/frameworks/iso-27001
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/security/overview
- rule_id: gcp.aiplatform.model.ai_services_supply_chain_model_artifact_image_scann_critical
  service: aiplatform
  resource: model
  requirement: Ai Services Supply Chain Model Artifact Image Scann Critical
  scope: aiplatform.model.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Critical Vulnerability Scanning for AI Model Artifacts
  rationale: Scanning AI model artifacts for critical vulnerabilities is essential to prevent the deployment of compromised software which could lead to data breaches or unauthorized access. Neglecting this can result in significant business risks, including loss of customer trust and potential non-compliance with regulatory standards that mandate secure software supply chains.
  description: This rule checks whether AI model artifacts in Google Cloud AI Platform undergo critical vulnerability scanning to identify and mitigate risks before deployment. Ensure that Artifact Registry or Container Analysis is configured to automatically scan for vulnerabilities in model artifacts. If critical vulnerabilities are found, remediation involves updating the artifact with a secure version or applying security patches. Regular monitoring and scanning are crucial to maintain a robust security posture.
  references:
  - https://cloud.google.com/container-analysis/docs
  - https://cloud.google.com/artifact-registry/docs/enable-vulnerability-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/architecture/implementing-devops/security
- rule_id: gcp.aiplatform.model.ai_services_supply_chain_model_artifact_sbom_present
  service: aiplatform
  resource: model
  requirement: Ai Services Supply Chain Model Artifact Sbom Present
  scope: aiplatform.model.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure SBOM Present for AI Services Supply Chain Model Artifacts
  rationale: Having a Software Bill of Materials (SBOM) for AI models is critical for transparency and security in the AI supply chain. It helps identify components and dependencies, mitigating risks of vulnerabilities and compliance failures due to unverified third-party software. This practice supports regulatory requirements and enhances trust in AI deployments by providing a clear understanding of software components used.
  description: This rule checks that every AI model artifact in the AI Platform has an associated SBOM. An SBOM provides a detailed inventory of all software components, including versioning and licensing information. To verify, ensure the SBOM is generated and stored alongside the model artifacts in your repository. Regularly update the SBOM as part of your CI/CD pipeline to reflect any changes. Remediation involves generating an SBOM using tools like Syft or CycloneDX and ensuring it's linked to the model artifact.
  references:
  - https://cloud.google.com/vertex-ai/docs/training-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/news-events/news/2021/05/nist-releases-software-supply-chain-security-guidance
  - https://www.iso.org/iso-27001-information-security.html
  - https://owasp.org/www-project-cyclonedx/
  - https://slsa.dev/spec/v0.1/
- rule_id: gcp.aiplatform.model.ai_services_supply_chain_model_artifact_signed_and_verified
  service: aiplatform
  resource: model
  requirement: Ai Services Supply Chain Model Artifact Signed And Verified
  scope: aiplatform.model.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Verify AI Model Artifacts with Signed and Trusted Certificates
  rationale: Ensuring that AI model artifacts are signed and verified protects against the introduction of malicious or tampered code, which can lead to unauthorized data access or manipulation. This practice is essential for maintaining the integrity of AI systems, meeting compliance requirements, and safeguarding business operations from potential threats.
  description: This rule checks that AI model artifacts in Google Cloud's AI Platform are signed with a trusted certificate and verified before deployment. To verify, ensure that all model artifacts have been signed using a certificate from a trusted Certificate Authority (CA) and that the verification process is in place to check the signatures before model execution. Remediation involves configuring the AI Platform to enforce signature verification before loading models, using tools like Binary Authorization for verification workflows.
  references:
  - https://cloud.google.com/ai-platform/deep-learning-vm/docs/security-best-practices
  - https://cloud.google.com/binary-authorization/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.data_governance_package_artifact_signed_and_verified
  service: aiplatform
  resource: model
  requirement: Data Governance Package Artifact Signed And Verified
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Model Artifacts Are Signed and Verified
  rationale: Signing and verifying AI model artifacts is crucial to ensure the integrity and authenticity of the models being deployed. Unsigned or improperly verified artifacts pose risks such as model tampering, which can lead to incorrect predictions, data leaks, or unauthorized access. Compliance with data protection regulations like GDPR and industry standards necessitates proper data governance and artifact management.
  description: This rule checks that AI Platform model artifacts are signed and verified before deployment. To achieve this, configure your models to include a digital signature using a secure key management system. Verify these signatures upon deployment to ensure they have not been altered. Remediation involves implementing artifact signing processes and using Cloud KMS for secure key management. Regular audits should be conducted to ensure compliance with this policy.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.model.data_governance_package_encrypted
  service: aiplatform
  resource: model
  requirement: Data Governance Package Encrypted
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Models Use Encrypted Data Governance Packages
  rationale: Encrypting data governance packages for AI Platform models is crucial to protect sensitive information from unauthorized access and data breaches. This practice mitigates risks associated with data leakage and ensures compliance with data protection regulations like GDPR and CCPA, reducing potential legal and financial penalties.
  description: This rule checks if the data governance packages for AI Platform models are encrypted using Google Cloud's Customer-Managed Encryption Keys (CMEK). To verify compliance, ensure that the AI Platform model resources specify an encryption key from Cloud Key Management Service (KMS). Remediation involves setting up a CMEK in Cloud KMS and configuring the model to use this key during deployment.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/data-governance
  - https://cloud.google.com/security/compliance/cis#cis-google-cloud-computing-foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.model.data_governance_package_not_publicly_shared
  service: aiplatform
  resource: model
  requirement: Data Governance Package Not Publicly Shared
  scope: aiplatform.model.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure AI Platform Model Data Governance Package is Not Publicly Shared
  rationale: Publicly sharing AI model data governance packages can lead to unauthorized access, data breaches, and exploitation of sensitive information. This exposure can result in financial loss, reputational damage, and non-compliance with privacy regulations such as GDPR and CCPA, undermining trust and operational integrity.
  description: This rule checks the access permissions of AI Platform model data governance packages to ensure they are not publicly accessible. It verifies that IAM policies restrict access only to authorized users and groups. If public sharing is detected, it is critical to immediately revise IAM settings to limit access. Remediation involves removing 'allUsers' or 'allAuthenticatedUsers' from the IAM policy bindings of the model resource.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/permissions-ai-platform
  - https://cloud.google.com/security-command-center/docs/how-to-remediate-security-findings
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.model.data_governance_registry_artifact_encryption_enabled
  service: aiplatform
  resource: model
  requirement: Data Governance Registry Artifact Encryption Enabled
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption of AI Platform Model Artifacts
  rationale: Enabling encryption for AI Platform model artifacts protects sensitive data from unauthorized access and potential data breaches. This is critical for maintaining data integrity and confidentiality, especially when dealing with proprietary algorithms and datasets. Compliance with regulations such as GDPR, HIPAA, and CCPA mandates robust data protection measures to safeguard user data and intellectual property.
  description: This rule checks whether encryption is enabled for artifacts stored in the AI Platform's Data Governance Registry. It verifies that the 'encryptionSpec' field is configured with a customer-managed encryption key (CMEK). To ensure compliance, navigate to the AI Platform model settings in the Google Cloud Console and configure the 'encryptionSpec' using a CMEK. This enhances data security by allowing users to control the encryption keys used for their data.
  references:
  - https://cloud.google.com/ai-platform/docs/general/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57pt1r4.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.aiplatform.model.data_governance_registry_cross_account_sharing_restricted
  service: aiplatform
  resource: model
  requirement: Data Governance Registry Cross Account Sharing Restricted
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Sharing in AI Platform Models
  rationale: Restricting cross-account sharing of AI models in GCP mitigates the risk of unauthorized access and data breaches, thereby protecting sensitive data and intellectual property. This practice is crucial to maintaining data privacy and meeting compliance requirements such as GDPR and CCPA, which mandate stringent data protection measures.
  description: This rule checks if AI Platform models are configured to prevent cross-account sharing, ensuring that access is limited to authorized accounts only. To verify, review the IAM policies associated with the AI model and ensure no permissions allow cross-account access. Remediation involves updating IAM policies to restrict sharing to within the organization or specific trusted accounts only.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.aiplatform.model.data_governance_registry_rbac_least_privilege
  service: aiplatform
  resource: model
  requirement: Data Governance Registry RBAC Least Privilege
  scope: aiplatform.model.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Platform Model Uses Least Privilege RBAC
  rationale: Implementing least privilege RBAC in the AI Platform models minimizes the risk of unauthorized access, which can lead to data breaches or malicious manipulation of machine learning models. This helps in maintaining data integrity, complying with regulatory standards such as GDPR and CCPA, and protecting sensitive intellectual property.
  description: This rule checks if the AI Platform model employs least privilege principles by verifying the role assignments in the Data Governance Registry. Ensure that only necessary roles with minimal permissions are granted to users and service accounts. To remediate, review and adjust IAM policies to remove excessive permissions while ensuring operational needs are met.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.data_privacy_package_artifact_signed_and_verified
  service: aiplatform
  resource: model
  requirement: Data Privacy Package Artifact Signed And Verified
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Model Artifacts are Signed and Verified
  rationale: Ensuring that AI Platform model artifacts are signed and verified is crucial for maintaining data integrity and preventing unauthorized modifications. Unsigned or unverified artifacts pose a risk of data breaches, intellectual property theft, and non-compliance with data protection regulations such as GDPR and HIPAA. This practice helps protect sensitive data and intellectual property while supporting trust in AI models.
  description: This rule checks that all data privacy package artifacts for AI Platform models are signed and verified to ensure authenticity and integrity. Organizations should leverage GCP's Artifact Registry to sign artifacts and use Binary Authorization to verify signatures before deployment. Regularly audit artifact signing processes and implement automated alerts for unsigned or tampered artifacts to mitigate risks. Remediation involves signing all unsigned artifacts and configuring verification policies.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/artifact-registry/docs/signing
  - https://cloud.google.com/binary-authorization/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.model.data_privacy_package_encrypted
  service: aiplatform
  resource: model
  requirement: Data Privacy Package Encrypted
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Model Data Privacy Package is Encrypted
  rationale: Encrypting data privacy packages for AI models mitigates the risk of unauthorized access to sensitive data, which can result in data breaches and compliance violations. This measure is crucial for protecting intellectual property and maintaining trust with customers and stakeholders. Non-compliance with encryption standards can lead to severe regulatory penalties and reputational damage.
  description: This rule checks whether the data privacy packages associated with AI Platform models are encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that the 'encryption_spec' field is specified in the model's configuration settings, which should point to a valid Cloud KMS key. Remediation involves updating the model configuration to include an appropriate encryption key for securing the data privacy package.
  references:
  - https://cloud.google.com/ai-platform/training/docs/model-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.aiplatform.model.data_privacy_package_not_publicly_shared
  service: aiplatform
  resource: model
  requirement: Data Privacy Package Not Publicly Shared
  scope: aiplatform.model.public_access
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: critical
  title: Ensure AI Platform Model Data Privacy Packages Not Publicly Shared
  rationale: Publicly sharing AI model data privacy packages can lead to unauthorized access to sensitive data, increasing the risk of data breaches and non-compliance with data protection regulations such as GDPR and CCPA. This could expose the organization to significant legal liabilities and reputational damage.
  description: This rule checks whether the data privacy packages associated with AI Platform models are publicly shared. Ensuring these packages are not publicly accessible is crucial for safeguarding sensitive data. Administrators should verify that IAM policies are correctly configured to restrict public access and apply encryption at rest to protect data integrity. Remediation involves reviewing and updating access controls and encryption settings in the GCP Console under the AI Platform's model permissions.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.model.data_privacy_registry_artifact_encryption_enabled
  service: aiplatform
  resource: model
  requirement: Data Privacy Registry Artifact Encryption Enabled
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure AI Platform Model Registry Artifacts are Encrypted
  rationale: Encrypting registry artifacts in AI Platform is crucial to protect sensitive data from unauthorized access and potential breaches, especially given the valuable nature of ML models and training datasets. It helps in mitigating risks of data leakage and loss, thus preserving intellectual property and maintaining regulatory compliance with standards like GDPR and HIPAA.
  description: This rule checks if encryption is enabled for data artifacts stored within the AI Platform's model registry. To verify, ensure that the AI Platform models utilize Customer-Managed Encryption Keys (CMEK) for data protection at rest. Remediation involves configuring the model registry to use CMEK by specifying a valid key from Cloud Key Management Service (KMS) during model creation or update.
  references:
  - https://cloud.google.com/ai-platform/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.data_privacy_registry_cross_account_sharing_restricted
  service: aiplatform
  resource: model
  requirement: Data Privacy Registry Cross Account Sharing Restricted
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict AI Model Cross-Account Sharing for Data Privacy
  rationale: Restricting cross-account sharing of AI models is crucial for maintaining data privacy and preventing unauthorized access to sensitive information. This control mitigates risks such as data breaches and intellectual property theft, which can lead to significant financial and reputational damage. Ensuring compliance with privacy regulations like GDPR and CCPA is essential to avoid legal penalties and maintain trust with stakeholders.
  description: This rule checks whether AI model resources in Google Cloud's AI Platform are configured to prevent cross-account sharing. It ensures that models are not shared with accounts outside the designated organization without explicit authorization. To verify, review the sharing settings of AI models and ensure they are not accessible to unauthorized external accounts. Remediation involves adjusting IAM policies to restrict model access to trusted accounts only.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.aiplatform.model.data_privacy_registry_rbac_least_privilege
  service: aiplatform
  resource: model
  requirement: Data Privacy Registry RBAC Least Privilege
  scope: aiplatform.model.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Model RBAC Follows Least Privilege Principle
  rationale: Applying the least privilege principle to AI model access controls minimizes the risk of unauthorized data access, which could lead to data breaches and non-compliance with data protection regulations. Improper access management can result in sensitive data exposure and potential financial and reputational damage.
  description: This rule checks that roles and permissions assigned to users accessing AI models are limited to the minimum necessary for their job functions, adhering to the least privilege principle. Verification involves reviewing IAM policies and ensuring that roles such as 'Viewer', 'Editor', and 'Owner' are not excessively granted. Remediation includes auditing IAM roles, removing unnecessary permissions, and employing custom roles for specific needs.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.aiplatform.model.machine_learning_package_artifact_signed_and_verified
  service: aiplatform
  resource: model
  requirement: Machine Learning Package Artifact Signed And Verified
  scope: aiplatform.model.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure ML Artifacts are Signed and Verified
  rationale: Ensuring that machine learning package artifacts are signed and verified mitigates the risk of deploying malicious code, which can lead to data breaches, model corruption, and business disruption. This practice is crucial for maintaining integrity and trust in AI models, aligning with compliance standards such as SOC2 and ISO 27001.
  description: This rule checks whether all machine learning package artifacts in the AI Platform are signed and verified before deployment. Verification ensures that only authorized and untampered artifacts are deployed, preventing potential security breaches. To remediate, ensure the signing process is integrated into the CI/CD pipeline and that verification occurs before deployment. Use GCP's Binary Authorization to enforce this policy.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/security-overview
  - https://cloud.google.com/binary-authorization/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.machine_learning_package_encrypted
  service: aiplatform
  resource: model
  requirement: Machine Learning Package Encrypted
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Packages in AI Platform are Encrypted at Rest
  rationale: Encrypting machine learning packages at rest in AI Platform protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining the confidentiality and integrity of data, especially in regulated industries where compliance with standards such as HIPAA and PCI-DSS is mandatory. Encryption helps mitigate risks related to data exposure in case of unauthorized access to storage systems.
  description: This rule checks whether machine learning packages stored in AI Platform are encrypted using Google-managed or customer-managed encryption keys. To verify, ensure that the AI Platform models have encryption configurations enabled in their settings. Remediation involves configuring the AI Platform to use Google Cloud Key Management Service (KMS) for encrypting data at rest by specifying a customer-managed key during model creation or update.
  references:
  - https://cloud.google.com/ai-platform/docs/model-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption/
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.aiplatform.model.machine_learning_package_not_publicly_shared
  service: aiplatform
  resource: model
  requirement: Machine Learning Package Not Publicly Shared
  scope: aiplatform.model.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Sharing of AI Platform Models
  rationale: Publicly shared machine learning models can expose sensitive algorithms and data, leading to intellectual property theft or misuse. Unauthorized access can result in model tampering or data leaks, violating compliance with frameworks like GDPR and HIPAA, and potentially causing reputational and financial damage.
  description: This rule checks if AI Platform models are publicly accessible, which could lead to unauthorized use or data breaches. Ensure that model IAM policies do not grant 'allUsers' or 'allAuthenticatedUsers' roles. Remediation involves reviewing model access policies and restricting permissions to specified users or service accounts only. Verify settings using the Google Cloud Console or CLI, and update policies to comply with the principle of least privilege.
  references:
  - https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/iam/docs/overview
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.aiplatform.model.machine_learning_registry_artifact_encryption_enabled
  service: aiplatform
  resource: model
  requirement: Machine Learning Registry Artifact Encryption Enabled
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption for AI Platform Model Artifacts
  rationale: Encrypting machine learning registry artifacts helps protect sensitive model data from unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality, meeting legal and regulatory requirements such as GDPR or HIPAA, and preserving the integrity of AI models used in critical business processes.
  description: This rule verifies that all machine learning model artifacts stored in Google Cloud AI Platform are encrypted at rest using customer-managed encryption keys (CMEK). To comply, ensure that each model in the AI Platform has encryption enabled with a specified CMEK in the model's settings. This can be validated by checking the encryption configuration in the Google Cloud Console under AI Platform settings or via the gcloud command-line tool. To remediate, configure the AI Platform model to use a CMEK through the Google Cloud Console or update the model configuration via the AI Platform API.
  references:
  - https://cloud.google.com/ai-platform/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.aiplatform.model.machine_learning_registry_cross_account_sharing_restricted
  service: aiplatform
  resource: model
  requirement: Machine Learning Registry Cross Account Sharing Restricted
  scope: aiplatform.model.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Restrict Cross-Account Sharing in AI Platform Model Registry
  rationale: Restricting cross-account sharing in the AI Platform Model Registry is essential to prevent unauthorized access to machine learning models, which may contain sensitive data or proprietary algorithms. Unrestricted sharing can lead to data breaches, intellectual property theft, and compliance violations, such as failing to adhere to GDPR or CCPA privacy regulations.
  description: This rule checks that machine learning models in Google Cloud's AI Platform Model Registry are not shared across accounts unless explicitly authorized. To verify, review the sharing settings of your AI models to ensure they are not publicly accessible or shared with unauthorized accounts. Remediation involves setting appropriate IAM policies to limit access to trusted users within the same organization and auditing access logs regularly.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.model.machine_learning_registry_rbac_least_privilege
  service: aiplatform
  resource: model
  requirement: Machine Learning Registry RBAC Least Privilege
  scope: aiplatform.model.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce RBAC Least Privilege for AI Platform Models
  rationale: Implementing least privilege in ML registry access minimizes the risk of unauthorized data exposure and model manipulation, which could lead to operational disruptions or regulatory non-compliance. Misconfigured permissions can result in data breaches, model theft, or biased model training, impacting business reputation and financial stability.
  description: This rule checks if permissions assigned to users or service accounts accessing AI Platform models adhere to the principle of least privilege. Users should only have the minimum roles necessary to perform their tasks, such as Viewer or Editor, rather than Owner. Verification involves auditing IAM policies associated with AI Platform models for overly permissive roles. Remediation includes adjusting IAM roles to limit access based on role-specific duties and regularly reviewing access logs for anomalies.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.notebook.ai_services_secrets_vault_access_rbac_least_privilege
  service: aiplatform
  resource: notebook
  requirement: Ai Services Secrets Vault Access RBAC Least Privilege
  scope: aiplatform.notebook.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Notebook Vault Access in AI Platform
  rationale: Implementing least privilege for access to AI Services Secrets Vault ensures that only authorized users and services can access sensitive secrets, minimizing the risk of data breaches. This control is critical for protecting intellectual property and maintaining compliance with data protection regulations like GDPR, HIPAA, and others, as unauthorized access could lead to significant financial and reputational harm.
  description: This rule verifies that access to the AI Services Secrets Vault from AI Platform notebooks is restricted to only those roles and identities that absolutely require it. Users should configure Identity and Access Management (IAM) roles to grant the minimum permissions necessary to perform their intended functions. To comply, audit current access policies using the GCP Console or CLI, ensuring that only essential service accounts and users have roles such as 'Secret Manager Secret Accessor'. Revoke any unnecessary permissions immediately.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.aiplatform.notebook.ai_services_secrets_vault_kms_encryption_and_rotatio_enabled
  service: aiplatform
  resource: notebook
  requirement: Ai Services Secrets Vault KMS Encryption And Rotatio Enabled
  scope: aiplatform.notebook.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption & Rotation for AI Notebook Secrets
  rationale: Using Cloud KMS for encrypting secrets in AI Platform notebooks enhances data security by protecting sensitive information against unauthorized access and potential data breaches. Regular key rotation further mitigates risks by ensuring that encryption keys are periodically changed, aligning with compliance standards like NIST and PCI-DSS, and thereby reducing the risk of key compromise.
  description: This rule checks if AI Platform notebook instances use Cloud KMS for encrypting secrets stored in the AI Services Secrets Vault and if key rotation is enabled. Verify that the notebooks have KMS keys configured for encryption, and key rotation periods are set in accordance with security policies. To remediate, configure AI Platform notebooks to use KMS keys by navigating to the notebook instance settings and specifying a KMS key, ensuring that automatic key rotation is enabled in the Cloud KMS settings.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/security
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-risk-management-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.notebook.ai_services_secrets_vault_no_plaintext_secrets_in_config
  service: aiplatform
  resource: notebook
  requirement: Ai Services Secrets Vault No Plaintext Secrets In Config
  scope: aiplatform.notebook.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure No Plaintext Secrets in AI Platform Notebook Configurations
  rationale: Storing plaintext secrets in AI Platform notebooks poses a significant security risk by potentially exposing sensitive data to unauthorized access. This can lead to data breaches, intellectual property theft, and non-compliance with regulatory frameworks such as GDPR, HIPAA, or PCI-DSS, which mandate the protection of sensitive information. Ensuring secrets are stored securely mitigates these risks and upholds data integrity and confidentiality.
  description: This rule checks for the presence of plaintext secrets in the configuration of AI Platform notebooks, ensuring that all secrets are stored securely using a dedicated secret management service such as Google Cloud Secret Manager. To verify compliance, inspect notebook configurations for hardcoded secrets and replace them with references to the secret management solution. Remediation involves migrating any plaintext secrets found to the secret manager and updating the notebook configuration to access secrets programmatically.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/best-practices-security
  - https://cloud.google.com/secret-manager/docs
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 6.4
  - NIST SP 800-53 Rev. 5 - System and Communications Protection
  - 'PCI DSS v3.2.1 - Requirement 3: Protect Stored Cardholder Data'
  - 'ISO/IEC 27001:2013 - Annex A.10: Cryptography'
- rule_id: gcp.aiplatform.notebook.ai_services_workspace_encryption_at_rest_enabled
  service: aiplatform
  resource: notebook
  requirement: Ai Services Workspace Encryption At Rest Enabled
  scope: aiplatform.notebook.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure AI Notebook Workspace Encryption at Rest is Enabled
  rationale: Enabling encryption at rest for AI Notebook workspaces protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining confidentiality and integrity, especially in environments where sensitive or proprietary data is processed, supporting compliance with regulations like GDPR and HIPAA.
  description: This rule checks whether encryption at rest is enabled for AI Notebook workspaces within Google Cloud's AI Platform. The configuration ensures that all data stored within the workspace is encrypted using Google-managed encryption keys. To verify, navigate to the AI Platform Notebooks in the GCP Console and ensure encryption settings are configured. If not enabled, update the notebook instance settings to use encryption at rest, leveraging Google-managed keys or customer-managed keys for enhanced security.
  references:
  - https://cloud.google.com/ai-platform-notebooks/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.aiplatform.notebook.ai_services_workspace_iam_role_least_privileged
  service: aiplatform
  resource: notebook
  requirement: Ai Services Workspace IAM Role Least Privileged
  scope: aiplatform.notebook.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privileged IAM Roles for AI Notebook Workspaces
  rationale: Applying the principle of least privilege to IAM roles for AI Notebook workspaces minimizes the risk of unauthorized access, data leakage, and potential misuse of AI resources. This is crucial for maintaining a secure environment, protecting sensitive data, and adhering to compliance standards such as PCI-DSS, HIPAA, and ISO 27001.
  description: This rule verifies that IAM roles assigned to AI Notebook workspaces are granted only the permissions necessary for their function. It checks for overly permissive roles and suggests adjustments to align with the least privilege principle. Remediation involves reviewing assigned roles, identifying excessive permissions, and modifying or creating custom roles with restricted privileges. Ensure only essential permissions are granted by using Google Cloud IAM's predefined or custom roles appropriately.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
  - https://cloud.google.com/iam/docs/roles-best-practices
- rule_id: gcp.aiplatform.notebook.ai_services_workspace_idle_shutdown_configured
  service: aiplatform
  resource: notebook
  requirement: Ai Services Workspace Idle Shutdown Configured
  scope: aiplatform.notebook.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure AI Services Workspace Idle Shutdown is Configured
  rationale: Configuring idle shutdown for AI Services Workspaces helps optimize resource usage and reduce costs by automatically powering down idle resources. This minimizes the attack surface by ensuring resources are not left running unnecessarily, which could be exploited by attackers. It also supports compliance with cost management and operational efficiency standards.
  description: This check verifies that AI Services Workspaces in GCP have idle shutdown configured. Idle shutdown automatically shuts down workspaces after a specified period of inactivity, which can be configured in the GCP Console under the AI Platform Notebook settings. To verify, navigate to the AI Platform Notebooks section and ensure that the 'Idle Shutdown' option is enabled with an appropriate time limit set. Remediation involves enabling this setting and selecting a suitable time interval to balance resource availability with security and cost considerations.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/monitoring-and-logging#idle_shutdown
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-using-google-cloud-identity-and-access-management
  - https://cloud.google.com/blog/products/data-analytics/reducing-ai-ml-costs-with-gcp
- rule_id: gcp.aiplatform.notebook.ai_services_workspace_private_networking_enforced
  service: aiplatform
  resource: notebook
  requirement: Ai Services Workspace Private Networking Enforced
  scope: aiplatform.notebook.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Services Notebooks
  rationale: Enforcing private networking for AI Services notebooks reduces the attack surface by preventing exposure to the public internet. This minimizes the risk of unauthorized access, data breaches, and ensures compliance with regulations that mandate secure data handling, such as HIPAA and GDPR. It is crucial for protecting sensitive machine learning models and data.
  description: This rule checks if AI Services notebooks are configured to use private networking, ensuring that they are only accessible within a VPC. To verify, ensure that the notebooks have the 'network' attribute set to a valid VPC network. Remediation involves updating the notebook configuration to specify a private VPC network, which can be done through the Google Cloud Console or gcloud CLI. This configuration enhances security by limiting access to internal IPs only.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/configure-private-ip
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.8
  - https://cloud.google.com/architecture/best-practices-vpc-design
  - NIST SP 800-53 Rev. 5, AC-17 Remote Access
  - ISO/IEC 27001:2013 A.13.1 Network Security Management
- rule_id: gcp.aiplatform.pipeline_job.ai_services_agent_orchestrator_data_exfiltration_po_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Agent Orchestrator Data Exfiltration Po Enforced
  scope: aiplatform.pipeline_job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Services Agent Orchestrator Prevents Data Exfiltration
  rationale: Preventing data exfiltration in AI services is crucial to protect sensitive data and comply with data protection regulations. Uncontrolled data flow can lead to unauthorized data access, violating privacy laws such as GDPR and impacting organizational reputation and trust.
  description: This rule checks if the AI Services Agent Orchestrator within AI Platform Pipeline Jobs is configured to prevent data exfiltration. Ensure the orchestration policies enforce data encryption and restrict network access to authorized endpoints only. Verify compliance by reviewing the pipeline job's configuration in the GCP console and apply necessary IAM policies and VPC Service Controls to mitigate exfiltration risks.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security-overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.aiplatform.pipeline_job.ai_services_agent_orchestrator_secrets_isolation_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Agent Orchestrator Secrets Isolation Enabled
  scope: aiplatform.pipeline_job.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure AI Services Agent Orchestrator Secrets Isolation
  rationale: Isolating orchestrator secrets in AI Platform ensures that sensitive credentials and keys are not inadvertently exposed or misused, mitigating the risk of unauthorized access and potential data breaches. This practice is crucial for maintaining the confidentiality and integrity of operations, especially in environments where multiple AI models and pipelines are managed. It also aligns with compliance requirements such as PCI-DSS and HIPAA, which mandate stringent controls on sensitive data handling.
  description: This rule checks if AI Services Agent Orchestrator secrets are isolated in AI Platform Pipeline Jobs by verifying specific configuration settings that separate and protect sensitive information. To enable secrets isolation, users must configure their pipeline jobs to use dedicated secret management tools, such as Secret Manager, and ensure that secrets are not hardcoded or stored in plaintext. Remediation involves auditing existing pipeline configurations, updating them to utilize secure secret storage, and enforcing access controls. Verification can be performed by reviewing pipeline job configurations for compliance with these practices.
  references:
  - https://cloud.google.com/ai-platform/docs/pipelines/secrets
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/iam/docs
- rule_id: gcp.aiplatform.pipeline_job.ai_services_agent_orchestrator_tool_use_allowlist_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Agent Orchestrator Tool Use Allowlist Enforced
  scope: aiplatform.pipeline_job.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce AI Services Agent Orchestrator Tool Allowlist
  rationale: Enforcing an allowlist for AI Services Agent Orchestrator tools helps mitigate security risks by ensuring only authorized tools can orchestrate AI workloads, reducing the attack surface. This approach is crucial for maintaining data integrity and preventing unauthorized access or manipulation of AI pipelines, aligning with compliance requirements like SOC2 and HIPAA.
  description: This rule checks whether an allowlist is enforced for tools used by the AI Services Agent Orchestrator in GCP AI Platform pipeline jobs. To verify, ensure that the allowlist is configured in the IAM policies, restricting tool access to only those explicitly approved. Remediation involves updating IAM policies to include a specific list of permitted tools, thereby preventing unauthorized orchestration activities.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.pipeline_job.ai_services_data_pipeline_access_controls_least_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Data Pipeline Access Controls Least Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Pipeline Job Access
  rationale: Implementing least privilege access for AI Platform pipeline jobs minimizes the risk of unauthorized data access and potential data breaches. Over-permissioned roles can lead to exploitation by malicious actors, impacting business operations and violating compliance standards such as GDPR and CCPA.
  description: This rule verifies that AI Platform pipeline jobs are configured with the least privilege necessary, ensuring that service accounts and users have only the permissions required to perform their tasks. To verify compliance, review the IAM roles assigned to the pipeline jobs and ensure they align with the principle of least privilege. Remediation involves adjusting IAM policies to limit permissions, removing unnecessary roles, and applying custom roles where appropriate.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/security-best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/enterprise/best-practices-for-identity-and-access-management
- rule_id: gcp.aiplatform.pipeline_job.ai_services_data_pipeline_private_networking_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Data Pipeline Private Networking Enforced
  scope: aiplatform.pipeline_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Data Pipelines
  rationale: Enforcing private networking for AI services data pipelines helps protect sensitive data from exposure to the public internet, reducing the risk of unauthorized access and data breaches. This is particularly important for organizations handling proprietary or regulated data, as it supports compliance with industry standards and regulations while ensuring the integrity and confidentiality of data during processing.
  description: This rule checks that AI Platform pipeline jobs are configured to use private networking, which restricts network access to authorized internal IPs only. To verify compliance, inspect the pipeline job configuration to ensure that private networking is enabled, which typically involves setting up a VPC with appropriate firewall rules. Remediation involves modifying pipeline configurations to route traffic through a Virtual Private Cloud (VPC) and applying necessary firewall rules to allow only trusted sources.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/configure-private-networking
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations-vpc-design
- rule_id: gcp.aiplatform.pipeline_job.ai_services_pipeline_iam_role_least_privileged
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Pipeline IAM Role Least Privileged
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Platform Pipeline IAM Roles
  rationale: Implementing least privilege for AI Platform Pipeline IAM roles minimizes the risk of unauthorized access to sensitive data and resources, reducing potential security breaches. This aligns with compliance requirements, such as NIST and ISO 27001, which mandate strict access controls to protect data integrity and confidentiality. Inadequate privilege management can lead to data leaks, financial loss, and reputational damage.
  description: This rule checks if AI Platform Pipeline jobs are assigned IAM roles that adhere to the principle of least privilege. It ensures that roles only have permissions necessary to perform their tasks, without excess access. To verify, review IAM role assignments for pipeline jobs and adjust permissions to the minimum required. Remediation involves auditing current roles and applying more restrictive policies using GCP's IAM policy management tools.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.pipeline_job.ai_services_pipeline_kms_encryption_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Pipeline KMS Encryption Enabled
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption for AI Platform Pipeline Jobs
  rationale: Enabling KMS encryption for AI Platform pipeline jobs is crucial to protect sensitive data processed and stored by AI models. This reduces the risk of data breaches by ensuring that data is encrypted at rest using customer-managed keys. It helps organizations meet compliance requirements such as GDPR and HIPAA, by ensuring that sensitive information remains secure and under the control of the organization.
  description: This rule checks whether AI Platform pipeline jobs are configured to use Customer-Managed Encryption Keys (CMEK) with Google Cloud Key Management Service (KMS). To verify, ensure that the `kmsKeyName` attribute is set in the pipeline job configuration. To remediate, update the pipeline job definition to include a valid KMS key name, ensuring that the key is properly granted permissions to the AI Platform service account for encryption and decryption operations.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security#encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.pipeline_job.ai_services_pipeline_logging_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Pipeline Logging Enabled
  scope: aiplatform.pipeline_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure AI Services Pipeline Logging is Enabled
  rationale: Enabling logging for AI Services pipeline jobs is crucial for monitoring and auditing purposes. It helps in detecting anomalies, understanding user behavior, and maintaining compliance with data protection regulations such as GDPR and CCPA. Inadequate logging can result in undetected security incidents and non-compliance with legal and regulatory mandates.
  description: This check verifies that logging is enabled for AI Platform pipeline jobs, ensuring that all operations are recorded and auditable. It involves configuring the AI Platform to capture logs related to pipeline execution, which can be reviewed for troubleshooting and compliance audits. To enable logging, configure the AI Platform to integrate with Cloud Logging and ensure that the appropriate log sinks are set up to capture pipeline job activities. This can be done through the Google Cloud Console or using gcloud commands.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.pipeline_job.data_governance_ai_human_review_artifacts_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Ai Human Review Artifacts Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Human Review Artifacts are Encrypted at Rest
  rationale: Encrypting AI human review artifacts at rest protects sensitive data from unauthorized access, reducing the risk of data breaches and ensuring compliance with data protection regulations such as GDPR and HIPAA. This is crucial for maintaining trust with stakeholders and preventing potential financial and reputational damage from data leaks.
  description: This rule checks whether AI human review artifacts within AI Platform Pipeline Jobs are encrypted at rest using Customer-Managed Encryption Keys (CMEK). To verify, ensure that encryption settings in the pipeline job configurations are set to use CMEK. Remediation involves updating the pipeline job configuration to specify a CMEK for encrypting artifacts, which can be done via the GCP Console or gcloud command-line tool by setting the appropriate encryption key during job creation.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.pipeline_job.data_governance_ai_human_review_ui_private_networki_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Ai Human Review Ui Private Networki Enforced
  scope: aiplatform.pipeline_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Human Review UI Uses Private Network in AI Platform Pipelines
  rationale: Enforcing private network connections for the AI Human Review UI in AI Platform pipelines reduces exposure to unauthorized access, minimizing data breaches and protecting sensitive information. This control is crucial for maintaining confidentiality and integrity, particularly when handling regulated or proprietary data, and supports compliance with data protection regulations.
  description: This rule verifies that AI Platform pipeline jobs utilize a private network for the AI Human Review UI, enhancing data governance through network isolation. Check that the 'network' field in pipeline job configuration specifies a private VPC. Remediation involves updating the pipeline job configuration to include a private VPC network, ensuring all data interactions are confined within a controlled environment.
  references:
  - https://cloud.google.com/ai-platform/docs/pipelines/configure-networking
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/security-foundations/networking
  - https://cloud.google.com/vpc/docs/vpc
- rule_id: gcp.aiplatform.pipeline_job.data_governance_ai_human_review_workteam_access_rb_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Ai Human Review Workteam Access Rb Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict Human Review Workteam Access in AI Platform Pipelines
  rationale: Ensuring least privilege access for AI human review workteams is critical to prevent unauthorized data exposure and maintain compliance with data protection regulations. Improper access controls can lead to data breaches, impacting both business reputation and customer trust. Adhering to access control best practices mitigates the risk of insider threats and aligns with regulatory standards.
  description: This rule checks if AI Platform's pipeline job configurations grant only necessary privileges to human review workteams. It verifies that access roles are restricted to the minimum required for task execution, reducing the risk of data leakage. To remediate, review and adjust IAM policies to ensure users have the least privilege necessary for their roles. Regular access reviews and audits should also be conducted to maintain compliance.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.pipeline_job.data_governance_artifacts_private_and_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Artifacts Private And Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Pipeline Job Artifacts are Encrypted and Private
  rationale: Encrypting and restricting access to AI Platform Pipeline Job artifacts helps protect sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining data integrity and confidentiality, especially in industries with strict data protection regulations like healthcare and finance. Non-compliance can lead to reputational damage and financial penalties.
  description: This rule checks if AI Platform Pipeline Job artifacts are stored in private and encrypted storage buckets. Verify that the Google Cloud Storage buckets used for storing pipeline job outputs have Bucket Policy Only or Uniform Bucket-Level Access enabled, and that they use Customer-Managed Encryption Keys (CMEK) for encryption. Remediation involves configuring the storage bucket settings to ensure private access and enabling CMEK for encryption.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://cloud.google.com/storage/docs/access-control/uniform-bucket-level-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.pipeline_job.data_governance_kms_encryption_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance KMS Encryption Enabled
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Platform Pipeline Jobs
  rationale: Enabling KMS encryption for AI Platform pipeline jobs protects sensitive data by encrypting it at rest using a customer-managed key. This reduces the risk of unauthorized data access and helps meet compliance requirements for data protection standards such as HIPAA, PCI-DSS, and GDPR. It ensures that only authorized users with access to the encryption key can decrypt the data, mitigating potential data breaches.
  description: This rule checks whether AI Platform pipeline jobs are configured to use Cloud KMS customer-managed keys for data encryption at rest. To verify, ensure the pipeline job configuration includes an 'encryptionSpec' with a specified 'kmsKeyName'. If not configured, update the pipeline job settings to include a valid KMS key from your Cloud KMS service. This enhances data security by leveraging encryption best practices.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/kms/docs
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.pipeline_job.data_governance_logging_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Logging Enabled
  scope: aiplatform.pipeline_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Data Governance Logging for AI Platform Pipeline Jobs
  rationale: Data governance logging is crucial for monitoring and auditing AI workflows, providing visibility into data access and manipulation within pipeline jobs. Without proper logging, organizations risk unauthorized data access, potential data breaches, and non-compliance with regulations such as GDPR or HIPAA, which can lead to severe legal and financial repercussions.
  description: This rule verifies that data governance logging is enabled for AI Platform pipeline jobs to ensure all data activities are captured in audit logs. To implement, configure AI Platform pipeline jobs in the Google Cloud Console to ensure Cloud Audit Logs are activated for data access. Remediation involves enabling logging through IAM permissions and configuring logging sinks to capture and store logs in Cloud Logging for review and analysis.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/audit-logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.aiplatform.pipeline_job.data_governance_roles_least_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Roles Least Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Pipeline Jobs
  rationale: Limiting roles to the least privilege necessary for AI Platform pipeline jobs reduces the risk of unauthorized access and data breaches. Over-privileged access can lead to accidental or malicious data manipulation, violating compliance requirements such as GDPR and HIPAA, and potentially causing reputational damage and financial loss.
  description: This rule checks that users and service accounts assigned to AI Platform pipeline jobs have the minimal necessary permissions. Ensure roles like 'Viewer' or 'Custom Role' with specific permissions are used instead of broad roles like 'Editor'. Verify by reviewing IAM policies and adjust using the GCP Console or gcloud CLI to enforce strict access controls, ensuring compliance with organizational policies.
  references:
  - https://cloud.google.com/ai-platform/docs/pipelines/security
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_ai_human_review_artifacts_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Ai Human Review Artifacts Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Human Review Artifacts Are Encrypted
  rationale: Encrypting human review artifacts in AI Platform ensures that sensitive data used in machine learning processes is protected from unauthorized access. This mitigates the risk of data breaches and aligns with compliance requirements such as GDPR and HIPAA, which mandate the protection of personally identifiable information (PII) and sensitive data. Insufficient encryption can lead to significant financial and reputational damage due to data exposure.
  description: This rule verifies that all human review artifacts generated by AI Platform Pipeline Jobs are encrypted at rest using customer-managed encryption keys (CMEK). To ensure compliance, configure the AI Platform to use CMEK for encryption by specifying the key in the pipeline job configuration. Regularly audit your encryption settings to ensure they align with your organization's data protection policies. Remediation involves updating the pipeline job configurations to specify a valid CMEK.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - 'CIS GCP Benchmark: 4.7 Ensure that Cloud Storage bucket is encrypted with CMK'
  - 'NIST SP 800-57 Part 1: Recommendation for Key Management'
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_ai_human_review_ui_private_networking_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Ai Human Review Ui Private Networking Enforced
  scope: aiplatform.pipeline_job.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Private Networking for AI Human Review UI
  rationale: Enforcing private networking for AI Human Review UI mitigates the risk of data exposure over the internet, reducing vulnerability to unauthorized access and data breaches. This is crucial for maintaining data privacy and meeting compliance requirements such as GDPR and HIPAA, especially when handling sensitive data through AI platforms.
  description: This rule checks that AI Human Review UI is configured to operate within a private network, ensuring data is not exposed to public networks. Verify that the 'network' field in pipeline job configurations references a valid VPC network. Remediation involves configuring the AI Platform to use private networking by setting up a private Google Access and ensuring the pipeline job is deployed within this network. This setup helps in securing data in transit and aligns with organizational data protection policies.
  references:
  - https://cloud.google.com/ai-platform/docs/pipelines/networking
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_ai_human_review_workteam_access_rbac__privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Ai Human Review Workteam Access RBAC Privilege
  scope: aiplatform.pipeline_job.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure RBAC for AI Human Review Workteam in Pipeline Jobs
  rationale: Proper Role-Based Access Control (RBAC) for AI Human Review Workteams is critical to prevent unauthorized access to sensitive data processed in AI pipelines. Unauthorized access can lead to data breaches, compliance violations, and loss of trust. Ensuring appropriate privileges helps mitigate risks associated with data privacy regulations such as GDPR and CCPA.
  description: This rule checks if AI Human Review Workteams involved in AI Platform pipeline jobs have the necessary and appropriate roles assigned. Configurations should ensure that only authorized personnel have access to sensitive data, following the principle of least privilege. To verify, review IAM permissions for workteams associated with pipeline jobs and adjust roles to match their responsibilities. Remediation involves updating IAM policies to restrict access to only those who require it for their job functions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/pipelines
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_artifacts_private_and_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Artifacts Private And Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Pipeline Data is Private and Encrypted
  rationale: Data privacy is critical for protecting sensitive information processed by AI Platform pipelines. Unencrypted data artifacts can be exposed to unauthorized access, leading to data breaches and non-compliance with regulations such as GDPR and HIPAA. Ensuring data privacy and encryption mitigates risks of data leakage and supports maintaining trust and regulatory adherence.
  description: This rule verifies that artifacts created by AI Platform pipeline jobs are stored privately and encrypted at rest. Ensure that all output artifacts are stored in Google Cloud Storage buckets with bucket policies enforcing encryption, such as Customer-Managed Encryption Keys (CMEK). Verify encryption settings through the Google Cloud Console or CLI, and apply necessary policies to restrict access and enforce encryption standards.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/architecture/guidelines-for-securing-ai-platform-pipelines
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_kms_encryption_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy KMS Encryption Enabled
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Platform Pipeline Jobs
  rationale: Using KMS encryption for AI Platform pipeline jobs protects sensitive data at rest, reducing the risk of unauthorized access or data breaches. This practice helps organizations comply with regulatory requirements such as GDPR and HIPAA, which mandate robust data protection measures. Encrypting data also mitigates potential financial and reputational damage from data leaks.
  description: This rule checks that all AI Platform pipeline jobs are configured with Cloud KMS keys for encryption. The pipeline job must specify a customer-managed encryption key (CMEK) to ensure that data is securely encrypted at rest. To verify, review pipeline job configurations for the `encryptionSpec` field and confirm it references a valid KMS key. To remediate, update the pipeline job to include a KMS key in its encryption settings.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_logging_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Logging Enabled
  scope: aiplatform.pipeline_job.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Privacy Logging is Enabled for AI Platform Pipeline Jobs
  rationale: Enabling data privacy logging for AI Platform pipeline jobs is crucial for monitoring access and usage of sensitive data, which helps in identifying unauthorized access and mitigating data breaches. It supports compliance with data protection regulations such as GDPR and CCPA by providing audit trails that demonstrate adherence to privacy policies. This practice also aids in detecting anomalies and ensuring accountability in data processing activities.
  description: This rule checks whether data privacy logging is enabled for AI Platform pipeline jobs, ensuring that all data access and processing activities are logged and monitored. To verify, ensure that logging is configured in the AI Platform settings by enabling 'logConfig' for pipeline jobs. Remediation involves updating the pipeline job specifications to include the necessary logging configurations, allowing for comprehensive data access tracking.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/reference
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_roles_least_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Roles Least Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Pipeline Job Roles
  rationale: Properly assigning roles with the least privilege principle is crucial to minimize the risk of unauthorized access or data leakage. Misconfigured roles can lead to excessive permissions, increasing the attack surface and potential for data breaches. Adhering to least privilege is often a requirement under compliance frameworks such as GDPR and HIPAA, which emphasize data protection and access control.
  description: This rule checks that roles assigned to users and service accounts for AI Platform pipeline jobs are limited to the minimum necessary permissions. It verifies that custom roles or predefined roles do not grant broader access than required for their function. To remediate, review and adjust IAM policies to ensure roles are aligned with job responsibilities and audit access logs regularly to identify any deviations from expected usage.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_human_review_artifacts_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Human Review Artifacts Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt Human Review Artifacts in AI Platform Pipelines
  rationale: Encrypting human review artifacts in AI Platform pipelines is crucial to protect sensitive data from unauthorized access and potential breaches. This ensures compliance with data protection regulations such as GDPR and HIPAA, which mandate encryption of data at rest. Failure to encrypt can lead to data exposure, resulting in legal penalties and reputational damage.
  description: This rule verifies that machine learning human review artifacts generated by AI Platform pipeline jobs are encrypted using Customer Managed Encryption Keys (CMEK). To ensure compliance, configure the pipeline job to use a CMEK by specifying the `encryptionSpec` field in the job's configuration. Validate the setup by checking the job's metadata in the Cloud Console or using gcloud CLI. Remediate by updating pipeline configurations to include a valid CMEK.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/pipelines/create-pipeline#encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_human_review_ui_private_networking_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Human Review Ui Private Networking Enforced
  scope: aiplatform.pipeline_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure ML Human Review UI Uses Private Networking
  rationale: Enforcing private networking for the Machine Learning Human Review UI mitigates potential data breaches by restricting network access to authorized and internal networks only. This reduces exposure to the public internet, thereby protecting sensitive machine learning data and models from unauthorized access and tampering. It also helps meet compliance requirements for data protection and privacy standards, such as GDPR and HIPAA, by ensuring secure data transmission within a controlled environment.
  description: This rule checks if the Machine Learning Human Review UI for AI Platform pipeline jobs is configured to use private networking. To verify, ensure that the UI is accessed through a VPC Service Control perimeter, blocking access from public IP addresses. Remediation involves configuring private Google Access and setting up appropriate firewall rules within the VPC to allow only necessary internal traffic. This enhances security by ensuring that sensitive machine learning operations are conducted over a secure and private network.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/reference/rest/v1beta1/projects.locations.pipelineJobs
  - https://cloud.google.com/vpc/docs/private-access-options
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 6.6 Ensure VPC Flow Logs is enabled for every subnet in VPC Network
  - NIST SP 800-53 Rev. 5 - SC-7 Boundary Protection
  - PCI-DSS v3.2.1 - 1.3 Prohibit direct public access between the Internet and any system component in the cardholder data environment
  - ISO 27001:2013 - A.13.1 Network Security Management
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_human_review_workteam_access_rbac_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Human Review Workteam Access RBAC Privilege
  scope: aiplatform.pipeline_job.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure ML Human Review Workteam RBAC Privilege is Minimally Scoped
  rationale: Properly scoping access for machine learning human review workteams is crucial to prevent unauthorized data exposure and manipulation, which can lead to data breaches and compromise model integrity. Inadequate access management can result in non-compliance with regulatory frameworks such as GDPR and CCPA, posing legal and financial risks.
  description: This rule checks if the role-based access control (RBAC) privileges assigned to machine learning human review workteams are aligned with the principle of least privilege. Verify that only necessary permissions are granted to perform specific review tasks by auditing IAM policies associated with pipeline jobs. Remediation involves adjusting access levels to ensure compliance with organizational policy and relevant regulations, removing unnecessary roles, and periodically reviewing access permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/ai-platform/docs/pipelines/iam
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_pipeline_artifacts_private_and_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Pipeline Artifacts Private And Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Pipeline Artifacts Are Private and Encrypted
  rationale: Encrypting machine learning pipeline artifacts at rest ensures data confidentiality and integrity, reducing the risk of unauthorized access and data breaches. This is crucial for maintaining trust with stakeholders, meeting compliance requirements (such as GDPR and HIPAA), and protecting sensitive intellectual property or personal data involved in machine learning processes.
  description: This rule checks if AI Platform pipeline artifacts are stored with encryption at rest and access is restricted to authorized users only. Ensure that encryption keys are managed securely using Google Cloud Key Management Service (KMS) and access is controlled via IAM roles with least privilege. Verify encryption settings in AI Platform's pipeline job configurations and update policies to enforce encryption and privacy controls across all artifacts.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_pipeline_kms_encryption_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Pipeline KMS Encryption Enabled
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption for AI Platform Pipeline Jobs
  rationale: Enabling KMS encryption for machine learning pipeline jobs on GCP secures sensitive data by ensuring that it is encrypted at rest. This reduces the risk of unauthorized access and data breaches, safeguarding intellectual property and personal data. Compliance with industry standards such as PCI-DSS and HIPAA often requires encryption of sensitive data, making this a critical control for organizations handling regulated data.
  description: This rule checks whether AI Platform pipeline jobs have Cloud Key Management Service (KMS) encryption enabled. To verify, ensure the 'encryptionSpec.kmsKeyName' field is specified in the pipeline job configuration. Remediation involves updating the pipeline job settings to include a valid KMS key, providing enhanced security by encrypting the data processed by the pipeline. This configuration helps maintain data confidentiality and integrity.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_pipeline_logging_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Pipeline Logging Enabled
  scope: aiplatform.pipeline_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for Machine Learning Pipelines in GCP AI Platform
  rationale: Enabling logging for machine learning pipelines ensures comprehensive audit trails, aiding in the detection of anomalies, troubleshooting, and ensuring compliance with data governance policies. Insufficient logging can lead to undetected unauthorized activities or misconfigurations, posing a risk to data integrity and confidentiality. Compliance with frameworks like SOC2 and ISO 27001 often requires detailed logging for accountability and auditability.
  description: This rule checks if logging is enabled for machine learning pipelines in GCP AI Platform. To verify, ensure that the pipeline job configurations include logging options that capture necessary events and activities. Remediation involves configuring the pipeline jobs to utilize Google's Cloud Logging service, ensuring all relevant logs are captured and retained according to your organization's policy. This can be done through the AI Platform Console or via API configurations.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_pipeline_roles_least_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Pipeline Roles Least Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for ML Pipeline Roles
  rationale: Implementing least privilege for machine learning pipeline roles minimizes the attack surface by ensuring that users and service accounts have only the necessary permissions to perform their tasks. This reduces the risk of unauthorized data access, accidental data loss, and potential exploitation by malicious actors. Adhering to this principle helps in meeting compliance requirements such as GDPR and ISO 27001.
  description: This rule audits the permissions associated with roles used in AI Platform pipeline jobs, ensuring they adhere to the principle of least privilege. It checks for excessive permissions that are not required for the pipeline's functionality, such as permissions allowing access to unrelated resources. Remediation involves reviewing the roles assigned to users and service accounts and tailoring them to include only the necessary permissions. This can be done using the Google Cloud Console or gcloud CLI to update IAM policies.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/architecture/identity-access-management
  - https://www.nist.gov/privacy-framework
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_automl_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Automl Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable AI Platform AutoML Training Pipeline Audit Logs
  rationale: Enabling audit logs for AI Platform AutoML training pipelines is crucial for tracking access and changes to machine learning models, which helps in detecting unauthorized or anomalous activities. This logging is vital for forensic analysis in case of data breaches, ensuring accountability and transparency, and supporting compliance with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks whether audit logs are enabled for the AI Platform AutoML training pipelines to ensure that all significant actions are recorded. To verify, ensure that logging is configured in the Google Cloud Console under the AI Platform settings, specifically under 'Audit Logs'. If not enabled, configure audit logs by navigating to the IAM & Admin > Audit Logs section and enabling 'Admin Read', 'Data Read', and 'Data Write' for the AI Platform service. This ensures comprehensive logging of administrative and data access activities.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_automl_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Automl Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Platform AutoML
  rationale: Enforcing private networking for AI Platform AutoML training pipelines helps mitigate the risk of unauthorized access and data exfiltration. By ensuring that models are trained in a secure, private network, organizations can maintain data confidentiality and integrity, which is crucial for compliance with data protection regulations such as GDPR and CCPA. This also reduces exposure to potential cyber threats and enhances the overall security posture of the AI infrastructure.
  description: This rule checks if AI Platform AutoML training pipelines are configured to use private networking, ensuring that all data and model training processes remain within a secure internal network. To verify, inspect the network configuration settings of the training pipeline to confirm that it is set to use a private IP range. Remediation involves updating the network settings of the pipeline to utilize a VPC that does not allow public IP addresses, ensuring all traffic remains private and internal.
  references:
  - https://cloud.google.com/ai-platform/docs/general/deployment-overview#private-networking
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_automl_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Automl Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Platform Training Pipelines Use Least Privilege Roles
  rationale: Assigning excessive permissions to AI Platform training pipelines can lead to unauthorized data access, accidental data modification, and increased risk of insider threats. Ensuring least privilege helps protect sensitive data, supports compliance with data protection regulations, and minimizes potential attack vectors.
  description: This rule checks that AI Platform training pipelines are assigned only the minimal roles necessary for their operation, specifically focusing on Automl roles. Verify roles by reviewing IAM policies attached to training pipelines and ensure they are limited to essential permissions. If over-privileged roles are identified, modify IAM policies to restrict access to only what is necessary for the pipeline's functionality.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_hpo_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Hpo Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure AI Platform HPO Logs Are Enabled for Training Pipelines
  rationale: Enabling HPO logs for AI Platform training pipelines is critical for tracking hyperparameter optimization processes, which can help in auditing AI model training activities. This logging aids in identifying potential anomalies or misconfigurations, and ensures transparency and accountability, which are essential for complying with data governance frameworks and regulatory standards.
  description: This rule checks if hyperparameter optimization logs are enabled for AI Platform training pipelines on GCP. Proper logging settings should be configured to capture all relevant events, which can be verified through the AI Platform console or via the gcloud CLI. To remediate, ensure that logging is activated under the 'Training Pipelines' section in AI Platform, confirming that the 'Hyperparameter Optimization Logs' option is set to active.
  references:
  - https://cloud.google.com/ai-platform/training/docs/monitor-debug
  - https://cloud.google.com/solutions/best-practices-for-ai-ml
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_hpo_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Hpo Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI HPO Training Pipelines
  rationale: Private networking in AI HPO (Hyperparameter Optimization) ensures that data and model training processes are protected from unauthorized access and potential data breaches. By enforcing private networking, organizations can mitigate risks associated with exposure to public networks, ensuring compliance with data protection regulations such as GDPR and CCPA. This practice also aligns with security best practices, reducing the attack surface and enhancing data confidentiality.
  description: This rule checks that AI Platform Training Pipelines with hyperparameter optimization (HPO) are configured to use private networking, preventing exposure to public IP addresses. To verify, ensure that the 'network' parameter is set to a VPC network in the training pipeline configuration. Remediation involves updating pipeline configurations to specify a private VPC network, providing an isolated and secure environment for data processing and model training.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-containers-training#using_private_ips
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#private_networking
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_hpo_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Hpo Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Platform Training Pipelines
  rationale: Ensuring least privilege access for AI Platform training pipelines minimizes the risk of unauthorized data access and manipulation. Misconfigured access can lead to data breaches, intellectual property theft, and non-compliance with regulatory standards such as GDPR and CCPA, which mandate strict data protection and privacy. By restricting roles to only necessary permissions, you reduce the attack surface and potential for insider threats.
  description: This rule checks that roles assigned to AI Platform training pipelines adhere to the principle of least privilege. Specifically, it ensures that only the necessary permissions are granted for data governance and hyperparameter optimization. To verify, review IAM policies associated with AI Platform resources and confirm that roles are appropriately scoped. Remediation involves auditing current roles, removing excessive permissions, and only assigning predefined roles like 'AI Platform User', ensuring they are limited to required operations.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/aiplatform/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_automl_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Automl Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform AutoML Logging is Enabled for Training Pipelines
  rationale: Enabling Ai Automl logs for training pipelines is crucial for maintaining data privacy and security. Without logging, it is difficult to track data access and modifications, which increases the risk of unauthorized data exposure and potential compliance violations. Logging also aids in forensic investigations and monitoring for suspicious activities, which are vital for achieving compliance with standards such as GDPR and HIPAA.
  description: This rule checks if logging is enabled for AutoML training pipelines in Google Cloud's AI Platform. To ensure data privacy, logs should be configured to capture access and modification events for all training data processed by AutoML. Administrators can verify this by checking the logging configuration in the AI Platform settings. If logging is not enabled, it should be configured through the Google Cloud Console or gcloud CLI by setting the appropriate log sinks to capture the required events.
  references:
  - https://cloud.google.com/ai-platform/docs/getting-started-automl#logging
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/logging/docs/setup
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_automl_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Automl Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Private Networking for AI Platform Training Pipelines
  rationale: Enforcing private networking for AI Platform training pipelines mitigates the risk of data exposure by restricting access to sensitive data over the public internet. This ensures that training data remains protected from unauthorized access and potential interception, aligning with data privacy regulations and reducing the risk of data breaches. It supports compliance with frameworks like GDPR and HIPAA that mandate stringent data protection measures.
  description: This rule checks if AI Platform training pipelines are configured to use private networking, which involves setting up VPC Peering or Private Google Access. To verify, ensure the training pipeline's execution environment is within a VPC network and that no public IP addresses are assigned. Remediation involves configuring private services access and updating the pipeline to operate within the secure network settings. This enhances security by isolating traffic within a controlled network perimeter.
  references:
  - https://cloud.google.com/ai-platform/training/docs/networking
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/itl/applied-cybersecurity/nist-special-publication-800-53
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_automl_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Automl Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Training Pipeline Roles
  rationale: Implementing least privilege in AI Platform training pipelines minimizes the risk of unauthorized access to sensitive data and operations, reducing the potential for data breaches and service disruptions. This approach aligns with compliance requirements such as GDPR and CCPA, which mandate strict data access controls to protect personal data. Additionally, by limiting permissions, organizations can prevent privilege escalation attacks and maintain the integrity of their AI models.
  description: This rule checks that roles assigned to AI Platform training pipelines follow the principle of least privilege. It verifies that service account permissions are restricted to only those necessary for executing specific tasks within the training pipeline. To remediate, review and adjust IAM roles, removing unnecessary permissions and ensuring only essential access is granted. This can be verified through the IAM policy analysis tools available in the GCP Console, focusing on roles such as 'roles/aiplatform.user' and 'roles/aiplatform.viewer' as opposed to broader roles like 'owner' or 'editor'.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_hpo_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Hpo Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable AI HPO Logs for Enhanced Data Privacy in Training Pipelines
  rationale: Enabling AI Hyperparameter Optimization (HPO) logs is crucial for maintaining data privacy, as it helps track and audit model training processes. This reduces the risk of unauthorized data access and supports compliance with data protection regulations such as GDPR and CCPA. Without these logs, organizations may face data breaches or non-compliance fines, potentially harming their reputation and financial standing.
  description: This rule checks whether AI HPO logs are enabled for training pipelines on Google Cloud AI Platform. To verify, ensure that logging is configured to capture all relevant HPO activities within the AI Platform's training pipelines. Remediation involves enabling logging through the AI Platform settings and ensuring that logs are stored securely with appropriate access controls. This helps in monitoring and auditing AI training activities, providing an additional layer of security for sensitive data.
  references:
  - https://cloud.google.com/ai-platform/training/docs/logging
  - https://cloud.google.com/architecture/framework/security/encryption
  - CIS Google Cloud Platform Foundation Benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_hpo_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Hpo Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Private Networking for AI Platform Training Pipelines
  rationale: Enforcing private networking for AI Platform training pipelines mitigates the risk of unauthorized access and data exposure by restricting network traffic to a secure VPC. This reduces the threat surface for potential attackers and complies with data privacy regulations such as GDPR and HIPAA, which mandate stringent data protection measures.
  description: This rule checks whether AI Platform training pipelines are configured to use private networking, which ensures data is transmitted over a secure, internal network rather than the public internet. To verify, ensure the training pipeline's networking configuration specifies a VPC network. Remediation involves updating the training pipeline configuration to include a VPC network, ensuring all data traffic remains within Google Cloud's secure infrastructure.
  references:
  - https://cloud.google.com/ai-platform/training/docs/private-ip
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_hpo_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Hpo Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Training Pipeline Roles
  rationale: Applying the principle of least privilege to AI Platform roles minimizes the risk of unauthorized access to sensitive data and operations, reducing potential data breaches and violations of compliance standards. This approach helps maintain data privacy, especially when handling sensitive datasets in training pipelines, aligning with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks that roles assigned to users and service accounts for managing AI Platform training pipelines are configured with the least privilege principle. Verify that users only have permissions necessary for their tasks, avoiding roles with excessive access like 'Owner'. Remediate by auditing current role assignments and adjusting permissions to adhere strictly to required operations, using predefined roles such as 'AI Platform Viewer' or custom roles with minimum necessary permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/training/docs/using-iam
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/standard/54534.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_automl_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Automl Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Audit Logs for AutoML Training Pipelines
  rationale: Enabling audit logs for AutoML training pipelines is crucial for tracking access and modifications to machine learning resources, which helps prevent unauthorized actions and ensures data integrity. This logging capability supports forensic investigations by providing a detailed record of operations, which is essential for compliance with data protection regulations and maintaining accountability in AI operations.
  description: This rule checks if audit logs are enabled for AutoML training pipelines in Google Cloud AI Platform. Specifically, it verifies that all read, write, and administrative access logs are captured and stored in Cloud Logging. To enable logging, ensure that the AI Platform service account has the appropriate roles to access Logging, and configure the logging level in the AI Platform settings. Compliance can be verified by reviewing the 'Audit Logs' section under IAM & Admin in the GCP Console and ensuring logs are being generated for all relevant pipeline activities.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/automl/docs
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_automl_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Automl Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for Automl Training Pipelines
  rationale: Enforcing private networking for Machine Learning AutoML training pipelines minimizes exposure to public internet threats, reducing the risk of data breaches and unauthorized access. This measure supports compliance with regulatory standards such as GDPR and HIPAA by ensuring sensitive data remains within the controlled environment of the Google Cloud Platform. It also enhances the overall security posture by limiting attack vectors and potential vulnerabilities.
  description: This rule checks if the AutoML training pipelines in GCP's AI Platform are configured to use private networking, which means they communicate over a Virtual Private Cloud (VPC) instead of the public internet. Ensure that the 'enablePrivateServiceConnect' option is set to true in the training pipeline configuration. Verify this setting via the Google Cloud Console or by using the gcloud CLI. To remediate, update existing pipelines to enforce private networking by modifying the network configuration to include a private connection.
  references:
  - https://cloud.google.com/ai-platform/training/docs/using-vpc
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/network-connectivity/docs/service-directory
  - https://cloud.google.com/vpc
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_automl_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Automl Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AutoML Roles in Training Pipelines
  rationale: Implementing least privilege for AutoML roles in GCP's AI Platform is crucial to minimize potential unauthorized access and data breaches. Over-privileged accounts can lead to accidental or malicious misuse of resources, causing significant business disruption and financial loss. Furthermore, adhering to least privilege principles helps in meeting compliance requirements such as GDPR and HIPAA, which mandate strict access controls.
  description: This rule checks that roles assigned to users and service accounts in AI Platform training pipelines are limited to only those permissions necessary for their tasks. It involves reviewing IAM policies to ensure no excessive permissions are granted beyond what is needed for AutoML operations. Remediation involves adjusting IAM roles to align with the principle of least privilege by using predefined roles or creating custom roles tailored to specific needs.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/ai-platform/training/docs/custom-service-account
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/creating-custom-roles
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_hpo_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Hpo Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for ML Hyperparameter Optimization in AI Platform
  rationale: Enabling logs for Machine Learning hyperparameter optimization (HPO) pipelines in Google Cloud AI Platform helps organizations monitor, troubleshoot, and optimize their ML models effectively. This practice reduces security risks by enabling visibility into data processing activities and supports compliance with regulatory requirements that mandate logging of critical operations. It also aids in detecting potential misconfigurations and unauthorized access.
  description: This rule checks whether logging is enabled for hyperparameter optimization (HPO) in Google Cloud AI Platform training pipelines. Ensuring logs are active allows teams to capture detailed information about the HPO process, which is essential for debugging and improving model performance. To enable logging, navigate to the AI Platform's Training Pipelines section, and configure logging settings to capture detailed operational data. Regularly review these logs to maintain optimal security and compliance posture.
  references:
  - https://cloud.google.com/ai-platform/training/docs/auditing
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_hpo_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Hpo Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for ML HPO in AI Platform
  rationale: Enforcing private networking for Machine Learning Hyperparameter Optimization (HPO) ensures that data and model training communications are restricted to secure, internal networks. This reduces the risk of data breaches and unauthorized access, which can lead to intellectual property theft and compliance violations with standards like GDPR and HIPAA.
  description: This rule checks whether Machine Learning HPO processes in Google AI Platform use private networking configurations. Specifically, it verifies that training pipelines are set to use a VPC network, preventing exposure to the public internet. To remediate, configure the AI Platform training pipeline with a VPC network by setting the 'network' field in the pipeline's trainingInput to the appropriate VPC name.
  references:
  - https://cloud.google.com/ai-platform/training/docs/using-vpc
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_hpo_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Hpo Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for ML HPO Roles in AI Platform
  rationale: Granting excessive permissions to roles associated with machine learning hyperparameter optimization (HPO) can expose sensitive data and increase the attack surface. Least privilege access helps mitigate risks such as unauthorized data access, accidental deletion, or misuse of resources, which are critical for maintaining the integrity and confidentiality of your ML workloads. Additionally, adhering to least privilege principles aligns with compliance requirements such as PCI-DSS and ISO 27001, ensuring an organization's data protection posture is robust.
  description: This rule checks for adherence to the principle of least privilege in AI Platform training pipelines, specifically for roles involved in machine learning hyperparameter optimization. It ensures that roles assigned to users, service accounts, or applications have only the necessary permissions needed to perform their functions. To verify, audit IAM policies associated with AI Platform and modify roles to restrict permissions to the minimum necessary. Remediation involves updating IAM policies and roles to remove excess permissions and regularly reviewing access logs for compliance.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_training_job_input_output_encrypted
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Training Job Input Output Encrypted
  scope: aiplatform.training_pipeline.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Training Job Data is Encrypted at Rest
  rationale: Encrypting machine learning training job input and output data at rest mitigates the risk of unauthorized data access, which is crucial for maintaining data confidentiality and integrity. This is particularly important in industries with strict compliance requirements such as healthcare and finance, where sensitive data protection is mandated by regulations like HIPAA and PCI-DSS. Failing to encrypt data can lead to data breaches, financial loss, and reputational damage.
  description: This rule checks whether the data used in and produced by AI Platform training pipelines is encrypted at rest using customer-managed encryption keys (CMEK). To verify, ensure that the training pipeline is configured with CMEK through the Google Cloud Console or using the AI Platform API. Remediation involves specifying a customer-managed key during the creation of the training pipeline, which provides greater control over data security and compliance with organizational policies.
  references:
  - https://cloud.google.com/ai-platform/training/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_training_job_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Training Job Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for AI Platform Training Jobs
  rationale: Enabling logs for AI Platform training jobs is crucial for monitoring and troubleshooting machine learning workflows. This practice helps in detecting anomalies, understanding model performance, and ensuring accountability by maintaining an audit trail of all activities. It also supports compliance with regulations that mandate logging of data processing activities, such as GDPR and HIPAA.
  description: This rule checks if logging is enabled for AI Platform training jobs, ensuring that the logs are captured and stored for review and analysis. To verify, ensure that the 'enableLogging' flag is set to true in the training pipeline configuration. Remediation involves updating the training pipeline's configuration to include logging settings, which can be done via the GCP Console, gcloud command-line tool, or by updating the pipeline configuration script.
  references:
  - https://cloud.google.com/ai-platform/training/docs/audit-logging
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/architecture/framework/security
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_training_job_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Training Job Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for ML Training Jobs
  rationale: Enforcing private networking for Machine Learning training jobs on GCP ensures that communication between AI Platform resources and the training environment occurs over a secure, private network. This reduces exposure to external threats such as unauthorized access or data exfiltration. It also aids in compliance with regulatory standards that mandate secure data handling, such as PCI-DSS and HIPAA, by minimizing network attack surfaces.
  description: This rule checks that all Machine Learning training jobs within AI Platform Training Pipelines are configured to use private networking. Specifically, it verifies that the 'enablePrivateServiceConnect' option is set to true, ensuring that traffic stays within Googleâ€™s private network. To remediate non-compliant resources, configure the training job to use a VPC network by setting the 'network' field in the TrainingPipeline configuration. This helps protect sensitive data and aligns with best practices for network access control.
  references:
  - https://cloud.google.com/ai-platform/training/docs/networking
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/vpc/docs/private-service-connect
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_training_job_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Training Job Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for ML Training Job Roles
  rationale: Utilizing least privilege access for machine learning training jobs minimizes the risk of unauthorized access and potential data breaches. It is crucial for maintaining data confidentiality and integrity, especially when handling sensitive data within AI models. Adhering to this principle supports compliance with regulations such as GDPR and HIPAA, and helps prevent privilege escalation attacks.
  description: This rule verifies that roles assigned to AI Platform training jobs have the minimum necessary permissions. Review IAM policies associated with training pipelines to ensure roles are limited to only essential actions. Remediation involves auditing current permissions, using predefined roles where possible, and removing unnecessary access rights. Implementing these changes can be verified through IAM policy reviews in the Google Cloud Console or using gcloud CLI commands.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/training/docs/using-iam-permissions
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#access
- rule_id: gcp.apigateway.api.api_access_logging_enabled
  service: apigateway
  resource: api
  requirement: API Access Logging Enabled
  scope: apigateway.api.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Gateway Access Logging is Enabled
  rationale: Enabling access logging for API Gateway is crucial for monitoring API usage, detecting unauthorized access attempts, and conducting forensic investigations in case of security incidents. It aids in compliance with regulatory requirements like GDPR, PCI-DSS, and HIPAA, which mandate detailed access logs to ensure accountability and data protection.
  description: This rule checks if access logging is enabled for API Gateway resources in GCP. Without access logging, you may miss critical insights into API usage patterns and potential security threats. To verify, navigate to the API Gateway settings in the GCP Console and ensure that logging is activated. Remediation involves enabling logging by configuring the API Gateway to send logs to Cloud Logging, ensuring all access requests are logged and monitored.
  references:
  - https://cloud.google.com/api-gateway/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api.api_key_restrictions_verified
  service: apigateway
  resource: api
  requirement: API Key Restrictions Verified
  scope: apigateway.api.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure API Key Restrictions are Properly Configured
  rationale: Unrestricted API keys pose significant risks as they can be easily exploited by malicious actors, leading to unauthorized access to sensitive data and services. Implementing API key restrictions minimizes the attack surface and aligns with regulatory requirements such as GDPR for protecting user data.
  description: This rule checks whether API keys used in Google Cloud's API Gateway are configured with appropriate restrictions, such as IP address restrictions, application restrictions, and service restrictions. To verify, navigate to the API key settings in the Google Cloud Console and ensure that restrictions are applied. Remediate by editing the API key to add necessary restrictions, which helps prevent misuse and unauthorized access.
  references:
  - https://cloud.google.com/docs/authentication/api-keys#api_key_restrictions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/security-best-practices
- rule_id: gcp.apigateway.api.certificate_enabled
  service: apigateway
  resource: api
  requirement: Certificate Enabled
  scope: apigateway.api.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Certificates are Enabled for API Gateway
  rationale: Enabling certificates for API Gateway ensures secure communication between clients and the API, protecting sensitive data from interception and man-in-the-middle attacks. This is crucial for maintaining data privacy and integrity, especially for APIs handling personal or sensitive data, and is often required for compliance with regulations such as PCI-DSS and HIPAA.
  description: This rule checks if SSL/TLS certificates are enabled for API Gateway endpoints, ensuring encrypted data transmission. To verify, confirm that API Gateway configurations include SSL/TLS certificates for all endpoints. Remediation involves configuring API Gateway with a valid SSL/TLS certificate, which can be obtained from a trusted Certificate Authority or managed through Google Cloud's Certificate Manager.
  references:
  - https://cloud.google.com/api-gateway/docs/get-started
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/load-balancing/docs/ssl-certificates
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.apigateway.api.gateway_api_gateway_https_required
  service: apigateway
  resource: api
  requirement: Gateway API Gateway HTTPS Required
  scope: apigateway.api.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure HTTPS is Enabled for API Gateway Endpoints
  rationale: Using HTTPS for API Gateway endpoints is crucial to protect the data in transit from eavesdropping and man-in-the-middle attacks, ensuring the confidentiality and integrity of information exchanged. This is especially important for APIs handling sensitive data or personal information, as it helps organizations comply with regulatory standards like GDPR, PCI-DSS, and HIPAA, which mandate secure data transmission.
  description: This rule checks if API Gateway endpoints are configured to use HTTPS protocols. Configuring HTTPS involves setting up SSL/TLS certificates for secure communication. To verify, review the API Gateway settings to ensure 'https' is specified in the gateway URL. Remediation involves updating the gateway configuration to include SSL certificates and enforcing HTTPS-only connections, which can be done via the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/api-gateway/docs/secure-traffic
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api.gateway_authorization_enabled
  service: apigateway
  resource: api
  requirement: Gateway Authorization Enabled
  scope: apigateway.api.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure API Gateway Authorization is Enabled
  rationale: Enabling authorization in GCP API Gateway is critical to ensure that only authenticated and authorized users can access API resources. This mitigates the risk of unauthorized data exposure and potential data breaches, aligning with regulatory requirements like PCI-DSS and HIPAA, which mandate secure access controls for sensitive data.
  description: This rule checks if authorization is enabled for API Gateways in GCP. It verifies that an authorization configuration, such as a JWT token validation, is in place for APIs to prevent unauthorized access. To remediate, ensure that API Gateways are configured with appropriate authentication mechanisms like Google ID tokens or OAuth2. This can be done through the GCP Console by navigating to the API Gateway service, selecting the relevant API, and configuring the authorization settings under the 'Gateway' section.
  references:
  - https://cloud.google.com/api-gateway/docs/auth-overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.apigateway.api.gateway_restapi_logging_enabled
  service: apigateway
  resource: api
  requirement: Gateway Restapi Logging Enabled
  scope: apigateway.api.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for API Gateway REST APIs
  rationale: Enabling logging for API Gateway REST APIs is crucial for monitoring and forensic analysis. Without logging, detecting and investigating security incidents such as unauthorized access or data breaches becomes challenging, potentially leading to compliance issues with frameworks like PCI-DSS and HIPAA.
  description: This rule checks if logging is enabled for API Gateway REST APIs in Google Cloud Platform. Logging should be configured to capture all relevant request and response metadata to facilitate auditing and troubleshooting. To enable logging, navigate to the API Gateway console, select the API, and configure the logging settings to send logs to Cloud Logging. Regularly review and analyze these logs for unusual activities.
  references:
  - https://cloud.google.com/api-gateway/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.apigateway.api.gateway_restapi_waf_acl_attached_configured
  service: apigateway
  resource: api
  requirement: Gateway Restapi Waf ACL Attached Configured
  scope: apigateway.api.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure WAF Configured for API Gateway REST APIs
  rationale: Configuring a Web Application Firewall (WAF) for API Gateway REST APIs is crucial for protecting against common web exploits and vulnerabilities such as SQL injection, cross-site scripting (XSS), and others. Without a WAF, APIs are vulnerable to attacks that can lead to data breaches, service interruptions, and non-compliance with security standards like PCI-DSS and NIST. Proper WAF configuration helps mitigate these risks and ensures that sensitive data is adequately protected from external threats.
  description: This rule checks that a Web Application Firewall (WAF) is attached and properly configured for each API Gateway REST API. It involves verifying that the WAF policies are applied and that they align with security requirements to filter malicious traffic. To ensure compliance, review the WAF configuration in the GCP Console under API Gateway settings and confirm that the appropriate firewall rules are active. Remediation involves attaching a suitable WAF policy to the API Gateway and regularly updating the rules to respond to new threats.
  references:
  - https://cloud.google.com/api-gateway/docs/configure-waf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api.platform_endpoint_authn_required
  service: apigateway
  resource: api
  requirement: Platform Endpoint Authn Required
  scope: apigateway.api.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure API Gateway Uses Endpoint Authentication
  rationale: Requiring authentication for API Gateway endpoints is crucial to prevent unauthorized access to APIs, which can lead to data breaches and unauthorized data manipulation. It helps in complying with regulatory requirements such as GDPR and HIPAA, which mandate secure access controls for sensitive data. Without authentication, APIs are vulnerable to abuse, potentially impacting business operations and customer trust.
  description: This rule checks if API Gateway endpoints require authentication to ensure that only authorized users can access the API resources. To verify, inspect the API Gateway configurations to ensure that authentication mechanisms such as OAuth 2.0 or API keys are enabled. Remediation involves configuring the API Gateway to require authentication by setting up a proper authentication provider and associating it with the API endpoint.
  references:
  - https://cloud.google.com/api-gateway/docs/authenticating-users
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigateway.api.platform_endpoint_authz_policies_enforced
  service: apigateway
  resource: api
  requirement: Platform Endpoint Authz Policies Enforced
  scope: apigateway.api.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce Platform Endpoint Authz Policies for API Gateway
  rationale: Enforcing authorization policies on API Gateway endpoints mitigates risks of unauthorized access and data breaches, which could lead to financial loss, reputational damage, and compliance violations. It is crucial for maintaining secure communication channels and protecting sensitive data flowing through APIs.
  description: This rule checks whether authorization policies are enforced on all API Gateway platform endpoints. Specifically, it verifies that IAM roles and conditions are correctly configured to restrict access based on the principle of least privilege. To verify, review API Gateway configurations and ensure that appropriate authorization mechanisms, such as OAuth 2.0 or API keys, are implemented. Remediate by updating IAM policies to include necessary roles and conditions to enforce access control.
  references:
  - https://cloud.google.com/api-gateway/docs/authz
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.apigateway.api.platform_endpoint_private_networking_enforced
  service: apigateway
  resource: api
  requirement: Platform Endpoint Private Networking Enforced
  scope: apigateway.api.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for API Gateway Endpoints
  rationale: Enforcing private networking for API Gateway endpoints protects sensitive data from exposure to the public internet, mitigating risks such as unauthorized access and data breaches. This control is crucial for maintaining data confidentiality and integrity, especially for organizations handling sensitive information subject to compliance requirements like PCI-DSS and HIPAA.
  description: This rule checks that all API Gateway endpoints are configured to use private networking, ensuring they are only accessible within a VPC. To verify, inspect the API Gateway configuration to ensure the 'network' attribute is set to a valid VPC network. Remediation involves updating the API configuration to enforce private networking, which can be achieved by using the 'gcloud apigateway apis update' command with the appropriate network settings.
  references:
  - https://cloud.google.com/api-gateway/docs/network-security
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/
  - https://cloud.google.com/blog/products/api-management/using-google-cloud-api-gateway-to-securely-manage-and-monitor-apis
- rule_id: gcp.apigateway.api.platform_endpoint_waf_attached
  service: apigateway
  resource: api
  requirement: Platform Endpoint Waf Attached
  scope: apigateway.api.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure WAF is Attached to API Gateway Endpoints
  rationale: Attaching a Web Application Firewall (WAF) to API Gateway endpoints helps protect against common web exploits that could compromise application security, such as SQL injection and cross-site scripting (XSS). This is crucial for maintaining the integrity and availability of APIs, which are often critical components of business operations. Additionally, this can aid in compliance with regulatory standards that mandate the protection of sensitive data and the implementation of security controls.
  description: This rule checks if a Web Application Firewall is attached to API Gateway endpoints to filter and monitor HTTP traffic. To verify, inspect the API Gateway configuration in the GCP Console to ensure a WAF policy is associated with the API. Remediation involves configuring a WAF policy using Google Cloud Armor and attaching it to the API Gateway. This setup helps mitigate risks from malicious traffic and enhances the security posture of your APIs.
  references:
  - https://cloud.google.com/api-gateway/docs
  - https://cloud.google.com/armor/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.apigateway.api_config.platform_authorizer_cache_ttl_reasonable
  service: apigateway
  resource: api_config
  requirement: Platform Authorizer Cache Ttl Reasonable
  scope: apigateway.api_config.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Reasonable TTL for API Gateway Authorizer Cache
  rationale: Setting a reasonable time-to-live (TTL) for the API Gateway authorizer cache helps balance security with performance. Short TTLs can lead to increased latency and higher costs due to frequent authorizer invocations, while overly long TTLs may allow outdated authorization data to persist, potentially exposing the API to unauthorized access. Ensuring an appropriate TTL supports compliance with security policies and reduces the risk of exposing sensitive data.
  description: This rule checks if the TTL for the API Gateway authorizer cache is set to a reasonable duration. Verify the TTL setting in the api_config resource of the API Gateway service to ensure it aligns with security best practices, typically not exceeding 300 seconds. To remediate, adjust the TTL value in the API Gateway configuration to ensure it is neither too short, causing performance issues, nor too long, risking stale authorization data.
  references:
  - https://cloud.google.com/api-gateway/docs/using-api-gateway
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigateway.api_config.platform_authorizer_tls_min_1_2_enforced
  service: apigateway
  resource: api_config
  requirement: Platform Authorizer TLS Min 1 2 Enforced
  scope: apigateway.api_config.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS 1.2+ for API Gateway Platform Authorizer
  rationale: Enforcing TLS 1.2 or higher for API Gateway Platform Authorizer is crucial for protecting data in transit. Using outdated TLS versions exposes APIs to vulnerabilities such as man-in-the-middle attacks, potentially leading to unauthorized data access and breaches. Ensuring compliance with industry standards like PCI-DSS and NIST enhances trust and security posture.
  description: This rule verifies that the API Gateway Platform Authorizer is configured to enforce a minimum of TLS 1.2 for securing communications. Check the API Gateway configuration settings to ensure TLS 1.2 or higher is specified. Remediation involves updating the API Gateway settings to disable lower TLS versions, ensuring that only secure connections are permitted.
  references:
  - https://cloud.google.com/api-gateway/docs/security-overview
  - https://cloud.google.com/security/compliance/cis-2-0-0
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.apigateway.api_config.platform_authorizer_token_audience_restricted
  service: apigateway
  resource: api_config
  requirement: Platform Authorizer Token Audience Restricted
  scope: apigateway.api_config.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Restrict API Gateway Token Audience for Security
  rationale: Restricting the token audience for API Gateway's platform authorizer is crucial to prevent unauthorized access to APIs. An unrestricted audience can lead to token misuse, potentially allowing attackers to impersonate clients and access sensitive data. This practice supports compliance with security standards like PCI-DSS and SOC2, ensuring that only designated services can interact with your APIs.
  description: This rule checks that the API Gateway configuration specifies a restricted token audience in the platform authorizer settings. This ensures that only authorized services can use the token to access the API. To verify, examine the API Gateway's configuration settings for 'audiences' under the 'authorizer' section. Remediation involves updating the API configuration to specify a limited set of audience values that match only intended client identifiers.
  references:
  - https://cloud.google.com/api-gateway/docs/authenticating-users
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://trust.salesforce.com/en/compliance/soc-2/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigateway.api_config.platform_stage_logging_enabled
  service: apigateway
  resource: api_config
  requirement: Platform Stage Logging Enabled
  scope: apigateway.api_config.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Gateway Platform Stage Logging is Enabled
  rationale: Enabling logging for API Gateway's platform stages is critical for tracking access patterns, identifying potential misuse, and ensuring compliance with data protection regulations. Without logging, organizations may be unaware of unauthorized access, leading to data breaches and non-compliance with frameworks like PCI-DSS and SOC2.
  description: This rule checks that logging is enabled for all API Gateway platform stages in GCP. Logging provides visibility into request and response activities, which is essential for auditing and forensic investigations. To verify, ensure the 'logConfig' setting is populated in the API Gateway configurations. Remediation involves configuring the API Gateway to send logs to a designated Cloud Logging sink, ensuring all platform stages are properly monitored.
  references:
  - https://cloud.google.com/api-gateway/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs/setup
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api_config.platform_stage_require_usage_plan_for_api_keys
  service: apigateway
  resource: api_config
  requirement: Platform Stage Require Usage Plan For API Keys
  scope: apigateway.api_config.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Usage Plans for API Keys in API Gateway
  rationale: Requiring usage plans for API keys is critical in managing API access, preventing abuse, and ensuring fair usage. Without enforcing usage plans, there's a higher risk of API misuse, leading to potential data breaches or service disruptions. This measure also aligns with compliance requirements for data protection and access control.
  description: This rule checks that all API configurations in the API Gateway have usage plans associated with API keys. Usage plans define request quotas and rate limits, which are essential for controlling access to your APIs. To verify, review the API Gateway settings to ensure that all API keys are linked to a defined usage plan. Remediation involves creating or updating usage plans and associating them with the relevant API keys.
  references:
  - https://cloud.google.com/api-gateway/docs/quotas-overview
  - https://cloud.google.com/api-gateway/docs/using-api-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api_config.platform_stage_throttling_enabled
  service: apigateway
  resource: api_config
  requirement: Platform Stage Throttling Enabled
  scope: apigateway.api_config.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Throttling for API Gateway Configurations
  rationale: Enabling throttling on API Gateway configurations helps prevent abuse and overuse of APIs, which can lead to service degradation or denial of service. It ensures that APIs are only used within the limits intended by the organization, protecting backend services from excessive load and potential security threats. Compliance with regulations like NIST and PCI-DSS often requires such controls to manage access and protect data integrity.
  description: This rule checks if throttling is enabled on the API Gateway platform for each API configuration. Throttling controls the rate of requests to ensure API stability and security. To verify, check the API Gateway settings for configured limits on requests per second. If throttling is not enabled, configure it by setting appropriate limits in the GCP Console under the API Gateway configuration section. This helps mitigate risks of API misuse and supports compliance with security standards.
  references:
  - https://cloud.google.com/api-gateway/docs/quotas-overview
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/api-gateway/docs/quotas
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigateway.api_config.platform_usage_plan_quota_limits_configured
  service: apigateway
  resource: api_config
  requirement: Platform Usage Plan Quota Limits Configured
  scope: apigateway.api_config.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Gateway Usage Plan Quotas Are Configured
  rationale: Configuring usage plan quotas for API Gateway is critical to control and limit the usage of your APIs, preventing misuse and potential denial of service attacks. It helps in managing costs by avoiding unexpected high usage and ensuring fair usage across different clients, aligning with compliance requirements for resource management and availability.
  description: This rule checks whether usage plan quotas are configured for API Gateway on GCP. Quotas should be set to restrict the number of requests or data that can be sent through your APIs. To verify, review the API configuration settings in the GCP Console under API Gateway and ensure quotas are defined and active. Remediation involves setting appropriate quotas via the GCP Console or API to enforce limits according to your operational and security needs.
  references:
  - https://cloud.google.com/api-gateway/docs/quotas-overview
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/blog/products/api-management/api-gateway-is-now-generally-available
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.apigateway.api_config.platform_usage_plan_rate_limits_configured
  service: apigateway
  resource: api_config
  requirement: Platform Usage Plan Rate Limits Configured
  scope: apigateway.api_config.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: API Gateway Rate Limits for Usage Plans Configured
  rationale: Configuring platform usage plan rate limits for API Gateway ensures that APIs are protected against denial-of-service attacks and prevents overconsumption of resources, which could lead to increased costs and potential service disruptions. By setting appropriate rate limits, organizations can improve the reliability and availability of their services while adhering to compliance requirements for resource usage management.
  description: This rule checks if usage plans in API Gateway have rate limits configured to control the number of requests that can be made to an API. Proper configuration involves setting both request and burst limits to safeguard against traffic spikes. To verify, inspect the API Gateway configurations for set usage plans and ensure they include defined request quotas. Remediation involves using the GCP Console or gcloud CLI to define rate limits in the API configurations, thus ensuring robust traffic management.
  references:
  - https://cloud.google.com/api-gateway/docs/quotas-overview
  - https://cloud.google.com/security/compliance/cis#section-6.7
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/73906.html
  - https://cloud.google.com/architecture/api-design-best-practices
- rule_id: gcp.apigee.api_proxy.api_api_content_type_whitelist_enforced
  service: apigee
  resource: api_proxy
  requirement: API API Content Type Whitelist Enforced
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce API Proxy Content Type Whitelist in Apigee
  rationale: Enforcing a content type whitelist for API proxies in Apigee helps mitigate security risks such as injection attacks, data breaches, and service misuse by ensuring only allowed content types are processed. This is crucial for maintaining data integrity, meeting compliance requirements, and protecting sensitive information from unauthorized access.
  description: This rule checks if API proxies in Apigee have a content type whitelist enforced, ensuring that only predefined content types are accepted. To verify, inspect the API proxy configurations to ensure the 'Content-Type' header is validated against an approved list. To remediate, configure Apigee API proxies to include a 'Content-Type' validation policy that specifies allowed content types, thereby preventing the processing of potentially harmful or unexpected data.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/security/secure-api-proxies
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/json-threat-protection-policy
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/xml-threat-protection-policy
- rule_id: gcp.apigee.api_proxy.api_api_parameters_validation_enabled
  service: apigee
  resource: api_proxy
  requirement: API API Parameters Validation Enabled
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Apigee API Proxy Parameters Validation is Enabled
  rationale: Enabling API parameters validation in Apigee API Proxies is crucial for preventing malicious inputs that could lead to data breaches or unauthorized access. It helps mitigate security risks such as injection attacks and ensures compliance with data integrity and protection standards. This practice supports adherence to regulatory frameworks like PCI-DSS and NIST, thereby safeguarding business operations and client trust.
  description: This check verifies that API parameters validation is enabled for Apigee API Proxies. Parameter validation is an essential security measure that ensures only expected and safe inputs are processed by the API, reducing the risk of attacks. To implement this, configure policies in Apigee to validate parameters, ensuring they conform to expected formats and values. Regularly audit and update these policies to adapt to evolving security threats.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/security/parameter-validation
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigee.api_proxy.api_api_request_body_size_limit_configured
  service: apigee
  resource: api_proxy
  requirement: API API Request Body Size Limit Configured
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Request Body Size Limit is Configured for Apigee Proxies
  rationale: Configuring an API request body size limit helps prevent denial-of-service (DoS) attacks and resource exhaustion by limiting the size of incoming requests. This is crucial for maintaining system performance and reliability, especially in environments with high transaction volumes. Additionally, it supports compliance with security standards that require protections against resource abuse.
  description: This rule checks if an API request body size limit is configured for Apigee API proxies. To verify, navigate to the Apigee console, select the relevant API proxy, and ensure that a 'Request Body Size Limit' policy is applied. This configuration prevents excessively large requests from impacting system performance. Remediation involves setting a maximum allowable request size, thereby mitigating potential DoS risks.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/request-body-size-limit-policy
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/best-practices-for-content-filtering
- rule_id: gcp.apigee.api_proxy.api_api_request_schema_validation_enabled
  service: apigee
  resource: api_proxy
  requirement: API API Request Schema Validation Enabled
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable API Request Schema Validation in Apigee API Proxies
  rationale: Enabling schema validation for API requests in Apigee helps prevent unauthorized data input, reduces the risk of malicious payloads, and ensures data integrity. This is crucial for maintaining the security posture of applications that rely on Apigee for API management, especially in environments where regulatory compliance such as PCI-DSS or HIPAA is required.
  description: This rule checks if API request schema validation is enabled for Apigee API proxies. Schema validation ensures incoming API requests adhere to a defined structure, preventing unexpected or harmful data from being processed. To verify and enable this setting, navigate to the Apigee Management Console, access the specific API proxy, and ensure that the request schema validation is configured in the proxy's policies. Remediation involves updating the API proxy configuration to include proper JSON or XML schema validation policies.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/json-schema-validator-policy
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/xml-schema-validator-policy
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.hipaajournal.com/hipaa-compliance-guide
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.apigee.api_proxy.api_api_required_security_headers_enforced
  service: apigee
  resource: api_proxy
  requirement: API API Required Security Headers Enforced
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Proxies Enforce Required Security Headers
  rationale: Enforcing security headers in API proxies is critical to protect against common web vulnerabilities such as cross-site scripting (XSS) and clickjacking. Failure to enforce these headers can lead to unauthorized access or data breaches, impacting business operations and customer trust. Compliance with security standards often mandates the use of such protective measures to mitigate risks.
  description: This rule checks whether API proxies in Apigee enforce necessary security headers such as Content-Security-Policy, X-Frame-Options, and X-Content-Type-Options. Verifying this involves reviewing API proxy configurations to ensure these headers are included in HTTP responses. Remediation involves updating the API proxy policies to set these headers correctly, thereby enhancing the security posture against web-based attacks.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/security/security-best-practices
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://owasp.org/www-project-secure-headers/
- rule_id: gcp.apigee.api_proxy.api_api_response_schema_validation_enabled
  service: apigee
  resource: api_proxy
  requirement: API API Response Schema Validation Enabled
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Response Schema Validation is Enabled in Apigee
  rationale: Enabling API response schema validation in Apigee ensures that the data returned by APIs conforms to the defined structure, reducing the risk of unexpected data exposure and ensuring data integrity. This helps maintain API reliability and trustworthiness while meeting compliance requirements related to data protection and privacy such as GDPR and CCPA.
  description: This rule checks if response schema validation is enabled for API proxies in Apigee. API response schema validation helps in verifying that the response payloads match the expected schema definitions, preventing schema violations that could lead to data leakage or security vulnerabilities. To enable this, configure the `Response` policy with schema validation in the Apigee API proxy settings. Regular audits and updates to the schema definitions are recommended to adapt to new compliance standards.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/fundamentals/security
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/response-schema-validation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/apigee/docs/api-platform/fundamentals/proxy-endpoints
- rule_id: gcp.apigee.environment.api_rate_limiting_api_quota_limit_configured
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Quota Limit Configured
  scope: apigee.environment.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Rate Limiting and Quota Limits in Apigee Environments
  rationale: Configuring API rate limiting and quota limits in Apigee environments is crucial for preventing service abuse, ensuring fair usage among clients, and protecting backend services from overuse. Lack of such configurations can lead to resource exhaustion, degraded service performance, and increased operational costs, while also potentially violating regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule checks for the configuration of API rate limiting and quota limits within Apigee environments. Properly setting these limits helps manage traffic, control usage, and enforce policies that safeguard the API infrastructure. To verify, review the Apigee environment configurations for defined rate limits and quotas. Remediation involves using the Apigee management interface to set appropriate rate and quota policies aligned with business needs and compliance standards.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/config-overview
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/apigee/docs/api-platform/antipatterns/rate-limiting
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
- rule_id: gcp.apigee.environment.api_rate_limiting_api_stage_burst_limit_configured
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Stage Burst Limit Configured
  scope: apigee.environment.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Stage Burst Limit Configured for Apigee Environments
  rationale: Configuring burst limits in Apigee environments helps prevent API abuse, ensuring service stability and protecting against denial-of-service attacks. It mitigates the risk of unexpected spikes in request traffic that can deplete resources, leading to service disruption and potential financial losses. Proper rate limiting also aids in compliance with industry regulations that mandate robust security controls for API management.
  description: This rule checks that burst limits are configured for APIs in Apigee environments to control the maximum number of requests allowed over a short time. To verify, review the API proxy configurations in the Apigee console or via Apigee API Management APIs, ensuring that burst limit policies are applied. To remediate, define and implement burst limit policies according to your organization's traffic management policies, using the Apigee Edge management interface or API.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
  - https://cloud.google.com/security/compliance/cis/gcp/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/apigee/docs/api-platform/antipatterns/rate-limiting
- rule_id: gcp.apigee.environment.api_rate_limiting_api_stage_configured
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Stage Configured
  scope: apigee.environment.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Rate Limiting is Configured for Apigee Environments
  rationale: Configuring API rate limiting in Apigee environments is crucial to prevent abuse and ensure the availability of APIs. Without proper rate limits, an API can be overwhelmed by excessive requests, leading to service degradation or downtime. This configuration helps in meeting compliance with data protection regulations and preventing potential denial-of-service attacks.
  description: This rule checks whether API rate limiting is configured for each API stage within an Apigee environment. It verifies that appropriate thresholds are set to control the number of requests an API can handle over a specified period. To verify, review the API proxy settings in the Apigee management console and ensure rate-limiting policies are applied. Remediation involves configuring rate limit policies in the Apigee Edge UI by specifying the allowed request rate and burst capacity.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigee.environment.api_rate_limiting_api_stage_throttle_overrides_not_unbounded
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Stage Throttle Overrides Not Unbounded
  scope: apigee.environment.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Rate Limits Are Set for Apigee Environments
  rationale: Unbounded API rate limits can lead to resource exhaustion, service degradation, and increased operational costs. By setting proper rate limits, organizations can mitigate the risk of denial-of-service attacks and ensure fair usage of APIs. This aligns with compliance frameworks that require managing system capacity and maintaining service availability.
  description: This rule checks that API rate limiting and stage throttle overrides in Apigee environments are not set to unbounded values. Administrators should configure specific limit values to control the number of requests per second allowed for each API. Verify this by reviewing the Apigee environment's API proxy configurations. To remediate, set appropriate throttle limits in the API proxy settings to prevent excessive use and potential abuse.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/architecture/best-practices-for-building-api-products
- rule_id: gcp.apigee.environment.api_rate_limiting_api_usage_plan_required_for_api_keys
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Usage Plan Required For API Keys
  scope: apigee.environment.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enforce API Rate Limiting with Usage Plans for API Keys in Apigee
  rationale: Implementing API rate limiting with usage plans is crucial to protect APIs from abuse and unexpected traffic spikes that could lead to degraded performance or denial of service. It also helps in managing costs and maintaining service quality by controlling the consumption of API resources. Regulatory frameworks like PCI-DSS require mechanisms to prevent excessive user activity that could compromise system integrity.
  description: This rule checks if API rate limiting is enforced for API keys through usage plans in Apigee environments. Without usage plans, API keys might allow unregulated access, leading to potential service disruptions and security incidents. To verify, ensure that all APIs associated with API keys have a corresponding usage plan that defines limits. Remediation involves creating or updating usage plans to include specific rate limits for APIs accessed via API keys.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
  - https://cloud.google.com/apigee/docs/api-platform/security/api-security-best-practices
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/apigee/docs/api-platform/antipatterns/rate-limiting
- rule_id: gcp.apikeys.key.api_restrictions_configured
  service: apikeys
  resource: key
  requirement: API Restrictions Configured
  scope: apikeys.key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure API Keys Have Configured API Restrictions
  rationale: Configuring API restrictions for API keys limits their use to specific APIs, minimizing the risk of unauthorized access and misuse. Without restrictions, compromised keys can be exploited to access multiple services, leading to data breaches and potential financial and reputational damage. Compliance with standards such as GDPR and PCI-DSS often requires strict access controls on sensitive APIs.
  description: This rule checks if API keys in GCP have API restrictions configured to limit their use to authorized APIs only. To verify, review the API key settings in the Google Cloud Console under 'API & Services > Credentials' and ensure that each key has specific APIs selected in the 'API restrictions' section. Remediation involves adding the necessary API restrictions to each key, ensuring they are only usable with the intended services.
  references:
  - https://cloud.google.com/docs/authentication/api-keys#restricting_api_keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apikeys.key.platform_api_expiration_required
  service: apikeys
  resource: key
  requirement: Platform API Expiration Required
  scope: apikeys.key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Require API Key Expiration for Platform APIs
  rationale: Requiring an expiration date for API keys mitigates the risk of unauthorized access by ensuring that unused or compromised keys are invalidated after a certain period. This practice reduces the attack surface and aligns with best practices for key management, supporting compliance with standards like PCI-DSS and SOC2. Expiring API keys also helps prevent data breaches and unauthorized data exposure, protecting sensitive business information.
  description: This rule checks if API keys used for accessing platform APIs have an expiration date set. To verify, examine the API key configuration in the GCP Console or using the gcloud command-line tool. If an expiration date is not set, update the key settings to include one, ensuring it aligns with your organization's security policy on key rotation and lifecycle management. Regularly review and update expiration dates to adapt to changing security requirements.
  references:
  - https://cloud.google.com/docs/authentication/api-keys#enforcing_expiration_dates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/iam/docs/using-manage-api-keys
  - https://cloud.google.com/security/best-practices/encryption
- rule_id: gcp.apikeys.key.platform_api_rotation_policy_defined
  service: apikeys
  resource: key
  requirement: Platform API Rotation Policy Defined
  scope: apikeys.key.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure API Key Rotation Policy is Defined for Platform APIs
  rationale: Defining a key rotation policy for API keys is crucial to mitigate the risk of key compromise. Regular rotation reduces the window of opportunity for an attacker who obtains an API key, enhancing the overall security posture. It also aligns with compliance requirements such as PCI-DSS and ISO 27001, which mandate regular review and rotation of keys to protect sensitive data.
  description: This rule checks whether a key rotation policy is defined for API keys used to access Google Cloud Platform services. A rotation policy should specify the frequency of key changes and ensure that keys are not used beyond their intended lifespan. To verify, ensure that API keys have an associated rotation schedule in the GCP Console under the 'API & Services' section. Remediation involves setting up a key rotation policy using automation tools like Cloud Functions or external scripts to regularly rotate keys.
  references:
  - https://cloud.google.com/docs/authentication/api-keys
  - https://cloud.google.com/iam/docs/managing-api-keys
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0, Control 1.9
  - PCI-DSS Requirement 3.6.4
  - ISO 27001:2013 Section A.9.2.3
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apikeys.key.platform_api_scopes_least_privilege
  service: apikeys
  resource: key
  requirement: Platform API Scopes Least Privilege
  scope: apikeys.key.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure API Keys Use Least Privilege Platform API Scopes
  rationale: Limiting API key permissions to the minimum required reduces the risk of unauthorized access and data breaches. By adhering to the principle of least privilege, organizations can prevent privilege escalation and minimize the attack surface, meeting compliance requirements for frameworks like NIST and ISO 27001.
  description: This rule checks that API keys are configured with the minimum necessary platform API scopes to perform their intended functions. Ensure that each API key is scoped to access only the required APIs, avoiding overly broad permissions. To verify, review API key configurations in the Google Cloud Console or via the gcloud command-line tool. Remediation involves updating the API key settings to restrict scopes to the least privilege necessary for application functionality.
  references:
  - https://cloud.google.com/docs/authentication/api-keys
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundation
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.apikeys.key.rotated_in_90_days
  service: apikeys
  resource: key
  requirement: Rotated In 90 Days
  scope: apikeys.key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: API Key Rotation Every 90 Days
  rationale: Regular rotation of API keys reduces the risk of unauthorized access by minimizing the window of opportunity for credential compromise. This practice is crucial for protecting sensitive data and maintaining compliance with security standards, such as PCI-DSS, which mandate strict access control measures. By rotating API keys, organizations can mitigate risks associated with leaked or exposed credentials, thereby enhancing overall data security.
  description: This rule checks whether API keys in Google Cloud Platform are rotated at least every 90 days. Regular key rotation is a best practice to ensure that any compromised keys are quickly rendered useless. To verify compliance, review the creation and last rotation dates of API keys in the GCP console or through the API. If a key is older than 90 days, it should be regenerated and updated in all applications that use it. This process involves updating configurations and ensuring that old keys are properly revoked.
  references:
  - https://cloud.google.com/docs/authentication/api-keys
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/cyberframework
- rule_id: gcp.appengine.application.enforce_https_in_transit
  service: appengine
  resource: application
  requirement: Enforce HTTPS In Transit
  scope: appengine.application.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce HTTPS for App Engine Application Traffic
  rationale: Enforcing HTTPS ensures that data in transit is encrypted, protecting it from interception and tampering. This is crucial for maintaining user trust and meeting compliance requirements such as PCI-DSS and HIPAA, which mandate secure transmission of sensitive information. Unsecured data transmission can lead to data breaches, resulting in financial losses and reputational damage.
  description: This rule checks that your App Engine applications are configured to enforce HTTPS for all incoming traffic. To verify, ensure that the 'secure' option for URL handlers in the app.yaml file is set to 'always'. Remediation involves updating the app.yaml configuration and redeploying the application to require HTTPS. This practice prevents unencrypted data from being transmitted over the network, mitigating the risk of interception.
  references:
  - https://cloud.google.com/appengine/docs/standard/python/config/appref
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-52r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.appengine.application.engine_managed_updates_enabled
  service: appengine
  resource: application
  requirement: Engine Managed Updates Enabled
  scope: appengine.application.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure App Engine Managed Updates Are Enabled
  rationale: Enabling managed updates for App Engine applications helps maintain the security and stability of applications by automatically applying security patches and updates. This reduces the risk of vulnerabilities being exploited and ensures compliance with security best practices, which is critical for protecting sensitive data and maintaining customer trust.
  description: This rule checks if managed updates are enabled for App Engine applications. Managed updates automatically apply the latest security patches and updates, minimizing the need for manual intervention. To verify, check the App Engine settings in the Google Cloud Console or use the gcloud command-line tool. To enable, navigate to the App Engine settings and select the option to enable managed updates, ensuring your application remains secure and compliant.
  references:
  - https://cloud.google.com/appengine/docs/standard#automatic_updates
  - https://cloud.google.com/security/compliance/fedramp
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.appengine.application.paas_app_env_secrets_from_vault_only
  service: appengine
  resource: application
  requirement: Paas App Env Secrets From Vault Only
  scope: appengine.application.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure App Engine Uses Vault for Environment Secrets
  rationale: Storing application secrets directly within code or environment variables poses a significant security risk, including unauthorized access and data breaches. Using a centralized secret management solution like Google Cloud Secret Manager ensures that secrets are securely stored, accessed, and audited, aligning with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks that all environment secrets used in Google App Engine applications are sourced exclusively from Google Cloud Secret Manager. Verify that applications access secrets via the Secret Manager API and do not hardcode them in configuration files or environment variables. To remediate, refactor applications to retrieve secrets using the Secret Manager client library and update deployment procedures to remove sensitive information from environment settings.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/appengine/docs/security
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/kms/docs/secrets
- rule_id: gcp.appengine.application.paas_app_logging_enabled
  service: appengine
  resource: application
  requirement: Paas App Logging Enabled
  scope: appengine.application.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure App Engine Logging is Enabled
  rationale: Enabling logging for App Engine applications is crucial for monitoring application behavior, detecting anomalies, and responding to security incidents. Without logging, it becomes difficult to trace actions and events within the application, increasing the risk of undetected breaches and non-compliance with regulations like PCI-DSS and ISO 27001.
  description: This rule checks that logging is enabled for App Engine applications, ensuring that application requests and system events are recorded in Stackdriver. Verify by checking the App Engine settings in the Google Cloud Console under the 'Logging' section. Enable logging by setting the appropriate logging level and ensuring logs are exported to a centralized logging solution. This facilitates audit trails and enhances incident response capabilities.
  references:
  - https://cloud.google.com/appengine/docs/standard/python/logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.appengine.application.paas_app_private_networking_enforced
  service: appengine
  resource: application
  requirement: Paas App Private Networking Enforced
  scope: appengine.application.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for App Engine Applications
  rationale: Enforcing private networking for App Engine applications ensures that the services are only accessible within your VPC, reducing exposure to the public internet. This minimizes the risk of unauthorized access and potential data breaches, aligning with compliance requirements such as PCI-DSS and HIPAA. Private networking also supports better control and monitoring of traffic, enhancing overall security posture.
  description: This rule checks whether App Engine applications are configured to use private networking, restricting access to the application to internal VPC resources. To verify, ensure that the 'app.yaml' file includes 'vpc_access_connector' settings or that the App Engine is using a serverless VPC access connector. Remediation involves configuring a serverless VPC access connector and updating the App Engine app configuration to reference this connector, thereby routing traffic through the VPC.
  references:
  - https://cloud.google.com/appengine/docs/standard/networking-and-firewalls
  - https://cloud.google.com/vpc/docs/configure-serverless-vpc-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.appengine.application.paas_app_tls_min_1_2_enforced
  service: appengine
  resource: application
  requirement: Paas App TLS Min 1 2 Enforced
  scope: appengine.application.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce TLS 1.2+ for App Engine Applications
  rationale: Ensuring that App Engine applications enforce a minimum of TLS 1.2 is critical for safeguarding data in transit against interception and tampering. This reduces vulnerability to known cryptographic attacks targeting older versions of TLS, thereby complying with industry standards such as PCI-DSS and enhancing customer trust by protecting sensitive data.
  description: This rule checks if Google App Engine applications are configured to enforce a minimum TLS version of 1.2. Applications not meeting this requirement should be updated to specify TLS 1.2 or higher to ensure secure communication. This can be verified and configured through the Google Cloud Console under App Engine settings, or via the gcloud CLI tool by setting the 'min_tls_version' parameter in the 'app.yaml' file.
  references:
  - https://cloud.google.com/appengine/docs/standard#languages
  - https://cloud.google.com/security/encryption-in-transit/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/publications/security-and-privacy-controls-information-systems-and-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.appengine.version.paas_app_artifact_encrypted
  service: appengine
  resource: version
  requirement: Paas App Artifact Encrypted
  scope: appengine.version.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt App Engine Version Artifacts at Rest
  rationale: Encrypting App Engine artifacts at rest is crucial for protecting sensitive data from unauthorized access and potential breaches. It mitigates the risks of data exposure in case of storage compromise and ensures compliance with data protection regulations such as GDPR and HIPAA, which mandate encryption of sensitive information. Implementing encryption also aligns with industry best practices to safeguard intellectual property and maintain customer trust.
  description: This rule checks whether the artifacts for App Engine versions are encrypted at rest using Google-managed keys. To verify, ensure that the App Engine service is configured to utilize Google Cloud's default encryption mechanisms. Remediation involves reviewing and confirming the encryption settings in the App Engine configuration and enabling encryption if not already set. Consider using Customer-Managed Encryption Keys (CMEK) for additional control over encryption processes.
  references:
  - https://cloud.google.com/appengine/docs/standard#encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
- rule_id: gcp.appengine.version.paas_app_immutable
  service: appengine
  resource: version
  requirement: Paas App Immutable
  scope: appengine.version.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure App Engine Versions Are Immutable
  rationale: Immutable application versions in GCP's App Engine prevent unauthorized changes to application code and configurations post-deployment, reducing the risk of introducing vulnerabilities or errors. This practice supports compliance with standards requiring controlled software deployment processes, such as ISO 27001 and SOC 2, and mitigates the risk of inadvertent or malicious alterations that could lead to service disruptions or data breaches.
  description: This rule checks that deployed versions of applications in GCP App Engine are immutable, meaning no changes can be made once a version is deployed. To verify, ensure that versioning and deployment practices enforce immutability, such as using CI/CD pipelines that lock versions post-deployment. Remediation involves adjusting deployment strategies and scripts to disallow updates to existing versions, instead deploying new versions for changes. This ensures a consistent and auditable deployment process.
  references:
  - https://cloud.google.com/appengine/docs/standard
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/cis-benchmarks/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/solutions/devops/devops-process
- rule_id: gcp.artifactregistry.repository.container_analysis_enabled
  service: artifactregistry
  resource: repository
  requirement: Container Analysis Enabled
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Container Analysis is Enabled for Artifact Registry Repositories
  rationale: Enabling Container Analysis for repositories is crucial as it provides vulnerability scanning and metadata storage for container images, enhancing the security posture by identifying and mitigating potential vulnerabilities early. This is particularly important for maintaining the integrity and security of containerized applications, reducing the risk of deploying vulnerable containers into production environments, and aiding in compliance with security standards and regulations.
  description: This rule checks if Container Analysis is enabled for repositories in Google Artifact Registry. Container Analysis automatically scans images for vulnerabilities and stores metadata, ensuring that security issues are identified and addressed before deployment. To verify, navigate to the Artifact Registry in the GCP Console and check the settings for each repository to ensure Container Analysis is activated. If not enabled, configure the repository settings to include vulnerability scanning. This enhances security monitoring and helps maintain compliance with industry standards.
  references:
  - https://cloud.google.com/container-analysis/docs/container-analysis-overview
  - https://cloud.google.com/artifact-registry/docs/overview
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/container-analysis/docs/setting-up
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
- rule_id: gcp.artifactregistry.repository.container_scanning_enabled
  service: artifactregistry
  resource: repository
  requirement: Container Scanning Enabled
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Container Scanning in Artifact Registry
  rationale: Enabling container scanning in GCP Artifact Registry is crucial for identifying vulnerabilities and security misconfigurations in container images before they are deployed. This proactive measure helps prevent potential breaches and data leaks, minimizing the risk of exposure to malware and ensuring compliance with industry standards such as PCI-DSS and HIPAA.
  description: This rule checks whether container scanning is enabled for repositories in Google Cloud Artifact Registry. To verify, navigate to the Artifact Registry in the GCP Console, select the desired repository, and ensure that the 'Enable Vulnerability Scanning' option is activated. Remediate by configuring the repository settings to automatically scan images upon upload, providing an additional security layer in your CI/CD pipeline.
  references:
  - https://cloud.google.com/artifact-registry/docs/enable-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-security-rule-requirements/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.artifactregistry.repository.registry_image_scan_on_push_enabled
  service: artifactregistry
  resource: repository
  requirement: Registry Image Scan On Push Enabled
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Image Scan on Push for Artifact Registry Repositories
  rationale: Enabling image scan on push in Artifact Registry helps identify vulnerabilities in container images early in the development process, reducing the risk of deploying insecure applications. This proactive approach aligns with security best practices and regulatory requirements such as PCI-DSS and ISO 27001, which emphasize the importance of vulnerability management and secure software development lifecycle.
  description: This rule checks if the image scan on push feature is enabled for repositories in Google's Artifact Registry. Enabling this setting ensures that each image pushed to the registry is automatically scanned for vulnerabilities, providing immediate feedback to developers and preventing vulnerable images from progressing through the deployment pipeline. To verify and enable this setting, navigate to the GCP Console, select Artifact Registry, and configure the repository settings to enable automatic scanning. Alternatively, use the `gcloud` command-line tool to set the `scan-on-push` flag to true.
  references:
  - https://cloud.google.com/artifact-registry/docs/scan-images
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.artifactregistry.repository.registry_repository_encryption_enabled
  service: artifactregistry
  resource: repository
  requirement: Registry Repository Encryption Enabled
  scope: artifactregistry.repository.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Artifact Registry Repositories Use Encryption at Rest
  rationale: Enabling encryption at rest for Artifact Registry repositories ensures that sensitive data stored within the repositories is protected against unauthorized access and potential data breaches. This is crucial for maintaining the confidentiality and integrity of artifacts, especially in environments subject to regulatory requirements like GDPR, HIPAA, and PCI-DSS, which mandate strong data protection measures.
  description: This rule checks whether encryption at rest is enabled for Google Cloud Artifact Registry repositories. Encryption at rest is a security feature that encrypts your data while stored on disk, preventing exposure from unauthorized access. To verify, ensure that each repository is configured with Google's default encryption or a customer-managed encryption key (CMEK). Remediation involves configuring the repository settings to include encryption options, either through the console or CLI.
  references:
  - https://cloud.google.com/artifact-registry/docs/manage-repos#encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.0.0 - 6.6
  - https://cloud.google.com/security/compliance
  - 'NIST Special Publication 800-57: Recommendation for Key Management'
- rule_id: gcp.artifactregistry.repository.repository_minimum_user_access
  service: artifactregistry
  resource: repository
  requirement: Repository Minimum User Access
  scope: artifactregistry.repository.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Minimal User Access to Artifact Registry Repositories
  rationale: Restricting user access to GCP Artifact Registry repositories minimizes the risk of unauthorized data exposure, accidental deletions, or malicious modifications. Inadequate access controls can lead to compliance issues with standards like PCI-DSS and HIPAA, impacting both business reputation and financial standing.
  description: This rule checks that user access to Artifact Registry repositories is restricted to the minimum necessary permissions. It involves auditing the IAM policies associated with these repositories to ensure that users only have roles that are essential for their tasks. To verify, review IAM policies and adjust roles to comply with the principle of least privilege. Remediation includes removing unnecessary user roles and periodically auditing access controls.
  references:
  - https://cloud.google.com/artifact-registry/docs/access-control
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.artifactregistry.repository.supply_chain_policy_storage_encrypted
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Policy Storage Encrypted
  scope: artifactregistry.repository.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Artifact Registry Repositories Use Encrypted Storage
  rationale: Encrypting storage containing supply chain artifacts protects against unauthorized data access and potential data breaches, which can compromise the integrity of software supply chains. This is crucial for compliance with data protection regulations like PCI-DSS and HIPAA, and mitigates risks of intellectual property theft and reputational damage.
  description: This rule checks if Artifact Registry repositories have encryption enabled at rest. Ensure that Google-managed keys or Customer-Managed Encryption Keys (CMEK) are used to encrypt repository storage. Verification can be done through the GCP Console or gcloud CLI by inspecting repository configurations. To remediate, enable encryption by configuring CMEK or using default encryption settings provided by Google.
  references:
  - https://cloud.google.com/artifact-registry/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-at-rest/
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_image_scanning_enabled
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Image Scanning Enabled
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Image Scanning in Artifact Registry Repositories
  rationale: Enabling image scanning in Artifact Registry repositories helps identify vulnerabilities in container images before they are deployed. This proactive measure mitigates the risk of deploying compromised or vulnerable software, thereby enhancing the security posture of your containerized applications. It also supports compliance with regulations that mandate vulnerability management and secure software development practices.
  description: This rule checks whether image scanning is enabled for repositories in Google Cloud's Artifact Registry. Image scanning should be configured to automatically scan container images for known vulnerabilities, ensuring that only secure images are used within your environment. Verification can be done by checking the repository settings in the Google Cloud Console to ensure that the 'Enable Vulnerability Scanning' option is active. If not enabled, navigate to the Artifact Registry section, select the repository, and activate the image scanning feature under 'Vulnerability Scanning'.
  references:
  - https://cloud.google.com/artifact-registry/docs/monitoring-and-securing
  - https://cloud.google.com/artifact-registry/docs/enable-vulnerability-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_immutable_tags_or_signing_enforced
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Immutable Tags Or Signing Enforced
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Immutable Tags or Signing in Artifact Registry
  rationale: Ensuring that tags in the Artifact Registry are immutable or that all artifacts are signed enhances the integrity and authenticity of software components. This practice mitigates risks such as unauthorized changes to artifacts, which could introduce vulnerabilities or malicious code, impacting business operations and compliance with standards like PCI-DSS and SOC2.
  description: This rule checks that all repositories in the Artifact Registry enforce either immutable tags or artifact signing. Immutable tags prevent changes to existing tags, ensuring that once a version is published, it cannot be altered. Alternatively, enforcing artifact signing requires that all artifacts are cryptographically signed, verifying their origin and integrity. To remediate, configure the repository settings in the Google Cloud Console to enable either immutable tags or set up a signing policy, ensuring all users are aware of the new requirements.
  references:
  - https://cloud.google.com/artifact-registry/docs/creating-repositories
  - https://cloud.google.com/artifact-registry/docs/configuring-repo-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/soc2.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_no_public_pull_push
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry No Public Pull Push
  scope: artifactregistry.repository.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Pull/Push on Artifact Registry Repositories
  rationale: Allowing public pull and push access to artifact repositories poses a significant security risk, as unauthorized users could introduce malicious artifacts or exfiltrate sensitive data. This exposure could lead to supply chain attacks, compromising the integrity of software deployments and violating compliance mandates such as SOC 2, which emphasize access control. Limiting repository access helps protect against these threats, ensuring that only trusted entities can interact with critical software components.
  description: This rule checks for repositories in Google Cloud's Artifact Registry that have public pull or push access enabled. Ensuring that repositories are not publicly accessible prevents unauthorized access and modifications. To verify and remediate, review the IAM policies of each repository to ensure that roles like 'roles/artifactregistry.reader' and 'roles/artifactregistry.writer' are not granted to 'allUsers' or 'allAuthenticatedUsers'. Update these policies to restrict access to only necessary and trusted Google Cloud identities.
  references:
  - https://cloud.google.com/artifact-registry/docs/access-control
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_no_wildcard_admin
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry No Wildcard Admin
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Restrict Admin Rights in Artifact Registry Repositories
  rationale: Using wildcard entries for admin access in Artifact Registry repositories poses a significant security risk by allowing unrestricted access, which can lead to unauthorized actions and potential data breaches. This can compromise the integrity of the software supply chain, impact business operations, and violate compliance standards such as PCI-DSS and SOC2.
  description: This rule ensures that no wildcard entries are used for granting admin roles within Artifact Registry repositories. It checks for IAM policies that assign roles/artifactregistry.admin with wildcard identities, such as 'allUsers' or 'allAuthenticatedUsers.' To verify, review and modify IAM policies to replace wildcard entries with specific user or service account identities. Remediation involves updating IAM policies in the Google Cloud Console or using gcloud CLI to remove wildcard permissions.
  references:
  - https://cloud.google.com/artifact-registry/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_private_or_access_restricted
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Private Or Access Restricted
  scope: artifactregistry.repository.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Artifact Registry Repositories Are Private or Access Restricted
  rationale: Restricting access to Artifact Registry repositories is essential to prevent unauthorized access and potential supply chain attacks. Publicly accessible repositories can expose sensitive artifacts to malicious actors, increasing the risk of data breaches and non-compliance with regulations such as GDPR and PCI-DSS. Limiting access helps maintain the integrity and confidentiality of your software supply chain.
  description: This rule checks if your Artifact Registry repositories are configured to be private or have restricted access to trusted identities only. To verify, ensure that IAM policies are applied to limit access to specific service accounts, groups, or users. Remediation involves updating IAM settings and network policies to restrict repository access to approved identities and networks. Utilize VPC Service Controls to further enhance security by defining a service perimeter.
  references:
  - https://cloud.google.com/artifact-registry/docs/repositories
  - 'CIS GCP Benchmark: Ensure Artifact Registry repositories are not publicly accessible'
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc-service-controls/docs/concepts
  - 'NIST SP 800-53: AC-3 Access Enforcement'
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_replication_cross_region_encrypted
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Replication Cross Region Encrypted
  scope: artifactregistry.repository.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region Encrypted Replication for Artifact Repositories
  rationale: Encrypting cross-region replicated artifacts mitigates the risk of unauthorized data access during transit and storage, protecting sensitive data from exposure and ensuring compliance with data protection standards such as GDPR and HIPAA. It also reduces the impact of potential data breaches and aligns with best practices for securing supply chain integrity in cloud environments.
  description: This rule checks that all cross-region replications of Artifact Registry repositories are encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). Verify configuration by ensuring that encryption settings are enabled for replicated repositories. To remediate, configure the repository settings to use CMEK or GMEK for cross-region replication, ensuring that the encryption keys are properly managed and rotated according to security policies.
  references:
  - https://cloud.google.com/artifact-registry/docs/repositories
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/artifact-registry/docs/cmek
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_replication_destinations_lea_privilege
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Replication Destinations Lea Privilege
  scope: artifactregistry.repository.replication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Limit Artifact Registry Replication Destination Privileges
  rationale: Improperly configured replication destinations in the Artifact Registry can introduce vulnerabilities by allowing unauthorized access or replication of sensitive artifacts. This can lead to data exfiltration, unauthorized access to proprietary software, and potential breaches of compliance with standards such as PCI-DSS and ISO 27001. Limiting privileges helps mitigate these risks by ensuring that only authorized destinations can replicate sensitive data.
  description: This rule checks that replication destinations for Artifact Registry repositories are configured with the least privilege necessary to perform their functions. Specifically, it ensures that only designated and trusted projects or repositories are permitted as replication destinations. Verification involves reviewing IAM policies and permissions associated with the Artifact Registry repositories and ensuring they adhere to the principle of least privilege. Remediation includes updating IAM policies to restrict replication permissions to trusted destinations only.
  references:
  - https://cloud.google.com/artifact-registry/docs/access-control
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundation
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/blog/products/identity-security/understanding-the-shared-responsibility-model-in-google-cloud
- rule_id: gcp.artifactregistry.repository.supply_chain_unused_or_old_tags_expire
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Unused Or Old Tags Expire
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Expiration of Unused or Old Tags in Artifact Repositories
  rationale: Allowing unused or old tags to persist in artifact repositories can result in increased storage costs, clutter, and potential security vulnerabilities. By expiring these tags, organizations reduce the risk of deploying outdated or vulnerable artifacts and maintain a cleaner, more efficient repository. This practice supports compliance with best practices in supply chain security by ensuring only current and necessary artifacts are available for deployment.
  description: This rule checks that unused or old tags within Artifact Registry repositories are configured to expire after a defined period. This involves setting lifecycle management policies to automatically remove tags that are not accessed or modified within a specified timeframe. Verification can be done by reviewing and configuring lifecycle policies in the Artifact Registry settings. Remediation includes implementing expiration policies that align with organizational security and operational requirements.
  references:
  - https://cloud.google.com/artifact-registry/docs/tagging
  - https://cloud.google.com/artifact-registry/docs/manage-repos#lifecycle-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.artifactregistry.repository.vulnerability_scanning_enabled
  service: artifactregistry
  resource: repository
  requirement: Vulnerability Scanning Enabled
  scope: artifactregistry.repository.vulnerability_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Enable Vulnerability Scanning for Artifact Registry Repositories
  rationale: Enabling vulnerability scanning on Artifact Registry repositories is crucial for identifying and mitigating security flaws in container images and other artifacts. This proactive measure helps prevent the deployment of vulnerable dependencies, reduces the attack surface, and supports compliance with security standards such as NIST and PCI-DSS. Failing to scan for vulnerabilities can expose your workloads to potential exploits and data breaches.
  description: This rule checks whether vulnerability scanning is enabled for repositories in Google Cloud Artifact Registry. It ensures that images and artifacts stored in the registry are automatically scanned for known vulnerabilities, providing detailed reports on potential security issues. To verify, navigate to the repository settings in the GCP Console and confirm that vulnerability scanning is enabled. Enable it by setting up a scanning configuration in the Artifact Registry settings, which will automatically scan all new and existing artifacts.
  references:
  - https://cloud.google.com/artifact-registry/docs/monitoring-and-scanning
  - https://cloud.google.com/security/compliance/cis#section-5
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
- rule_id: gcp.asset.asset.governance_aggregation_authorization_not_expired_or_revoked
  service: asset
  resource: asset
  requirement: Governance Aggregation Authorization Not Expired Or Revoked
  scope: asset.asset.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Governance Authorization is Valid and Not Revoked
  rationale: Expired or revoked governance aggregation authorizations pose a risk by allowing unauthorized access to sensitive asset data, potentially leading to data breaches and non-compliance with regulatory frameworks like GDPR or SOC2. Maintaining valid authorizations ensures that only designated entities can view and aggregate asset data, mitigating unauthorized data access and maintaining organizational security posture.
  description: This rule checks if governance aggregation authorizations for Google Cloud assets are neither expired nor revoked. It verifies that each authorization token used for asset aggregation is active and has not been revoked, ensuring secure and compliant aggregation processes. To remediate, review the asset governance authorization settings, renew expired authorizations, and revoke any that are no longer needed or have been compromised. Regularly auditing these settings helps maintain a secure and compliant environment.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.asset.asset.governance_aggregation_authorization_target_accounts_correct
  service: asset
  resource: asset
  requirement: Governance Aggregation Authorization Target Accounts Correct
  scope: asset.asset.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Proper Authorization for Governance Aggregation
  rationale: Proper authorization for governance aggregation in GCP is crucial to prevent unauthorized access and potential data breaches. Incorrect configurations may lead to unauthorized users gaining access to sensitive data, posing compliance risks with regulations like GDPR and CCPA. This rule helps maintain the integrity and confidentiality of aggregated governance data across accounts.
  description: This rule checks that target accounts for governance aggregation in GCP have the correct authorization settings. It verifies that only authorized accounts are allowed to aggregate governance data, preventing unauthorized access. To remediate, review IAM policies and ensure that only necessary accounts have permissions for governance aggregation. Confirm with the GCP Console or gcloud CLI that the IAM roles and permissions are correctly set for each target account.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.asset.asset.governance_aggregation_authorization_trusts_least_privilege
  service: asset
  resource: asset
  requirement: Governance Aggregation Authorization Trusts Least Privilege
  scope: asset.asset.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege in Governance Aggregation Authorization
  rationale: Ensuring that governance aggregation authorizations adhere to the principle of least privilege is crucial to minimizing the risk of unauthorized access and potential data breaches. This approach reduces the attack surface by ensuring that users or services have only the permissions necessary to perform their functions. Compliance with least privilege principles helps meet regulatory requirements such as PCI-DSS and ISO 27001, which mandate strict access controls.
  description: This rule checks that permissions granted for governance aggregation in Google Cloud are limited to the minimum necessary. Specifically, it verifies that roles assigned to service accounts or users for asset aggregation tasks do not exceed the required permissions. To verify, review IAM policies for roles associated with asset aggregation and adjust any overly permissive roles. Use predefined roles like 'roles/viewer' instead of 'roles/editor' or 'roles/owner'.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.asset.asset.governance_config_recorder_enabled
  service: asset
  resource: asset
  requirement: Governance Config Recorder Enabled
  scope: asset.asset.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Governance Config Recorder is Enabled for Asset Monitoring
  rationale: Enabling the Governance Config Recorder is crucial for maintaining visibility into asset configurations and changes within your GCP environment. This helps in identifying unauthorized alterations, ensuring compliance with regulatory frameworks, and mitigating risks associated with misconfigurations or malicious activities. Without it, organizations may face compliance violations and increased security vulnerabilities.
  description: This rule checks if the Governance Config Recorder is enabled on your GCP assets. The Governance Config Recorder is a critical component for tracking configuration changes and maintaining a historical record of asset states. To verify, ensure that the Asset Inventory API is enabled and properly configured in your GCP project. Remediation involves enabling the API via the GCP Console under APIs & Services, and setting up the necessary permissions for continuous asset monitoring.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.asset.asset.governance_config_recorder_global_resource_types_tracked
  service: asset
  resource: asset
  requirement: Governance Config Recorder Global Resource Types Tracked
  scope: asset.asset.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Track Global Resource Types with Governance Config Recorder
  rationale: Ensuring that all global resource types in Google Cloud are tracked by the Governance Config Recorder is crucial for maintaining visibility and compliance across cloud assets. Failing to track these resources could lead to undetected policy violations, configuration drift, and security vulnerabilities, which could have significant business and regulatory impacts.
  description: This check verifies that the Governance Config Recorder has been configured to track all global resource types across your Google Cloud environment. Administrators should ensure that the Asset Inventory service is properly set up to record changes in global resources, such as organization policies and IAM settings. To remediate, update the recorder settings to include all necessary global resource types and regularly review the asset inventory reports for anomalies.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.asset.asset.governance_delivery_channel_secure_destination_configured
  service: asset
  resource: asset
  requirement: Governance Delivery Channel Secure Destination Configured
  scope: asset.asset.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Secure Configuration for Governance Delivery Channel
  rationale: Misconfigured delivery channels can expose sensitive data to unauthorized entities, leading to potential data breaches or non-compliance with regulatory standards such as GDPR and CCPA. Ensuring secure destinations for governance data helps protect against unauthorized access and eavesdropping, reducing the risk of data leaks and enhancing the overall security posture.
  description: This rule checks for the secure configuration of destination settings in the Governance Delivery Channel for GCP assets. It verifies that delivery channels are configured to send data to secure, authorized destinations using encrypted protocols (e.g., HTTPS, TLS). To verify, ensure that all delivery channel destinations are correctly configured with encryption and access controls. Remediation involves updating the delivery channel settings to use secure endpoints and validating access permissions.
  references:
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/framework/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/docs/security/security-controls
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.asset.feed.governance_aggregation_authorization_present_and_valid
  service: asset
  resource: feed
  requirement: Governance Aggregation Authorization Present And Valid
  scope: asset.feed.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Valid Authorization for Asset Feed Governance Aggregation
  rationale: Proper authorization for governance aggregation is crucial as it ensures that only legitimate and verified entities can access and aggregate sensitive asset metadata. A lack of this oversight can lead to unauthorized access, data breaches, or inadvertent exposure of confidential information, potentially resulting in non-compliance with data protection regulations such as GDPR or CCPA.
  description: This rule checks whether the authorization for asset feed governance aggregation is present and valid within GCP. It verifies that correct IAM policies are applied to ensure only authorized users or service accounts have access to create and manage asset feeds. To remediate any issues, review and update IAM policies to include only necessary permissions, ensuring that the principle of least privilege is adhered to. Regularly audit and validate these configurations to maintain compliance and security integrity.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/impersonating-service-accounts
- rule_id: gcp.asset.feed.governance_aggregator_auto_enroll_new_accounts
  service: asset
  resource: feed
  requirement: Governance Aggregator Auto Enroll New Accounts
  scope: asset.feed.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Auto-Enroll New Accounts in Governance Aggregator
  rationale: Automatically enrolling new accounts in an asset governance aggregator ensures that all resources are monitored from the outset, reducing the risk of unmanaged assets which can lead to compliance failures and security vulnerabilities. This proactive approach supports regulatory compliance by maintaining visibility and control over the entire asset inventory as per standards such as ISO 27001 and NIST.
  description: This rule verifies that new accounts are automatically enrolled in the Governance Aggregator within the GCP Asset service. It ensures that asset feeds are configured to include new projects, allowing for continuous security monitoring and compliance checks. To verify, check the asset feed configuration for automatic enrollment settings. Remediation involves setting up the asset feed with the necessary permissions and configuration to include new accounts by default.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.asset.feed.governance_aggregator_org_aggregator_enabled
  service: asset
  resource: feed
  requirement: Governance Aggregator Org Aggregator Enabled
  scope: asset.feed.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Governance Aggregator for Organization Asset Monitoring
  rationale: Enabling Governance Aggregator Org Aggregator allows centralized visibility into asset changes across the organization, which is crucial for detecting unauthorized modifications, ensuring policy adherence, and maintaining compliance with regulatory frameworks such as NIST and ISO 27001. Without this feature, organizations risk missing critical asset changes that could lead to data breaches or non-compliance penalties.
  description: This rule checks whether the Governance Aggregator Org Aggregator is enabled in GCP Asset Inventory. This feature aggregates asset data across all projects within an organization, providing a comprehensive view for security and compliance monitoring. To verify, ensure that asset feeds are configured with an organizational scope and that necessary permissions are granted. Remediation involves configuring the asset feed using Cloud Asset Inventory's API or Console, ensuring it's set to monitor organization-wide asset changes.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/asset-inventory/docs/monitoring-asset-changes
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.asset.feed.governance_delivery_channel_destination_access_lea_privilege
  service: asset
  resource: feed
  requirement: Governance Delivery Channel Destination Access Lea Privilege
  scope: asset.feed.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Asset Feed Delivery Channel
  rationale: Limiting access privileges for asset feed delivery channels minimizes the risk of unauthorized access to sensitive data, ensuring compliance with security standards such as NIST and ISO 27001. By enforcing least privilege, organizations can reduce the attack surface and protect against potential data breaches and insider threats.
  description: This rule verifies that the IAM policies associated with the destination of asset feed delivery channels are configured with the principle of least privilege. It checks for overly permissive roles that may grant unnecessary access to sensitive data. To remediate, review and adjust IAM policies to only include roles essential for operational needs, prioritizing roles with least privilege access. Regular audits should be performed to maintain compliance.
  references:
  - https://cloud.google.com/asset-inventory/docs/manage-feeds
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/best-practices/identity
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.asset.feed.governance_delivery_channel_kms_encryption_enabled
  service: asset
  resource: feed
  requirement: Governance Delivery Channel KMS Encryption Enabled
  scope: asset.feed.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Asset Feed Governance Channels
  rationale: Enabling KMS encryption for asset feed governance channels is crucial to protect sensitive data from unauthorized access and potential breaches. Without encryption, data at rest could be exposed to malicious actors, leading to legal penalties and damage to the organization's reputation. This practice aligns with compliance requirements such as GDPR and HIPAA, ensuring that encryption is enforced as a standard security measure.
  description: This rule checks whether Cloud Asset Governance Delivery Channels have Google Cloud Key Management Service (KMS) encryption enabled. To verify, ensure that each asset feed within your GCP environment is configured to use a customer-managed encryption key (CMEK) from KMS. To remediate, navigate to the Cloud Console, select the appropriate asset feed, and configure it to use a CMEK. This ensures that all data transmitted through governance channels is encrypted at rest, using strong encryption standards.
  references:
  - https://cloud.google.com/asset-inventory/docs/feeds-overview
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.backupdr.backup_plan.backup_access_backup_admins_mfa_required
  service: backupdr
  resource: backup_plan
  requirement: Backup Access Backup Admins MFA Required
  scope: backupdr.backup_plan.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure MFA for Backup Admins Accessing Backup Plans
  rationale: Requiring multi-factor authentication (MFA) for backup administrators helps mitigate the risk of unauthorized access to backup plans, reducing potential data breaches and ensuring data integrity. This practice is crucial for maintaining compliance with regulations such as NIST SP 800-63B, PCI-DSS, and ISO 27001, which emphasize strong authentication mechanisms.
  description: This rule verifies that all accounts with administrative access to GCP Backup and DR backup plans have MFA enabled. To ensure compliance, verify that IAM policies enforce MFA for roles with backup admin privileges. Remediation involves configuring IAM policy bindings to require MFA and ensuring that all backup admin accounts are enrolled in an MFA program. Regularly audit IAM roles and policies to confirm ongoing compliance.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/backup-and-dr/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/digital-identity-guidelines
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_plan.backup_access_backup_keys_access_least_privilege
  service: backupdr
  resource: backup_plan
  requirement: Backup Access Backup Keys Access Least Privilege
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: high
  title: Ensure Least Privilege Access to Backup Keys in Backup Plans
  rationale: Limiting access to backup keys is crucial to prevent unauthorized data retrieval, which could lead to data breaches or loss. Ensuring least privilege access aligns with best practices for data protection and is essential for meeting compliance standards such as ISO 27001 and PCI-DSS, which mandate stringent access controls to sensitive information.
  description: This rule checks that access to backup keys within GCP Backup and Disaster Recovery (backupdr) backup plans is restricted to only those identities that absolutely need it. Verify that IAM roles associated with backup keys are set to the minimum scope necessary by reviewing and adjusting IAM policies. Remediation involves auditing current access and revoking unnecessary permissions, ensuring only authorized personnel have access to these critical resources.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/backup-disaster-recovery/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.backupdr.backup_plan.backup_access_backup_vault_rbac_least_privilege
  service: backupdr
  resource: backup_plan
  requirement: Backup Access Backup Vault RBAC Least Privilege
  scope: backupdr.backup_plan.backup_recovery
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Backup Vault Access Follows Least Privilege Principle
  rationale: Implementing least privilege for Backup Vault access helps prevent unauthorized data access and potential data breaches. It minimizes the risk of insider threats and limits the impact of compromised accounts. Adhering to least privilege is crucial for compliance with standards such as ISO 27001 and NIST SP 800-53, which mandate strict access controls.
  description: This rule checks if backup plans in GCP Backup and DR service implement RBAC policies that adhere to the least privilege principle. It verifies that users and service accounts have only the necessary permissions to perform their roles. Remediation involves auditing current IAM roles associated with backup plans and adjusting permissions to ensure minimal necessary access. Verify configurations in the IAM section of the Google Cloud Console and adjust roles as needed to comply with best practices.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/backup-disaster-recovery/docs/access-control
  - https://www.nist.gov/publications/security-and-privacy-controls-information-systems-and-organizations
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.backupdr.backup_plan.backup_access_no_public_access_to_backup_vault
  service: backupdr
  resource: backup_plan
  requirement: Backup Access No Public Access To Backup Vault
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Restrict Public Access to Backup Vaults in GCP
  rationale: Public access to backup vaults can lead to unauthorized data exposure and potential data breaches. This poses significant risks to business continuity and compliance with data protection regulations such as GDPR or HIPAA. Threat actors could exploit publicly accessible backups to extract sensitive information or disrupt recovery processes.
  description: This rule checks that backup vaults in GCP's Backup and DR service are not publicly accessible. Verify that IAM policies do not grant 'allUsers' or 'allAuthenticatedUsers' roles on backup vault resources. Remediation involves adjusting IAM policies to restrict access to only trusted identities and roles, ensuring that backup data remains secure and accessible only to authorized personnel.
  references:
  - https://cloud.google.com/backup-and-dr/docs/security-iam
  - CIS Google Cloud Platform Foundation Benchmark, v1.2.0, Section 7.2
  - NIST SP 800-53 Rev. 5, CP-9 Information System Backup
  - 'PCI-DSS Requirement 3: Protect stored cardholder data'
  - ISO/IEC 27001:2013 - A.12.3 Backup
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.backupdr.backup_plan.backup_monitoring_alert_destinations_configured
  service: backupdr
  resource: backup_plan
  requirement: Backup Monitoring Alert Destinations Configured
  scope: backupdr.backup_plan.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Backup Plan Alert Destinations Are Configured
  rationale: Configuring alert destinations for backup plans is crucial for timely notifications about backup failures or issues. Without proper alerting, organizations risk data loss due to unmonitored backup failures, which can lead to business disruptions, financial loss, and non-compliance with data protection regulations.
  description: This rule checks whether alert destinations are configured for GCP backup plans. Proper configuration ensures that alerts are sent to designated personnel or systems whenever a backup operation fails or encounters issues. To verify, navigate to the Backup and DR service in the GCP Console, select the backup plan, and review the alert configurations under monitoring settings. Remediation includes setting up alert policies in Cloud Monitoring to notify stakeholders via email, SMS, or webhook.
  references:
  - https://cloud.google.com/backup-dr/docs
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.backupdr.backup_plan.backup_monitoring_alert_rules_for_job_failures_configured
  service: backupdr
  resource: backup_plan
  requirement: Backup Monitoring Alert Rules For Job Failures Configured
  scope: backupdr.backup_plan.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Configure Alert Rules for Backup Job Failures
  rationale: Configuring backup monitoring alert rules is crucial for ensuring data resilience and integrity. Without timely alerts on backup job failures, organizations risk data loss, impacting business continuity and compliance with data protection regulations like GDPR and HIPAA. Early detection of backup failures helps mitigate risks associated with data breaches and operational disruptions.
  description: This rule checks if alerting rules are configured for monitoring backup job failures in Google Cloud Backup and DR service. It ensures that alerts are set up to notify administrators of any failed backup operations, enabling prompt investigation and remediation. To verify, access the Google Cloud Console, navigate to Monitoring, and ensure that alerting policies are defined for backup job statuses. Remediation involves creating alerting policies that target backup job failure logs and configuring notification channels.
  references:
  - https://cloud.google.com/backup-dr/docs/monitoring
  - https://cloud.google.com/monitoring/alerts
  - 'CIS GCP Benchmark: 4.5 Ensure Cloud Monitoring is Configured for All Backup Plans'
  - 'NIST SP 800-53: CP-9 - Information System Backup'
  - PCI DSS Requirement 10.6 - Review Logs and Security Events
  - https://cloud.google.com/security
- rule_id: gcp.backupdr.backup_plan.backup_monitoring_alert_rules_for_sla_breach_configured
  service: backupdr
  resource: backup_plan
  requirement: Backup Monitoring Alert Rules For Sla Breach Configured
  scope: backupdr.backup_plan.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Configure SLA Breach Alerts for Backup Plans
  rationale: Configuring SLA breach alerts for backup plans is crucial to ensure timely responses to potential data loss incidents, minimizing downtime and protecting business continuity. Failure to respond promptly to SLA breaches can result in data integrity issues, financial losses, and potential non-compliance with data protection regulations.
  description: This rule checks that Google Cloud Backup and DR (backupdr) service has monitoring and alerting rules configured to notify stakeholders when a Service Level Agreement (SLA) breach occurs. To verify, ensure that alert policies are set up in Cloud Monitoring to track backup operations status and trigger notifications when SLAs are not met. Remediation involves setting up specific alert policies via Google Cloud Console or using Terraform scripts to automate alert configurations.
  references:
  - https://cloud.google.com/backup-and-dr/docs/monitoring-and-alerting
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/terraform/resource-docs/monitoring/alert_policy
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.backupdr.backup_plan.backup_monitoring_metrics_export_enabled
  service: backupdr
  resource: backup_plan
  requirement: Backup Monitoring Metrics Export Enabled
  scope: backupdr.backup_plan.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Backup Monitoring Metrics Export for Backup Plans
  rationale: Exporting backup monitoring metrics is crucial for maintaining visibility into the health and performance of backup operations. This enables organizations to quickly identify and respond to potential issues, ensuring data integrity and compliance with regulations like GDPR and HIPAA that mandate reliable data protection. Without these metrics, organizations face increased risk of undetected backup failures, leading to data loss and potential non-compliance fines.
  description: This rule checks if backup monitoring metrics export is enabled for backup plans on GCP. Ensure that the `monitoring_metrics_enabled` flag is set to true in your backup configuration. Verify this setting in the Cloud Console under Backup and DR service, or use the CLI command `gcloud backup plans describe [PLAN_NAME]` to confirm. If not enabled, update the backup plan settings to include metric exports or configure it using Infrastructure as Code (IaC) tools like Terraform.
  references:
  - https://cloud.google.com/backup-and-dr/docs/monitoring
  - https://cloud.google.com/backup-and-dr/docs/creating-backup-plans
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.backupdr.backup_plan.dr_configured
  service: backupdr
  resource: backup_plan
  requirement: DR Configured
  scope: backupdr.backup_plan.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Backup Plans Have Disaster Recovery Configured
  rationale: Configuring disaster recovery (DR) for backup plans is crucial to minimize data loss and ensure business continuity in the event of an incident. Without a proper DR strategy, organizations are at risk of prolonged downtime and potential non-compliance with industry regulations such as ISO 27001 or SOC2, which could lead to financial and reputational damage.
  description: This rule checks if disaster recovery configurations are in place for Google Cloud backup plans. Specifically, it verifies that the backup plans include settings for cross-region replication and regular DR testing. To verify, review your backup plans in the Google Cloud Console under 'Backup & DR' to ensure DR configurations are active. To remediate, enable cross-region replication and schedule regular DR drills to ensure data resilience across geographic locations.
  references:
  - https://cloud.google.com/backup-and-dr/docs/configure-disaster-recovery
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/architecture/dr-scenarios
- rule_id: gcp.backupdr.backup_plan.plan_min_retention_configured
  service: backupdr
  resource: backup_plan
  requirement: Plan Min Retention Configured
  scope: backupdr.backup_plan.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Backup Plan Minimum Retention Period is Configured
  rationale: Configuring a minimum retention period for backup plans ensures that data is retained for a sufficient time to recover from data loss incidents, supporting business continuity and compliance with data retention policies. Without a defined retention period, critical data might be prematurely deleted, increasing the risk of data loss and potential non-compliance with regulatory requirements.
  description: This rule checks whether a minimum retention period is set for each backup plan in GCP's Backup for DR service. To verify, ensure that each backup plan has the 'minimum retention period' parameter configured according to your organization's data retention policy. Failure to configure this setting could result in insufficient data availability for recovery. To remediate, access the GCP Console, navigate to Backup for DR, and set the 'minimum retention period' for each backup plan based on organizational or regulatory requirements.
  references:
  - https://cloud.google.com/backup-dr/docs/create-backup-plan#retention
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_plan.recovery_point_retention_configured
  service: backupdr
  resource: backup_plan
  requirement: Recovery Point Retention Configured
  scope: backupdr.backup_plan.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Recovery Point Retention is Configured for Backup Plans
  rationale: Configuring recovery point retention ensures that backup data is available for restoration when needed, minimizing data loss in disaster scenarios. This is crucial for meeting business continuity requirements and compliance obligations, such as data retention policies mandated by regulations like GDPR and HIPAA.
  description: This rule checks if recovery point retention is configured for GCP Backup and DR backup plans. To verify, inspect the backup plan settings in the GCP Console under Backup and DR, ensuring that a retention policy is specified. Remediation involves setting a retention period based on business needs and compliance requirements, using the GCP Console or gcloud CLI to adjust settings as necessary.
  references:
  - https://cloud.google.com/backup-disaster-recovery/docs/backup-plan
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/backup-disaster-recovery/docs/how-to/configure-backup-plan
  - https://cloud.google.com/backup-disaster-recovery/docs/how-to/manage-recovery-points
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_coverage_policy_assignme_complete
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Coverage Policy Assignme Complete
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Complete Backup Coverage in Backup Plan
  rationale: Complete backup coverage is critical for ensuring that all crucial data can be recovered in the event of a disaster. Incomplete backup assignments can lead to significant data loss, disrupt business operations, and potentially violate regulatory requirements such as GDPR or HIPAA. Ensuring thorough backup coverage mitigates risks related to data breaches and operational downtime.
  description: This rule checks if all necessary resources are included in the backup plan to ensure full recovery capability. It verifies that the resilience recovery backup coverage policy is assigned to all relevant backup plans. To remediate, review your backup plans and confirm that all critical resources are included. Utilize the GCP Console or gcloud commands to update and assign policies where necessary.
  references:
  - https://cloud.google.com/backup-and-dr/docs
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-34/rev-1/final
  - https://cloud.google.com/backup-and-dr/docs/configuring-backup-plans
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_coverage_unprotected_asse_enabled
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Coverage Unprotected Asse Enabled
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup Coverage for All Critical Assets in GCP Backup Plan
  rationale: Ensuring all critical assets are covered in backup plans enhances resilience by minimizing data loss and reducing downtime during disasters. Unprotected assets can lead to significant business impacts, including financial losses and operational disruptions. Compliance with standards like ISO 27001 and NIST requires comprehensive backup strategies, thus protecting the organization's reputation and customer trust.
  description: This rule checks that all critical assets are included in a GCP backup plan to ensure resilience and recovery capabilities. Verify that backup plans are configured to include all necessary resources by reviewing backup plan policies and settings in the GCP Console. Remediation involves identifying any unprotected assets and updating the backup plan to incorporate these resources, ensuring alignment with organizational recovery objectives.
  references:
  - https://cloud.google.com/backup-and-dr/docs/overview
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/backup-and-dr/docs/create-backup-plan
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_health_failed_jobs_alerti_enabled
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Health Failed Jobs Alerti Enabled
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Enable Alerting for Failed Backup Jobs in Backup Plans
  rationale: Alerting on failed backup jobs is crucial to ensure that data recovery processes are operational and reliable. Without timely alerts, failed backups could go unnoticed, potentially leading to data loss and impacting business continuity. This is especially critical in industries with stringent data protection regulations, where data availability and integrity are mandated.
  description: This rule checks whether alerting is enabled for failed backup jobs within Google Cloud Backup and DR backup plans. It verifies that notifications are configured to alert administrators in case of backup failures, allowing for prompt investigation and remediation. Administrators should ensure that alerting is enabled by configuring appropriate monitoring and notification settings in the Google Cloud Console. Remediation involves setting up Cloud Monitoring alerts to track backup job statuses and configuring notification channels for alert delivery.
  references:
  - https://cloud.google.com/backup-and-dr/docs/overview
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/backup-and-dr/docs/monitoring-backups
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_health_job_success_rate_minimum
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Health Job Success Rate Minimum
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup Plan Job Success Rate Meets Minimum Threshold
  rationale: Achieving a high success rate in backup jobs is crucial for ensuring data resilience and availability during disasters. A low success rate can lead to incomplete or unsuccessful data recovery, posing business continuity risks. Regulatory frameworks often mandate robust backup and recovery plans to protect sensitive data and maintain compliance.
  description: This rule verifies that the success rate of backup health jobs within a GCP Backup and Disaster Recovery (backupdr) plan meets or exceeds a specified minimum threshold. It checks the backup plan configurations to ensure that backups are consistently successful, identifying frequent failures that require attention. To remediate, review backup configurations, analyze failure logs, and adjust settings or resources to improve reliability. Regular monitoring and testing of backup operations are recommended to maintain compliance with resilience standards.
  references:
  - https://cloud.google.com/backup-and-dr/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/architecture/framework/security
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_health_last_backup_recency_w_days
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Health Last Backup Recency W Days
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup Health by Monitoring Last Backup Recency
  rationale: Timely and regular backups are crucial for maintaining business continuity and minimizing data loss in disaster recovery scenarios. Delayed or irregular backups can lead to significant risks, including data unavailability, potential non-compliance with data retention policies, and increased vulnerability to data loss from unforeseen incidents.
  description: This rule checks the recency of the last successful backup in the GCP Backup and DR service. It ensures that backup plans for critical resources are regularly executed within a specified number of days. To verify, access the Backup and DR dashboard and review the 'Last Backup' timestamp. Remediate by scheduling regular backups and configuring alerts for backup failures or delays to ensure compliance with organizational resilience policies.
  references:
  - https://cloud.google.com/backup-and-dr/docs/concepts
  - https://cloud.google.com/architecture/dr-scenarios
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_inventory_resource_discove_synced
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Inventory Resource Discove Synced
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup Inventory Resource Discovery is Synced
  rationale: Maintaining an up-to-date and accurate backup inventory is crucial for effective disaster recovery and business continuity planning. Unsynced or outdated resource inventories can lead to incomplete or ineffective recovery processes, potentially resulting in data loss, increased downtime, and non-compliance with industry regulations such as ISO 27001 or SOC2.
  description: This check verifies that the resource discovery for backup inventory within the backup plan is consistently synced. A correctly synchronized backup inventory ensures that all resources are accounted for in the event of a disaster recovery operation. To verify, ensure the backup plan settings include automatic synchronization of resource discovery and regularly audit the sync status. Remediation involves configuring the backup plan to enable regular sync operations and monitoring for any sync failures.
  references:
  - https://cloud.google.com/backup-and-dr/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/backup-and-dr/docs/troubleshooting
- rule_id: gcp.backupdr.backup_vault.backup_encryption_backup_storage_immutability_enabled
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Backup Storage Immutability Enabled
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Storage Immutability and Encryption
  rationale: Enabling encryption and immutability for backup storage ensures that sensitive data is protected from unauthorized access and tampering, reducing the risk of data breaches and ransomware attacks. This measure is critical for maintaining data integrity and meeting regulatory requirements such as GDPR and HIPAA, which mandate protection of personal and sensitive information.
  description: This rule checks that backup storage within the Google Cloud Backup and DR service is configured with encryption and immutability. Encryption at rest protects data by making it unreadable without the appropriate decryption keys, while immutability ensures that backups cannot be modified or deleted within a specified timeframe. Verify these settings in the GCP Console under Backup and DR policies, and enable encryption and immutability in the backup settings to comply with best practices and regulatory standards.
  references:
  - https://cloud.google.com/backup-dr/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.backupdr.backup_vault.backup_encryption_cmk_cmek_key_configured
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption CMK Cmek Key Configured
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vaults Use Customer-Managed Encryption Keys (CMEK)
  rationale: Encrypting backup data with Customer-Managed Encryption Keys (CMEK) strengthens data protection by providing customers with control over encryption keys. This enhances security against unauthorized access and aids in meeting regulatory compliance requirements such as GDPR and HIPAA by ensuring that sensitive data remains encrypted and under customer control.
  description: This rule checks whether backup vaults in GCP Backup and DR Service are configured to use Customer-Managed Encryption Keys (CMEK) for encrypting data at rest. To verify, examine the backup vault settings in the Google Cloud Console or use gcloud commands to ensure a CMEK is specified. Remediation involves configuring the backup vault to use a CMEK by assigning a Google Cloud Key Management Service (KMS) key to the vault settings, thereby enhancing data protection.
  references:
  - https://cloud.google.com/backup-dr/docs/using-cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/publications/sp-800-53r5-security-and-privacy-controls
- rule_id: gcp.backupdr.backup_vault.backup_encryption_enabled
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Enabled
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vault Encryption is Enabled
  rationale: Enabling encryption for backup vaults in GCP ensures that sensitive data is protected against unauthorized access, reducing the risk of data breaches and ensuring compliance with data protection regulations such as GDPR, HIPAA, and PCI-DSS. Without encryption, backups are vulnerable to unauthorized access, which could lead to data leaks and significant financial and reputational damage.
  description: This rule checks whether encryption is enabled for backup vaults in the GCP Backup and Disaster Recovery service. Encryption at rest should be enabled to protect backup data from unauthorized access. To verify, navigate to the Backup and Disaster Recovery settings in the GCP Console and ensure that encryption settings are configured. Remediation involves enabling Google-managed or customer-managed encryption keys (CMEK) for backup vaults.
  references:
  - https://cloud.google.com/backup-disaster-recovery/docs/encrypting-data
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_vault.backup_encryption_encryption_at_rest_enabled
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Encryption At Rest Enabled
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vaults Are Encrypted At Rest
  rationale: Encrypting backups at rest is critical to protect sensitive data from unauthorized access and potential data breaches. It ensures compliance with regulatory requirements such as GDPR and HIPAA, which mandate data protection measures. In the event of physical theft or unauthorized access to storage media, encryption at rest mitigates the risk of data compromise.
  description: This rule verifies that all backup vaults within the Backup and DR service have encryption at rest enabled. This is accomplished by checking the configuration settings of backup vaults to ensure that the 'encryption' property is set to 'true'. To remediate, ensure that encryption is enabled during the creation of backup vaults or by modifying existing vault settings. Regular audits should be conducted to maintain compliance.
  references:
  - https://cloud.google.com/backup-dr/docs/encryption-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
- rule_id: gcp.backupdr.backup_vault.backup_encryption_encryption_in_transit_tls_min_1_2
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Encryption In Transit TLS Min 1 2
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vault Uses TLS 1.2+ for In-Transit Encryption
  rationale: Utilizing TLS version 1.2 or higher for data in transit within Backup Vaults is crucial to protect sensitive data against interception and man-in-the-middle attacks. This practice helps avoid potential data breaches and aligns with compliance frameworks like PCI-DSS and HIPAA, which mandate strong encryption standards to safeguard customer information.
  description: This rule checks that Google Cloud's Backup Vault service is configured to use TLS version 1.2 or higher for encrypting data in transit. To verify compliance, ensure that all backup configurations use the appropriate encryption settings as specified in the GCP console under Backup settings. Remediation involves updating the configurations to enforce the use of TLS 1.2 or later, thereby enhancing the security posture of your backup data.
  references:
  - https://cloud.google.com/backup-and-dr/docs/encryption
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
- rule_id: gcp.backupdr.backup_vault.backup_encryption_key_policy_least_privilege
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Key Policy Least Privilege
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Least Privilege for Backup Vault Encryption Key Policy
  rationale: Applying the principle of least privilege to encryption key policies mitigates the risk of unauthorized access to backup vaults, preventing data breaches and ensuring compliance with data protection regulations. Limiting access reduces the potential attack surface, protecting sensitive data from malicious insiders or external threats.
  description: This rule checks whether the encryption key policy for backup vaults adheres to the principle of least privilege by ensuring only authorized users and services have access. Review IAM roles and permissions associated with the encryption keys to confirm that they are restricted to necessary personnel. Remediation involves auditing current permissions, removing unnecessary access, and aligning with organizational security policies.
  references:
  - https://cloud.google.com/backup-and-dr/docs/security
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.backupdr.backup_vault.backup_encryption_key_rotation_enabled
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Key Rotation Enabled
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vault Encryption Key Rotation is Enabled
  rationale: Regular rotation of encryption keys for backup vaults is crucial to mitigate the risk of unauthorized data access and to adhere to data protection regulations. Frequent key rotation reduces the potential impact of key compromise, ensuring that encrypted data remains secure over time. It is also a requirement for compliance with several data protection standards, providing a safeguard against evolving security threats.
  description: This rule checks whether the encryption keys for Google Cloud Backup Vaults are configured to rotate automatically. To verify, ensure that the key rotation schedule is set in the Cloud Key Management Service (KMS) for the keys associated with your backup vaults. Remediation involves configuring automatic key rotation in KMS and setting an appropriate rotation period, such as every 90 days, to enhance data security and compliance.
  references:
  - https://cloud.google.com/kms/docs/rotation
  - https://cloud.google.com/backup-and-dr/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_vault.enabled_for_vms
  service: backupdr
  resource: backup_vault
  requirement: Enabled For Vms
  scope: backupdr.backup_vault.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Backup Vaults are Enabled for VMs in GCP
  rationale: Enabling backup vaults for virtual machines (VMs) in GCP is critical for ensuring data resilience and disaster recovery. In the event of data loss or corruption, having backups allows for quick restoration, minimizing downtime and potential revenue loss. This practice also supports compliance with regulatory frameworks that mandate data integrity and availability, such as ISO 27001 and PCI-DSS.
  description: This rule checks if backup vaults are configured and enabled for virtual machines within the GCP environment. Ensuring that backup vaults are operational involves verifying that backup policies are correctly applied to VMs and that the backups are regularly scheduled. Remediation involves configuring the backup vault through the GCP Console or command-line interface to include all necessary VMs and setting appropriate backup schedules to meet organizational recovery objectives.
  references:
  - https://cloud.google.com/backup-and-disaster-recovery/docs/backup-overview
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_access_allowed_cidrs_minimized
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Access Allowed Cidrs Minimized
  scope: bigquery.connection.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit Allowed CIDRs for BigQuery Connection Endpoints
  rationale: Restricting CIDR ranges for BigQuery connection endpoints minimizes exposure to unauthorized access, thereby reducing the risk of data breaches and unauthorized data manipulation. This is crucial for protecting sensitive data and ensuring compliance with regulatory frameworks such as GDPR and CCPA, which mandate strict control over data access.
  description: This rule checks whether the CIDR ranges allowed for BigQuery connection endpoints are minimized to only those necessary for legitimate access. It ensures that only specific, trusted IP ranges can access the BigQuery data warehouse, lowering the risk of malicious actors exploiting overly permissive access controls. Verification involves reviewing the CIDR configurations in the BigQuery connection settings and adjusting them to the least permissive ranges necessary for operational requirements.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_access_private_only
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Access Private Only
  scope: bigquery.connection.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict BigQuery Connections to Private Networks Only
  rationale: Limiting BigQuery connections to private networks reduces exposure to unauthorized access and potential data breaches. This approach aligns with best practices for network security by minimizing attack surfaces and ensuring data is only accessible through secure, controlled environments. It is critical for compliance with regulations that require safeguarding data through restricted network access.
  description: This rule checks if all BigQuery connections are configured to use private networking, ensuring that data access is restricted to approved private endpoints. To verify, review the network settings in the BigQuery console to confirm the 'Private Service Connect' option is enabled. Remediation involves updating connections to use private endpoints, minimizing external network exposure.
  references:
  - https://cloud.google.com/bigquery/docs/reference/private-ip
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_access_tls_required
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Access TLS Required
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS for BigQuery Data Warehouse Endpoint Access
  rationale: Enforcing TLS for BigQuery connections ensures data integrity and confidentiality during transit. Without TLS, data transferred between clients and the BigQuery service can be intercepted or tampered with, posing a risk of data breaches. Ensuring TLS compliance helps meet regulatory standards such as GDPR and HIPAA, which mandate secure data transmission practices.
  description: This rule checks if TLS is enforced for data warehouse endpoints in BigQuery connections. It ensures that any connection to the BigQuery service uses TLS to encrypt data in transit. To verify, inspect the configuration settings under BigQuery connections and ensure the 'requireTLS' property is set to true. Remediation involves updating the settings to enforce TLS for all connections, which can typically be configured via the GCP Console or using the bq command-line tool.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-overview
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_authz_no_anonymous_access
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Authz No Anonymous Access
  scope: bigquery.connection.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Anonymous Access to BigQuery Connection Endpoints
  rationale: Allowing anonymous access to BigQuery connection endpoints can lead to unauthorized data retrieval or manipulation, posing significant risks to data confidentiality and integrity. This can result in non-compliance with regulations like GDPR and HIPAA, and expose organizations to potential data breaches, financial penalties, and reputational damage.
  description: This rule checks if BigQuery connection endpoints are configured to prevent anonymous access by ensuring authorization is required for all connections. To verify, ensure that all connection endpoints have IAM policies granting access only to authenticated users with appropriate roles. Remediation involves reviewing and updating IAM policies to restrict access to authorized identities only, removing any wildcard ('allUsers' or 'allAuthenticatedUsers') permissions.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/projects.connections
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/docs/enterprise/best-practices-for-identity-and-access-management#restrict-usage
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_authz_rbac_least_privilege
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Authz RBAC Least Privilege
  scope: bigquery.connection.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure BigQuery Endpoint Uses RBAC with Least Privilege
  rationale: Implementing RBAC with the principle of least privilege on BigQuery connections minimizes the risk of unauthorized data access and potential data breaches. This approach helps organizations comply with regulatory standards and reduces the attack surface by limiting permissions to only those necessary for specific roles. Ensuring least privilege is critical in environments where sensitive data is processed or stored, aligning with security best practices and compliance mandates.
  description: This rule checks that all BigQuery connection endpoints enforce Role-Based Access Control (RBAC) with the principle of least privilege. Verify that roles assigned to users or service accounts have only the necessary permissions required for their specific tasks. Remediation involves reviewing current roles and permissions, removing any excessive access rights, and utilizing predefined roles where applicable to enforce strict access controls.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_client_cert_key_length_minimum
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Client Cert Key Length Minimum
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure HSM Client Cert Key Length Meets Minimum Requirements
  rationale: Using a minimum key length for HSM client certificates in BigQuery connections ensures robust encryption, protecting sensitive data against brute-force attacks. Shorter key lengths can expose data to unauthorized access, which may lead to data breaches and violate compliance with standards such as PCI-DSS and HIPAA.
  description: This rule checks that the HSM client certificate key length for BigQuery connections is at least 2048 bits. To verify, review the key length configuration in the BigQuery connection settings. If the key length is shorter, regenerate the certificate with a compliant key length and update the connection configuration accordingly.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/Connection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_client_cert_not_expired
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Client Cert Not Expired
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery HSM Client Certificates Are Valid
  rationale: Expired HSM client certificates in BigQuery connections can lead to unauthorized data access, data breaches, and non-compliance with data protection regulations. Ensuring that these certificates are valid helps maintain secure data transfer and aligns with regulatory requirements such as PCI-DSS and HIPAA, reducing the risk of data exposure and financial penalties.
  description: This rule checks whether the HSM client certificates used in BigQuery connections have expired. Expired certificates can compromise encrypted data integrity and confidentiality. Administrators should regularly monitor certificate expiration dates and renew certificates before expiration to maintain secure connections. Verification can be done through the GCP Console or CLI by reviewing the certificate's validity period associated with the BigQuery connection.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/Connection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_client_cert_trusted_issuer
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Client Cert Trusted Issuer
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery HSM Client Cert Has Trusted Issuer
  rationale: Trusted issuers for HSM client certificates in BigQuery ensure that only authorized entities can establish secure connections, mitigating risks of data breaches and unauthorized access. This is crucial for maintaining data confidentiality and integrity, especially in environments handling sensitive information. Compliance with industry standards and regulatory requirements often mandates the use of trusted certificate authorities.
  description: This rule checks that BigQuery connections using HSM client certificates originate from trusted certificate issuers. To validate, ensure that the issuer's certificate is listed in the trusted certificate authorities of your GCP environment. Remediation involves updating the list of trusted issuers or contacting your certificate authority to address any discrepancies. Proper configuration helps prevent unauthorized data access and supports encryption at rest policies.
  references:
  - https://cloud.google.com/bigquery/docs/hardware-security-modules
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_configuration_present
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Configuration Present
  scope: bigquery.connection.configuration_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery HSM Configuration for Data Warehouse Connections
  rationale: Configuring a Hardware Security Module (HSM) for BigQuery connections enhances data protection by ensuring that encryption keys are securely stored and managed. This reduces the risk of unauthorized data access and helps meet compliance requirements for data protection standards such as PCI-DSS and HIPAA, which mandate strong encryption measures for sensitive data.
  description: This rule checks for the presence of an HSM configuration in BigQuery connection settings, ensuring that data is encrypted at rest using keys managed by a secure external source. To verify, inspect the BigQuery connection configurations to ensure they reference a Cloud HSM key. Remediation involves configuring your BigQuery connections to use Cloud HSM for managing encryption keys, which can be done via the GCP Console or the gcloud command-line tool.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/hsm/docs
  - https://cloud.google.com/bigquery/docs/reference/rest
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_keys_in_hsm_only
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Keys In Hsm Only
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure HSM-Only Storage for BigQuery HSM Keys
  rationale: Storing HSM keys securely in HSMs is critical to prevent unauthorized access and ensure that cryptographic operations are performed in a tamper-resistant environment. This reduces the risk of data breaches and supports compliance with data protection regulations like PCI-DSS and HIPAA, which mandate strong encryption management practices.
  description: This rule checks whether BigQuery connections are configured to use HSM-stored keys exclusively for encryption at rest. Verify that all relevant BigQuery configurations utilize Cloud HSM for key management by inspecting the key ring and key location settings in your GCP project. Remediation involves updating BigQuery connections to point to HSM-stored keys in Cloud KMS, ensuring that encryption keys never leave the secure boundaries of the HSM.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs/hsm
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt3r1.pdf
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_only_approved_hsm_endpoints
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Only Approved Hsm Endpoints
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict BigQuery HSM Connections to Approved Endpoints
  rationale: Restricting BigQuery connections to approved HSM endpoints ensures data encryption keys are managed securely, preventing unauthorized access and maintaining data integrity. This is critical for protecting sensitive data against breaches and meeting compliance with standards like PCI-DSS and HIPAA, which mandate stringent encryption practices.
  description: This rule checks that connections to BigQuery data warehouses are only established with approved HSM endpoints, ensuring that encryption keys are securely managed and stored. Verification involves reviewing the list of configured HSM endpoints against approved lists in your organization's security policies. Remediation requires updating connection settings to exclude unapproved endpoints and ensuring all connections route through approved HSMs.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.connection.datalake_credentials_in_secrets_manager
  service: bigquery
  resource: connection
  requirement: Datalake Credentials In Secrets Manager
  scope: bigquery.connection.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Store Datalake Credentials Using GCP Secrets Manager
  rationale: Storing Datalake credentials directly in code or unprotected environments poses significant security risks, including unauthorized data access and potential data breaches. By using GCP Secrets Manager, organizations can centrally manage, audit, and control access to these sensitive credentials, aligning with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that Datalake credentials used in BigQuery connections are securely stored in GCP Secrets Manager instead of being hardcoded or stored in less secure environments. To verify, review the BigQuery connection settings and ensure that any credentials are referenced via Secrets Manager. Remediation involves migrating existing credentials to Secrets Manager and updating connection configurations to retrieve credentials from there.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.connection.datalake_private_networking_enforced
  service: bigquery
  resource: connection
  requirement: Datalake Private Networking Enforced
  scope: bigquery.connection.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Datalake Private Networking for BigQuery Connections
  rationale: Enforcing private networking for BigQuery connections mitigates the risk of data exposure by restricting access to internal networks, reducing the attack surface for unauthorized access. This is crucial for maintaining data integrity and confidentiality, especially in environments handling sensitive or regulated data. Compliance with standards such as PCI-DSS and HIPAA often requires stringent network access controls to prevent unauthorized data access.
  description: This rule checks if BigQuery connections are configured to use private networking, ensuring that data interactions occur within the confines of a secure, internal network. To verify compliance, inspect the BigQuery connection settings to confirm that private IP is enforced. Remediation involves updating the connection settings to enforce private networking through the use of VPC Service Controls and private Google access. This configuration helps maintain a secure data environment by limiting exposure to the public internet.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/Connection
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.connection.datalake_role_least_privilege
  service: bigquery
  resource: connection
  requirement: Datalake Role Least Privilege
  scope: bigquery.connection.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure BigQuery Connections Use Least Privilege Roles
  rationale: Restricting roles to the least privilege necessary reduces the risk of unauthorized data access and potential data breaches. Over-privileged roles can lead to inadvertent data exposure, impacting business confidentiality and compliance with regulations such as GDPR and HIPAA.
  description: This rule checks that BigQuery connections are assigned the minimal roles necessary to perform their functions. Review and adjust IAM policies to ensure roles do not exceed the required permissions. Use predefined roles where possible and customize only when necessary to align with least privilege principles. Regular audits and updates of permissions should be conducted to adapt to changing requirements and threats.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0
  - NIST SP 800-53 Access Control (AC)
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.bigquery.connection.datalake_tls_required
  service: bigquery
  resource: connection
  requirement: Datalake TLS Required
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS is Enabled for BigQuery Datalake Connections
  rationale: Requiring TLS for BigQuery Datalake connections helps protect sensitive data from interception and unauthorized access during transmission. Failing to enforce TLS can lead to data breaches, non-compliance with data protection regulations like GDPR and HIPAA, and potentially severe financial and reputational damage to the organization.
  description: This rule checks if TLS is enforced for connections to BigQuery Datalake. TLS encryption ensures that data in transit is secure and protected against eavesdropping and man-in-the-middle attacks. To verify, review the connection settings in the Google Cloud Console or use gcloud commands to ensure `--require-tls` is set to true. If TLS is not enforced, update the connection settings to enable TLS by following the Google Cloud documentation for configuring secure connections.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.bigquery.dataset.backup_enabled
  service: bigquery
  resource: dataset
  requirement: Backup Enabled
  scope: bigquery.dataset.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure BigQuery Datasets Have Backup and Recovery Configured
  rationale: Enabling backups for BigQuery datasets is crucial for data resilience and disaster recovery. In the event of accidental data deletion or corruption, maintaining up-to-date backups enables quick restoration of data, minimizing business disruption. Non-compliance with data protection regulations like GDPR or HIPAA can result in significant legal and financial penalties.
  description: This rule verifies that BigQuery datasets have an appropriate backup and recovery configuration. It checks for the presence of mechanisms such as scheduled exports to Google Cloud Storage or Data Transfer Service for automated backups. To remediate, configure regular data exports to a secure storage location, ensuring backup integrity and accessibility. Verify the backup settings through the GCP Console or by using command-line tools like 'bq' to ensure that backup procedures are active and properly configured.
  references:
  - https://cloud.google.com/bigquery/docs/managing-tables#copying_a_table
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#resource
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 5.2
  - 'NIST SP 800-53 Rev. 5 CP-9: Information System Backup'
  - 'PCI-DSS Requirement 9.5.1: Protect stored cardholder data'
  - https://cloud.google.com/solutions/bigquery-data-transfer-service
- rule_id: gcp.bigquery.dataset.classification_configured
  service: bigquery
  resource: dataset
  requirement: Classification Configured
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Dataset Classification is Configured
  rationale: Configuring classification for BigQuery datasets is crucial for data protection and privacy, as it helps categorize and apply appropriate security controls to sensitive data. Failure to configure classification can result in inadequate data protection, potentially leading to unauthorized access, data breaches, and non-compliance with data privacy regulations such as GDPR, HIPAA, and PCI-DSS.
  description: This rule checks if classification is configured for BigQuery datasets, ensuring that data is properly categorized and protected. Verify that datasets have appropriate classification tags and labels configured in the BigQuery console or via API. Remediation involves reviewing your datasets, identifying the sensitivity of the data they contain, and applying suitable classification labels to enhance security measures and meet compliance requirements.
  references:
  - https://cloud.google.com/bigquery/docs/labeling-datasets
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/security
  - https://cloud.google.com/security/compliance/frameworks
- rule_id: gcp.bigquery.dataset.data_analytics_access_policies_least_privilege
  service: bigquery
  resource: dataset
  requirement: Data Analytics Access Policies Least Privilege
  scope: bigquery.dataset.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure BigQuery Datasets Use Least Privilege Access
  rationale: Implementing least privilege access for BigQuery datasets minimizes the risk of data breaches by ensuring users and services have only the permissions they need to perform their tasks. This approach reduces the potential attack surface and limits damage in case of compromised credentials, thus supporting compliance with privacy regulations such as GDPR and CCPA.
  description: This rule checks BigQuery dataset access policies to ensure they adhere to the principle of least privilege. It verifies that permissions are granted based on necessity rather than convenience, avoiding overly broad roles like 'Editor' or 'Owner' for routine data analytics tasks. To remediate, audit all IAM policies associated with BigQuery datasets and restrict roles to the minimum required for functionality, using predefined roles or custom roles where necessary.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 7.1
  - NIST SP 800-53 Rev. 5 - AC-6 Least Privilege
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.bigquery.dataset.data_analytics_catalog_cross_account_sharing_restricted
  service: bigquery
  resource: dataset
  requirement: Data Analytics Catalog Cross Account Sharing Restricted
  scope: bigquery.dataset.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict BigQuery Dataset Cross-Account Sharing
  rationale: Limiting cross-account sharing of BigQuery datasets mitigates security risks associated with unauthorized access and data leakage. This is crucial for maintaining data confidentiality, integrity, and availability, as well as ensuring compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks for any instances of BigQuery datasets being shared with external accounts or domains outside of your organization. It is important to verify that datasets are only shared with trusted entities and to audit sharing configurations regularly. Remediation involves reviewing dataset IAM policies, removing unauthorized accounts, and implementing strict sharing controls to prevent inadvertent data exposure.
  references:
  - https://cloud.google.com/bigquery/docs/shared-datasets#restricting_access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset.data_analytics_catalog_metadata_encryption_enabled
  service: bigquery
  resource: dataset
  requirement: Data Analytics Catalog Metadata Encryption Enabled
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Dataset Metadata Encryption is Enabled
  rationale: Enabling encryption for BigQuery dataset metadata protects sensitive data from unauthorized access and exposure. This is crucial for maintaining data confidentiality, preventing data breaches, and complying with regulatory standards such as HIPAA and PCI-DSS which mandate encryption of sensitive information. By encrypting metadata, organizations reduce the risk of data theft and enhance trust with stakeholders.
  description: This rule checks if BigQuery datasets have encryption enabled for their metadata using Customer-Managed Encryption Keys (CMEK). By default, Google Cloud encrypts data at rest using Google-managed keys. For enhanced security, configure datasets to use CMEK, which offers greater control over encryption keys. Verification can be done via the Google Cloud Console or gcloud CLI by inspecting the encryption configuration of each dataset. To remediate, update the dataset's encryption settings to use a Cloud Key Management Service (KMS) key.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.hhs.gov/hipaa
- rule_id: gcp.bigquery.dataset.data_analytics_catalog_rbac_least_privilege
  service: bigquery
  resource: dataset
  requirement: Data Analytics Catalog RBAC Least Privilege
  scope: bigquery.dataset.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege in BigQuery Dataset Access
  rationale: Adhering to the principle of least privilege in BigQuery ensures that users have only the permissions necessary to perform their job functions, reducing the risk of data breaches and unauthorized data access. This is crucial for protecting sensitive data and maintaining compliance with regulatory requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strict access controls to safeguard personal and financial information.
  description: This rule checks that IAM roles assigned to users and service accounts on BigQuery datasets are limited to the minimum necessary permissions. Roles such as 'bigquery.dataViewer' should be used instead of broader roles like 'bigquery.admin'. Regularly audit and review IAM policies associated with datasets to ensure compliance with the least privilege principle. Implementing resource-level access controls can effectively minimize the risk of unauthorized access and data leaks.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.bigquery.dataset.data_analytics_encrypted_at_rest_cmek
  service: bigquery
  resource: dataset
  requirement: Data Analytics Encrypted At Rest Cmek
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Datasets Use CMEK for Encryption at Rest
  rationale: Encrypting BigQuery datasets with Customer-Managed Encryption Keys (CMEK) at rest enhances data security by providing greater control over encryption keys. This reduces the risk of unauthorized access and meets compliance requirements for data protection, such as those in GDPR, HIPAA, and other frameworks. It helps safeguard sensitive data from potential breaches and insider threats.
  description: This rule checks if BigQuery datasets are encrypted using CMEK, offering stronger security by allowing customers to manage and rotate their own encryption keys. Verify that CMEK is configured by checking the dataset's encryption configuration settings in the GCP Console or via the `bq` command-line tool. To remediate, update the dataset to use a CMEK by applying a key from Cloud Key Management Service (KMS) and ensure that the necessary IAM permissions are granted to BigQuery to use the key.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.dataset.data_analytics_group_external_group_sharing_restricted
  service: bigquery
  resource: dataset
  requirement: Data Analytics Group External Group Sharing Restricted
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict External Sharing for BigQuery Data Analytics Groups
  rationale: Restricting external sharing of datasets in BigQuery minimizes the risk of unauthorized data access, which could lead to data breaches or loss of sensitive information. This control helps organizations comply with data protection regulations such as GDPR and CCPA by ensuring that data is only shared with trusted entities.
  description: This rule checks that datasets in BigQuery are not shared with external groups outside of the organization. To verify compliance, inspect the dataset's IAM policies to ensure that no external identities have been granted access. Remediation involves reviewing and modifying IAM policies to remove any permissions granted to external groups, thus limiting data sharing to internal users or trusted partners only.
  references:
  - https://cloud.google.com/bigquery/docs/security-iam
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.bigquery.dataset.data_analytics_group_roles_minimal
  service: bigquery
  resource: dataset
  requirement: Data Analytics Group Roles Minimal
  scope: bigquery.dataset.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict BigQuery Dataset Roles for Data Analytics Group
  rationale: Limiting roles to the Data Analytics Group ensures that users have only the necessary permissions, reducing the risk of data exposure and unauthorized access. This adherence to the principle of least privilege helps prevent potential data breaches and supports compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks that BigQuery datasets do not grant excessive permissions to data analytics groups, specifically ensuring roles are limited to only what is necessary for analytics tasks. Verify that roles such as 'roles/bigquery.dataViewer' or 'roles/bigquery.dataEditor' are assigned instead of broader roles like 'roles/bigquery.admin'. Remediation involves reviewing IAM policies and adjusting the roles to align with the principle of least privilege, ensuring compliance with security best practices.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/bigquery/docs/security
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset.data_analytics_public_sharing_disabled
  service: bigquery
  resource: dataset
  requirement: Data Analytics Public Sharing Disabled
  scope: bigquery.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Sharing for BigQuery Datasets
  rationale: Allowing public sharing of BigQuery datasets can expose sensitive data to unauthorized users, leading to data breaches and compliance violations. Public access increases the risk of data exfiltration and non-compliance with regulations such as GDPR, HIPAA, and PCI-DSS. Ensuring datasets are not publicly shared helps mitigate these threats and protect organizational assets.
  description: This rule checks for public access permissions on BigQuery datasets and ensures that sharing with 'allUsers' and 'allAuthenticatedUsers' is disabled. To verify, review the Access Control settings of BigQuery datasets and remove any entries that allow public access. Remediation involves using the GCP Console or CLI to modify IAM policies, ensuring datasets are shared only with specific, authorized users or groups.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/itl/applied-cybersecurity/nist-special-publication-800-53
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.dataset.data_analytics_row_level_security_enabled
  service: bigquery
  resource: dataset
  requirement: Data Analytics Row Level Security Enabled
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Row-Level Security for BigQuery Datasets
  rationale: Enabling row-level security in BigQuery datasets is crucial for protecting sensitive data by restricting access to specific rows based on user roles. This minimizes the risk of unauthorized data exposure and helps meet compliance requirements like HIPAA and GDPR, which mandate strict data access controls.
  description: This rule checks if row-level security is enabled for BigQuery datasets, ensuring that access to data is controlled at a granular level. To verify, examine the dataset's access policies to see if row-level access controls are configured. Remediation involves setting up appropriate row-level security policies using GCP's BigQuery interface or API, ensuring that only authorized users can view specific data rows.
  references:
  - https://cloud.google.com/bigquery/docs/row-level-security-intro
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://hipaa.jotform.com/what-is-hipaa-compliance/
  - https://gdpr.eu/what-is-gdpr/
- rule_id: gcp.bigquery.dataset.data_analytics_workgroup_allowed_data_sources_allowlist
  service: bigquery
  resource: dataset
  requirement: Data Analytics Workgroup Allowed Data Sources Allowlist
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Data Analytics Workgroup to Approved BigQuery Data Sources
  rationale: Allowing unrestricted data source access can lead to unauthorized data exposure and breaches. Limiting access to predefined data sources mitigates the risk of data leakage and ensures compliance with regulations such as GDPR and HIPAA by enforcing strict data governance and control.
  description: This rule checks if BigQuery datasets accessed by the Data Analytics Workgroup are limited to a specific allowlist of data sources. Verify that the IAM policies and access configurations for the datasets are set to restrict access only to datasets approved by your organizationâ€™s data governance policies. To remediate, configure IAM roles and permissions to enforce access only to the allowed data sources, thereby ensuring data integrity and compliance with internal policies.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/security-and-compliance-overview
  - https://cloud.google.com/architecture/bigquery-data-governance-guide
- rule_id: gcp.bigquery.dataset.data_analytics_workgroup_network_access_private
  service: bigquery
  resource: dataset
  requirement: Data Analytics Workgroup Network Access Private
  scope: bigquery.dataset.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure BigQuery Dataset Access via Private Networks
  rationale: Restricting BigQuery dataset access to private networks reduces exposure to unauthorized access and potential data breaches. This practice aligns with security best practices by limiting the attack surface and ensures compliance with regulations like HIPAA and PCI-DSS that emphasize data security and privacy.
  description: This rule verifies that BigQuery datasets are only accessible through private IPs by configuring VPC Service Controls and enabling Private Google Access. To confirm, ensure that datasets are part of a service perimeter and that network access is restricted to private subnets. Remediation involves setting up appropriate network policies and VPC configurations to enforce private access.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/security-controls
- rule_id: gcp.bigquery.dataset.data_analytics_workgroup_query_result_encryption_enabled
  service: bigquery
  resource: dataset
  requirement: Data Analytics Workgroup Query Result Encryption Enabled
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Query Result Encryption for Data Analytics
  rationale: Encrypting query results at rest protects sensitive data from unauthorized access, minimizing the risk of data breaches and ensuring compliance with data protection regulations such as GDPR and HIPAA. Without encryption, query results may be exposed to internal and external threats, potentially leading to significant financial and reputational damage.
  description: This rule checks that BigQuery datasets used by data analytics workgroups have query result encryption enabled, using either Google-managed or customer-managed encryption keys (CMEK). To verify, ensure that datasets are configured with the 'encryptionConfiguration.kmsKeyName' property set. Remediation involves updating the dataset's encryption settings through the GCP Console or gcloud command-line tool by specifying a valid KMS key.
  references:
  - https://cloud.google.com/bigquery/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_admin_access_least_privilege
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster Admin Access Least Privilege
  scope: bigquery.dataset.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Least Privilege for BigQuery Dataset Admin Access
  rationale: Over-provisioned access to BigQuery datasets can lead to unauthorized data exposure and manipulation, potentially resulting in data breaches and regulatory compliance failures (e.g., GDPR, HIPAA). Minimizing access rights to only what is necessary reduces the attack surface and helps prevent both insider and external threats.
  description: This rule verifies that the IAM roles associated with BigQuery datasets are configured to adhere to the principle of least privilege. Specifically, it checks for and flags overly permissive roles like 'roles/bigquery.admin' assigned at the dataset level. Remediation involves auditing existing permissions, removing unnecessary roles, and using predefined roles such as 'roles/bigquery.dataEditor' or custom roles tailored to specific user needs to limit access appropriately.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_audit_logging_enabled
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster Audit Logging Enabled
  scope: bigquery.dataset.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure BigQuery Audit Logging is Enabled for Datasets
  rationale: Enabling audit logging for BigQuery datasets is crucial for monitoring access and changes to data, which helps in detecting unauthorized access and potential data breaches. Audit logs play a key role in fulfilling compliance requirements like GDPR and HIPAA, which mandate tracking data access and modifications to protect sensitive information.
  description: This rule verifies that audit logging is enabled for BigQuery datasets, ensuring all access and changes are logged for review. To check, navigate to the Google Cloud Console, select the relevant dataset, and ensure that audit logging is configured. Remediation involves enabling audit logging by setting up the appropriate logging configurations in the GCP Console under the 'Logging' section for BigQuery datasets.
  references:
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_encryption_at_rest_cmek
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster Encryption At Rest Cmek
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Dataset Uses CMEK for Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encrypting BigQuery datasets at rest enhances data security by allowing organizations to have full control over their encryption keys. This reduces the risk of unauthorized data access and can help meet regulatory requirements such as GDPR and HIPAA, which mandate stringent data protection measures.
  description: This rule checks if BigQuery datasets are encrypted with Customer-Managed Encryption Keys (CMEK) instead of Google-managed keys. To verify, ensure that the 'encryptionConfiguration' field is set with a valid 'kmsKeyName' in the dataset's configuration. If not configured, update the dataset settings to use a key from Google Cloud KMS to enable CMEK. This ensures that datasets are protected with keys managed by your organization, offering an additional layer of security.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_private_networking_enforced
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster Private Networking Enforced
  scope: bigquery.dataset.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for BigQuery Data Warehouse Clusters
  rationale: Ensuring that BigQuery data warehouse clusters are configured with private networking mitigates the risk of unauthorized access over the public internet. This configuration helps protect sensitive data from exposure and is crucial for compliance with regulations such as GDPR and HIPAA, which mandate stringent data protection measures. By restricting access to private IP addresses, organizations can better manage network traffic and reduce the attack surface.
  description: This rule verifies that BigQuery datasets designated as data warehouses are configured to enforce private networking. Specifically, this involves ensuring that the datasets are only accessible via internal IP addresses and not exposed to the public internet. To verify compliance, check the dataset's network configuration in the GCP Console or via the gcloud CLI, ensuring that 'privateServiceConnect' is enabled. Remediation involves updating the dataset's networking settings to restrict access only to the private network.
  references:
  - https://cloud.google.com/bigquery/docs/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/architecture/using-vpc-service-controls-to-protect-data
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_tls_min_1_2_enforced
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster TLS Min 1 2 Enforced
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery TLS Version 1.2+ for Data Warehouse Cluster
  rationale: Enforcing a minimum TLS version of 1.2 for BigQuery datasets helps protect sensitive data from interception and tampering during transit. This is crucial for maintaining data privacy and integrity, reducing the risk of data breaches and ensuring compliance with data protection regulations like GDPR and HIPAA. Organizations can avoid potential financial penalties and reputational damage by adhering to these security standards.
  description: This rule checks that all BigQuery datasets in a data warehouse cluster enforce a minimum TLS version of 1.2. To verify, ensure that your BigQuery configurations specify TLS 1.2 or higher for data in transit. If a dataset does not meet this requirement, update the dataset configuration via the Google Cloud Console or use the bq command-line tool to enforce the required TLS version. Regular audits should be conducted to ensure compliance.
  references:
  - https://cloud.google.com/bigquery/docs/encryption#encryption_in_transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/nist-special-publication-800-52-revision-2-guidelines-selection-configuration-and-use
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.bigquery.dataset.datalake_database_cross_account_sharing_restricted
  service: bigquery
  resource: dataset
  requirement: Datalake Database Cross Account Sharing Restricted
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account BigQuery Dataset Sharing
  rationale: Restricting cross-account sharing of BigQuery datasets helps prevent unauthorized access to sensitive data, mitigating risks of data breaches and ensuring compliance with privacy laws such as GDPR and CCPA. It also protects intellectual property by limiting data access to trusted entities within the organization, reducing the attack surface.
  description: This rule checks if BigQuery datasets are being shared with accounts outside the organization's domain. By ensuring datasets are only shared internally, organizations can protect data from unauthorized access. Verify by reviewing dataset IAM policies and ensure external accounts are not granted roles. Remediate by removing any permissions granted to external accounts and regularly auditing access controls.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.bigquery.dataset.datalake_database_encrypted
  service: bigquery
  resource: dataset
  requirement: Datalake Database Encrypted
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Datasets Are Encrypted At Rest
  rationale: Encrypting BigQuery datasets at rest protects sensitive data from unauthorized access and breaches, which can lead to data theft and financial losses. It is crucial for meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS, as well as maintaining trust with stakeholders by ensuring that data is stored securely.
  description: This rule checks if BigQuery datasets are encrypted using Customer-Managed Encryption Keys (CMEK) to enhance data protection over the default Google-managed keys. Verify that datasets have CMEK configured by reviewing the dataset encryption settings in the Google Cloud Console or via the gcloud command-line tool. To remediate, configure CMEK for BigQuery datasets by creating a Cloud Key Management Service (KMS) key and associating it with the dataset.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/patch
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.bigquery.dataset.datalake_database_policy_least_privilege
  service: bigquery
  resource: dataset
  requirement: Datalake Database Policy Least Privilege
  scope: bigquery.dataset.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure BigQuery Dataset Policies Enforce Least Privilege
  rationale: Implementing the principle of least privilege in BigQuery dataset policies minimizes the risk of unauthorized access and data breaches. Over-permissioned roles can lead to data exfiltration, unauthorized data manipulation, and compliance violations, impacting business continuity and integrity. Compliance with frameworks like NIST and HIPAA requires stringent access controls to protect sensitive data.
  description: This rule checks that all BigQuery datasets have IAM policies granting only the necessary permissions required for specific roles, avoiding broad permissions like 'Owner' or overly permissive custom roles. Verify the IAM policies using the Google Cloud Console or gcloud CLI to ensure roles are appropriately assigned. Remediation involves reviewing and modifying IAM policies to align with least privilege principles, creating custom roles if necessary, and regularly auditing permissions.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.bigquery.dataset.dataset_cmk_encryption_configured
  service: bigquery
  resource: dataset
  requirement: Dataset CMK Encryption Configured
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Datasets Use Customer-Managed Encryption Keys
  rationale: Using Customer-Managed Encryption Keys (CMKs) for BigQuery datasets enhances data protection by allowing organizations to control and rotate encryption keys, mitigating the risk of unauthorized access. This is especially crucial for sensitive data, helping to meet regulatory requirements such as GDPR, HIPAA, and PCI-DSS, and protecting against data breaches and insider threats.
  description: This rule verifies that BigQuery datasets are configured to use Customer-Managed Encryption Keys (CMKs) rather than the default Google-managed keys. To ensure compliance, check the dataset settings in the Google Cloud Console or use the bq command-line tool to confirm that a CMK is specified. To remediate, create a Cloud KMS key in the desired location and update the dataset's encryption settings to use this key. This process involves setting the kmsKeyName property in the dataset's encryptionConfiguration.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 6.8
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource
  - https://cloud.google.com/kms/docs/rotating-keys
- rule_id: gcp.bigquery.dataset.dataset_public_access_configured
  service: bigquery
  resource: dataset
  requirement: Dataset Public Access Configured
  scope: bigquery.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure BigQuery Datasets Are Not Publicly Accessible
  rationale: Public access to BigQuery datasets poses significant security risks, including unauthorized data exposure and potential data breaches. This can lead to non-compliance with data protection regulations such as GDPR and CCPA, resulting in financial penalties and damage to organizational reputation. Protecting datasets by restricting public access helps mitigate these risks and safeguard sensitive information.
  description: This rule checks for datasets in BigQuery that are configured with public access, meaning any user on the internet can access the data without authentication. Verify that the dataset's permissions do not include 'allUsers' or 'allAuthenticatedUsers' roles, which grant public access. To remediate, update the IAM policy of the dataset to remove these roles and ensure access is limited to authorized users only. This can be done via the Google Cloud Console, gcloud command-line tool, or the BigQuery API.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/setIamPolicy
- rule_id: gcp.bigquery.dataset.default_cmek_encryption_configured
  service: bigquery
  resource: dataset
  requirement: Default Cmek Encryption Configured
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Datasets Use Default CMEK Encryption
  rationale: Configuring Customer-Managed Encryption Keys (CMEK) for BigQuery datasets protects sensitive data by providing control over encryption keys, reducing the risk of unauthorized data access. This is essential for meeting compliance requirements like GDPR and HIPAA, which mandate strict data protection measures. Additionally, using CMEK helps mitigate the risk of data breaches by ensuring that access to encryption keys is tightly controlled and auditable.
  description: This rule checks that all BigQuery datasets have a default Customer-Managed Encryption Key (CMEK) configured. To verify, ensure that each dataset in your Google Cloud project specifies a CMEK in its encryption configuration. If not configured, update the dataset settings in the Google Cloud Console or use the `bq` command-line tool to specify a key from Cloud Key Management Service (Cloud KMS). This will ensure that data is encrypted at rest using keys that you control.
  references:
  - https://cloud.google.com/bigquery/docs/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.bigquery.dataset.public_access_configured
  service: bigquery
  resource: dataset
  requirement: Public Access Configured
  scope: bigquery.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Access to BigQuery Datasets
  rationale: Public access to BigQuery datasets can expose sensitive data to unauthorized users, leading to data breaches and compliance violations. Organizations risk reputational damage and financial penalties if confidential information is leaked. Securing datasets aligns with regulatory requirements like GDPR, HIPAA, and PCI-DSS, which mandate strict access controls to protect personal and financial data.
  description: This rule checks if any BigQuery datasets are configured with public access, which allows any user with an internet connection to read or modify data. To verify, inspect the dataset's permissions in the Google Cloud Console or via gcloud CLI for entries granting roles to 'allUsers' or 'allAuthenticatedUsers'. To remediate, remove such entries and set permissions to only include authorized users or groups with specific roles like 'READER' or 'WRITER'.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis#section_5.8
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset_access_entry.data_analytics_external_user_access_reviewed
  service: bigquery
  resource: dataset_access_entry
  requirement: Data Analytics External User Access Reviewed
  scope: bigquery.dataset_access_entry.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Review External User Access to BigQuery Datasets
  rationale: Regularly reviewing external user access to BigQuery datasets is crucial because unauthorized or outdated access can lead to data breaches, compromising sensitive information and violating compliance regulations such as GDPR, HIPAA, and SOC2. Ensuring that only necessary external users have access minimizes the attack surface and helps maintain data integrity and confidentiality.
  description: This rule checks whether access entries for external users on BigQuery datasets have been reviewed and updated regularly. It involves verifying access configurations through the IAM policy bindings for datasets, ensuring that permissions are granted based on the principle of least privilege. Remediation includes conducting a periodic audit of dataset access entries, removing unnecessary external user permissions, and documenting the review process for accountability.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset_access_entry.data_analytics_roles_minimal
  service: bigquery
  resource: dataset_access_entry
  requirement: Data Analytics Roles Minimal
  scope: bigquery.dataset_access_entry.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit BigQuery Dataset Access to Essential Data Analytics Roles
  rationale: Restricting dataset access to minimal required roles reduces the risk of unauthorized data exposure, which is crucial for maintaining data confidentiality and integrity. Over-permissioned roles can lead to accidental or malicious data leakage, impacting business reputation and violating data protection regulations like GDPR and HIPAA.
  description: This rule checks that BigQuery dataset access entries are assigned only to necessary data analytics roles, such as 'roles/bigquery.dataViewer' or 'roles/bigquery.metadataViewer', to minimize privilege levels. Verify by reviewing IAM policies attached to datasets and adjust roles to ensure least privilege. Remediate by removing excessive roles and assigning only essential roles needed for specific data analysis tasks.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles#bigquery-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.bigquery.dataset_access_entry.data_analytics_sso_required
  service: bigquery
  resource: dataset_access_entry
  requirement: Data Analytics Sso Required
  scope: bigquery.dataset_access_entry.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Require SSO for BigQuery Dataset Access
  rationale: Requiring Single Sign-On (SSO) for BigQuery dataset access enhances security by ensuring that user identities are verified through a centralized authentication service. This mitigates risks such as unauthorized access due to compromised credentials and reduces the attack surface by managing access through a single point of enforcement. It also helps organizations comply with identity and access management requirements in compliance frameworks like SOC 2 and ISO 27001.
  description: This rule checks whether Single Sign-On (SSO) is enforced for accessing BigQuery datasets. To verify, ensure that all dataset access entries are configured to authenticate through an enterprise SSO provider. Remediate by integrating GCP IAM policies with your organization's Identity Provider (IdP) to enforce SSO. Update IAM roles and permissions to align with the centralized authentication strategy, minimizing direct assignments of roles to users and leveraging group-based access.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/best-practices-for-enterprise-organizations
  - https://cloud.google.com/architecture/identity-federation-best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.bigquery.reservation.capacity_monitoring_configured
  service: bigquery
  resource: reservation
  requirement: Capacity Monitoring Configured
  scope: bigquery.reservation.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure BigQuery Reservation Capacity Monitoring is Configured
  rationale: Monitoring BigQuery reservation capacity is crucial for optimizing resource allocation and cost management. Without proper monitoring, organizations risk over-provisioning, leading to unnecessary expenses, or under-provisioning, which can cause performance issues and service disruptions. Ensuring capacity monitoring helps in maintaining efficient resource utilization and meeting compliance requirements for data usage transparency.
  description: This rule checks if capacity monitoring is configured for BigQuery reservations. To verify, ensure that the BigQuery reservation settings include monitoring metrics for slot utilization and reservation usage. Remediation involves configuring Stackdriver Monitoring to track these metrics and setting up alerts for threshold breaches. Such configurations enable proactive management of resource capacity and prevent unexpected billing spikes due to inefficient resource use.
  references:
  - https://cloud.google.com/bigquery/docs/reservations-intro
  - https://cloud.google.com/monitoring
  - 'CIS GCP Benchmark: Section 4.6 - Ensure logging and monitoring is configured for BigQuery'
  - 'NIST SP 800-53: SI-4 - Monitoring tools and techniques'
  - ISO/IEC 27001:2013 - A.12.4 Logging and monitoring
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf (PCI DSS 10.6)
- rule_id: gcp.bigquery.routine.data_analytics_group_logging_enabled_where_supported
  service: bigquery
  resource: routine
  requirement: Data Analytics Group Logging Enabled Where Supported
  scope: bigquery.routine.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for BigQuery Routines in Data Analytics Group
  rationale: Enabling logging for BigQuery routines helps organizations track and monitor access and modifications, providing a comprehensive audit trail essential for data integrity and compliance. It mitigates security risks by identifying unauthorized access and allows for timely incident response. Logging is crucial for compliance with regulations such as GDPR, HIPAA, and SOC2 which require detailed activity records.
  description: This rule checks whether logging is enabled for BigQuery routines within the Data Analytics group. To ensure proper logging, configure the appropriate logging sinks in Google Cloud's Logging service to capture routine execution. Verify that logs are being generated and stored securely by checking the integration with Cloud Logging. Remediation involves enabling logging settings in the Cloud Console or using gcloud CLI commands to capture detailed audit logs for routine operations.
  references:
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.bigquery.routine.data_analytics_group_tls_required
  service: bigquery
  resource: routine
  requirement: Data Analytics Group TLS Required
  scope: bigquery.routine.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS is Enforced for BigQuery Routine Connections
  rationale: Enforcing TLS for BigQuery routines is critical to protect data in transit from unauthorized access and potential interception. Failure to secure these connections can lead to data breaches, exposing sensitive analytics and violating compliance standards such as GDPR and HIPAA.
  description: This rule checks if TLS is required for all connections to BigQuery routines, ensuring that data transmitted between clients and BigQuery services is encrypted. To verify, review the IAM policies and network settings associated with BigQuery routines for TLS enforcement. Remediation involves configuring network security settings to mandate TLS for all external and internal communications, ensuring alignment with organizational security policies.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/routines
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.routine.data_warehouse_group_logging_enabled_where_supported
  service: bigquery
  resource: routine
  requirement: Data Warehouse Group Logging Enabled Where Supported
  scope: bigquery.routine.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for BigQuery Routines in Data Warehouse Group
  rationale: Enabling logging for BigQuery routines is crucial for monitoring and auditing data access and manipulation activities. This helps identify unauthorized access or malicious activities, ensuring data integrity and supporting compliance with regulations like GDPR and CCPA. Without logging, detecting anomalies or tracing data breaches becomes challenging, increasing the risk of data loss and non-compliance fines.
  description: This rule checks whether logging is enabled for BigQuery routines within the data warehouse group. To verify, ensure audit logging is configured for all relevant BigQuery datasets and projects. If not enabled, configure audit logs by setting up Google Cloud's Audit Logs for BigQuery to capture Data Access logs. This involves enabling the BigQuery Data Access logs in Google Cloud Logging under the project's IAM & Admin section.
  references:
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/security/compliance/cis/gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.bigquery.routine.data_warehouse_group_require_ssl
  service: bigquery
  resource: routine
  requirement: Data Warehouse Group Require SSL
  scope: bigquery.routine.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Routines Use SSL for Data Warehouse Access
  rationale: Requiring SSL for BigQuery routines accessing data warehouses protects data in transit from eavesdropping and man-in-the-middle attacks. This practice is essential for maintaining data confidentiality and integrity, especially when handling sensitive or regulated data. It aligns with compliance requirements like GDPR, HIPAA, and PCI-DSS that mandate secure data transmission.
  description: This rule checks if BigQuery routines are configured to use SSL when accessing data warehouses. SSL ensures encrypted connections, protecting data from interception during transmission. To verify, inspect routine settings in BigQuery to ensure SSL is enforced. Remediation involves modifying routine configurations to mandate SSL usage, which can be done via the GCP Console or by updating routine definitions using SQL scripts.
  references:
  - https://cloud.google.com/bigquery/docs/reference/standard-sql/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
- rule_id: gcp.bigquery.table.audit_logging
  service: bigquery
  resource: table
  requirement: Audit Logging
  scope: bigquery.table.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for BigQuery Tables
  rationale: Audit logging is crucial in detecting unauthorized access and changes to BigQuery tables, which can lead to data breaches or non-compliance with regulations like GDPR and HIPAA. It provides a comprehensive record of data access and modifications, enabling organizations to monitor and respond to security incidents effectively.
  description: This rule checks if audit logging is enabled for all BigQuery tables within the project. Audit logs should capture 'Admin Read', 'Admin Write', and 'Data Read' events to ensure visibility into all access and modification actions. To verify, check the Logging section under IAM & Admin in the GCP Console and ensure that the appropriate log types are enabled. Remediate by configuring the necessary log types in the IAM policies for BigQuery resources.
  references:
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.bigquery.table.automated_snapshot
  service: bigquery
  resource: table
  requirement: Automated Snapshot
  scope: bigquery.table.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Enable BigQuery Table Automated Snapshots
  rationale: Automated snapshots of BigQuery tables enhance data resilience by allowing recovery from data loss events, such as accidental deletions or corruptions. This practice ensures business continuity, minimizes downtime, and supports compliance with data protection regulations by providing a recovery point objective (RPO).
  description: This rule checks if automated snapshots are enabled for BigQuery tables, ensuring regular backups. Automated snapshots should be configured through the BigQuery service to create point-in-time copies of data, which can be retained for a defined period. To enable, use the Google Cloud Console or gcloud command-line tool to set up snapshot schedules. Remediation involves reviewing table configurations and enabling automated snapshots where missing.
  references:
  - https://cloud.google.com/bigquery/docs/managing-snapshots
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.bigquery.table.automatic_upgrades
  service: bigquery
  resource: table
  requirement: Automatic Upgrades
  scope: bigquery.table.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure BigQuery Tables Use Automatic Security Updates
  rationale: Enabling automatic upgrades for BigQuery tables ensures that the latest security patches and enhancements are applied without delay. This reduces the risk of data breaches and compliance violations by protecting against newly discovered vulnerabilities. It also aligns with regulatory requirements for maintaining up-to-date security measures, thereby safeguarding sensitive data and maintaining trust with stakeholders.
  description: This rule checks whether automatic security updates are enabled for BigQuery tables. To verify, navigate to the Google Cloud Console, select 'BigQuery', and review the settings under 'Automatic Updates' for each table. If not enabled, configure the table settings to allow automatic updates, ensuring that security patches are applied regularly. This ensures that security vulnerabilities are mitigated promptly, minimizing exposure to potential threats.
  references:
  - https://cloud.google.com/bigquery/docs/security
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/document-1000
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.table.cmek_at_rest_encryption_configured
  service: bigquery
  resource: table
  requirement: Cmek At Rest Encryption Configured
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Tables Use CMEK for Data Encryption
  rationale: Configuring Customer-Managed Encryption Keys (CMEK) for BigQuery tables ensures that sensitive data is protected using encryption keys managed by the organization. This approach mitigates risks of unauthorized data access and enhances control over data encryption policies. It also supports compliance with regulations such as GDPR, HIPAA, and PCI-DSS that require stringent data protection measures.
  description: This rule verifies that all BigQuery tables are configured to use CMEK for encryption at rest. Specifically, it checks if a customer-managed key is specified in the table's encryption configuration. To comply, update your table settings to reference a Cloud Key Management Service (KMS) key. This can be done via the GCP Console, gcloud CLI, or REST API by specifying the appropriate KMS key resource ID.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs
  - 'CIS Google Cloud Platform Foundation Benchmark v1.3.0, ID: 8.1'
  - 'NIST SP 800-53: SC-13, SC-28'
  - 'PCI-DSS v3.2.1: Requirement 3.5'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.table.data_analytics_encrypted_at_rest_cmek
  service: bigquery
  resource: table
  requirement: Data Analytics Encrypted At Rest Cmek
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Tables Use CMEK for Data Encryption at Rest
  rationale: Encrypting data at rest using Customer-Managed Encryption Keys (CMEK) in BigQuery enhances data security by allowing organizations to control the encryption keys, reducing the risk of unauthorized data access. This is crucial for protecting sensitive analytics data against potential threats and complying with strict regulatory requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strong data protection measures.
  description: This rule checks if BigQuery tables are encrypted at rest using CMEK instead of Google-managed keys. To verify, ensure that the 'encryptionConfiguration' property of a BigQuery table includes a valid KMS key reference. Remediation involves configuring BigQuery table settings to specify a CMEK by setting the 'kmsKeyName' parameter. This process not only enhances data security but also provides compliance with regulatory standards requiring encryption at rest.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 7.2
  - NIST SP 800-53 Rev. 5 - SC-13 Cryptographic Protection
  - 'PCI-DSS v3.2.1 - Requirement 3: Protect Stored Cardholder Data'
- rule_id: gcp.bigquery.table.data_analytics_kms_key_policy_least_privilege
  service: bigquery
  resource: table
  requirement: Data Analytics KMS Key Policy Least Privilege
  scope: bigquery.table.policy_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Table KMS Key Policy Uses Least Privilege
  rationale: Implementing the least privilege principle for KMS key policies in BigQuery ensures that only authorized users have access to decrypt data, reducing the risk of data breaches. This is crucial for complying with data protection regulations like GDPR and CCPA, and for safeguarding sensitive business information from unauthorized access or misuse.
  description: This rule checks that BigQuery tables encrypted with Customer-Managed Encryption Keys (CMEK) have KMS key policies granting minimal access only to necessary identities. Verify that the IAM policy on the KMS key does not include overly broad roles, such as roles/cloudkms.admin. Remediation involves reviewing the KMS key IAM policy, ensuring only specific roles like roles/cloudkms.cryptoKeyEncrypterDecrypter are granted to trusted identities, and removing unnecessary permissions.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.table.data_analytics_not_publicly_shared
  service: bigquery
  resource: table
  requirement: Data Analytics Not Publicly Shared
  scope: bigquery.table.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Sharing of BigQuery Tables
  rationale: Publicly sharing BigQuery tables can lead to unauthorized data access, exposing sensitive information to external threats and increasing the risk of data breaches. This practice can result in compliance violations with regulations such as GDPR and HIPAA, potentially resulting in financial penalties and reputational damage.
  description: This rule checks for BigQuery tables that have been configured with public access, which allows anyone on the Internet to view or download the data. To verify, review the table's IAM policy bindings and ensure that 'allUsers' or 'allAuthenticatedUsers' are not granted roles such as 'roles/bigquery.dataViewer'. Remediation involves removing any public bindings and applying the principle of least privilege by granting access only to specific users or groups who require it.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.table.data_warehouse_cross_region_copy_encrypted
  service: bigquery
  resource: table
  requirement: Data Warehouse Cross Region Copy Encrypted
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region BigQuery Table Copies Use Encryption
  rationale: Encrypting cross-region BigQuery table copies protects sensitive data from unauthorized access during transit and storage. Without encryption, data is vulnerable to interception and breaches, posing risks to data integrity and compliance with regulations such as GDPR and CCPA. Encryption at rest helps mitigate potential data exposure in the event of unauthorized access to the storage infrastructure.
  description: This rule checks that BigQuery tables copied across regions are encrypted. Ensure that Customer-Managed Encryption Keys (CMEK) are applied when creating cross-region copies to secure data against unauthorized access. Verify table settings in the BigQuery console or via CLI to ensure encryption is enabled and configured with CMEK. If encryption is not enabled, update the table settings to apply CMEK or transfer data with encryption options enabled.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS GCP Benchmark 1.2.0, Section 7.3
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
- rule_id: gcp.bigquery.table.data_warehouse_encrypted_at_rest_cmek
  service: bigquery
  resource: table
  requirement: Data Warehouse Encrypted At Rest Cmek
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Tables Use CMEK for Data Encryption at Rest
  rationale: Encrypting BigQuery data at rest with Customer-Managed Encryption Keys (CMEK) provides enhanced security by allowing control over cryptographic keys, thus protecting sensitive data against unauthorized access. This approach mitigates risks associated with data breaches and non-compliance with data protection regulations like GDPR and HIPAA, which mandate stringent encryption standards.
  description: This rule checks if BigQuery tables are encrypted using Customer-Managed Encryption Keys (CMEK) rather than Google-managed keys. To verify, inspect the table metadata for the encryption configuration. If not using CMEK, update the table settings to specify a CMEK, ensuring the key is stored in Cloud Key Management Service (KMS). This action enhances data security by giving organizations full control over encryption keys, including rotation and access permissions.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
- rule_id: gcp.bigquery.table.data_warehouse_not_publicly_shared
  service: bigquery
  resource: table
  requirement: Data Warehouse Not Publicly Shared
  scope: bigquery.table.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Sharing of BigQuery Data Warehouse Tables
  rationale: Publicly shared BigQuery tables can expose sensitive data to unauthorized users, increasing the risk of data breaches and non-compliance with privacy regulations such as GDPR and CCPA. Limiting access to only authorized personnel helps protect sensitive information and maintain data integrity.
  description: This rule checks if any BigQuery tables, particularly those used as data warehouses, are publicly accessible. Public access can be verified through the IAM policy of the table, ensuring that no 'allUsers' or 'allAuthenticatedUsers' roles are granted. To remediate, review and update the IAM policy to restrict access to necessary users only, removing any public roles.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.bigquery.table.datalake_access_policies_least_privilege
  service: bigquery
  resource: table
  requirement: Datalake Access Policies Least Privilege
  scope: bigquery.table.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege Access to BigQuery Datalake Tables
  rationale: Implementing least privilege access controls for BigQuery tables mitigates the risk of unauthorized data access, reducing the attack surface and potential data breaches. It ensures compliance with data protection regulations such as GDPR and CCPA by restricting access to sensitive data. Inadequate access management could lead to data leaks, insider threats, and non-compliance with industry standards.
  description: This rule checks whether access to BigQuery tables is granted based on the principle of least privilege. It verifies that IAM roles assigned to users and service accounts have only the necessary permissions for their tasks. To remediate, audit table-level IAM policies and remove any users or roles with excessive permissions. Use predefined roles where appropriate and regularly review access logs to ensure continued compliance.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.bigquery.table.datalake_column_level_access_controls_enabled
  service: bigquery
  resource: table
  requirement: Datalake Column Level Access Controls Enabled
  scope: bigquery.table.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Column-Level Access Controls on BigQuery Tables
  rationale: Implementing column-level access controls in BigQuery is crucial to protecting sensitive data by ensuring that users only access data necessary for their roles. This reduces the risk of data breaches and unauthorized access, which can lead to significant financial and reputational damage, and ensures compliance with regulations like GDPR that mandate data minimization and access restrictions.
  description: This rule checks if column-level access controls are enabled on BigQuery tables, which restrict access to certain columns based on user roles. To verify, inspect the table's access policy configuration in the GCP Console or via the bq command-line tool, and ensure that appropriate IAM roles are defined for sensitive columns. To remediate, define and apply IAM policies that specify who can access sensitive columns, using the GCP Console or the bq tool.
  references:
  - https://cloud.google.com/bigquery/docs/column-level-security-intro
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/Table
  - https://cloud.google.com/bigquery/docs/access-control
- rule_id: gcp.bigquery.table.datalake_encrypted
  service: bigquery
  resource: table
  requirement: Datalake Encrypted
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Tables are Encrypted at Rest
  rationale: Encrypting BigQuery tables at rest protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, especially for organizations handling regulated data. Compliance with encryption requirements is necessary for adhering to standards like GDPR, HIPAA, and PCI-DSS, reducing legal and financial risks.
  description: This rule verifies that all BigQuery tables in the datalake are encrypted at rest using Google-managed or customer-managed keys. Ensure that the `encryptionConfiguration` property is set appropriately for each table. Remediation involves configuring BigQuery to use either Google Cloud's default encryption or specifying a customer-managed encryption key (CMEK) through Cloud KMS. Regular audits and updates to key management policies are recommended to maintain security posture.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#resource
- rule_id: gcp.bigquery.table.datalake_partition_access_policies_least_privilege
  service: bigquery
  resource: table
  requirement: Datalake Partition Access Policies Least Privilege
  scope: bigquery.table.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for BigQuery Table Partition Access
  rationale: Implementing least privilege access policies for BigQuery table partitions reduces the risk of unauthorized data access and potential data leaks. By minimizing permissions, organizations can mitigate insider threats and comply with regulatory requirements such as GDPR and HIPAA that mandate strict access controls on sensitive data.
  description: This rule checks that access to BigQuery table partitions is restricted based on the principle of least privilege. Ensure that only necessary roles are granted to users or service accounts accessing table partitions. Regularly audit and review IAM policies to remove excessive permissions. Utilize GCP's IAM policy analysis tools to identify and remediate over-privileged access.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/bigquery/docs/best-practices-security
- rule_id: gcp.bigquery.table.datalake_partition_catalog_encrypted
  service: bigquery
  resource: table
  requirement: Datalake Partition Catalog Encrypted
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Table Partitions Have Encryption at Rest
  rationale: Encrypting data in BigQuery tables, particularly in datalake partitions, safeguards against unauthorized access and data breaches, which can lead to significant financial and reputational damage. It is critical for meeting compliance requirements such as GDPR and HIPAA that mandate data protection measures. Without encryption, sensitive information is vulnerable to exposure from both internal and external threats.
  description: This rule verifies that all BigQuery table partitions in the datalake catalog are encrypted using Google-managed or customer-managed encryption keys. To check compliance, review the encryption configuration of BigQuery tables via the GCP Console or CLI. If tables are not encrypted, apply Google-managed encryption keys by default, or configure Customer-Managed Encryption Keys (CMEK) for enhanced control. Ensure that all partitions of each table are consistently encrypted at rest.
  references:
  - https://cloud.google.com/bigquery/docs/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
- rule_id: gcp.bigquery.table.datalake_public_sharing_disabled
  service: bigquery
  resource: table
  requirement: Datalake Public Sharing Disabled
  scope: bigquery.table.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Sharing for BigQuery Datalake Tables
  rationale: Allowing public sharing of BigQuery tables can lead to unauthorized access to sensitive data, resulting in data breaches and regulatory non-compliance. Publicly accessible data lakes may expose personally identifiable information (PII) or intellectual property, leading to financial and reputational damage. Ensuring data is only accessible to authorized users mitigates the risk of data leaks and supports compliance with regulations such as GDPR and HIPAA.
  description: This rule checks if any BigQuery tables within the datalake are publicly accessible, which is a critical security concern. Public access should be explicitly disabled by setting the appropriate IAM policies, ensuring that only authorized users or service accounts can access the data. To remediate, review table permissions and remove 'allUsers' or 'allAuthenticatedUsers' from the IAM policy bindings. Regular audits and monitoring should be conducted to prevent unauthorized access.
  references:
  - https://cloud.google.com/bigquery/docs/security
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.bigquery.table.datalake_rbac_least_privilege
  service: bigquery
  resource: table
  requirement: Datalake RBAC Least Privilege
  scope: bigquery.table.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for BigQuery Table Access
  rationale: Implementing least privilege access for BigQuery tables reduces the risk of unauthorized data exposure and potential data breaches. It helps in maintaining compliance with data protection regulations like GDPR and HIPAA, where access to sensitive data must be strictly controlled and auditable. Failing to enforce this can lead to significant financial penalties and reputational damage.
  description: This rule checks for overly permissive roles assigned to BigQuery tables within your GCP projects. Specifically, it ensures that only necessary roles are granted at the table level, thereby adhering to the principle of least privilege. To verify, review the IAM policies and ensure that roles like 'Owner', 'Editor', or custom roles with excessive permissions are not broadly granted. Remediate by assigning specific roles that limit access to only what is required for users to perform their job functions.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Computing Foundations Benchmark v1.1.0, 4.2 Ensure that BigQuery datasets are not anonymously or publicly accessible
  - NIST SP 800-53 AC-6 Least Privilege
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.table.datalake_version_immutability_enforced
  service: bigquery
  resource: table
  requirement: Datalake Version Immutability Enforced
  scope: bigquery.table.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Immutability for BigQuery Table Versions
  rationale: Enforcing immutability on BigQuery table versions ensures data integrity and prevents unauthorized or accidental modifications. This is crucial for maintaining data trustworthiness, supporting forensic investigations, and complying with regulations such as GDPR and HIPAA, which mandate stringent data protection and privacy measures.
  description: This rule checks if BigQuery tables have immutability enforced on their versions to prevent changes post-creation. Immutability can be achieved by configuring appropriate access controls and using table snapshots. To verify, ensure that table access policies restrict editing permissions and that table snapshots are used for historical data. Remediation involves reviewing IAM roles and permissions and setting up a regular snapshot schedule to preserve data states.
  references:
  - https://cloud.google.com/bigquery/docs/managing-table-snapshots
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://documentation.compliance.io/gdpr-overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.table.encryption_enabled
  service: bigquery
  resource: table
  requirement: Encryption Enabled
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Table Encryption at Rest
  rationale: Encrypting BigQuery tables at rest mitigates the risk of unauthorized data access and data breaches, which can have significant business and reputational impacts. It is crucial for meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate encryption of sensitive data to protect user privacy and integrity.
  description: This rule checks whether encryption is enabled for BigQuery tables, ensuring that all data stored is encrypted using Google-managed or customer-managed encryption keys. To verify, ensure that BigQuery tables are configured with the appropriate encryption settings in the GCP Console or via the bq command-line tool. If encryption is not enabled, configure the tables using default Google-managed keys or specify customer-managed keys for enhanced control.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
- rule_id: gcp.bigquery.table.maintenance_settings
  service: bigquery
  resource: table
  requirement: Maintenance Settings
  scope: bigquery.table.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure BigQuery Table Maintenance Settings are Configured
  rationale: Proper maintenance settings in BigQuery tables ensure that data access and changes are monitored and controlled, reducing the risk of unauthorized modifications or data loss. This is crucial for maintaining data integrity and compliance with regulatory standards such as GDPR and HIPAA, which require strict data handling and auditing practices.
  description: This check verifies that BigQuery tables have appropriate maintenance settings configured, including setting up update and delete protections and ensuring that audit logging is enabled. To verify, check the table configurations for the presence of IAM policies that restrict access to data modification operations and ensure that logging is enabled via the GCP Console or gcloud CLI. Remediation involves setting up IAM roles with the principle of least privilege and configuring audit logs in the Cloud Logging service.
  references:
  - https://cloud.google.com/bigquery/docs/table-deletion
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.bigquery.table.public_access
  service: bigquery
  resource: table
  requirement: Public Access
  scope: bigquery.table.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to BigQuery Tables
  rationale: Allowing public access to BigQuery tables can lead to unauthorized data exposure, resulting in potential data breaches and compliance violations. This is critical for organizations handling sensitive or regulated data, as it can impact customer trust and lead to significant financial penalties under frameworks like GDPR, HIPAA, or PCI-DSS.
  description: This rule checks if any BigQuery tables are accessible by the 'allUsers' or 'allAuthenticatedUsers' groups, which indicates public access. To verify, inspect the IAM policy bindings of each table for these entries. Remediation involves removing these public entries and ensuring that only specific, authenticated users or groups have access, achieved via the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/getIamPolicy
  - https://cloud.google.com/security-command-center/docs/how-to-remediate-biqquery-findings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/bigquery/docs/table-access-controls-intro
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.bigquery.table.table_cmk_encryption_configured
  service: bigquery
  resource: table
  requirement: Table CMK Encryption Configured
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Tables Use Customer-Managed Keys (CMK) for Encryption
  rationale: Using Customer-Managed Keys (CMK) for BigQuery table encryption ensures that organizations maintain control over encryption keys, thereby reducing the risk of unauthorized data access. This approach aligns with data protection regulations such as GDPR and CCPA, which necessitate strict control over data security and privacy. Additionally, CMK provides an additional layer of security against potential insider threats and supports forensic investigations.
  description: This rule verifies that BigQuery tables are encrypted using Customer-Managed Keys (CMK) instead of the default Google-managed keys. To ensure compliance, configure the BigQuery table to use a Cloud Key Management Service (KMS) key by specifying the 'encryptionConfiguration.kmsKeyName' property. Remediation involves updating table settings to include a specific KMS key for encryption, which can be done via the GCP Console or gcloud command-line tool. Regular audits should be conducted to ensure ongoing compliance with this encryption policy.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://cloud.google.com/kms/docs/key-rotation
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigtable.instance.backup_enabled
  service: bigtable
  resource: instance
  requirement: Backup Enabled
  scope: bigtable.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Enable Backups for Bigtable Instances
  rationale: Enabling backups for Bigtable instances ensures data resilience and disaster recovery capabilities, mitigating risks of data loss due to accidental deletions, corruption, or regional outages. This practice supports business continuity and aligns with compliance requirements by preserving critical data integrity in regulated environments.
  description: This rule checks if backups are enabled for Bigtable instances, a vital step in ensuring data recovery and protection against data loss. To verify, navigate to the Bigtable console, select your instance, and ensure that automated backups are configured. Remediation involves setting up a backup schedule and retention policy appropriate to your organizational needs, leveraging GCP's Bigtable backup capabilities.
  references:
  - https://cloud.google.com/bigtable/docs/backup-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigtable.table.data_protection_storage_global_cross_region_replic_encrypted
  service: bigtable
  resource: table
  requirement: Data Protection Storage Global Cross Region Replic Encrypted
  scope: bigtable.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region Replicated Bigtable Data is Encrypted
  rationale: Encrypting data across regions protects sensitive information from unauthorized access during transit and storage. Unencrypted data replication can expose your organization to data breaches, resulting in financial loss and compliance violations with regulations like GDPR and CCPA.
  description: This rule checks that all Bigtable tables replicated across regions use encryption to safeguard data at rest. Ensure that the service account associated with Bigtable has the necessary permissions to encrypt data with customer-managed encryption keys (CMEK). Verify this by checking Bigtable table settings in the Google Cloud Console or using gcloud commands. To remediate, configure Bigtable to use CMEK for cross-region replication.
  references:
  - https://cloud.google.com/bigtable/docs/encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/security/encryption-in-transit/
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigtable.table.data_protection_storage_global_encryption_at_rest_al_regions
  service: bigtable
  resource: table
  requirement: Data Protection Storage Global Encryption At Rest Al Regions
  scope: bigtable.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Bigtable Tables Use CMEK for Encryption At Rest in All Regions
  rationale: Using customer-managed encryption keys (CMEK) for Bigtable ensures that your organization retains control over the encryption keys that protect data at rest, which is crucial for meeting stringent compliance requirements and mitigating risks of unauthorized data access. This setup is especially important for businesses with sensitive data in highly regulated industries such as finance and healthcare.
  description: This rule verifies that all Bigtable tables across all regions are configured to use customer-managed encryption keys (CMEK) instead of default Google-managed keys. To check, ensure that each Bigtable instance is associated with a key from Cloud Key Management Service (KMS). Remediation involves creating a Cloud KMS key and applying it to your Bigtable tables via instance settings. This helps maintain data integrity and confidentiality by leveraging the enhanced security controls of CMEK.
  references:
  - https://cloud.google.com/bigtable/docs/using-cmek
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57pt1r4.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/resource-hierarchy
- rule_id: gcp.bigtable.table.data_protection_storage_global_pitr_enabled_where_supported
  service: bigtable
  resource: table
  requirement: Data Protection Storage Global Pitr Enabled Where Supported
  scope: bigtable.table.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Bigtable Global PITR is Enabled Where Supported
  rationale: Enabling Point-in-Time Recovery (PITR) for Bigtable provides a critical safeguard against data loss due to accidental deletion or corruption. This feature is essential in maintaining data integrity and availability, which are key to business continuity and compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks if Point-in-Time Recovery (PITR) is enabled for Google Cloud Bigtable tables where it is supported. To verify, ensure that the 'Enable PITR' option is activated in the Bigtable instance settings. If not enabled, data recovery to a specific point in time may not be possible, increasing the risk of data loss. Remediation involves accessing the Bigtable console, navigating to the table settings, and enabling the PITR feature.
  references:
  - https://cloud.google.com/bigtable/docs/data-protection
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.billing.anomaly.cost_alert_destinations_configured
  service: billing
  resource: anomaly
  requirement: Cost Alert Destinations Configured
  scope: billing.anomaly.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Billing Anomaly Cost Alert Destinations Configured
  rationale: Configuring cost alert destinations for billing anomalies is crucial to timely detect and respond to unexpected expenses, which can indicate security breaches or misconfigurations leading to cost spikes. This reduces financial risk and ensures accountability in billing management, while supporting compliance with financial governance standards.
  description: This rule checks if alert destinations are configured for billing anomalies in GCP. It requires setting up notifications to specific channels (e.g., email, Pub/Sub) to alert stakeholders of abnormal cost activities. To verify, ensure that alert policies are configured in the Cloud Console under Billing > Budgets & alerts, and specify destinations. Remediation involves setting up an alert policy with notification channels to promptly detect and handle billing anomalies.
  references:
  - https://cloud.google.com/billing/docs/how-to/notifications
  - https://cloud.google.com/iam/docs/roles-and-permissions#billing
  - https://cloud.google.com/billing/docs/how-to/budgets
  - CIS GCP Benchmark - Billing Alerts
  - 'NIST SP 800-53: SA-9 External Information System Services'
  - ISO/IEC 27001:2013 - A.15 Supplier Relationships
- rule_id: gcp.billing.anomaly.cost_detectors_enabled
  service: billing
  resource: anomaly
  requirement: Cost Detectors Enabled
  scope: billing.anomaly.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable GCP Billing Cost Anomaly Detectors
  rationale: Enabling cost anomaly detection in GCP billing helps organizations proactively identify unexpected charges, potentially indicating security breaches or misconfigurations. Early detection of billing anomalies can prevent financial loss and ensure compliance with budgetary constraints, aligning with governance and compliance frameworks.
  description: This rule checks whether cost anomaly detectors are enabled in GCP billing. To verify, navigate to the 'Billing' section in the GCP Console, and ensure that anomaly detection is configured under 'Budgets & alerts.' If not enabled, configure cost anomaly detectors by setting thresholds and notification preferences to receive alerts on any unusual billing activities promptly.
  references:
  - https://cloud.google.com/billing/docs/how-to/monitor-anomalies
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.billing.anomaly.cost_severity_thresholds_configured
  service: billing
  resource: anomaly
  requirement: Cost Severity Thresholds Configured
  scope: billing.anomaly.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Cost Severity Thresholds Are Configured for Billing Anomalies
  rationale: Configuring cost severity thresholds is crucial to promptly identify unusual billing activities that could indicate misconfigurations or unauthorized usage, potentially leading to significant financial losses. It helps organizations detect and address anomalies before they escalate, ensuring budget adherence and financial governance. Compliance with financial controls and governance frameworks such as SOC2 and ISO 27001 often requires proactive monitoring of billing anomalies.
  description: This rule checks whether cost severity thresholds have been configured for billing anomalies in GCP. Proper configuration involves setting thresholds that trigger alerts when billing exceeds expected levels, allowing for timely investigation. To verify, access the Google Cloud Console, navigate to the Billing section, and ensure that anomaly detection is enabled with defined severity thresholds. Remediate by configuring these thresholds to align with your organization's budget and risk tolerance.
  references:
  - https://cloud.google.com/billing/docs/how-to/notifications
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/billing/docs/how-to/budgets
- rule_id: gcp.billing.billing_account.cost_access_admins_mfa_required
  service: billing
  resource: billing_account
  requirement: Cost Access Admins MFA Required
  scope: billing.billing_account.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce MFA for Cost Access Admins on Billing Accounts
  rationale: Requiring Multi-Factor Authentication (MFA) for Cost Access Admins reduces the risk of unauthorized access to sensitive billing data, which can lead to financial losses and data breaches. This practice helps to protect against credential theft and account hijacking, ensuring compliance with regulations such as PCI-DSS and ISO 27001 that mandate strong authentication controls.
  description: This rule checks whether MFA is enabled for users with Cost Access Admin privileges on GCP billing accounts. To verify, ensure that all IAM users with `billing.accounts.get` or `billing.accounts.list` permissions have MFA enforced through Identity and Access Management (IAM) policies. Remediation involves configuring IAM to require MFA for these roles, typically by enabling security key enforcement or app-based authentication for user accounts.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/managing-policies
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/identity
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.billing.billing_account.cost_access_console_rbac_least_privilege
  service: billing
  resource: billing_account
  requirement: Cost Access Console RBAC Least Privilege
  scope: billing.billing_account.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Billing Account Access Uses Least Privilege RBAC
  rationale: Implementing least privilege for billing account access reduces the risk of unauthorized financial data exposure and potential mismanagement of billing settings, which could lead to financial loss or non-compliance with regulations requiring data confidentiality and integrity. Proper access control is crucial for maintaining organizational security posture and meeting compliance standards such as PCI-DSS and SOC 2.
  description: This rule checks if billing account access is granted based on the principle of least privilege using Role-Based Access Control (RBAC). Ensure users have only the permissions necessary for their roles by assigning predefined roles such as 'Billing Viewer' or 'Billing Admin' instead of overly permissive custom roles. Verify and adjust permissions through the Google Cloud Console by reviewing IAM policies for each billing account and removing excess permissions. Regular audits should be conducted to ensure ongoing compliance.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.billing.billing_account.cost_access_cost_data_exports_private_and_encrypted
  service: billing
  resource: billing_account
  requirement: Cost Access Cost Data Exports Private And Encrypted
  scope: billing.billing_account.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cost Data Exports are Private and Encrypted
  rationale: Encrypting cost data exports ensures that sensitive financial information is protected from unauthorized access and potential breaches. This practice mitigates risks such as data leakage and supports compliance with financial regulations, safeguarding organizational reputation and maintaining customer trust.
  description: This rule checks that billing account cost data exports are configured to be private and encrypted at rest. To verify, ensure that cost data export settings are configured to use Google Cloud Storage buckets with default encryption enabled and access permissions restricted to necessary personnel. Remediation involves setting up a bucket with encryption and applying IAM policies to limit access.
  references:
  - https://cloud.google.com/billing/docs/how-to/export-data-bigquery
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.billing.billing_account.cost_access_cost_export_destinations_least_privilege
  service: billing
  resource: billing_account
  requirement: Cost Access Cost Export Destinations Least Privilege
  scope: billing.billing_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict Cost Export Destinations to Least Privilege
  rationale: Implementing least privilege for cost export destinations reduces the risk of unauthorized access to sensitive billing data, which can lead to financial exposure and compliance violations. Ensuring that only necessary permissions are granted helps prevent data leaks and unauthorized financial insights, aligning with regulatory standards such as PCI-DSS and SOC2.
  description: This rule checks whether cost export destinations in GCP Billing Accounts are configured with the least privilege principle. Specifically, it verifies that only necessary roles and permissions are assigned to users or service accounts accessing cost export destinations. To remediate, review IAM policies associated with billing accounts and adjust roles to ensure they have only the permissions needed for their function. Use the GCP Console or gcloud command-line tool to audit and modify IAM roles.
  references:
  - https://cloud.google.com/billing/docs/how-to/export-data-bigquery
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/security/access-management
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.billing.billing_account.cost_savings_plan_admins_mfa_required
  service: billing
  resource: billing_account
  requirement: Cost Savings Plan Admins MFA Required
  scope: billing.billing_account.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enable MFA for Cost Savings Plan Admins on Billing Accounts
  rationale: Requiring multi-factor authentication (MFA) for Cost Savings Plan Admins mitigates the risk of unauthorized access to critical financial settings and data. This prevents potential financial losses and exposure of sensitive billing information, which could arise from compromised credentials. Ensuring MFA aligns with compliance requirements such as PCI-DSS and enhances security posture against identity-based threats.
  description: This rule checks that all users with the 'Cost Savings Plan Admin' role on GCP billing accounts have MFA enabled. Without MFA, accounts are vulnerable to unauthorized access from compromised passwords. Verify by reviewing identity settings in the IAM section of the GCP Console and ensure MFA is enforced for these users. Remediate by configuring identity policies to enforce MFA for all billing account administrators.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/mfa
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-63/3/final
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.billing.billing_account.cost_savings_plan_approval_workflow_required
  service: billing
  resource: billing_account
  requirement: Cost Savings Plan Approval Workflow Required
  scope: billing.billing_account.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce Approval Workflow for Cost Savings Plans
  rationale: Implementing an approval workflow for Cost Savings Plans helps prevent unauthorized commitments that could lead to unexpected financial liabilities. This measure mitigates risks of overspending and helps maintain budgetary control, ensuring alignment with strategic financial goals and compliance with organizational policies.
  description: This rule checks if an approval workflow is in place for any Cost Savings Plans created under the billing account. The workflow should require authorization from designated stakeholders prior to plan activation. Verification involves reviewing the billing account's configuration settings to ensure that requests for Cost Savings Plans trigger approval mechanisms. To remediate, configure billing account settings to enforce an approval workflow, ensuring all cost-saving initiatives are vetted and authorized appropriately.
  references:
  - https://cloud.google.com/billing/docs/how-to/manage-billing-account
  - https://cloud.google.com/docs/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.billing.billing_account.cost_savings_plan_purchase_permissions_restricted
  service: billing
  resource: billing_account
  requirement: Cost Savings Plan Purchase Permissions Restricted
  scope: billing.billing_account.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Cost Savings Plan Purchase Permissions
  rationale: Restricting permissions for purchasing Cost Savings Plans mitigates the risk of unauthorized financial commitments that could lead to unexpected expenses and budget overruns. It ensures that only authorized personnel with a clear understanding of the organization's financial strategy can make such purchases, aligning with financial governance policies and reducing the risk of financial mismanagement.
  description: This rule checks that only designated IAM roles have permissions to purchase Cost Savings Plans in GCP. Specifically, it ensures that roles like 'roles/billing.admin' are assigned to users who are responsible for financial decisions. To verify, review IAM policies for the billing account and limit permissions using the principle of least privilege. Remediation involves auditing IAM policies and adjusting permissions as needed to restrict access to authorized personnel only.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/understanding-roles#billing-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.billing.budget.cost_alert_destinations_configured
  service: billing
  resource: budget
  requirement: Cost Alert Destinations Configured
  scope: billing.budget.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Cost Alert Destinations Are Configured
  rationale: Configuring cost alert destinations in GCP budgets helps organizations manage spending effectively, preventing unexpected costs and ensuring financial accountability. Without these alerts, businesses might exceed budgets, leading to financial risk and impacting profitability. Compliance frameworks often require monitoring of spending to manage financial risk, making this practice essential for adhering to standards like ISO 27001 and SOC2.
  description: This rule checks whether cost alert destinations are configured within GCP budgets. To verify, ensure that each budget has designated alert recipients, such as email addresses, configured in the Google Cloud Console under 'Budgets & alerts' settings. Remediation involves accessing the 'Budgets & alerts' section, selecting a budget, and adding notification recipients to receive alerts when spending exceeds specified thresholds.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/billing/docs/how-to/notifications
- rule_id: gcp.billing.budget.cost_alert_thresholds_configured
  service: billing
  resource: budget
  requirement: Cost Alert Thresholds Configured
  scope: billing.budget.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Billing Budget Cost Alert Thresholds Are Configured
  rationale: Configuring cost alert thresholds in GCP billing budgets helps organizations proactively manage and monitor their cloud expenditure, preventing unexpected overspending. This is crucial for maintaining budget discipline and ensuring financial accountability. By setting cost alerts, businesses can mitigate the risk of financial mismanagement and align their cloud spending with strategic financial goals.
  description: This rule checks if cost alert thresholds are configured for GCP billing budgets. Proper configuration involves setting thresholds at specific percentage levels of the budget that trigger notifications when exceeded. To verify, review your GCP Console under Billing > Budgets & alerts and ensure thresholds are set at strategic levels like 50%, 90%, and 100%. Remediation involves accessing the GCP Console and configuring these thresholds to align with your financial policies.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0 - Section 7.7
  - NIST SP 800-53 Rev. 5 - CP-11
  - ISO/IEC 27001:2013 - A.8.3.3
- rule_id: gcp.billing.budget.cost_budgets_defined_for_accounts_or_projects
  service: billing
  resource: budget
  requirement: Cost Budgets Defined For Accounts Or Projects
  scope: billing.budget.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Cost Budgets Are Set for GCP Projects or Accounts
  rationale: Setting cost budgets for GCP projects or accounts is crucial to prevent unexpected overspending, which can lead to financial constraints and impact operational budgets. Without defined budgets, organizations risk exceeding financial limits, potentially violating internal governance policies or external compliance requirements related to cost management.
  description: This rule checks whether cost budgets are established for GCP billing accounts or specific projects. A budget helps monitor spending and sends alerts when costs exceed defined thresholds. To verify, navigate to the Google Cloud Console, select 'Billing', and ensure that budgets are set under the 'Budgets & alerts' tab. To remediate, create a budget by specifying a target amount and configuring alert thresholds to align with financial policies.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-171/rev-2/final
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.billing.budget.cost_cost_tag_policy_enforced
  service: billing
  resource: budget
  requirement: Cost Cost Tag Policy Enforced
  scope: billing.budget.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Cost Tag Policy on GCP Billing Budgets
  rationale: Enforcing cost tag policies on billing budgets ensures that costs are accurately tracked and attributed to the appropriate projects or departments, which aids in financial management and accountability. Without this enforcement, there is a risk of cost misallocation, leading to budget overruns and potential financial loss. This practice also supports compliance with financial regulations and audit requirements by ensuring transparent and traceable spending.
  description: This rule checks whether a cost tag policy is enforced on all GCP billing budgets to ensure that each expenditure is tagged appropriately. To verify, check the billing settings in the GCP Console to confirm that a cost tag policy is associated with each budget. To remediate, navigate to the Billing section of the GCP Console, create or edit a budget, and apply a cost tag policy to ensure all expenditures are categorized correctly. This configuration helps in maintaining strict budgetary controls and financial transparency.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/resource-manager/docs/tags/tags-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.billing.budget.cost_modify_permissions_restricted
  service: billing
  resource: budget
  requirement: Cost Modify Permissions Restricted
  scope: billing.budget.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Cost Modification Permissions for GCP Budgets
  rationale: Restricting cost modification permissions for GCP budgets is crucial to prevent unauthorized changes that could lead to financial mismanagement and unexpected expenditures. This control helps mitigate insider threats, where individuals with excessive permissions might alter budget settings, resulting in budget overruns or misaligned financial reporting. Ensuring only authorized personnel have these permissions supports compliance with financial governance requirements and audit readiness.
  description: This rule verifies that only designated users or groups have permissions to modify cost settings in GCP Budgets. To check this, review IAM roles assigned to users for billing accounts and ensure that only roles with necessary permissions, like 'Billing Account Admin', are assigned. Remediation involves auditing current IAM policies, revoking excessive permissions, and implementing least privilege principles by assigning roles like 'Billing Account Viewer' to users who do not need modification capabilities.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.billing.budget.cost_required_cost_tags_present
  service: billing
  resource: budget
  requirement: Cost Required Cost Tags Present
  scope: billing.budget.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Cost Tags are Present for GCP Billing Budgets
  rationale: Having cost tags in place allows organizations to effectively track and allocate spending within GCP, ensuring financial accountability and enabling cost optimization. Without these tags, it becomes challenging to identify unnecessary expenditures and allocate costs accurately across departments, potentially leading to overspending and budget mismanagement. Regulatory requirements may also necessitate detailed financial reporting, making proper cost tagging essential for compliance.
  description: This rule checks if all required cost tags are applied to GCP billing budgets. Cost tags help categorize and track expenses related to specific projects or departments within an organization. To verify compliance, ensure that each budget within your GCP billing account has the necessary tags applied. If tags are missing, update your budget settings in the GCP Console to include all required tags. This can typically be done under the 'Budgets & Alerts' section by editing the budget settings.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/docs/overview/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53
  - https://cloud.google.com/billing/docs/how-to/export-data
  - ISO/IEC 27001
- rule_id: gcp.billing.budget.cost_untagged_resource_alerts_enabled
  service: billing
  resource: budget
  requirement: Cost Untagged Resource Alerts Enabled
  scope: billing.budget.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Alerts for Untagged Resource Costs
  rationale: Enabling alerts for untagged resource costs helps organizations identify and manage potential cost overruns due to misconfigured or untracked resources. This can mitigate the risk of unexpected expenses, ensure better budget adherence, and support financial compliance. Untagged resources can lead to inefficiencies and lack of accountability, impacting overall cloud cost management.
  description: This rule checks that alerts are configured for resources without tags that incur costs, enabling proactive monitoring. To verify, ensure that budget alerts in the GCP Billing console are set up for untagged resource expenditures. Remediation involves configuring alerts via the GCP Console or using Cloud Monitoring to notify stakeholders when costs for untagged resources exceed predefined thresholds.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/monitoring/docs/alerting
  - https://cloud.google.com/billing/docs/how-to/cost-allocation-tagging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.certificatemanager.certificate.certificates_expiration_check_gcp_load_balancing_s_protocols
  service: certificatemanager
  resource: certificate
  requirement: Certificates Expiration Check Gcp Load Balancing S Protocols
  scope: certificatemanager.certificate.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Certificates Are Valid for GCP Load Balancing Protocols
  rationale: Expired SSL/TLS certificates on GCP load balancers can lead to service disruptions and expose data in transit to interception, increasing vulnerability to man-in-the-middle attacks. Regularly checking certificate expiration supports compliance with data protection standards such as PCI-DSS, ensuring secure data transmission.
  description: This rule verifies that SSL/TLS certificates managed by GCP Certificate Manager for load balancing are not expired. It is essential to monitor certificate expiration dates and set up alerts for timely renewals. To remediate, implement automated certificate renewal processes and regularly audit certificate validity through the GCP console or API. Ensure all certificates associated with load balancers are valid and support secure protocols.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-certificates
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/certificate-manager/docs/using
- rule_id: gcp.certificatemanager.certificate.certificates_expiration_check_gcp_storage_bucket_enf_enabled
  service: certificatemanager
  resource: certificate
  requirement: Certificates Expiration Check Gcp Storage Bucket Enf Enabled
  scope: certificatemanager.certificate.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Certificates Expiration Check for GCP Storage Buckets
  rationale: Regularly checking certificate expiration is crucial to prevent service disruptions and maintain secure communications. Expired certificates can lead to failed connections, exposing data to potential interception or unauthorized access. This practice supports compliance with security standards and reduces the risk of data breaches and operational downtime.
  description: This rule checks whether the expiration of SSL/TLS certificates stored in GCP storage buckets is monitored and enforced. It requires configuring alerts for approaching expiration dates to ensure timely renewal. To verify, review certificate manager settings for alert configurations and automate renewal processes where possible. Enabling this check helps mitigate risks associated with expired certificates by maintaining continuous and secure operations.
  references:
  - https://cloud.google.com/certificate-manager/docs
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.certificatemanager.certificate.expiration
  service: certificatemanager
  resource: certificate
  requirement: Expiration
  scope: certificatemanager.certificate.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Monitor and Renew Expiring Certificates
  rationale: Certificates play a critical role in securing data transmission. Expired certificates can lead to disrupted services, loss of customer trust, and exposure to man-in-the-middle attacks. Ensuring timely renewal is essential for maintaining secure and compliant operations, particularly under regulations like PCI-DSS and HIPAA.
  description: This rule checks for certificates managed by GCP's Certificate Manager that are nearing expiration. Regular monitoring and renewal of these certificates are crucial to prevent service interruptions and vulnerabilities. To verify, review the certificate expiration date through the GCP Console or CLI. Set up alerts for certificates expiring within a 30-day window and automate renewals where possible.
  references:
  - https://cloud.google.com/certificate-manager/docs/concepts
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.certificatemanager.certificate.expiration_configured
  service: certificatemanager
  resource: certificate
  requirement: Expiration Configured
  scope: certificatemanager.certificate.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Certificate Expiration is Configured in Certificate Manager
  rationale: Properly configuring certificate expiration is crucial to prevent service disruptions and ensure data integrity. Without expiration settings, certificates may become invalid without notice, leading to potential downtime, security vulnerabilities, and non-compliance with industry regulations such as PCI-DSS and ISO 27001.
  description: This rule checks whether expiration settings are defined for certificates managed in GCP Certificate Manager. To verify, ensure that each certificate has a defined expiration date and that renewal processes are in place. To remediate, configure automatic renewals or set alerts for manual renewals to avoid unexpected certificate expiry, which can lead to service interruptions and security risks.
  references:
  - https://cloud.google.com/certificate-manager/docs/managing-certificates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.certificatemanager.certificate.manager_certificates_expiration_configured
  service: certificatemanager
  resource: certificate
  requirement: Manager Certificates Expiration Configured
  scope: certificatemanager.certificate.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Manager Certificates Have Expiration Configured
  rationale: Properly configuring certificate expiration is critical to preventing the use of outdated cryptographic credentials, which can lead to vulnerabilities such as man-in-the-middle attacks. Regularly expiring certificates ensure that they are rotated and updated, reducing the risk of unauthorized access and helping to maintain trust in secure communications. Compliance with industry standards and regulations like PCI-DSS and ISO 27001 often mandates periodic certificate renewal.
  description: This rule checks that all certificates managed by Certificate Manager have an expiration date configured. Certificates without expiration dates may remain in use long after their intended lifespan, exposing systems to security risks. Verify that each certificate has an expiration policy set by using the Google Cloud Console or gcloud command-line tool. Remediation involves setting an appropriate expiration date for certificates, and configuring alerts for certificate renewal prior to expiration.
  references:
  - https://cloud.google.com/certificate-manager/docs/overview
  - https://cloud.google.com/solutions/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://owasp.org/www-project-top-ten/
- rule_id: gcp.cloudfunctions.function.concurrency_limit_configured
  service: cloudfunctions
  resource: function
  requirement: Concurrency Limit Configured
  scope: cloudfunctions.function.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Concurrency Limit is Configured for Cloud Functions
  rationale: Configuring a concurrency limit for Cloud Functions helps prevent resource exhaustion and ensures predictable performance under load. Without a set limit, functions may consume excessive resources, leading to increased costs, potential denial-of-service conditions, and performance degradation. This practice helps meet compliance requirements by maintaining system stability and availability.
  description: This rule checks whether Cloud Functions have a concurrency limit configured. A specified limit restricts the number of concurrent executions, helping to control resource usage and costs. To verify, review the Cloud Function's settings in the GCP Console or use the gcloud CLI. Remediation involves setting a suitable concurrency limit based on expected load and performance requirements, which can be done via the GCP Console or by updating the function configuration using gcloud commands.
  references:
  - https://cloud.google.com/functions/docs/concepts/exec#concurrency
  - https://cloud.google.com/functions/docs/bestpractices/tips#concurrency
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudfunctions.function.not_publicly_accessible
  service: cloudfunctions
  resource: function
  requirement: Not Publicly Accessible
  scope: cloudfunctions.function.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Cloud Functions are Not Publicly Accessible
  rationale: Publicly accessible Cloud Functions can be exploited by unauthorized parties, leading to potential data breaches and service disruptions. This exposure increases the risk of malicious actors executing harmful code, consuming resources, or accessing sensitive information. Compliance with standards such as PCI-DSS and ISO 27001 requires limiting public access to minimize attack surfaces.
  description: This check verifies that Google Cloud Functions are not set to be publicly accessible by reviewing IAM policies for roles that grant 'allUsers' or 'allAuthenticatedUsers' permissions. To remediate, adjust the function's IAM policies to restrict access to specific users or service accounts that require it. Ensure the 'roles/cloudfunctions.invoker' role is only granted to trusted entities by modifying permissions in the GCP Console or using the gcloud CLI.
  references:
  - https://cloud.google.com/functions/docs/securing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.cloudfunctions.function.serverless_dead_letter_queue_configured
  service: cloudfunctions
  resource: function
  requirement: Serverless Dead Letter Queue Configured
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Dead Letter Queue Configured for Cloud Functions
  rationale: Configuring a Dead Letter Queue (DLQ) for Cloud Functions ensures that unprocessed messages are not lost, helping maintain data integrity and enabling easier troubleshooting in case of failures. Without a DLQ, failed executions can result in data loss and impact business continuity, potentially leading to non-compliance with data handling regulations such as GDPR and HIPAA.
  description: This rule checks if a Dead Letter Queue is configured for Google Cloud Functions. A DLQ captures messages that cannot be processed successfully, providing a mechanism for reprocessing or analyzing failures. To verify, ensure that the 'deadLetterPolicy' is set within the function's 'eventTrigger' configuration. Remediation involves specifying a Pub/Sub topic for unprocessed messages in the Cloud Function's settings, ensuring failed events are retained for further analysis.
  references:
  - https://cloud.google.com/functions/docs/monitoring/logging
  - https://cloud.google.com/functions/docs/bestpractices/reliability
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/pubsub/docs/dead-letter-topics
- rule_id: gcp.cloudfunctions.function.serverless_env_no_plaintext_secrets
  service: cloudfunctions
  resource: function
  requirement: Serverless Env No Plaintext Secrets
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Avoid Plaintext Secrets in Cloud Functions Environment Variables
  rationale: Storing plaintext secrets in environment variables of Cloud Functions can lead to unauthorized access if the function's configuration is exposed. This exposure can result in data breaches or system compromise, particularly if sensitive credentials are leaked. Ensuring secrets are encrypted helps meet compliance requirements like PCI-DSS and protects against potential insider threats and external attacks.
  description: This rule checks whether Google Cloud Functions use plaintext environment variables to store sensitive information, such as API keys or passwords. Functions should leverage Google Secret Manager for storing sensitive data securely. To verify, inspect the function's configuration for any hardcoded secrets in the environment variables. Remediation involves migrating these secrets to Secret Manager and updating the function to access secrets at runtime securely.
  references:
  - https://cloud.google.com/functions/docs/configuring/secrets
  - https://cloud.google.com/secret-manager/docs/best-practices
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.1
  - NIST SP 800-53 Rev. 5, System and Communications Protection
  - 'PCI-DSS v3.2.1, Requirement 3: Protect Stored Cardholder Data'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudfunctions.function.serverless_logging_and_tracing_enabled
  service: cloudfunctions
  resource: function
  requirement: Serverless Logging And Tracing Enabled
  scope: cloudfunctions.function.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging and Tracing for Cloud Functions
  rationale: Enabling logging and tracing for Google Cloud Functions is crucial for monitoring function executions, diagnosing issues, and ensuring compliance with regulatory standards. This capability provides insights into the function's behavior and performance, helping to detect anomalies and unauthorized access, thereby mitigating potential security threats and operational inefficiencies.
  description: This rule checks whether logging and tracing are enabled for Cloud Functions. To ensure compliance, verify that each function has 'logging' and 'tracing' configurations enabled within the Google Cloud Console or via deployment scripts. Remediation involves setting up Stackdriver Logging and enabling Cloud Trace to capture detailed execution logs and performance traces, which can be done by adjusting the function's environment variables and permissions.
  references:
  - https://cloud.google.com/functions/docs/monitoring/logging
  - https://cloud.google.com/functions/docs/monitoring/error-reporting
  - https://cloud.google.com/functions/docs/monitoring
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/stackdriver/docs/solutions/functions
- rule_id: gcp.cloudfunctions.function.serverless_permissions_least_privilege
  service: cloudfunctions
  resource: function
  requirement: Serverless Permissions Least Privilege
  scope: cloudfunctions.function.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for GCP Cloud Functions Permissions
  rationale: Enforcing least privilege on Cloud Functions reduces the risk of unauthorized access and potential data breaches. Excessive permissions can lead to privilege escalation and exploitation, posing significant security risks and compliance violations with frameworks like PCI-DSS and ISO 27001.
  description: This rule checks if Cloud Functions are assigned only the necessary permissions to perform their tasks. Verify that roles granted do not exceed the minimum required and remove any excess permissions. Use IAM roles such as 'roles/cloudfunctions.invoker' judiciously to avoid broad access. Review and adjust IAM policies regularly to maintain compliance with least privilege principles.
  references:
  - https://cloud.google.com/functions/docs/securing
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices-for-managing-iam-policies
- rule_id: gcp.cloudfunctions.function.serverless_prov_conc_execution_role_least_privilege
  service: cloudfunctions
  resource: function
  requirement: Serverless Prov Conc Execution Role Least Privilege
  scope: cloudfunctions.function.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Cloud Functions Use Least Privilege for Execution Roles
  rationale: Using the least privilege principle for Cloud Functions execution roles minimizes the risk of unauthorized access and data breaches. Over-privileged roles can lead to potential exploitation if compromised, impacting data confidentiality and integrity. Adhering to this principle supports compliance with regulatory frameworks such as NIST and PCI-DSS, which mandate minimal access necessary for functionality.
  description: This rule checks whether Cloud Functions in GCP are configured to use execution roles with the least privilege necessary. It ensures that the role assigned to a function has only the necessary permissions to perform its operations, reducing the attack surface. Verify by reviewing the IAM policies attached to execution roles and adjust permissions to align with the function's operational requirements. Remediation involves editing the IAM policy to remove any excessive permissions not required by the function.
  references:
  - https://cloud.google.com/functions/docs/securing/function-identity
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/best-practices-for-using-granting-roles
- rule_id: gcp.cloudfunctions.function.serverless_published_and_immutable
  service: cloudfunctions
  resource: function
  requirement: Serverless Published And Immutable
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud Functions Are Published and Immutable
  rationale: Making serverless functions immutable after publication helps prevent unauthorized modifications that could introduce vulnerabilities or disrupt operations. This practice supports regulatory compliance by ensuring auditability and maintaining a secure baseline, reducing the risk of data breaches or service disruptions due to unapproved changes.
  description: This rule checks whether Google Cloud Functions are configured to be published and immutable, ensuring that once deployed, they cannot be altered without going through proper version control processes. Verification involves checking that the functions are deployed from source control systems or using versioned artifacts. Remediation includes implementing a CI/CD pipeline that enforces immutability by deploying only version-tagged code from secure repositories.
  references:
  - https://cloud.google.com/functions/docs/bestpractices/security
  - https://cloud.google.com/functions/docs/deploying
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudfunctions.function.serverless_role_least_privilege
  service: cloudfunctions
  resource: function
  requirement: Serverless Role Least Privilege
  scope: cloudfunctions.function.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Cloud Functions Service Accounts
  rationale: Assigning excessive permissions to Cloud Functions can lead to unauthorized access, data breaches, and compliance violations. By adhering to the principle of least privilege, organizations minimize the potential impact of compromised credentials or malicious actors exploiting over-privileged roles. This practice aligns with compliance frameworks such as PCI-DSS and ISO 27001, which require strict access controls.
  description: This rule checks that Cloud Functions are not granted excessive permissions beyond what is necessary for their operation. Ensure that the service accounts associated with Cloud Functions have roles that are specific to their function and activity. Remediation involves reviewing the roles and permissions associated with each function's service account and adjusting them to match the minimum necessary permissions. It is recommended to use IAM Policy Analyzer and IAM Recommender to evaluate and refine these permissions.
  references:
  - https://cloud.google.com/functions/docs/securing/function-identity
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudfunctions.function.serverless_rollforward_rollback_controls_present
  service: cloudfunctions
  resource: function
  requirement: Serverless Rollforward Rollback Controls Present
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Rollforward and Rollback Controls for Cloud Functions
  rationale: Having rollforward and rollback controls in place for Google Cloud Functions mitigates the risk of deploying faulty code, which can lead to service disruptions, security vulnerabilities, and non-compliance with industry regulations. Such controls are crucial in maintaining operational resilience and ensuring that any issues can be quickly remediated by reverting to a known good state.
  description: This rule checks that Cloud Functions have appropriate rollforward and rollback mechanisms implemented, allowing for safe deployment practices. Ensure versioning is enabled for functions to track changes and facilitate easy rollbacks if necessary. Remediation involves configuring deployment workflows to include automated rollback capabilities using tools like Google Cloud Build or third-party CI/CD solutions. Verification can be done by reviewing the deployment scripts and ensuring they include these controls.
  references:
  - https://cloud.google.com/functions/docs/bestpractices
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/devops/devops-processes
  - https://cloud.google.com/build/docs/configuring-builds/configure-deployment
  - https://cloud.google.com/solutions/best-practices-for-building-continuous-delivery-pipelines
- rule_id: gcp.cloudfunctions.function.serverless_source_authenticated
  service: cloudfunctions
  resource: function
  requirement: Serverless Source Authenticated
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud Functions Have Authenticated Source Traffic
  rationale: Allowing unauthenticated traffic to Cloud Functions can expose your application to unauthorized access, leading to data breaches or resource misuse. Enforcing authentication ensures that only intended and verified sources can invoke the functions, mitigating risks associated with unauthorized data exposure and potential exploitation. This is crucial for maintaining the integrity and confidentiality of your serverless applications and meeting compliance standards.
  description: This rule checks that Google Cloud Functions are configured to allow traffic only from authenticated sources. Specifically, it verifies the IAM policies and ingress settings to ensure only authorized entities can invoke the functions. To remediate, configure the Cloud Function's ingress settings to 'Allow internal traffic only' or set up identity-based authentication using IAM roles and policies. Verify using the GCP Console or gcloud CLI to ensure the function's ingress settings are correctly configured.
  references:
  - https://cloud.google.com/functions/docs/securing/authenticating
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/architecture/serverless/security
- rule_id: gcp.cloudfunctions.function.serverless_source_trusted
  service: cloudfunctions
  resource: function
  requirement: Serverless Source Trusted
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud Functions Source is from Trusted Locations
  rationale: Allowing Cloud Functions to execute code from untrusted sources poses security risks such as data exfiltration, unauthorized access, and execution of malicious code. This could lead to breaches of sensitive data, service disruptions, and non-compliance with regulations like GDPR, HIPAA, or PCI-DSS.
  description: This rule checks whether Cloud Functions are configured to execute code only from trusted and verified sources, such as authorized repositories or storage locations. Administrators should verify the source of the code before deployment by ensuring proper IAM roles and permissions are applied, and by utilizing version control systems with strong access controls. Remediation involves using Google Cloud's IAM to restrict who can modify the function's source and employing signed URLs or checksums for integrity verification.
  references:
  - https://cloud.google.com/functions/docs/bestpractices/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.cloudfunctions.function.serverless_version_immutable
  service: cloudfunctions
  resource: function
  requirement: Serverless Version Immutable
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud Functions Use Immutable Serverless Versions
  rationale: Immutable serverless versions in Google Cloud Functions prevent unintended changes and ensure consistent, repeatable deployments. This minimizes the risk of unauthorized code changes, which could introduce vulnerabilities or disrupt services. Regulatory frameworks often require integrity and consistency in deployment processes to ensure compliance with standards like SOC2 and ISO 27001.
  description: This rule checks that Google Cloud Functions are configured to use immutable versions, ensuring that once deployed, the function code cannot be altered without deploying a new version. To verify, ensure that your deployment process includes specific versioning practices for Cloud Functions. Remediation involves setting up deployment pipelines using tools like Cloud Build, which automatically version functions and maintain deployment integrity.
  references:
  - https://cloud.google.com/functions/docs/versioning
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/architecture/serverless-best-practices
  - https://cloud.google.com/cloud-build/docs/configuring-builds/create-basic-configuration
- rule_id: gcp.cloudfunctions.function.serverless_vpc_private_networking_enabled
  service: cloudfunctions
  resource: function
  requirement: Serverless VPC Private Networking Enabled
  scope: cloudfunctions.function.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enable Serverless VPC Access for Cloud Functions
  rationale: Enabling Serverless VPC Access for Cloud Functions is crucial for securing network traffic and ensuring that function executions remain within a controlled network environment. This reduces the risk of unauthorized access over the public internet and helps comply with stringent data protection regulations by maintaining data within private network boundaries.
  description: This rule verifies that Google Cloud Functions have Serverless VPC Access enabled, allowing them to communicate privately with resources in a Virtual Private Cloud (VPC). To configure, associate your function with a VPC connector. This can be done via the Google Cloud Console by navigating to the 'VPC Access' section under Cloud Functions or by using the gcloud CLI. Remediation involves creating a serverless VPC access connector if one does not exist and updating the function configuration to use this connector.
  references:
  - https://cloud.google.com/functions/docs/networking/network-settings
  - https://cloud.google.com/functions/docs/networking/connecting-vpc
  - https://cloud.google.com/architecture/serverless-vpc-access
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
- rule_id: gcp.cloudfunctions.function.url_public
  service: cloudfunctions
  resource: function
  requirement: Url Public
  scope: cloudfunctions.function.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to GCP Cloud Functions URLs
  rationale: Allowing public access to Cloud Functions URLs can expose sensitive data and operations to unauthorized users, leading to potential data breaches and service disruptions. Unrestricted access could be exploited by attackers to execute malicious actions or access confidential information, thereby violating compliance with regulations such as PCI-DSS, HIPAA, and SOC2.
  description: This rule checks for GCP Cloud Functions that are configured with publicly accessible URLs, which can be a significant security risk. To verify, ensure that Cloud Functions are only accessible to authorized users and service accounts using IAM policies. Remediation involves updating IAM settings to restrict access to specific users and roles, and removing 'allUsers' or 'allAuthenticatedUsers' from the function's permissions.
  references:
  - https://cloud.google.com/functions/docs/securing/managing-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
- rule_id: gcp.cloudidentity.group.identity_access_attached_policies_not_admin_star
  service: cloudidentity
  resource: group
  requirement: Identity Access Attached Policies Not Admin Star
  scope: cloudidentity.group.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Admin * Policies on Cloud Identity Groups
  rationale: Ensuring that Cloud Identity groups do not have overly permissive admin policies is crucial to maintaining the principle of least privilege. Overly broad permissions can lead to unauthorized access and potential data breaches, impacting the organization's security posture and compliance with regulations such as PCI-DSS and SOC2.
  description: This rule checks for Cloud Identity groups with attached policies that include administrative access ('admin' roles with '*') which may grant excessive permissions. Administrators should review and revise these policies to ensure they align with the principle of least privilege. Remediation involves auditing the policies attached to each group and removing or constraining permissions that are too broad.
  references:
  - https://cloud.google.com/identity/docs/admin-overview
  - https://cloud.google.com/iam/docs/understanding-roles#primitive_roles
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.isaca.org/resources/cobit
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
- rule_id: gcp.cloudidentity.group.identity_access_membership_review_enabled_where_supported
  service: cloudidentity
  resource: group
  requirement: Identity Access Membership Review Enabled Where Supported
  scope: cloudidentity.group.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Identity Access Membership Review in Cloud Identity Groups
  rationale: Enabling identity access membership review helps to ensure that only authorized users have access to sensitive resources, minimizing the risk of unauthorized access and potential data breaches. This practice supports compliance with regulations by maintaining up-to-date access controls and auditing capabilities, enhancing the organization's security posture.
  description: This rule checks if identity access membership review is enabled for Cloud Identity groups where supported. Regular reviews of group memberships are crucial to verify that users' access levels align with their current roles and responsibilities. To enable this, navigate to the Cloud Identity console, select the group, and configure the membership settings to require periodic reviews. Remediation involves setting an appropriate review frequency and ensuring responsible parties are notified of upcoming reviews.
  references:
  - https://cloud.google.com/identity/docs/manage-access
  - https://cloud.google.com/security/compliance/cis#section_8
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/identity-and-access-management
  - https://cloud.google.com/iam/docs/audit-config
- rule_id: gcp.cloudidentity.group.identity_access_no_inline_policies
  service: cloudidentity
  resource: group
  requirement: Identity Access No Inline Policies
  scope: cloudidentity.group.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure No Inline Policies on Cloud Identity Groups
  rationale: Inline policies directly attached to Cloud Identity Groups can lead to hidden permissions that are difficult to track and audit, increasing the risk of unauthorized access. By enforcing centralized policy management through IAM roles, organizations can better adhere to compliance requirements such as PCI-DSS and ISO 27001, and reduce the attack surface by ensuring consistent access controls.
  description: This rule checks for the presence of inline policies directly attached to Cloud Identity Groups, which can bypass the standard IAM roles and permissions model. Inline policies should be avoided to simplify policy management and auditing. Remediation involves transitioning any inline policies to managed IAM roles and ensuring that all permissions are granted through these roles. Verification can be done by reviewing the configuration of Cloud Identity Groups and ensuring no inline policies are present.
  references:
  - https://cloud.google.com/identity/docs/concepts/overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.cloudidentity.membership.identity_access_access_keys_rotated_90_days_or_less__present
  service: cloudidentity
  resource: membership
  requirement: Identity Access Access Keys Rotated 90 Days Or Less Present
  scope: cloudidentity.membership.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Access Keys Are Rotated Every 90 Days
  rationale: Regular rotation of access keys mitigates the risk of credential compromise, which can lead to unauthorized access to sensitive resources. This practice supports compliance with industry standards such as NIST and ISO 27001, reducing the risk of data breaches and ensuring a strong security posture against credential-based attacks.
  description: This rule checks whether access keys associated with Cloud Identity memberships have been rotated within the past 90 days. Regularly rotating access keys reduces the window of opportunity for malicious actors to exploit compromised credentials. To verify compliance, review the access key creation or last rotation dates within the Cloud Console or via the command line. Remediation involves scheduling regular key rotations and implementing automated notifications for keys nearing the rotation threshold.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.cloudidentity.membership.identity_access_console_password_present_only_if_required
  service: cloudidentity
  resource: membership
  requirement: Identity Access Console Password Present Only If Required
  scope: cloudidentity.membership.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Console Passwords Set Only When Necessary
  rationale: Unnecessary console passwords increase the risk of unauthorized access and potential data breaches. In environments where they are not required, they can introduce vulnerabilities that bypass more secure authentication methods. Minimizing password use aligns with least privilege principles and helps comply with regulations like PCI-DSS and ISO 27001, reducing attack surfaces and improving overall security posture.
  description: This rule checks that Identity Access Console passwords are present only when explicitly required for operational purposes. It ensures that console access is secured using alternative, more secure authentication methods like OAuth or SAML where possible. To verify, audit the membership settings in Cloud Identity to ensure that passwords are not set unless justified. If passwords are found without necessity, remove them and implement stronger authentication mechanisms.
  references:
  - https://cloud.google.com/identity/docs/setup
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices/identity-access-management
- rule_id: gcp.cloudidentity.membership.identity_access_inactive_90_days_disabled
  service: cloudidentity
  resource: membership
  requirement: Identity Access Inactive 90 Days Disabled
  scope: cloudidentity.membership.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Disable Inactive Identity Access After 90 Days
  rationale: Inactive identities pose a security risk as they can be exploited by unauthorized users to gain access to sensitive systems. Regularly disabling inactive accounts reduces the potential attack surface and helps organizations comply with regulatory standards like NIST and ISO 27001 which mandate secure management of user identities.
  description: This rule checks if user accounts in Google Cloud Identity have been inactive for over 90 days and ensures they are disabled. Inactive accounts are those that have not been logged into or used for any activities. Administrators can verify account activity through the Google Cloud Console under the 'Identity' section and disable inactive accounts manually or automate the process using identity management tools. Ensuring accounts are disabled after a period of inactivity mitigates the risk of unauthorized access.
  references:
  - https://cloud.google.com/identity
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-cloud-identity-security
- rule_id: gcp.cloudidentity.membership.identity_access_mfa_required
  service: cloudidentity
  resource: membership
  requirement: Identity Access MFA Required
  scope: cloudidentity.membership.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA for Cloud Identity Membership Access
  rationale: Requiring multi-factor authentication (MFA) for all members of Cloud Identity ensures that unauthorized access through compromised credentials is mitigated, reducing the risk of data breaches. This is critical for protecting sensitive information and ensuring compliance with security regulations such as PCI-DSS and HIPAA, which mandate strong authentication mechanisms.
  description: This rule checks whether all members in Cloud Identity have MFA enabled to access resources. MFA adds an extra layer of security by requiring users to provide two or more verification factors. To verify compliance, audit the identity provider settings or use the Google Cloud Console to ensure MFA is enforced. If MFA is not enabled, configure identity policies to require it for all users accessing the Cloud Identity services.
  references:
  - https://cloud.google.com/identity/docs/concepts/mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.cloudidentity.membership.identity_access_no_inline_policies_attached
  service: cloudidentity
  resource: membership
  requirement: Identity Access No Inline Policies Attached
  scope: cloudidentity.membership.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Inline Policies on Cloud Identity Memberships
  rationale: Attaching inline policies directly to Cloud Identity memberships can lead to inconsistent and difficult-to-manage access controls. This increases the risk of unintentional privilege escalation and makes it challenging to audit and comply with regulatory standards such as NIST and ISO 27001. By avoiding inline policies, organizations can ensure a more streamlined and secure access management strategy.
  description: This rule checks for the presence of inline policies directly attached to Cloud Identity memberships. Inline policies can lead to security risks by creating policy sprawl and making it difficult to maintain proper access controls. To verify, review membership configurations and ensure policies are attached through managed roles instead. Remediate by detaching inline policies and applying necessary permissions via predefined roles in IAM to improve manageability and security.
  references:
  - https://cloud.google.com/identity/docs
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudkms.crypto_key.cmk_rotation_enabled
  service: cloudkms
  resource: crypto_key
  requirement: CMK Rotation Enabled
  scope: cloudkms.crypto_key.key_rotation
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure CMK Rotation is Enabled for Crypto Keys
  rationale: Regular rotation of Customer Managed Keys (CMKs) mitigates the risk of data breaches and unauthorized access by limiting the lifespan of any single encryption key. This practice is crucial for maintaining compliance with data protection regulations like PCI-DSS and HIPAA, which mandate robust encryption management policies to safeguard sensitive data.
  description: This rule checks whether CMK rotation is enabled for Google Cloud KMS crypto keys. Enabling automatic rotation helps in minimizing potential exposure by automatically generating new encryption keys at regular intervals. To verify this setting, navigate to the Google Cloud Console, go to Cloud KMS, select the crypto key, and ensure the 'Rotation period' is configured. If not enabled, set an appropriate rotation period, such as 90 days, to enhance security posture.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 4.3
  - NIST SP 800-57 Part 1 Revision 5
  - PCI-DSS v3.2.1 Requirement 3.6.4
  - https://cloud.google.com/security/encryption-at-rest
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudkms.crypto_key.encryption_enabled
  service: cloudkms
  resource: crypto_key
  requirement: Encryption Enabled
  scope: cloudkms.crypto_key.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Cloud KMS Crypto Keys are Encrypted
  rationale: Ensuring encryption for Cloud KMS keys protects sensitive data from unauthorized access and potential breaches. Encryption is critical for maintaining data integrity and confidentiality, mitigating risks associated with data exposure. Compliance with regulations such as PCI-DSS and HIPAA often mandates encryption, ensuring that businesses meet legal requirements and avoid financial and reputational damages.
  description: This rule checks that Cloud KMS crypto keys are encrypted using appropriate algorithms. Ensure that all crypto keys are set up with encryption at rest enabled. Verification involves inspecting the key's configuration in the Google Cloud Console or via the gcloud command-line tool. Remediation includes configuring or updating crypto keys to enforce encryption settings. Use the GCP Console or gcloud commands to verify and adjust settings as needed.
  references:
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
- rule_id: gcp.cloudkms.crypto_key.expiration_set
  service: cloudkms
  resource: crypto_key
  requirement: Expiration Set
  scope: cloudkms.crypto_key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Expiration is Set for Cloud KMS Crypto Keys
  rationale: Setting an expiration date for Cloud KMS crypto keys is crucial to prevent the use of outdated keys, which can pose security risks such as unauthorized access to encrypted data. It helps in managing key lifecycle effectively, reducing the risk of key compromise and ensuring compliance with data protection regulations such as PCI-DSS and NIST SP 800-57, which mandate key rotation and expiration strategies.
  description: This rule checks whether an expiration date is set for all Cloud KMS crypto keys. By configuring an expiration date, organizations can ensure that keys are rotated or decommissioned after their intended usage period, preventing potential misuse. To verify, review the Cloud KMS key properties in the GCP Console or via gcloud command-line tool. Remediate by setting an expiration date through the GCP Console or using the gcloud command-line tool with the 'expiration-time' parameter.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://cloud.google.com/kms/docs/reference/rest/v1/projects.locations.keyRings.cryptoKeys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-4/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.cloudkms.crypto_key.key_iam_policy_public_access_configured
  service: cloudkms
  resource: crypto_key
  requirement: Key IAM Policy Public Access Configured
  scope: cloudkms.crypto_key.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Access to Cloud KMS Crypto Keys
  rationale: Allowing public access to Cloud KMS crypto keys poses significant security risks, including unauthorized data decryption and potential data breaches. Such access violates best practices and compliance standards like PCI-DSS and ISO 27001, which mandate strict control over cryptographic keys to safeguard sensitive data. Misconfigured access can lead to financial losses and damage to organizational reputation.
  description: This rule checks for configurations that grant public access to Cloud KMS crypto keys by reviewing IAM policies attached to the keys. A key that is publicly accessible can be exploited by unauthorized users, leading to potential data exposure. To remediate, ensure no IAM policy grants 'allUsers' or 'allAuthenticatedUsers' roles that allow access to the crypto key. Regularly audit key permissions using the GCP Console or gcloud command-line tool to maintain strict access controls.
  references:
  - https://cloud.google.com/kms/docs/iam
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/kms/docs/securing
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.cloudkms.crypto_key.lifecycle_policy
  service: cloudkms
  resource: crypto_key
  requirement: Lifecycle Policy
  scope: cloudkms.crypto_key.lifecycle_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Implement and Maintain KMS Key Lifecycle Management
  rationale: Proper lifecycle management of Cloud KMS keys is critical to ensuring data privacy and security. Without automated rotation and expiration policies, keys may become vulnerable to unauthorized access or cryptographic attacks, potentially leading to data breaches. Compliance with regulations such as PCI-DSS and HIPAA often mandates strict key management practices to protect sensitive information.
  description: This rule checks whether Cloud KMS keys have a defined lifecycle policy that includes key rotation and expiration schedules. Ensure each key has a 'nextRotationTime' and 'rotationPeriod' set to enforce automatic key rotation, reducing the risk of key compromise. Remediation involves configuring the key with a rotation period, typically every 90 days, via the GCP Console or gcloud command-line tool. Verify lifecycle settings by reviewing key properties in the Cloud KMS section of the GCP Console.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://cloud.google.com/security/compliance/cis#gcp_cis
  - https://www.pcisecuritystandards.org
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.cloudkms.crypto_key.not_publicly_accessible
  service: cloudkms
  resource: crypto_key
  requirement: Not Publicly Accessible
  scope: cloudkms.crypto_key.public_access
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: critical
  title: Ensure KMS Crypto Keys Are Not Publicly Accessible
  rationale: Publicly accessible KMS crypto keys pose a significant risk as unauthorized access could lead to data breaches, compromising sensitive information and violating data protection regulations. Ensuring these keys are not publicly accessible protects against unauthorized decryption of critical data and aligns with compliance requirements such as PCI-DSS and HIPAA.
  description: This rule checks the IAM policies associated with KMS crypto keys to ensure that no public access is granted. Verify that no 'allUsers' or 'allAuthenticatedUsers' entries exist in the key's IAM policy. To remediate, review the IAM policy of each crypto key and remove any public access entries. Implement principle of least privilege by granting access only to necessary users or service accounts.
  references:
  - https://cloud.google.com/kms/docs/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.cloudkms.crypto_key.not_scheduled_for_deletion
  service: cloudkms
  resource: crypto_key
  requirement: Not Scheduled For Deletion
  scope: cloudkms.crypto_key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure GCP KMS Keys Are Not Scheduled for Deletion
  rationale: Preventing KMS keys from being scheduled for deletion is critical to maintaining data integrity and availability. If a key is deleted, any data encrypted with that key becomes irretrievable, which can lead to data loss and business disruption. Moreover, unnecessary deletion of encryption keys can breach compliance requirements in frameworks like PCI-DSS and HIPAA that mandate data protection and retention.
  description: This rule checks for Google Cloud KMS keys that are scheduled for deletion to ensure they remain available for decrypting data. To verify this, inspect the key's lifecycle state using the Cloud Console or gcloud CLI. Remediation involves canceling the deletion operation if the key is scheduled for deletion by using the 'gcloud kms keys cancel-deletion' command. Ensure key management policies include regular audits to prevent accidental deletion.
  references:
  - https://cloud.google.com/kms/docs/key-states#key_scheduled_for_deletion
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/best-practices
- rule_id: gcp.cloudkms.crypto_key.privacy_encryption_at_rest_cmek_enabled
  service: cloudkms
  resource: crypto_key
  requirement: Privacy Encryption At Rest Cmek Enabled
  scope: cloudkms.crypto_key.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure CMEK is Enabled for KMS Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encrypting data at rest in Google Cloud's Key Management Service (KMS) enhances control over cryptographic operations and key lifecycle management. This reduces the risk of unauthorized access to sensitive data and ensures compliance with stringent data protection regulations such as GDPR and HIPAA. Without CMEK, organizations may face increased risk of data breaches and non-compliance penalties.
  description: This rule verifies that all KMS crypto keys are configured to use Customer-Managed Encryption Keys (CMEK) for encryption at rest. It checks the key configuration settings to ensure that CMEK is enabled and properly managed. To remediate, ensure that all crypto keys are created or updated with CMEK settings via the Cloud Console or gcloud CLI by specifying a user-managed key ring and key. Regular audits should be conducted to maintain compliance and security posture.
  references:
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/architecture/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/creating-keys
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudkms.crypto_key.privacy_encryption_in_transit_tls_min_1_2
  service: cloudkms
  resource: crypto_key
  requirement: Privacy Encryption In Transit TLS Min 1 2
  scope: cloudkms.crypto_key.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure TLS 1.2+ for Cloud KMS Crypto Key Encryption
  rationale: Using a minimum of TLS 1.2 for encryption in transit protects sensitive key material from interception and man-in-the-middle attacks, which can lead to unauthorized data access and breaches. This is crucial for maintaining the confidentiality and integrity of cryptographic operations and adhering to compliance standards like PCI-DSS and HIPAA.
  description: This rule checks that Google Cloud KMS Crypto Keys are using at least TLS 1.2 for encryption in transit. To verify, inspect the network configuration settings to ensure TLS 1.2 or higher is enforced. Remediation involves updating the TLS policies and ensuring all clients and services interacting with Cloud KMS support TLS 1.2 or newer versions. It is essential to regularly review and update configurations to comply with industry standards and best practices.
  references:
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudkms.crypto_key.project_no_secrets_in_variables_gcp_cloud_build_proj_logging
  service: cloudkms
  resource: crypto_key
  requirement: Project No Secrets In Variables Gcp Cloud Build Proj Logging
  scope: cloudkms.crypto_key.logging
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Prevent Secrets in Cloud Build Variables and Enable Logging
  rationale: Inadvertently storing secrets in Cloud Build environment variables can lead to unauthorized access to sensitive information, posing a significant security risk. Such exposure can result in data breaches, non-compliance with regulations like PCI-DSS and HIPAA, and reputational damage. Effective logging ensures that access and modifications to keys are monitored, providing an audit trail for compliance and forensic investigations.
  description: This rule checks that no sensitive secrets are stored in the environment variables of Cloud Build projects and that logging is enabled for cryptographic key activities. Verify that Cloud Build configuration files do not contain hardcoded secrets. Instead, use Google Cloud's Secret Manager. Ensure Cloud KMS key logging is enabled in Cloud Audit Logs to track access and usage. To remediate, audit Cloud Build project configurations, use Secret Manager for sensitive data, and configure logging in the Google Cloud Console under 'Logging' to monitor key usage.
  references:
  - https://cloud.google.com/build/docs/securing-builds/store-secrets
  - https://cloud.google.com/kms/docs/logging
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudkms.crypto_key.rotation_compliance_configured
  service: cloudkms
  resource: crypto_key
  requirement: Rotation Compliance Configured
  scope: cloudkms.crypto_key.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure Crypto Key Rotation Policy is Configured
  rationale: Regular rotation of cryptographic keys is crucial for maintaining security hygiene and minimizing the risk of key compromise. Without a defined rotation policy, keys may remain vulnerable to prolonged exposure, increasing the likelihood of unauthorized access. Compliance with standards like NIST and PCI-DSS often requires key rotation to ensure ongoing data protection and integrity.
  description: This rule checks if a rotation policy is set for each Cloud KMS crypto key, ensuring keys are automatically rotated at regular intervals. To verify, review the 'rotationPeriod' field in the crypto key's properties; it should be configured to a period that aligns with your security policy (e.g., every 90 days). To remediate, update the crypto key settings to include an appropriate 'rotationPeriod' using the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.cloudkms.crypto_key.rotation_enabled
  service: cloudkms
  resource: crypto_key
  requirement: Rotation Enabled
  scope: cloudkms.crypto_key.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure Cloud KMS Crypto Key Rotation is Enabled
  rationale: Enabling key rotation for Cloud KMS crypto keys reduces the risk of key compromise by limiting the duration a key is used to encrypt data. This practice is critical for maintaining data confidentiality and integrity, especially in environments with high compliance requirements such as PCI-DSS and HIPAA, where regular key rotation is mandated to protect sensitive data.
  description: This rule checks if automatic rotation is enabled for Cloud KMS crypto keys, which should be configured to rotate at least every 90 days. To verify, navigate to the Cloud KMS section in the GCP Console, select the crypto key, and check the rotation schedule settings. To enable rotation, set an automatic rotation schedule through the GCP Console or gcloud command-line tool. Regularly rotated keys minimize exposure time and help adhere to compliance standards.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57pt1r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudkms.crypto_key.secrets_kms_key_deletion_requires_waiting_period
  service: cloudkms
  resource: crypto_key
  requirement: Secrets KMS Key Deletion Requires Waiting Period
  scope: cloudkms.crypto_key.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Require Waiting Period for KMS Key Deletion
  rationale: Enforcing a waiting period for KMS key deletion prevents accidental or malicious data loss by allowing time to recover keys before permanent deletion. This reduces the risk of data breaches associated with lost encryption keys and complies with security standards that mandate data protection and integrity. Implementing this control aids in meeting regulatory requirements such as GDPR and PCI-DSS which emphasize data confidentiality and availability.
  description: This rule checks if a waiting period is configured for the deletion of Cloud KMS keys used to encrypt secrets. Specifically, it ensures that a minimum waiting period of 7 days is enforced, allowing sufficient time to recover keys if a deletion request was made in error. Verification can be done by reviewing the key lifecycle settings in the Google Cloud Console or using the Google Cloud SDK. Remediation involves setting a 'deleteProtection' attribute on the crypto key to true, ensuring a waiting period is enforced before deletion.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, 4.3
  - https://www.nist.gov/publications/nist-special-publication-800-57-part-1-revision-5
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.cloudkms.crypto_key.secrets_kms_key_not_publicly_accessible
  service: cloudkms
  resource: crypto_key
  requirement: Secrets KMS Key Not Publicly Accessible
  scope: cloudkms.crypto_key.public_access
  domain: secrets_and_key_management
  subcategory: key_management
  severity: critical
  title: Ensure KMS Keys Are Not Publicly Accessible
  rationale: Publicly accessible KMS keys pose a severe risk as they can be exploited by malicious actors to access sensitive data. Unauthorized access to keys can lead to data breaches and non-compliance with standards like PCI-DSS and HIPAA, resulting in legal and financial repercussions. Ensuring keys are private protects against these threats and maintains trust.
  description: This rule checks that KMS keys used for encrypting secrets are not publicly accessible. Public access should be disabled by configuring IAM policies to restrict permissions to only trusted users and services. Verify key access policies via the GCP Console or gcloud CLI, and remove any public bindings. Restrict access using the principle of least privilege to prevent unauthorized access.
  references:
  - https://cloud.google.com/kms/docs/securing-keys
  - https://cloud.google.com/kms/docs/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudkms.crypto_key.secrets_kms_key_policy_least_privilege
  service: cloudkms
  resource: crypto_key
  requirement: Secrets KMS Key Policy Least Privilege
  scope: cloudkms.crypto_key.policy_management
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure KMS Key Policies Enforce Least Privilege Access
  rationale: Applying least privilege principles to KMS key policies is crucial to minimize unauthorized access and potential data breaches. Overly permissive key policies can lead to misuse of encryption keys, resulting in exposure of sensitive data and non-compliance with regulations like GDPR and PCI-DSS. Ensuring restrictive access helps maintain data confidentiality and integrity, reducing potential attack vectors.
  description: This rule checks that Cloud KMS key policies grant only the necessary permissions to intended users or service accounts. Review and adjust the IAM policies associated with your KMS keys to ensure they follow the principle of least privilege. Audit the roles assigned to each key and remove any excessive permissions. Use the GCP Console or CLI to examine IAM policy bindings and update configurations to align with best practices.
  references:
  - https://cloud.google.com/kms/docs/resource-hierarchy-access-control
  - https://www.cisecurity.org/controls/cis-google-cloud-computing-foundations-benchmark-v1-2-0
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.cloudkms.crypto_key.secrets_kms_key_rotation_enabled
  service: cloudkms
  resource: crypto_key
  requirement: Secrets KMS Key Rotation Enabled
  scope: cloudkms.crypto_key.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Enable Rotation for Secrets KMS Crypto Keys
  rationale: Regular key rotation for Secrets KMS keys reduces the risk of key compromise by limiting the exposure of cryptographic keys over time. It aligns with compliance mandates such as PCI-DSS and NIST SP 800-57, which require periodic key changes to enhance security posture and protect sensitive data from unauthorized access.
  description: This rule verifies that all CryptoKeys used for secrets management have automatic rotation enabled in Google Cloud KMS. Automatic key rotation helps to mitigate the risk of key exposure and ensures keys are regularly updated without manual intervention. To remediate, set a rotation period for each CryptoKey using the Google Cloud Console or gcloud CLI, specifying a rotation period of no more than 365 days.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudkms.key_ring.secrets_kms_default_keys_disabled_where_cmek_required
  service: cloudkms
  resource: key_ring
  requirement: Secrets KMS Default Keys Disabled Where Cmek Required
  scope: cloudkms.key_ring.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Disable Default KMS Keys for Secrets When CMEK is Required
  rationale: Using default KMS keys for secrets management can expose your organization to potential data breaches, as these keys are not sufficiently isolated and may not meet compliance requirements for data encryption. By disabling default keys where Customer-Managed Encryption Keys (CMEK) are required, you ensure that sensitive data is protected with stronger, customer-controlled encryption, reducing the risk of unauthorized access and aligning with regulatory standards.
  description: This rule checks that default KMS keys are disabled for managing secrets in environments where Customer-Managed Encryption Keys (CMEK) are mandated. Organizations should configure their key rings to use CMEK instead of default Google-managed keys to maintain full control over encryption keys and compliance. Verify this by checking the key ring configuration settings in Cloud KMS and ensure that only CMEK are used. To remediate, update your Cloud KMS settings to disable default keys and configure CMEK usage as per your security policy.
  references:
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.cloudkms.key_ring.secrets_kms_logging_enabled
  service: cloudkms
  resource: key_ring
  requirement: Secrets KMS Logging Enabled
  scope: cloudkms.key_ring.logging
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Cloud KMS Key Ring Logging is Enabled
  rationale: Enabling logging for Cloud KMS key rings is crucial for monitoring access and changes to encryption keys. Without logging, unauthorized access or misuse of keys might go unnoticed, posing a significant security risk. Compliance with frameworks like PCI-DSS and HIPAA requires detailed record-keeping of key management activities to ensure data protection and accountability.
  description: This rule verifies that logging is enabled for Cloud KMS key rings to track all access and modifications. To ensure logging is active, check that key ring activity is being recorded in Cloud Audit Logs. Remediation involves configuring the appropriate logging settings in the Google Cloud Console under the 'Logging' section for each key ring, ensuring audit logs are set to capture 'Admin Read', 'Admin Write', and 'Data Access' events.
  references:
  - https://cloud.google.com/kms/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.cloudsql.backup.db_cluster_snapshot_cross_account_sharing_restricted
  service: cloudsql
  resource: backup
  requirement: Db Cluster Snapshot Cross Account Sharing Restricted
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Restrict Cross-Account Sharing of Cloud SQL Backups
  rationale: Restricting cross-account sharing of Cloud SQL backups is crucial to prevent unauthorized access to sensitive database content, which can lead to data breaches and compromise business operations. This control ensures that backups are only accessible by authorized accounts, mitigating risks of accidental exposure and aligning with compliance standards such as GDPR and PCI-DSS.
  description: This rule checks if Cloud SQL backups are shared across different GCP accounts without explicit authorization. Ensure that all snapshots are configured to deny access to external accounts unless explicitly required for business processes. To verify, review the IAM policies and permissions on Cloud SQL backups and ensure they are restricted to necessary accounts only. If cross-account access is detected, adjust the IAM settings to limit access to authorized users and accounts.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.backup.db_cluster_snapshot_cross_region_copy_encrypted
  service: cloudsql
  resource: backup
  requirement: Db Cluster Snapshot Cross Region Copy Encrypted
  scope: cloudsql.backup.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Snapshots Are Encrypted for Cross-Region Copies
  rationale: Encrypting database cluster snapshot copies across regions protects sensitive data from unauthorized access during transit and at rest. This measure mitigates the risk of data breaches, reduces potential exposure to legal liabilities, and aligns with compliance mandates such as GDPR and HIPAA, which require robust data protection strategies.
  description: This rule checks whether your Cloud SQL database snapshots that are copied across regions are encrypted. To verify this, ensure that the 'encryptionConfiguration' field is set in the Cloud SQL instance settings. If not enabled, configure Cloud SQL to use customer-managed encryption keys (CMEK) for snapshots. This ensures the data is encrypted using keys that you control, providing an additional layer of security.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/iam/docs/using-cmek
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.backup.db_cluster_snapshot_encrypted
  service: cloudsql
  resource: backup
  requirement: Db Cluster Snapshot Encrypted
  scope: cloudsql.backup.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Backup Snapshots Are Encrypted
  rationale: Encrypting Cloud SQL backup snapshots protects sensitive data from unauthorized access and potential data breaches. It mitigates risks related to data exfiltration and aligns with regulatory compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate encryption of data at rest. Unencrypted backups can expose sensitive information, leading to legal penalties and damage to organizational reputation.
  description: This rule checks that all Cloud SQL database backup snapshots are encrypted using Customer-Managed Encryption Keys (CMEK). To verify, ensure that the `encryptionConfiguration` field is set in the Cloud SQL instance settings. If not configured, enable encryption by setting up a CMEK in Google Cloud's Key Management Service (KMS) and associate it with your Cloud SQL instances. Regularly audit backup configurations to maintain compliance.
  references:
  - https://cloud.google.com/sql/docs/mysql/encrypt-data
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.cloudsql.backup.db_cluster_snapshot_not_publicly_shared
  service: cloudsql
  resource: backup
  requirement: Db Cluster Snapshot Not Publicly Shared
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Prevent Public Sharing of Cloud SQL Backup Snapshots
  rationale: Publicly shared database backups can expose sensitive data to unauthorized parties, increasing the risk of data breaches and non-compliance with regulations like GDPR and HIPAA. Properly securing these backups is critical to maintaining data integrity and ensuring that disaster recovery processes do not inadvertently lead to data leaks.
  description: This rule checks that Cloud SQL database cluster snapshots are not publicly shared. Public access to backups can be controlled by reviewing and configuring access policies in the Cloud SQL instance settings. To remediate, ensure that IAM policies do not grant public access to these resources and regularly audit permissions to maintain compliance.
  references:
  - https://cloud.google.com/sql/docs/mysql/manage-sql-instances#backup
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.backup.db_snapshot_cross_account_sharing_restricted
  service: cloudsql
  resource: backup
  requirement: Db Snapshot Cross Account Sharing Restricted
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Restrict Cross-Account Sharing of Cloud SQL Backups
  rationale: Restricting cross-account sharing of Cloud SQL backups is essential to prevent unauthorized access and potential data breaches. Sharing backups across accounts can expose sensitive data to unintended parties, increasing the risk of data exfiltration and compliance violations, particularly under regulations such as GDPR or HIPAA. Limiting sharing helps maintain data sovereignty and integrity, ensuring that only authorized personnel have access to critical backups.
  description: This rule checks for configurations that allow Cloud SQL database backups to be shared across different GCP accounts. To verify compliance, review the IAM policies associated with Cloud SQL backup resources and ensure that permissions for sharing are restricted to trusted accounts only. Remediation involves configuring IAM roles and policies to limit access strictly to necessary accounts, thereby reducing the risk of data leaks. It is advisable to audit and update these permissions regularly to align with the principle of least privilege.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.cloudsql.backup.db_snapshot_cross_region_copy_encrypted
  service: cloudsql
  resource: backup
  requirement: Db Snapshot Cross Region Copy Encrypted
  scope: cloudsql.backup.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region SQL Backups are Encrypted
  rationale: Encrypting cross-region backups of Cloud SQL instances protects sensitive data from unauthorized access during transit and storage in different geographic locations. This measure reduces the risk of data breaches that can lead to financial loss, reputational damage, and non-compliance with data protection regulations like GDPR and CCPA.
  description: This rule verifies that all cross-region copies of Cloud SQL backups are encrypted using Customer Managed Encryption Keys (CMEK). To ensure compliance, configure the Cloud SQL instance to use CMEK for backups and verify encryption settings in the GCP Console under the 'Backups' section for each instance. Remediation involves enabling CMEK and ensuring that the key is available and managed properly.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudsql.backup.db_snapshot_encrypted
  service: cloudsql
  resource: backup
  requirement: Db Snapshot Encrypted
  scope: cloudsql.backup.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Snapshots are Encrypted at Rest
  rationale: Encrypting database snapshots is crucial to prevent unauthorized access to sensitive data, which could lead to data breaches and non-compliance with regulatory standards such as GDPR and PCI-DSS. Without encryption, any snapshot could be accessed by malicious actors, posing a threat to data integrity and privacy.
  description: This rule checks whether Cloud SQL snapshots are encrypted at rest using customer-managed encryption keys (CMEK) or Google-managed encryption keys. To verify, ensure that the 'backups' setting for your Cloud SQL instance is enabled and that encryption configurations are properly set up. Remediation involves configuring your Cloud SQL instance to use the appropriate encryption keys and verifying the encryption status of existing snapshots.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.backup.db_snapshot_not_publicly_shared
  service: cloudsql
  resource: backup
  requirement: Db Snapshot Not Publicly Shared
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Prevent Public Access to Cloud SQL Backup Snapshots
  rationale: Publicly shared database snapshots can lead to unauthorized access to sensitive data, posing significant security risks and potential compliance violations. Unauthorized access could result in data breaches, affecting the organization's reputation and potentially leading to regulatory fines under frameworks such as GDPR and HIPAA.
  description: This rule checks that Cloud SQL backup snapshots are not publicly shared, ensuring they are protected from unauthorized access. To verify, review the access control settings in the Cloud SQL backup configuration and ensure that no public IAM policies or ACLs are applied. Remediation involves updating permissions to restrict access only to necessary users and roles, following the principle of least privilege.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/overview#policy_hierarchy
  - https://cloud.google.com/sql/docs/mysql/security
- rule_id: gcp.cloudsql.backup.snapshots_public_access_configured
  service: cloudsql
  resource: backup
  requirement: Snapshots Public Access Configured
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Ensure Cloud SQL Snapshots Do Not Have Public Access
  rationale: Public access to Cloud SQL snapshots poses significant security risks, including unauthorized data exposure and potential data breaches. This can lead to the compromise of sensitive information, financial loss, and non-compliance with data protection regulations such as GDPR and HIPAA. Protecting snapshots by restricting public access is crucial to maintaining the confidentiality and integrity of your database backups.
  description: This rule checks if any Cloud SQL backups have been configured with public access. Ensuring that snapshots are not publicly accessible prevents unauthorized entities from retrieving sensitive data. Verify the backup's access settings in the Google Cloud Console under Cloud SQL > Backups, and ensure that no public IPs are listed. Remediate by configuring access restrictions to allow only authorized users and IP addresses.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/security
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudsql.instance.auto_minor_version_upgrade_configured
  service: cloudsql
  resource: instance
  requirement: Auto Minor Version Upgrade Configured
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Auto Minor Version Upgrade for Cloud SQL Instances
  rationale: Configuring auto minor version upgrades for Cloud SQL instances helps maintain the security and stability of databases by automatically applying the latest minor updates and patches. This reduces the risk of vulnerabilities being exploited in outdated database versions, which can lead to data breaches, service disruptions, and non-compliance with industry regulations.
  description: This rule verifies that auto minor version upgrades are enabled for Cloud SQL instances. It checks the instance configuration to ensure that the 'auto_minor_version_upgrade' setting is set to true. To enable this setting, navigate to the Cloud SQL instance settings in the Google Cloud Console, edit the instance, and enable the 'Auto Minor Version Upgrade' option. This ensures that your databases receive critical security updates automatically, minimizing manual intervention.
  references:
  - https://cloud.google.com/sql/docs/mysql/upgrade-db-versions
  - https://cloud.google.com/sql/docs/mysql/instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.automated_backups
  service: cloudsql
  resource: instance
  requirement: Automated Backups
  scope: cloudsql.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Enable Automated Backups for Cloud SQL Instances
  rationale: Automated backups are crucial for data recovery in case of accidental data loss, corruption, or system failure. Enabling this feature helps ensure business continuity by minimizing downtime and data loss, meeting regulatory requirements for data protection and resilience in frameworks like SOC 2 and ISO 27001.
  description: This rule checks whether automated backups are enabled for Cloud SQL instances. Automated backups must be configured to run at regular intervals, ensuring that recent copies of the database are available for restoration. To verify, navigate to the Cloud SQL Instance settings and ensure that automated backups are activated. Remediation involves enabling automated backups through the GCP Console or using the gcloud command line tool, specifying the desired backup window and retention period.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/sql/docs/postgres/backup-recovery/backups
  - CIS GCP Benchmark v1.3.0, Section 7.5 - Ensure that automated backups are enabled
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.cloudsql.instance.automated_backups_enabled
  service: cloudsql
  resource: instance
  requirement: Automated Backups Enabled
  scope: cloudsql.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Enable Automated Backups for Cloud SQL Instances
  rationale: Automated backups are crucial for protecting data against accidental deletion, corruption, and other forms of data loss. They ensure business continuity by allowing for quick data recovery, thereby minimizing downtime and potential revenue loss. Moreover, they are often a requirement for compliance with regulations such as GDPR and HIPAA, which mandate data protection and recovery processes.
  description: This rule checks if automated backups are enabled for Cloud SQL instances. Automated backups should be configured to occur daily to ensure data is consistently protected. To verify, navigate to the Google Cloud Console, select the SQL instance, and ensure that the 'Automated backups' setting is enabled under the 'Backups' tab. If not enabled, adjust the settings to activate automated backups. This ensures that all critical data is regularly backed up and can be restored in case of failure.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.backup_enabled
  service: cloudsql
  resource: instance
  requirement: Backup Enabled
  scope: cloudsql.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Cloud SQL Instances Have Backups Enabled
  rationale: Enabling backups for Cloud SQL instances is crucial for data resilience and disaster recovery. Without regular backups, you risk data loss in events such as accidental deletions, data corruption, or system failures. This practice supports business continuity and meets compliance requirements like PCI-DSS and ISO 27001, which mandate data protection measures.
  description: This rule checks if automatic backups are enabled for Cloud SQL instances. Backups provide a point-in-time snapshot of the database, which is essential for restoring data in case of incidents. To verify, navigate to the Cloud SQL instances page in the GCP Console and ensure that automatic backups are enabled under the 'Backups' section. Remediation involves configuring a backup schedule and enabling automatic backups through the console or using gcloud CLI commands.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.backup_encrypted
  service: cloudsql
  resource: instance
  requirement: Backup Encrypted
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Backups are Encrypted at Rest
  rationale: Encrypting backups of Cloud SQL instances at rest protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining the confidentiality and integrity of data, meeting regulatory compliance requirements such as GDPR, HIPAA, and PCI-DSS, and mitigating risks associated with data leaks or theft.
  description: This rule checks whether backups for Cloud SQL instances are encrypted using Google-managed or customer-managed keys. To verify, navigate to the Cloud SQL instance settings in the GCP Console and ensure the 'Backups' section indicates that encryption is enabled. Remediation involves configuring encryption settings during the creation of the instance or by updating existing instances to use encrypted backups.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/security-foundations
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.cloudsql.instance.backup_retention_configured
  service: cloudsql
  resource: instance
  requirement: Backup Retention Configured
  scope: cloudsql.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Cloud SQL Backup Retention is Properly Configured
  rationale: Configuring backup retention for Cloud SQL instances is crucial for ensuring data can be restored in the event of accidental deletion, corruption, or other data loss scenarios. It helps maintain business continuity and meets compliance requirements for data protection and disaster recovery, such as those outlined by PCI-DSS and SOC2.
  description: This rule checks whether a backup retention policy is configured for Cloud SQL instances. Without this, data recovery may not be possible, leading to potential data loss. To verify, ensure that the 'backupConfiguration.enabled' setting is true and that a retention period is defined. Remediation involves setting an appropriate retention period in the Google Cloud Console under the SQL instance settings or using the gcloud CLI.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/sql/docs/postgres/backup-recovery/backups
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Section 6.1
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/soc2report.html
- rule_id: gcp.cloudsql.instance.contained_db_authentication_disabled
  service: cloudsql
  resource: instance
  requirement: Contained Db Authentication Disabled
  scope: cloudsql.instance.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Contained Database Authentication for Cloud SQL
  rationale: Disabling contained database authentication for Cloud SQL instances helps prevent unauthorized access by ensuring that authentication is managed at the instance level rather than within individual databases. This reduces the risk of privilege escalation and unauthorized access to sensitive data, aligning with compliance requirements for data protection and access control frameworks.
  description: This rule checks if contained database authentication is disabled for Cloud SQL instances. Contained database authentication allows users to authenticate directly within a database, which can bypass centralized access controls. To ensure stronger security management, it is recommended to disable this feature. Verification can be done by checking the instance settings, and remediation involves modifying the instance configuration to disable contained database authentication.
  references:
  - https://cloud.google.com/sql/docs/mysql/authentication
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.cloudsql.instance.cross_db_ownership_chaining_off
  service: cloudsql
  resource: instance
  requirement: Cross Db Ownership Chaining Off
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable Cross Database Ownership Chaining in Cloud SQL
  rationale: Disabling cross-database ownership chaining reduces the risk of unauthorized access between databases. This setting prevents users with permissions on one database from exploiting trust relationships to access objects in another database, thereby enhancing the overall security posture and supporting compliance with data separation policies.
  description: This rule checks if cross-database ownership chaining is disabled on Cloud SQL instances. Cross-database ownership chaining can be a security risk as it allows users to access objects they do not own across databases. To verify, inspect Cloud SQL instance settings for the 'cross_db_ownership_chaining' parameter and ensure it is set to 'off'. Remediation involves updating the instance configuration to disable this feature.
  references:
  - https://cloud.google.com/sql/docs/postgres/configure-instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://docs.microsoft.com/en-us/sql/relational-databases/security/database-engine-permissions-grants?view=sql-server-ver15
  - https://cloud.google.com/sql/docs/mysql/security
- rule_id: gcp.cloudsql.instance.db_cluster_audit_logging_enabled
  service: cloudsql
  resource: instance
  requirement: Db Cluster Audit Logging Enabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Cloud SQL Audit Logging is Enabled for Instances
  rationale: Enabling audit logging for Cloud SQL instances is crucial for tracking access and changes to your database configurations and data. It helps in identifying unauthorized access attempts and potential malicious activities, thus safeguarding sensitive data and meeting compliance requirements such as PCI-DSS and HIPAA. Without audit logs, organizations may face challenges in forensic investigations and risk non-compliance with regulatory standards.
  description: This rule checks whether audit logging is enabled for Cloud SQL instances in your GCP environment. Audit logs provide detailed insights into who accessed the database, what actions were taken, and when they occurred. To verify, navigate to the 'Logging' section under your Cloud SQL instance settings in the GCP Console and ensure 'Enable audit logs' is selected. Remediation involves configuring the Cloud SQL instance to enable audit logging via the GCP Console or using gcloud command-line tools.
  references:
  - https://cloud.google.com/sql/docs/mysql/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/instance-settings
- rule_id: gcp.cloudsql.instance.db_cluster_deletion_protection_enabled
  service: cloudsql
  resource: instance
  requirement: Db Cluster Deletion Protection Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Deletion Protection for Cloud SQL Instances
  rationale: Enabling deletion protection for Cloud SQL instances prevents accidental or malicious deletion of database clusters, which can lead to data loss, service downtime, and potential financial and reputational damage. This protection is crucial for maintaining data integrity and ensuring business continuity, especially in environments where multiple users have access to infrastructure resources.
  description: This rule checks if deletion protection is enabled for Cloud SQL instances. Deletion protection adds an additional layer of security by requiring a confirmation step before the instance can be deleted. To verify, navigate to the Google Cloud Console, access the SQL section, select your instance, and ensure that deletion protection is toggled on. Remediation involves enabling this setting in the instance configuration either via the console or using gcloud command-line tools.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings#deletion-protection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/architecture/security-best-practices
  - https://cloud.google.com/sql/docs/security-overview
- rule_id: gcp.cloudsql.instance.db_cluster_encryption_at_rest_cmek
  service: cloudsql
  resource: instance
  requirement: Db Cluster Encryption At Rest Cmek
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable CMEK for Cloud SQL Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for Cloud SQL ensures that organizations retain control over the encryption keys used to protect sensitive data at rest. This reduces the risk of unauthorized data access and enables compliance with regulations that require customer control over encryption keys, such as GDPR, HIPAA, and PCI-DSS.
  description: This rule checks that Cloud SQL instances are configured to use CMEK for encryption at rest. To verify, ensure that each Cloud SQL instance has a CMEK enabled in the settings. Remediation involves creating a Cloud Key Management Service (KMS) key and configuring the Cloud SQL instances to use this key for encrypting data at rest.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-cmek
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/sql/docs/mysql/encryption-at-rest
- rule_id: gcp.cloudsql.instance.db_cluster_minor_version_auto_upgrade_enabled
  service: cloudsql
  resource: instance
  requirement: Db Cluster Minor Version Auto Upgrade Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Auto Upgrade for Cloud SQL Minor Versions
  rationale: Enabling automatic minor version upgrades for Cloud SQL ensures that database instances receive the latest security patches and enhancements. This minimizes exposure to vulnerabilities and reduces the risk of data breaches that can lead to financial losses and reputational damage. It also aids in maintaining compliance with regulatory standards that require up-to-date software.
  description: This rule checks if Cloud SQL instances have the minor version auto upgrade feature enabled. This ensures that instances are automatically updated with the latest minor version releases, which often include critical security patches and performance improvements. To verify, check the Cloud SQL instance settings in the Google Cloud Console or use the gcloud CLI. If not enabled, configure the instance to allow minor version auto upgrades through the instance settings.
  references:
  - https://cloud.google.com/sql/docs/mysql/upgrade-db-instance
  - https://cloud.google.com/sql/docs/postgres/db-versions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings#auto-updates
- rule_id: gcp.cloudsql.instance.db_cluster_private_networking_enforced
  service: cloudsql
  resource: instance
  requirement: Db Cluster Private Networking Enforced
  scope: cloudsql.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Cloud SQL Instances Use Private IP Networking
  rationale: Enforcing private networking for Cloud SQL instances mitigates the risk of unauthorized access by ensuring that databases are accessible only through private IPs within a VPC. This reduces attack surface and aligns with compliance requirements like PCI-DSS and HIPAA, which mandate secure access controls for sensitive data.
  description: This rule checks if Cloud SQL instances are configured to use private IP addresses within a Virtual Private Cloud (VPC). By default, instances may be accessible over the public internet, increasing exposure to potential attacks. To remediate, enable private IP for the SQL instances under the 'Connections' settings in the Google Cloud Console, ensuring they are only accessible within authorized VPC networks.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://www.hipaajournal.com/hipaa-security-rule/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/security
- rule_id: gcp.cloudsql.instance.db_deletion_protection_enabled
  service: cloudsql
  resource: instance
  requirement: Db Deletion Protection Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Deletion Protection for Cloud SQL Instances
  rationale: Enabling deletion protection for Cloud SQL instances helps prevent accidental or malicious deletion of critical databases, which can lead to data loss, service downtime, and potential non-compliance with data retention policies. Protecting databases from deletion is crucial for maintaining business continuity and safeguarding sensitive data against unauthorized alterations.
  description: This rule checks if deletion protection is enabled for Cloud SQL instances. Deletion protection ensures that an instance cannot be deleted without first disabling this setting, providing a safeguard against unintended data loss. To verify, access the Cloud SQL instance settings in the Google Cloud Console and ensure the 'Deletion Protection' option is enabled. If not, update the instance configuration to enable this feature through the console or via gcloud command-line tool.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings#deletion_protection
  - https://cloud.google.com/security/compliance/cis#section-5.3
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/sql/docs/security
  - https://cloud.google.com/architecture/securing-sql-instances
- rule_id: gcp.cloudsql.instance.db_encryption_at_rest_enabled
  service: cloudsql
  resource: instance
  requirement: Db Encryption At Rest Enabled
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Encryption at Rest for Cloud SQL Instances
  rationale: Encryption at rest protects sensitive data stored in Cloud SQL instances from unauthorized access and potential data breaches. This is crucial for meeting compliance requirements such as PCI-DSS and HIPAA, and it helps prevent data theft in the event of physical hardware compromise or unauthorized database access.
  description: This rule checks if a Cloud SQL instance has encryption at rest enabled, which ensures that the data stored on the disk is encrypted using Google-managed encryption keys by default. To verify this setting, access the Cloud SQL instance settings in the Google Cloud Console and ensure that encryption is enabled. To remediate, create a new Cloud SQL instance with the 'Enable Encryption' option selected, or use customer-managed encryption keys (CMEK) for additional control.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings#enable-encryption
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/sites/default/files/hipaa-simplified.pdf
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/iam/docs/using-cmek
- rule_id: gcp.cloudsql.instance.db_iam_or_managed_identity_auth_enabled_where_supported
  service: cloudsql
  resource: instance
  requirement: Db IAM Or Managed Identity Auth Enabled Where Supported
  scope: cloudsql.instance.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable IAM or Managed Identity Auth for Cloud SQL Instances
  rationale: Enabling IAM or managed identity authentication for Cloud SQL instances enhances security by leveraging GCP's identity management capabilities. This reduces the risk of unauthorized access through better control and auditing of database access. It also aligns with compliance standards that require strict access management and audit trails for sensitive data stored in databases.
  description: This rule checks whether Cloud SQL instances have IAM or managed identity authentication enabled, which is crucial for securely managing database access. To verify, ensure that Cloud SQL instances are configured to use Cloud IAM or Managed Identity for database authentication. Remediation involves updating the database settings to enable these authentication methods, thereby improving access control and monitoring capabilities.
  references:
  - https://cloud.google.com/sql/docs/mysql/authentication
  - https://cloud.google.com/sql/docs/postgres/authentication
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - 'PCI-DSS Requirement 8: Identify and authenticate access to system components'
  - 'NIST SP 800-53: AC-2 Account Management'
- rule_id: gcp.cloudsql.instance.db_insecure_extensions_not_enabled
  service: cloudsql
  resource: instance
  requirement: Db Insecure Extensions Not Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable Insecure Extensions in Cloud SQL Instances
  rationale: Insecure database extensions can introduce vulnerabilities, allowing attackers to exploit these weaknesses to gain unauthorized access or escalate privileges. This is crucial for maintaining data integrity and protecting sensitive information, aligning with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks for the presence of insecure extensions in Cloud SQL instances. It ensures that such extensions, which are not recommended due to security risks, are disabled. To verify, review the list of enabled extensions in the Cloud SQL instance settings and disable any that are deemed insecure. Remediation involves updating the instance configuration to exclude these extensions, thereby reducing the attack surface.
  references:
  - https://cloud.google.com/sql/docs/mysql/extensions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/sql/docs/postgres/security-overview
- rule_id: gcp.cloudsql.instance.db_only_approved_extensions_enabled
  service: cloudsql
  resource: instance
  requirement: Db Only Approved Extensions Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Only Approved Extensions Enabled on Cloud SQL Instances
  rationale: Enabling only approved database extensions in Cloud SQL reduces the attack surface by preventing potentially vulnerable or unnecessary features from being used. This minimizes the risk of data breaches and ensures compliance with internal security policies and external regulations, such as PCI-DSS and HIPAA, which require minimizing the exposure of sensitive data to unvetted code.
  description: This rule verifies that only approved extensions are enabled on Cloud SQL instances. Administrators should maintain a list of allowed extensions, ensuring that any unapproved extensions are disabled. To verify, audit the extensions enabled on the instance via the Google Cloud Console or gcloud command-line tool, and compare them against the approved list. Remediation involves disabling unapproved extensions and revisiting database functionality to ensure business operations are not disrupted.
  references:
  - https://cloud.google.com/sql/docs/mysql/extensions
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-compliance-requirements/
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.db_parameter_group_audit_logging_enabled_where_supported
  service: cloudsql
  resource: instance
  requirement: Db Parameter Group Audit Logging Enabled Where Supported
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for Cloud SQL Instances Where Supported
  rationale: Audit logging for Cloud SQL instances helps track unauthorized access and changes, which is crucial for identifying potential security incidents and maintaining accountability. It supports compliance with regulations such as PCI-DSS and HIPAA by providing a detailed audit trail of database activities. Without audit logging, organizations may face undetected data breaches, leading to financial and reputational damage.
  description: This rule checks whether audit logging is enabled for Cloud SQL instances where supported. To verify, ensure the 'cloudsql.googleapis.com/sql/admin_audit' log is active in the Google Cloud Console under Logging > Logs Explorer. Remediation involves enabling audit logs by configuring the appropriate database flags and ensuring IAM roles have sufficient permissions for logging. This setting helps capture detailed logs of database activities for security monitoring and incident response.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-logs
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/logging/docs/audit/configure-data-access
- rule_id: gcp.cloudsql.instance.db_parameter_group_insecure_features_disabled_whe_applicable
  service: cloudsql
  resource: instance
  requirement: Db Parameter Group Insecure Features Disabled Whe Applicable
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable Insecure Features in Cloud SQL DB Parameter Groups
  rationale: Disabling insecure features in Cloud SQL DB parameter groups helps mitigate potential vulnerabilities that could be exploited by attackers to compromise database integrity or confidentiality. This is crucial for organizations handling sensitive data, as it reduces the risk of data breaches and aids in compliance with regulatory requirements such as PCI-DSS and HIPAA.
  description: This rule checks whether insecure features, such as unencrypted communication protocols and weak authentication mechanisms, are disabled in Cloud SQL DB parameter groups. Administrators should verify that these features are turned off by reviewing the DB parameter group's settings via the Cloud Console or using gcloud commands. Remediation involves adjusting the parameters to disable insecure options and enabling secure alternatives, such as SSL/TLS for data in transit and strong password policies.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://cloud.google.com/security/
  - https://cloud.google.com/sql/docs/mysql/security
- rule_id: gcp.cloudsql.instance.db_parameter_group_require_ssl
  service: cloudsql
  resource: instance
  requirement: Db Parameter Group Require SSL
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Instances Enforce SSL Connections
  rationale: Enforcing SSL connections for Cloud SQL instances is critical to protect data in transit from eavesdropping and man-in-the-middle attacks. This helps maintain data confidentiality and integrity, which is essential for meeting compliance requirements such as PCI-DSS and HIPAA. Without SSL, sensitive data may be exposed to unauthorized parties, leading to potential data breaches and financial loss.
  description: This rule verifies that Cloud SQL instances are configured to require SSL connections by checking the 'require_ssl' parameter in the database's parameter group. To ensure compliance, navigate to the Cloud SQL instance settings in the GCP Console and under 'Connections', enable 'SSL connections only'. Remediation involves updating the parameter group or applying a new one that enforces SSL, and ensuring that all client applications are configured to use SSL.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/sql/docs/mysql/security-overview
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.cloudsql.instance.db_public_access_disabled
  service: cloudsql
  resource: instance
  requirement: Db Public Access Disabled
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Access for Cloud SQL Instances
  rationale: Exposing Cloud SQL instances to the public internet increases the risk of unauthorized access, data breaches, and potential exploitation of database vulnerabilities. By disabling public access, organizations can ensure that only authorized internal networks can access database services, reducing the attack surface and aligning with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks if Cloud SQL instances have public IP addresses assigned, which allows connections from any IP address on the internet. To verify, inspect the SQL instance settings via the GCP Console or use gcloud commands to list instances with public IPs. Remediation involves configuring instances to use private IPs by modifying network settings, ensuring access is restricted to specific internal networks or VPNs.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.db_require_tls_in_transit
  service: cloudsql
  resource: instance
  requirement: Db Require TLS In Transit
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce TLS for Cloud SQL Instance Connections
  rationale: Requiring TLS for Cloud SQL instances ensures data is encrypted in transit, protecting against man-in-the-middle attacks and eavesdropping. This is crucial for maintaining data confidentiality and integrity, meeting compliance requirements such as PCI-DSS and HIPAA, and upholding customer trust.
  description: This rule verifies that Cloud SQL instances are configured to enforce TLS for all incoming connections. Administrators should enable 'require SSL' settings to ensure encrypted connections, thus safeguarding sensitive data against interception during transit. Verification can be done via the GCP Console or CLI by checking the instance's connection settings. Remediation involves updating the instance configuration to enforce SSL connections.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/sql/docs/postgres/configure-ssl-instance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.instance.db_subnet_group_private_subnets_only
  service: cloudsql
  resource: instance
  requirement: Db Subnet Group Private Subnets Only
  scope: cloudsql.instance.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Cloud SQL Instances Use Private Subnets Only
  rationale: Using private subnets for Cloud SQL instances reduces exposure to potential attacks from the internet and unauthorized access. It aligns with best practices for protecting sensitive data against breaches and complies with regulatory requirements such as HIPAA and PCI-DSS, which mandate strict control over network access to databases.
  description: This rule checks whether Cloud SQL instances are configured to use private subnets only within their Db Subnet Group, ensuring that instances do not have public IP addresses and are isolated from internet traffic. Verify that the 'Private Service Connect' is enabled, and ensure the instance is part of a VPC with private IP enabled. Remediation involves updating the instance's settings to disable public IP and configure networking to use private IPs only according to GCP documentation.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-private-ip
  - https://cloud.google.com/sql/docs/postgres/configure-private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/sqlserver/configure-private-ip
- rule_id: gcp.cloudsql.instance.default_admin_gcp_sql_instance_default_admin
  service: cloudsql
  resource: instance
  requirement: Default Admin Gcp Sql Instance Default Admin
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Disable Default Admin Account on GCP SQL Instances
  rationale: The presence of default admin accounts in Cloud SQL instances poses significant security risks, such as unauthorized access and potential data breaches. Attackers can exploit these accounts to gain full control over the database, compromising sensitive information. Ensuring these accounts are disabled aligns with compliance requirements like PCI-DSS and ISO 27001, which mandate strict access controls.
  description: This rule checks for the presence of default admin accounts on Cloud SQL instances, which should be disabled to prevent unauthorized access. Administrators should create custom accounts with the least privilege necessary for their operations. To verify, inspect the user list on each SQL instance and ensure no default admin accounts are enabled. Remediation involves disabling these accounts and setting up custom admin accounts with strong, unique credentials.
  references:
  - https://cloud.google.com/sql/docs/mysql/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings
- rule_id: gcp.cloudsql.instance.deletion_protection
  service: cloudsql
  resource: instance
  requirement: Deletion Protection
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Deletion Protection for Cloud SQL Instances
  rationale: Deletion protection prevents accidental or malicious deletion of Cloud SQL instances, which can lead to data loss and service disruption. This feature is crucial for maintaining data integrity and availability, especially for instances containing critical business or customer data. Compliance with data protection regulations may also necessitate safeguarding data against unintended deletions.
  description: This rule checks if deletion protection is enabled on Cloud SQL instances. Deletion protection is a setting that prevents the instance from being deleted via the console, gcloud command-line tool, or API. To verify, ensure the 'deletionProtection' field is set to true in the instance configuration. Remediation involves updating the instance settings to enable deletion protection using the Cloud Console, gcloud command-line tool, or REST API.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings#deletion-protection
  - https://cloud.google.com/sql/docs/postgres/instance-settings#deletion-protection
  - https://cloud.google.com/sql/docs/sqlserver/instance-settings#deletion-protection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudsql.instance.dynamodb_table_encryption_enabled
  service: cloudsql
  resource: instance
  requirement: Dynamodb Table Encryption Enabled
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Cloud SQL Instances Have Encryption at Rest Enabled
  rationale: Enabling encryption at rest for Cloud SQL instances protects sensitive data from unauthorized access and data breaches. It is crucial for maintaining data confidentiality and integrity, especially for workloads subject to regulatory standards such as PCI-DSS and HIPAA. This measure mitigates risks associated with data theft or loss in case of physical or logical breaches.
  description: This rule verifies that Google Cloud SQL instances have encryption at rest enabled. By default, Google automatically encrypts all data before it is written to disk using AES-256 encryption. To ensure compliance, users should verify that encryption settings align with organizational policies and regulatory requirements. Remediation involves confirming the use of Google's managed encryption keys or configuring customer-managed encryption keys (CMEK) for advanced control over encryption processes.
  references:
  - https://cloud.google.com/sql/docs/mysql/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.cloudsql.instance.encryption_at_rest_enabled
  service: cloudsql
  resource: instance
  requirement: Encryption At Rest Enabled
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Cloud SQL Instances Have Encryption at Rest Enabled
  rationale: Enabling encryption at rest for Cloud SQL instances is crucial for protecting sensitive data from unauthorized access and potential data breaches. It mitigates risks associated with data theft or exposure in cases of physical storage compromise. Compliance with standards such as PCI-DSS, HIPAA, and SOC2 often mandates encryption at rest for safeguarding sensitive information.
  description: This rule checks that all Cloud SQL instances have encryption at rest enabled by utilizing Google-managed keys or customer-managed keys. Verify this setting by accessing the Cloud SQL instance settings in the Google Cloud Console or using the gcloud CLI. To remediate, ensure that each Cloud SQL instance has the 'Enable Encryption at Rest' option checked during creation or update the instance to enable this option.
  references:
  - https://cloud.google.com/sql/docs/mysql/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.cloudsql.instance.enhanced_monitoring_enabled
  service: cloudsql
  resource: instance
  requirement: Enhanced Monitoring Enabled
  scope: cloudsql.instance.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Enhanced Monitoring for Cloud SQL Instances
  rationale: Enabling enhanced monitoring for Cloud SQL instances provides deeper insights into system performance, which is critical for identifying and mitigating potential security threats and ensuring optimal operation. It helps in meeting compliance requirements by providing detailed metrics and logs, which are necessary for audit trails and forensic investigations in case of a security incident.
  description: This rule checks whether enhanced monitoring is enabled for Cloud SQL instances, which involves configuring the instances to export detailed performance metrics to Cloud Monitoring. To verify, check the 'Database' section in the Google Cloud Console and ensure 'Enable Cloud Monitoring' is activated. Remediation involves enabling this feature, which can be done via the Cloud Console or gcloud command-line tool, ensuring that the Cloud SQL Admin API is enabled and proper IAM roles are assigned.
  references:
  - https://cloud.google.com/sql/docs/mysql/monitoring
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/sql/docs/postgres/monitoring
  - https://cloud.google.com/sql/docs/sqlserver/monitoring
- rule_id: gcp.cloudsql.instance.external_scripts_disabled
  service: cloudsql
  resource: instance
  requirement: External Scripts Disabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable External Scripts for Cloud SQL Instances
  rationale: Disabling external scripts execution in Cloud SQL reduces the potential attack surface by preventing malicious code from running within the database environment. This enhances data integrity and helps comply with regulatory frameworks such as PCI-DSS and HIPAA, which require secure data handling practices. It also mitigates the risk of unauthorized data access and compromise through external scripts that could exploit database vulnerabilities.
  description: This rule checks whether external scripts are disabled for Cloud SQL instances, ensuring that scripts cannot be executed from external sources. To verify compliance, review the Cloud SQL instance settings in the GCP Console or via gcloud CLI to ensure the 'external_scripts' flag is set to 'off'. Remediation involves updating the instance configuration to disable external script execution, which can be done through the GCP Console under the instance's 'Flags' section or using the gcloud command line tool.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.high_availability
  service: cloudsql
  resource: instance
  requirement: High Availability
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud SQL Instances Have High Availability Configured
  rationale: Configuring high availability for Cloud SQL instances is crucial to minimize service downtime and ensure business continuity in case of zone failures. Without high availability, database instances are vulnerable to disruptions, potentially leading to data loss and financial impacts. High availability is often a requirement for compliance with standards such as PCI-DSS and ISO 27001, which demand resilient and fault-tolerant systems.
  description: This rule checks that Cloud SQL instances are configured with high availability, which involves setting up a primary instance and one or more replicas in different zones. To verify, ensure that the 'availabilityType' is set to 'REGIONAL' in the Cloud SQL instance settings. If not configured, enable high availability by modifying the instance settings to include multi-zone replication. This ensures automatic failover and data redundancy across zones.
  references:
  - https://cloud.google.com/sql/docs/mysql/high-availability
  - https://cloudsecurityalliance.org/download/gcp-security-best-practices/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/sql/docs/mysql/configure-ha
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.cloudsql.instance.iam_authentication_enabled
  service: cloudsql
  resource: instance
  requirement: IAM Authentication Enabled
  scope: cloudsql.instance.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enable IAM Authentication for Cloud SQL Instances
  rationale: Enabling IAM authentication for Cloud SQL instances ensures that database access is integrated with Google Cloud's identity management, enhancing access control and reducing the risk of unauthorized access. This is crucial for protecting sensitive data, meeting compliance requirements such as GDPR and HIPAA, and mitigating potential data breaches and insider threats.
  description: This rule checks if IAM authentication is enabled for Cloud SQL instances, which allows users to connect to databases using their Google Cloud credentials. To verify, inspect the instance settings in the Google Cloud Console or use the gcloud CLI to confirm that IAM DB authentication is activated. Remediation involves configuring the instance to enable IAM authentication through the Cloud SQL settings panel or via the gcloud sql instances patch command.
  references:
  - https://cloud.google.com/sql/docs/mysql/authentication
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/iam-authentication
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.cloudsql.instance.instance_enabled
  service: cloudsql
  resource: instance
  requirement: Instance Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud SQL Instances Are Enabled and Configured Correctly
  rationale: Enabled Cloud SQL instances are crucial for maintaining business continuity and data accessibility. Disabling instances can lead to service disruption, affecting application availability and integrity. Properly configured instances ensure compliance with data protection regulations and mitigate risks of unauthorized access or data breaches.
  description: This rule verifies that Cloud SQL instances are enabled and operational. It checks the 'state' attribute of each Cloud SQL instance to confirm it is not in a 'disabled' state, which could indicate an unintentional service interruption. To remediate, ensure instances are correctly started and configured using the GCP Console or CLI. Regularly audit instance states to prevent accidental downtime.
  references:
  - https://cloud.google.com/sql/docs/mysql/start-stop-restart-instance
  - https://cloud.google.com/sql/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/instance-settings
  - https://cloud.google.com/sql/docs/postgres/instance-settings
- rule_id: gcp.cloudsql.instance.local_infile_flag_disabled
  service: cloudsql
  resource: instance
  requirement: Local Infile Flag Disabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Local Infile Flag is Disabled on Cloud SQL Instances
  rationale: Disabling the local infile flag in Cloud SQL prevents unauthorized data import operations, reducing the risk of data exfiltration and malicious data manipulation. This control is crucial for maintaining the integrity and confidentiality of data, especially for organizations that must adhere to stringent data protection regulations.
  description: This rule checks if the local infile flag is disabled on Cloud SQL instances, which is necessary to prevent potential security vulnerabilities associated with data import operations. To verify compliance, review the database flags in the Cloud SQL instance settings and ensure 'local_infile' is set to 'OFF'. Remediation involves configuring the database flags accordingly via the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.instance.log_connections_flag_configured
  service: cloudsql
  resource: instance
  requirement: Log Connections Flag Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Cloud SQL Log Connections Flag is Enabled
  rationale: Enabling the log connections flag for Cloud SQL instances is crucial for monitoring access and detecting potential unauthorized attempts. This setting aids in compliance with regulations that mandate logging of access to sensitive data, such as PCI-DSS and ISO 27001, and helps in forensic investigations by providing detailed connection logs.
  description: This rule checks whether the 'log_connections' flag is enabled for Cloud SQL instances, which logs all connection requests to the database. To verify, access the Cloud SQL instance settings in the GCP Console, navigate to the 'Flags' section, and ensure 'log_connections' is set to 'on'. If not enabled, modify the instance configuration to include this flag, which can be done via the GCP Console, gcloud command-line tool, or Terraform scripts.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://cloud.google.com/sql/docs/mysql/monitoring
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/sql/docs/mysql/configure-advanced-settings#flags
- rule_id: gcp.cloudsql.instance.log_disconnections_flag_enabled
  service: cloudsql
  resource: instance
  requirement: Log Disconnections Flag Enabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Log Disconnections for Cloud SQL Instances
  rationale: Logging disconnections in Cloud SQL helps identify potential issues such as network interruptions, unauthorized access attempts, or performance bottlenecks. This can be crucial for maintaining the integrity and reliability of database operations. Additionally, it aids in meeting compliance requirements by providing a detailed audit trail of all disconnection events.
  description: This rule checks if the 'log_disconnections' flag is enabled for Cloud SQL instances, ensuring that disconnection events are logged. To verify, review the instance configuration for the 'log_disconnections' setting. If not enabled, update the instance settings to include this flag. Logging disconnections can be done through the Google Cloud Console or using the gcloud command-line tool, providing visibility into database access patterns and potential anomalies.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/operations
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.cloudsql.instance.log_error_verbosity_configured
  service: cloudsql
  resource: instance
  requirement: Log Error Verbosity Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Cloud SQL Log Error Verbosity is Configured
  rationale: Configuring log error verbosity in Cloud SQL is essential for capturing detailed error logs, which help in diagnosing issues, ensuring accountability, and maintaining transparency. This setup aids in forensic investigations and is crucial for compliance with regulations like PCI-DSS and SOC 2, which require detailed logging for audit purposes.
  description: This rule checks whether the log error verbosity setting is configured on Cloud SQL instances. Proper configuration ensures that error logs are verbose enough to provide meaningful insights into database operations and issues. To verify, check the 'log_error_verbosity' setting in the database flags. Remediation involves setting the verbosity level to 'VERBOSE' or 'ERROR' as per the security policy requirements.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/sql/docs/postgres/flags
- rule_id: gcp.cloudsql.instance.log_min_duration_statement_disabled
  service: cloudsql
  resource: instance
  requirement: Log Min Duration Statement Disabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Cloud SQL Log Min Duration Statement is Enabled
  rationale: Without logging the minimum duration of SQL statements, it becomes difficult to identify and troubleshoot long-running queries which may indicate performance issues or potential security incidents, impacting business operations and data integrity. Enabling this feature aids in compliance with regulatory requirements for auditing and monitoring, such as PCI-DSS and SOC2, which mandate detailed logging.
  description: This rule checks if the 'log_min_duration_statement' setting is disabled on Cloud SQL instances. When disabled, SQL statement execution times are not logged, obscuring performance insights and security audit trails. To verify, ensure the 'log_min_duration_statement' is set to an appropriate threshold in the PostgreSQL or MySQL configuration of the Cloud SQL instance. Remediate by enabling and configuring this setting through the GCP Console or gcloud CLI to capture statements exceeding a specified duration.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/sql/docs/postgres/audit-logging
- rule_id: gcp.cloudsql.instance.log_min_error_statement_compliance_configured
  service: cloudsql
  resource: instance
  requirement: Log Min Error Statement Compliance Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Log Min Error Statement Configured for Cloud SQL Instances
  rationale: Configuring the 'log_min_error_statement' setting in Cloud SQL ensures that all error statements meeting or exceeding a specified severity level are logged. This helps in early detection of potential issues, supports forensic investigations, and adheres to compliance regulations like PCI-DSS and SOC2 that require detailed error logging for auditing purposes.
  description: This rule checks whether the 'log_min_error_statement' configuration is set appropriately on Cloud SQL instances to log minimum error statements. Proper configuration involves setting the parameter to a severity level (e.g., ERROR, WARNING) that aligns with your organization's logging policy. To verify, access the Cloud SQL instance settings in the GCP Console, navigate to 'Flags', and ensure 'log_min_error_statement' is configured. Remediation involves setting this flag to a suitable level and applying the changes.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.cloudsql.instance.log_min_messages_compliance_configured
  service: cloudsql
  resource: instance
  requirement: Log Min Messages Compliance Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Configure Cloud SQL Instances for Minimum Log Level Compliance
  rationale: Properly configuring the minimum log level for Cloud SQL instances is crucial to ensure that relevant security events are captured while reducing noise from less critical log messages. This aids in timely detection of anomalies and compliance with standards such as PCI-DSS and ISO 27001, which require effective logging mechanisms to track access and changes.
  description: This rule checks whether the 'log_min_messages' setting for Cloud SQL instances is set to a compliance-friendly level, such as 'ERROR' or 'WARNING'. This configuration is intended to log only significant events that might indicate potential security issues or system malfunctions. Verification can be performed by reviewing the Cloud SQL instance settings in the GCP Console under 'Logs' or using the gcloud command-line tool. Remediation involves updating the 'log_min_messages' setting in the Cloud SQL instance configuration to a more appropriate level if it is not already set.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags#mysql_flags
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/sql/docs/mysql/logging
- rule_id: gcp.cloudsql.instance.log_statement_flag_configured
  service: cloudsql
  resource: instance
  requirement: Log Statement Flag Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Cloud SQL Log Statement Flag is Configured
  rationale: Configuring the log statement flag in Cloud SQL is crucial for auditing and troubleshooting. It helps detect suspicious activities by capturing SQL statements executed on the database, which can aid in forensic investigations and compliance with standards like PCI-DSS and SOC2. Without logging, potential security incidents might go unnoticed, increasing the risk of data breaches.
  description: This rule checks if the log_statement flag is configured for Cloud SQL instances. The flag should be set to 'all' or 'mod' to capture all SQL statements or modifications, respectively. To verify, access the Cloud SQL instance settings and ensure the log_statement parameter is configured appropriately. If not configured, update the instance settings to enable logging. This enhances visibility into database operations, helping to monitor access and changes.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.cloudsql.instance.logging_enabled
  service: cloudsql
  resource: instance
  requirement: Logging Enabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for Cloud SQL Instances
  rationale: Enabling logging on Cloud SQL instances is crucial for monitoring access and activity, which helps in detecting unauthorized access and potential data breaches. Logging is also important for compliance with regulatory frameworks like PCI-DSS and HIPAA, which mandate detailed logging of access to sensitive data. Without logging, it is difficult to audit actions taken within the database, increasing the risk of undetected malicious activities.
  description: This rule checks whether logging is enabled for Cloud SQL instances, ensuring that all database operations are recorded. To verify, check the instance settings in the Google Cloud Console and ensure that the 'Enable Logging' option is activated. If not, navigate to the SQL instance settings and enable logging to capture all activities. This action allows for detailed auditing and issue resolution by maintaining an operational log of database activities.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings#enable-logging
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/logging/docs/audit/#cloudsql
  - https://cloud.google.com/sql/docs/mysql/maintain-log-files
- rule_id: gcp.cloudsql.instance.multi_az
  service: cloudsql
  resource: instance
  requirement: Multi Az
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud SQL Instances Use Multi-Region Deployment
  rationale: Deploying Cloud SQL instances across multiple availability zones (AZs) enhances fault tolerance and availability. This configuration protects against zonal outages, ensuring business continuity and minimizing downtime, which is critical for applications requiring high availability. It also aligns with compliance frameworks demanding robust failover mechanisms as part of disaster recovery strategies.
  description: This rule checks if Cloud SQL instances are configured to use multi-region (multi-AZ) deployment. To verify, inspect the instance settings in the Google Cloud Console or via the gcloud command-line tool to confirm that the 'region' setting includes multiple AZs. Remediation involves enabling the high-availability (HA) option during instance setup or modifying existing instances to enable HA, ensuring they are spread across multiple AZs for redundancy.
  references:
  - https://cloud.google.com/sql/docs/mysql/high-availability
  - https://cloud.google.com/sql/docs/postgres/high-availability
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.multi_az_enabled
  service: cloudsql
  resource: instance
  requirement: Multi Az Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud SQL Instances are Configured for Multi-AZ Deployment
  rationale: Enabling Multi-AZ deployment for Cloud SQL instances is crucial for high availability and fault tolerance, minimizing downtime risk. This configuration helps protect against zone failures and ensures continuous database service, which is vital for business continuity and meeting SLAs. It also supports compliance with regulations requiring robust disaster recovery plans.
  description: This rule checks if Cloud SQL instances have Multi-AZ configuration enabled, meaning they are deployed across multiple availability zones. Multi-AZ deployment allows automatic failover to a standby instance in a different zone in case of a failure. To verify, review the instance settings in the Google Cloud Console or use the gcloud CLI. Remediation involves modifying the instance to enable high availability settings, which can be done during instance creation or via configuration updates.
  references:
  - https://cloud.google.com/sql/docs/mysql/high-availability
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/instance-settings#availability
- rule_id: gcp.cloudsql.instance.no_public_access
  service: cloudsql
  resource: instance
  requirement: No Public Access
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public IP for Cloud SQL Instances
  rationale: Exposing Cloud SQL instances to the public internet increases the risk of unauthorized access and potential data breaches. Public IP access can be exploited by attackers to launch brute force attacks or exploit vulnerabilities. Restricting access to private IPs helps maintain data confidentiality and aligns with compliance requirements such as PCI-DSS and HIPAA, which mandate the protection of sensitive data.
  description: This rule checks if a Cloud SQL instance is configured with a public IP, which should be disabled to prevent unauthorized internet access. Verify that all instances use private IP connections by checking the 'IP configuration' settings in the GCP Console or via the gcloud command-line tool. To remediate, modify the instance settings to disable public IP and enable private IP connectivity, ensuring that access is controlled through authorized networks and identity-based access management.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 6.4
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.cloudsql.instance.no_public_access_gcp_compute_securitygroup_allow__configured
  service: cloudsql
  resource: instance
  requirement: No Public Access Gcp Compute Securitygroup Allow Configured
  scope: cloudsql.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Prevent Public Access to Cloud SQL Instances
  rationale: Allowing public access to Cloud SQL instances can expose sensitive data to unauthorized users, leading to potential data breaches. It increases the risk of attacks such as SQL injection and unauthorized data exfiltration. Compliance with regulations such as PCI-DSS, HIPAA, and SOC2 often mandates restricted access to secure databases, thereby ensuring data privacy and integrity.
  description: This rule checks for Cloud SQL instances that have public IP addresses or are configured to allow public access through GCP compute security groups. It is essential to ensure that these instances are not accessible from the public internet and are instead restricted to internal IP addresses or specific trusted networks. To remediate, disable public IP addresses in the instance settings and configure VPC firewall rules to limit access to authorized IP ranges only.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://cloud.google.com/sql/docs/mysql/sql-proxy
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.cloudsql.instance.paas_db_encryption_at_rest_enabled
  service: cloudsql
  resource: instance
  requirement: Paas Db Encryption At Rest Enabled
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption at Rest for GCP Cloud SQL Instances
  rationale: Encryption at rest protects sensitive data by ensuring it is not readable without proper authorization, reducing the risk of data breaches. It mitigates threats from unauthorized access and meets compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate data protection through encryption.
  description: This rule checks if Google Cloud SQL instances have encryption at rest enabled, which safeguards data stored within the databases. To verify, ensure that the 'disk_encryption_configuration' settings in the Cloud SQL instance configuration are applied. Remediation involves using the Google Cloud Console or gcloud CLI to set up Customer-Managed Encryption Keys (CMEK) or let Google manage encryption keys to secure data at rest.
  references:
  - https://cloud.google.com/sql/docs/mysql/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/encryption-at-rest/customer-managed-encryption
- rule_id: gcp.cloudsql.instance.paas_db_public_access_disabled
  service: cloudsql
  resource: instance
  requirement: Paas Db Public Access Disabled
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Access to Cloud SQL Instances
  rationale: Exposing Cloud SQL instances to the public internet poses significant security risks, including unauthorized access and potential data breaches. Disabling public access reduces the attack surface and helps comply with regulatory requirements such as PCI-DSS and HIPAA, which mandate the protection of sensitive data from unauthorized exposure.
  description: This rule checks whether Cloud SQL instances are configured to deny public IP access, ensuring they are only accessible via private IP addresses within Google Cloud VPCs. To verify, inspect the instance settings in the GCP Console and ensure 'Public IP' is not enabled. If necessary, modify the instance to use private IPs and configure appropriate VPC peering or VPN connections to maintain secure access.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.cloudsql.instance.paas_db_require_tls_in_transit
  service: cloudsql
  resource: instance
  requirement: Paas Db Require TLS In Transit
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Instances Use TLS for Data in Transit
  rationale: Enforcing TLS for data in transit protects against man-in-the-middle attacks, ensuring the confidentiality and integrity of data. This is crucial for maintaining trust with customers and complying with regulatory standards such as PCI-DSS and HIPAA, which mandate encryption of sensitive data during transmission.
  description: This rule checks that all Cloud SQL instances have TLS enabled for data in transit, ensuring secure communication between clients and databases. To verify, ensure the database instance is configured to require SSL/TLS connections. Remediation involves setting the 'require_ssl' flag to 'on' in the Cloud SQL instance settings and updating client configurations to use SSL/TLS for connections.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/encryption-in-transit/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.cloudsql.instance.pg_audit_logging_enabled
  service: cloudsql
  resource: instance
  requirement: Pg Audit Logging Enabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Pg Audit Logging is Enabled for Cloud SQL Instances
  rationale: Enabling Pg Audit logging for Cloud SQL instances is crucial for monitoring and tracking access to sensitive database resources. It helps in detecting unauthorized access attempts and potential data breaches, thereby safeguarding business-critical data. Compliance with regulations such as GDPR, PCI-DSS, and HIPAA often mandates comprehensive logging of database activities.
  description: This rule verifies that Pg Audit logging is enabled for PostgreSQL instances in Google Cloud SQL. Pg Audit provides detailed logging of database activities, including connection attempts, disconnections, and data access queries. To enable Pg Audit logging, configure the 'cloudsql.enable_pg_audit' flag for your Cloud SQL instance. This can be done via the GCP Console, gcloud command-line tool, or API. Ensure logs are regularly reviewed and stored securely for audit purposes.
  references:
  - https://cloud.google.com/sql/docs/postgres/pg-audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/sql/docs/postgres/configure-logs
- rule_id: gcp.cloudsql.instance.postgres_log_connections_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Connections Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Postgres Log Connections Flag is Enabled
  rationale: Enabling the Postgres log_connections flag helps in auditing and monitoring PostgreSQL database access patterns, which is crucial for detecting unauthorized access attempts and ensuring compliance with security policies. This logging capability supports forensic analysis and can assist in identifying potential security incidents or misuse.
  description: This rule checks for the activation of the log_connections flag on Cloud SQL PostgreSQL instances. When enabled, this flag logs each successful connection, aiding in the auditing of database access. To verify, navigate to the 'Flags' tab in the Cloud SQL instance settings and ensure 'log_connections' is set to 'on'. Remediation involves enabling this flag in the Cloud SQL instance configuration, either through the GCP console, gcloud CLI, or the API.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/postgres
  - https://cloud.google.com/sql/docs/postgres/logging
  - https://cloud.google.com/sql/docs/postgres/flags#log_connections
- rule_id: gcp.cloudsql.instance.postgres_log_disconnections_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Disconnections Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Postgres Log Disconnections for Cloud SQL Instances
  rationale: Logging disconnections in PostgreSQL helps in tracking user sessions and identifying unexpected disconnections, which can indicate potential security incidents or misconfigurations. This is crucial for forensic analysis and compliance with regulations that require detailed activity logs, such as PCI-DSS and SOC2.
  description: This rule checks whether the 'log_disconnections' flag is enabled for PostgreSQL instances on Cloud SQL. Enabling this flag ensures that all disconnection events are logged, providing visibility into session terminations. To verify, navigate to the Cloud SQL instance settings in the Google Cloud Console and ensure that 'log_disconnections' is set to 'on'. Remediation involves setting this flag to 'on' via the Cloud Console or using gcloud CLI.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags#postgres-flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/postgres/logging
- rule_id: gcp.cloudsql.instance.postgres_log_error_verbosity_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Error Verbosity Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Postgres Log Error Verbosity is Set to 'Default' or Lower
  rationale: Configuring the Postgres log error verbosity to an appropriate level is crucial for effective monitoring and troubleshooting. Excessive verbosity can lead to log flooding, obscuring important information and potentially leading to non-compliance with data protection regulations by exposing sensitive information. Ensuring the correct verbosity level helps maintain a balance between sufficient logging for security auditing and avoiding unnecessary exposure of sensitive data.
  description: This rule checks if the Postgres log error verbosity flag on Cloud SQL instances is set to 'Default' or lower. By default, this setting ensures that only essential error messages are logged, minimizing the risk of sensitive data exposure while maintaining necessary operational insights. To verify, access the Cloud SQL instance settings and review the 'log_error_verbosity' configuration. If set to 'Verbose', change it to 'Default' or 'Terse' to comply with best practices.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/postgres/configure-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-iec-27001-information-security.html
- rule_id: gcp.cloudsql.instance.postgres_log_min_duration_statement_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Min Duration Statement Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Postgres Log Min Duration Statement Configuration
  rationale: Configuring the log_min_duration_statement flag in PostgreSQL instances helps identify slow queries that might indicate performance bottlenecks or potential security vulnerabilities, such as SQL injection attempts. This setting aids in proactive database monitoring and optimization, aligning with compliance requirements for audit logging and performance monitoring.
  description: This rule checks whether the 'log_min_duration_statement' flag is set for PostgreSQL instances in Cloud SQL. The flag must be adjusted to log queries exceeding a specified execution time, aiding in the detection of inefficient queries and potential anomalies. To verify and remediate, access the Cloud SQL instance settings, navigate to the 'Flags' section, and set 'log_min_duration_statement' to a reasonable value, such as 1000 milliseconds, based on your application needs.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-working-with-postgresql-on-google-cloud
  - https://www.postgresql.org/docs/current/runtime-config-logging.html
- rule_id: gcp.cloudsql.instance.postgres_log_min_error_statement_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Min Error Statement Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Postgres Log Min Error Statement is Configured Correctly
  rationale: Proper configuration of the Postgres log_min_error_statement flag helps in identifying and troubleshooting SQL errors by capturing error statements in the logs. This is crucial for detecting potential security incidents and operational issues. Misconfiguration can lead to incomplete logging, hindering forensic investigations and failing to meet compliance standards such as PCI-DSS and HIPAA.
  description: This rule checks if the Postgres 'log_min_error_statement' flag is set to 'ERROR' or a more detailed level on Cloud SQL instances. This setting ensures that all SQL statements that generate errors, including syntax errors and permission denials, are logged. To verify, inspect the database flags configuration in the GCP Console or via gcloud CLI. Remediate by updating the flag in the Cloud SQL instance settings to 'ERROR' or 'ALL', and restart the instance for changes to take effect.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/postgres/diagnose-issues
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/postgres/configure-logging
  - https://cloud.google.com/sql/docs/postgres/audit-logging
- rule_id: gcp.cloudsql.instance.postgres_log_min_messages_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Min Messages Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Postgres Log Min Messages Flag is Configured
  rationale: Configuring the Postgres log_min_messages flag is crucial as it determines the minimum level of messages that are logged. This can help in identifying potential security incidents by capturing relevant logs. Not setting this flag appropriately can lead to insufficient logging, missing critical events, and potentially violating regulatory compliance requirements such as those outlined by HIPAA or PCI-DSS.
  description: This rule checks if the Postgres log_min_messages flag for Cloud SQL instances is set to a level that ensures adequate logging of important events, such as 'WARNING' or 'ERROR'. It is important to configure this setting to capture essential logs for security monitoring and incident response. To verify, navigate to the GCP Console, access the Cloud SQL instance settings, and ensure the log_min_messages flag is set appropriately. For remediation, update this setting under the 'Flags' section of your Cloud SQL instance.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/postgres/diagnose-issues
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.cloudsql.instance.postgres_log_statement_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Statement Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Postgres Log Statement for Cloud SQL Instances
  rationale: Enabling the log_statement flag in Cloud SQL for PostgreSQL instances is critical for auditing and monitoring activities within your database. This setting provides visibility into SQL queries, helping identify potential malicious activities such as SQL injection attempts or unauthorized data access. It supports compliance requirements by maintaining a clear record of database interactions, which is essential for audits and forensic investigations.
  description: This rule checks if the log_statement flag is enabled for PostgreSQL instances in Cloud SQL. This setting logs SQL statements executed against the database, which can be set to log all statements, none, or specific types of statements. To ensure comprehensive logging, configure the log_statement parameter to 'all'. This can be verified and set in the instance's database flags configuration. If not enabled, update the Cloud SQL instance settings to include the log_statement flag set to 'all' to ensure complete logging of database activities.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags#log_statement
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/sql/docs/audit-logs
  - https://cloud.google.com/sql/docs/postgres/configure-pg-flags
- rule_id: gcp.cloudsql.instance.public_access
  service: cloudsql
  resource: instance
  requirement: Public Access
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to Cloud SQL Instances
  rationale: Allowing public access to Cloud SQL instances exposes them to potential unauthorized access and attacks, such as brute force or SQL injection. This increases the risk of data breaches, compromising sensitive information, and potentially violating compliance standards like PCI-DSS, SOC2, or HIPAA, which mandate strict access controls.
  description: This rule checks if Cloud SQL instances are configured to allow public IP access. Instances should be configured to use private IPs and accessed via a secure, private network or VPN. To remediate, ensure that the 'Public IP' setting is disabled in the GCP console under the SQL instance settings or configure a private IP on the instance. Regularly audit and monitor network configurations to ensure no public endpoints are exposed.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/sql/docs/mysql/private-ip
- rule_id: gcp.cloudsql.instance.public_ip
  service: cloudsql
  resource: instance
  requirement: Public Ip
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Cloud SQL Instances Do Not Use Public IPs
  rationale: Public IP exposure of Cloud SQL instances can lead to unauthorized access and data breaches, increasing the attack surface. Limiting access to private IPs helps in compliance with regulations such as PCI-DSS and HIPAA that mandate secure data environments and reduces the risk of exposure to external threats.
  description: This rule checks for Cloud SQL instances configured with a public IP address, which can be accessed from the internet. To enhance security, disable public IP and configure a private IP within a VPC. Verification can be done through the GCP Console by navigating to the SQL instances and checking the IP configuration. Remediation involves switching the instance to use only private IPs and ensuring access through secure, internal networks.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/architecture/using-private-google-access-and-cloud-nat
  - https://cloud.google.com/sql/docs/mysql/sql-proxy
- rule_id: gcp.cloudsql.instance.public_ip_configured
  service: cloudsql
  resource: instance
  requirement: Public Ip Configured
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Cloud SQL Instances Do Not Use Public IP Addresses
  rationale: Exposing Cloud SQL instances to the internet via public IPs increases the risk of unauthorized access and data breaches. This can lead to sensitive information exposure, non-compliance with regulations like GDPR and HIPAA, and potential financial and reputational damage to the organization. Limiting access to private IPs within VPC networks enhances security by reducing the attack surface.
  description: This rule checks if a Cloud SQL instance is configured with a public IP address. Public IPs allow direct internet access, which can be exploited by attackers. To verify, check the instance's IP configuration in the GCP Console or via gcloud CLI. Remediation involves disabling the public IP and ensuring the instance is accessed through a private IP within a VPC network, possibly utilizing Cloud SQL Proxy for secure connections.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-private-ip
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/sql-proxy
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.public_ip_whitelist_configured
  service: cloudsql
  resource: instance
  requirement: Public Ip Whitelist Configured
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce Public IP Whitelisting for Cloud SQL Instances
  rationale: Unrestricted public access to Cloud SQL instances can expose sensitive data to unauthorized users, leading to potential data breaches and compliance violations. Restricting access through IP whitelisting helps mitigate risks from unauthorized access, ensuring only trusted networks can connect, aligning with regulatory requirements such as PCI-DSS and GDPR.
  description: This rule verifies that public IP addresses accessing Cloud SQL instances are restricted to a defined set of authorized IP ranges. To ensure security, configure the 'Authorized networks' settings in the Cloud SQL instance to include only trusted IP addresses. Regularly review and update the IP whitelist to reflect changes in trusted network infrastructure. Remediation involves navigating to the Google Cloud Console, accessing the Cloud SQL instance settings, and updating the public IP whitelist under 'Connections'.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/gdpr/
  - https://cloud.google.com/security/compliance/pci-dss/
  - https://cloud.google.com/sql/docs/postgres/authorized-networks
  - https://cloud.google.com/sql/docs/sqlserver/configure-ip
- rule_id: gcp.cloudsql.instance.remote_access_flag_configured
  service: cloudsql
  resource: instance
  requirement: Remote Access Flag Configured
  scope: cloudsql.instance.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Cloud SQL Remote Access is Securely Configured
  rationale: Improper configuration of remote access settings in Cloud SQL can expose databases to unauthorized access, leading to data breaches or loss. Configuring remote access controls helps in mitigating risks associated with unauthorized data exposure and ensures compliance with data protection regulations such as PCI-DSS and HIPAA.
  description: This rule checks if Cloud SQL instances have the 'allow only SSL connections' flag enabled, ensuring that all remote connections use SSL encryption. To verify, navigate to the Cloud SQL instance settings in the GCP Console and confirm that the 'Require SSL' option is activated under 'Connections'. To remediate, enable SSL enforcement and configure SSL certificates for client connections. This enhances data security by encrypting data in transit.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.require_ssl_enforcement
  service: cloudsql
  resource: instance
  requirement: Require SSL Enforcement
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce SSL for Cloud SQL Instances
  rationale: Enforcing SSL for Cloud SQL instances prevents unauthorized interception and tampering of data in transit, which is crucial for protecting sensitive information and maintaining privacy. This measure mitigates the risk of man-in-the-middle attacks and is often required for compliance with data protection regulations such as PCI-DSS and HIPAA.
  description: This rule checks if SSL is enforced for all Cloud SQL instances, ensuring that database connections are encrypted. To verify, inspect the 'require_ssl' setting in the Cloud SQL instance configuration. If not enabled, configure the instance to require SSL by setting the 'require_ssl' option to true in the Google Cloud Console or using gcloud command-line tools. This action ensures that all incoming connections use SSL for encryption.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://cloud.google.com/security/encryption-in-transit/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-52r1.pdf
- rule_id: gcp.cloudsql.instance.skip_show_database_flag_configured
  service: cloudsql
  resource: instance
  requirement: Skip Show Database Flag Configured
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure 'skip_show_database' Flag is Properly Configured in Cloud SQL
  rationale: The 'skip_show_database' flag limits the information an unauthorized user can retrieve from the server, reducing the risk of data exposure and potential information leakage. This is crucial for protecting sensitive metadata about databases, which could otherwise be exploited in targeted attacks. Ensuring this flag is configured aligns with data protection policies and helps meet compliance requirements related to data privacy standards.
  description: This rule checks if the 'skip_show_database' flag is set on Cloud SQL instances. When enabled, this flag prevents users from seeing all available databases on the instance, enhancing security by limiting information exposure. To verify, navigate to the Cloud SQL instance settings and confirm that the flag is set. Remediation involves enabling this setting through the Google Cloud Console, gcloud command-line tool, or API. It is critical for maintaining data confidentiality and minimizing the risk of unauthorized access to database metadata.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/configure-ip
- rule_id: gcp.cloudsql.instance.ssl_connections
  service: cloudsql
  resource: instance
  requirement: SSL Connections
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure SSL Connections for Cloud SQL Instances
  rationale: Enforcing SSL connections for Cloud SQL instances mitigates the risk of data interception and unauthorized access to sensitive data during transit. This is crucial for maintaining data confidentiality and integrity, especially for organizations handling regulated data under compliance frameworks like PCI-DSS and HIPAA. Failure to encrypt data in transit can lead to severe business impacts, including data breaches and non-compliance penalties.
  description: This rule checks whether SSL connections are enforced for Cloud SQL instances. To verify, ensure that the 'Require SSL' option is enabled in the Cloud SQL instance settings. This can be done via the Google Cloud Console, gcloud command-line tool, or REST API. Remediation involves enabling SSL enforcement and distributing the necessary SSL certificates to all database clients to secure the data transmission between clients and the Cloud SQL instance.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/sql/docs/security-overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.storage_encrypted
  service: cloudsql
  resource: instance
  requirement: Storage Encrypted
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Instances Have Storage Encryption Enabled
  rationale: Encrypting storage of Cloud SQL instances is crucial for protecting sensitive data from unauthorized access, particularly in the event of a data breach or physical media loss. This is vital for maintaining customer trust, adhering to data privacy regulations such as GDPR, and fulfilling compliance requirements of standards like PCI-DSS and HIPAA, which mandate encryption of data at rest.
  description: This rule verifies that Cloud SQL instances have storage encryption enabled, ensuring data is encrypted at rest using Google-managed keys. To check, verify the 'diskEncryptionConfiguration' parameter in the Cloud SQL instance settings. If not configured, enable encryption by navigating to the Cloud SQL instance settings in the GCP Console and selecting 'Enable Encryption' under storage options. This is an essential step in securing data and preventing unauthorized access.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/sql/docs/mysql/instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.cloudsql.instance.trace_flag_3625_enabled
  service: cloudsql
  resource: instance
  requirement: Trace Flag 3625 Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Trace Flag 3625 for Cloud SQL Instances
  rationale: Enabling Trace Flag 3625 on SQL Server instances in Cloud SQL helps limit the amount of sensitive information logged in error messages, reducing the risk of data leaks. This is particularly important for protecting personally identifiable information (PII) and complying with data protection regulations such as GDPR and CCPA.
  description: This rule checks whether Trace Flag 3625 is enabled on your Cloud SQL for SQL Server instances. Trace Flag 3625 suppresses detailed error messages that could reveal sensitive information, ensuring error logs do not inadvertently disclose PII. To verify, review the Cloud SQL instance settings and ensure the flag is active. Remediation involves enabling the flag via a Cloud SQL instance configuration change, which can be done through the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/sql/docs/sqlserver/flags
  - https://cloud.google.com/sql/docs/sqlserver/manage-flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
- rule_id: gcp.cloudsql.instance.user_connections_flag_non_limiting
  service: cloudsql
  resource: instance
  requirement: User Connections Flag Non Limiting
  scope: cloudsql.instance.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit User Connections in Cloud SQL Instances
  rationale: Unrestricted user connections to Cloud SQL instances can lead to resource exhaustion, potentially causing service disruptions and increased vulnerability to denial-of-service attacks. Limiting connections helps maintain availability, ensures fair resource usage, and complies with best practices for access management. It also aids in meeting compliance requirements for data protection and system integrity.
  description: This rule checks whether the 'max_connections' flag is set to a limiting value for Cloud SQL instances. A non-limiting configuration can allow excessive simultaneous connections, impacting performance and security. To verify, ensure that the 'max_connections' flag is configured with a value appropriate for your workload and usage patterns. Remediation involves accessing the Cloud SQL instance settings and setting a reasonable limit based on expected traffic and resource capacity.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.cloudsql.instance.user_options_flag_not_configured
  service: cloudsql
  resource: instance
  requirement: User Options Flag Not Configured
  scope: cloudsql.instance.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Cloud SQL User Options Flag is Properly Configured
  rationale: Improper configuration of the user options flag in Cloud SQL instances can lead to unauthorized access and potential data breaches. This setting controls user-defined parameters that can affect the security posture of the database. Ensuring it is configured appropriately helps mitigate risks associated with unauthorized database modifications and enhance regulatory compliance with standards such as PCI-DSS and HIPAA.
  description: This rule verifies that the user options flag is correctly configured on Cloud SQL instances to prevent security vulnerabilities. It checks whether any user-defined options that could compromise security are enabled. To verify, review the SQL instance settings in the GCP Console or use the gcloud command-line tool. Remediation involves setting a secure configuration by adjusting the user options flag via the Cloud SQL settings interface or by using gcloud CLI commands.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/security-overview
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.cloudsql.ssl_cert.db_sg_egress_restricted
  service: cloudsql
  resource: ssl_cert
  requirement: Db Sg Egress Restricted
  scope: cloudsql.ssl_cert.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Egress for Cloud SQL SSL Certificates
  rationale: Restricting egress traffic for SSL certificates in Cloud SQL is crucial to prevent unauthorized data exfiltration and to ensure compliance with data protection regulations. Unrestricted egress can lead to potential exposure of sensitive data and can increase the risk of data breaches, affecting business continuity and trust.
  description: This rule checks if egress traffic from Cloud SQL SSL certificates is restricted to specific IPs or networks. Proper configuration involves setting up firewall rules that limit outbound connections to only trusted destinations. Verify that network security groups associated with Cloud SQL instances have specific egress rules. Remediation includes configuring egress rules in the VPC firewall to permit only necessary traffic, ensuring that all data transfers are encrypted and securely transmitted.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/sql/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudsql.ssl_cert.db_sg_no_0_0_0_0_ingress_on_db_ports
  service: cloudsql
  resource: ssl_cert
  requirement: Db Sg No 0 0 0 0 Ingress On Db Ports
  scope: cloudsql.ssl_cert.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cloud SQL Ingress from 0.0.0.0/0 on DB Ports
  rationale: Allowing ingress to Cloud SQL from 0.0.0.0/0 poses a significant security risk as it exposes databases to potential attacks from any IP address on the internet. This can lead to unauthorized access, data breaches, or exploitation of database vulnerabilities. Compliance frameworks such as PCI-DSS and HIPAA require restricted access to sensitive data, ensuring only trusted sources can connect.
  description: This rule checks for Cloud SQL instances allowing ingress traffic from 0.0.0.0/0 on database ports. To secure your database, configure the instance's authorized networks to allow connections only from trusted IP ranges or VPNs. Review and modify the VPC firewall rules associated with your Cloud SQL instances to ensure they do not permit unrestricted access. Regularly audit and update these settings to comply with best practices and regulatory requirements.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.cloudsql.ssl_cert.db_sg_only_required_ports_open
  service: cloudsql
  resource: ssl_cert
  requirement: Db Sg Only Required Ports Open
  scope: cloudsql.ssl_cert.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL SSL Certs Have Only Required Ports Open
  rationale: Limiting open ports for Cloud SQL SSL certificates reduces the attack surface, mitigating risks of unauthorized access and potential data breaches. This is crucial for protecting sensitive data hosted in databases, ensuring compliance with data privacy regulations such as GDPR and HIPAA, and maintaining operational integrity by preventing service disruptions from malicious activities.
  description: This rule checks that only necessary ports are open for Cloud SQL instances utilizing SSL certificates, in accordance with security best practices. Administrators should verify that the database security groups (SG) only allow traffic on ports required for application functionality, typically port 3306 for MySQL or port 5432 for PostgreSQL. Remediation involves reviewing and updating firewall rules to restrict open ports to only those essential for legitimate database operations, minimizing exposure to potential threats.
  references:
  - https://cloud.google.com/sql/docs/mysql/authorize-networks
  - https://cloud.google.com/sql/docs/postgres/configure-ssl-instance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.user.db_approved_list_of_superusers_only
  service: cloudsql
  resource: user
  requirement: Db Approved List Of Superusers Only
  scope: cloudsql.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Cloud SQL Superusers to Approved List Only
  rationale: Limiting superuser access to Cloud SQL databases to an approved list mitigates the risk of unauthorized data access and potential breaches. Superusers have elevated privileges, making them prime targets for exploitation in cyber attacks. Ensuring only vetted individuals have such access helps maintain data integrity and supports compliance with regulations like PCI-DSS and HIPAA.
  description: This rule checks that only users from a predefined, approved list have superuser privileges in Cloud SQL instances. It verifies that no unauthorized users can escalate privileges to a superuser level, which could lead to unauthorized access and data manipulation. To remediate, regularly audit user roles and update the approved list in your IAM policy settings, ensuring only necessary personnel have superuser access. Use the Google Cloud Console or gcloud CLI to manage and verify these configurations.
  references:
  - https://cloud.google.com/sql/docs/mysql/users
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/sql/docs/mysql/roles-and-permissions
- rule_id: gcp.cloudsql.user.db_no_unused_or_default_superusers
  service: cloudsql
  resource: user
  requirement: Db No Unused Or Default Superusers
  scope: cloudsql.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Remove Unused or Default Superusers in Cloud SQL
  rationale: Leaving unused or default superuser accounts active in Cloud SQL can increase the risk of unauthorized access, data breaches, and privilege escalation. Superusers have elevated privileges, and if left unsecured, they can be exploited by attackers to gain complete control over the database. Regulatory frameworks such as PCI-DSS and HIPAA emphasize the principle of least privilege, making it essential to manage superuser accounts diligently.
  description: This rule checks for the presence of unused or default superuser accounts in Cloud SQL databases. Default accounts like 'root' or other vendor-provided superusers should be renamed, disabled, or deleted if not required. Additionally, regularly review user activity logs to identify and remove unused accounts. Remediation involves connecting to the database and executing SQL commands to manage or delete these accounts, ensuring that only necessary and active superuser accounts are retained.
  references:
  - https://cloud.google.com/sql/docs/mysql/users
  - https://cloud.google.com/sql/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.user.db_password_auth_hardened_or_iam_auth_preferred_wh_supported
  service: cloudsql
  resource: user
  requirement: Db Password Auth Hardened Or IAM Auth Preferred Wh Supported
  scope: cloudsql.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Cloud SQL Users Use Hardened Passwords or IAM Authentication
  rationale: Weak database password authentication increases the risk of unauthorized access to sensitive data, potentially leading to data breaches and non-compliance with regulations like PCI-DSS and HIPAA. Enforcing IAM authentication or hardened password policies reduces these risks by implementing stronger access controls and centralized identity management.
  description: This rule checks Cloud SQL user accounts to ensure they are using either IAM authentication, where supported, or database passwords that meet hardened security requirements. Verify that IAM authentication is enabled for databases that support it, or enforce strong password policies, such as a minimum length of 12 characters with complexity requirements, for database password authentication. To remediate, configure IAM roles for database access or update the password policy settings in the Google Cloud Console for Cloud SQL.
  references:
  - https://cloud.google.com/sql/docs/mysql/authentication
  - https://cloud.google.com/sql/docs/postgres/authentication
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
- rule_id: gcp.compute.address.attached
  service: compute
  resource: address
  requirement: Attached
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Static IP Addresses Are Attached to Resources
  rationale: Unattached static IP addresses may lead to unexpected billing and may be unintentionally left unused, posing a security risk if mistakenly allocated to unauthorized resources. Ensuring IP addresses are attached helps maintain an accurate inventory of network resources and supports compliance with network configuration management best practices.
  description: This rule checks for any static IP addresses in Google Cloud Platform's Compute Engine that are not attached to any resources, such as virtual machines or load balancers. Unattached IP addresses should be reviewed and either attached to the necessary resources or released if no longer needed. This can be verified through the Google Cloud Console under VPC Network > External IP addresses or using the gcloud command-line tool. Remediation involves attaching the IP to an appropriate resource or releasing it to avoid unnecessary charges and potential misuse.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address
  - https://cloud.google.com/compute/docs/instances/connecting-vm-instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.compute.address.edge_ip_set_cidrs_valid_and_minimized
  service: compute
  resource: address
  requirement: Edge Ip Set Cidrs Valid And Minimized
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge IP CIDRs Are Valid and Minimized
  rationale: Proper configuration of IP address CIDR ranges ensures that resources are not exposed to unnecessary network traffic, reducing the attack surface and preventing unauthorized access. Minimizing CIDR ranges helps in adhering to the principle of least privilege and can mitigate risks of data breaches and unauthorized access, which are critical for regulatory compliance with frameworks like PCI-DSS and SOC 2.
  description: This rule checks that the CIDR ranges assigned to edge IP addresses in GCP are valid, non-overlapping, and as narrow as possible. It involves verifying current CIDR configurations against access needs and adjusting them to allow only the required IP ranges. Remediation includes reviewing the IP address configurations in the GCP Console under 'VPC network' settings and using 'gcloud' commands to update CIDR ranges, ensuring they are appropriately limited to necessary IPs only.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.compute.address.edge_ip_set_cross_account_sharing_restricted
  service: compute
  resource: address
  requirement: Edge Ip Set Cross Account Sharing Restricted
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict Cross-Account Sharing of Edge IP Sets
  rationale: Restricting cross-account sharing of edge IP sets reduces the risk of unauthorized access and potential data breaches. It ensures that sensitive network configurations are not inadvertently exposed to other accounts, which could be exploited by malicious actors to compromise the network. This practice aligns with compliance requirements for data protection and access control.
  description: This rule checks if edge IP sets are shared across different GCP accounts. To verify, review the IAM policies and ensure that edge IP sets are not granted access to external accounts. Remediation involves adjusting IAM policies to limit access to trusted accounts only. This helps in maintaining network integrity and adheres to best practices for least privilege access.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#ephemeral
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloudsecurityalliance.org/research/guidance/
- rule_id: gcp.compute.address.edge_ip_set_not_empty
  service: compute
  resource: address
  requirement: Edge Ip Set Not Empty
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge IP Set on Compute Addresses is Configured
  rationale: Having an empty Edge IP set on a compute address can lead to potential security risks including unauthorized access or exposure of sensitive data. Proper configuration ensures that only designated IPs can access resources, aligning with security best practices and compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that the Edge IP set for compute addresses in Google Cloud Platform is not empty, ensuring that access is limited to specified IP ranges. To verify, inspect the configuration of each address in the GCP Console or via gcloud CLI to confirm that Edge IPs are properly set. Remediation involves updating the Edge IP set configuration to include all necessary IP ranges, ensuring compliance with your organization's access policies.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/docs/security-compliance
- rule_id: gcp.compute.address.edge_ip_set_used_by_at_least_one_rule
  service: compute
  resource: address
  requirement: Edge Ip Set Used By At Least One Rule
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge IP Sets are Actively Used by Firewall Rules
  rationale: Unused edge IP sets may indicate misconfiguration or unused resources, leading to potential security risks such as unintended exposure of network interfaces or inefficient resource utilization. Ensuring that edge IP sets are used by at least one firewall rule reduces attack surface and aligns with best practice for resource management and security compliance.
  description: This rule checks if edge IP sets in Google Cloud Platform's Compute service are actively used by at least one firewall rule. Unused IP sets should be reviewed and either associated with a relevant firewall rule or decommissioned. This ensures that all network configurations are intentional and necessary, reducing risk of exposure or unauthorized access. To verify, review the list of edge IP sets and corresponding firewall rules in the GCP Console or via gcloud CLI, and update configurations as needed.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
- rule_id: gcp.compute.address.ip_attached_configured
  service: compute
  resource: address
  requirement: Ip Attached Configured
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Attached IP Addresses Are Properly Configured
  rationale: Proper configuration of attached IP addresses is critical to maintaining network security and preventing unauthorized access or data leakage. Misconfigured IP addresses can lead to exposure of internal services to the internet, increasing the risk of attacks and non-compliance with standards such as PCI-DSS and ISO 27001.
  description: This rule checks that all attached IP addresses in Google Cloud Platform are correctly configured according to best practices. It ensures that IP addresses are not unnecessarily exposed to the public internet and are restricted to specific services or resources that require them. Verification involves reviewing the network interface settings on instances to ensure IP addresses are appropriately allocated and configured. Remediation may include re-assigning IP addresses to private networks or adjusting firewall rules to limit exposure.
  references:
  - https://cloud.google.com/vpc/docs/using-routes
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/ip-addresses
- rule_id: gcp.compute.address.ip_unassigned
  service: compute
  resource: address
  requirement: Ip Unassigned
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Unassigned IP Addresses in Google Compute Engine
  rationale: Unassigned IP addresses in Google Compute Engine can lead to inefficient resource utilization and increased costs. They may also pose a security risk if accidentally re-assigned to unauthorized or malicious workloads, potentially enabling network exploitation or unauthorized access to resources. Proper management of IP addresses is crucial for compliance with resource governance and budgetary controls.
  description: This rule checks for IP addresses in Google Compute Engine that have been allocated but are not currently assigned to any resource. Unassigned IP addresses should be regularly reviewed and released if not in use to prevent unnecessary charges. To remediate, identify unassigned IP addresses via the GCP Console or CLI and release them using the 'gcloud compute addresses delete' command or the Console UI to avoid incurring charges.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/compute/docs/instances/networking
  - https://cloud.google.com/docs/overview/security
- rule_id: gcp.compute.address.network_endpoint_policy_least_privilege
  service: compute
  resource: address
  requirement: Network Endpoint Policy Least Privilege
  scope: compute.address.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Network Endpoint Policies
  rationale: Applying the principle of least privilege to network endpoint policies minimizes access to only what is necessary, reducing potential attack vectors and preventing unauthorized access. This is crucial for protecting sensitive data and maintaining compliance with standards like NIST and PCI-DSS, which require strict access control measures.
  description: This rule checks if network endpoint policies associated with compute addresses are configured with least privilege in mind. Verify that IAM roles assigned to these policies are limited to the minimum permissions necessary. Remediation involves auditing the current roles and permissions and adjusting them to ensure they only allow essential actions, thus minimizing potential security risks.
  references:
  - https://cloud.google.com/compute/docs/reference/rest/v1/networkEndpointGroups
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.compute.address.network_endpoint_private_dns_enabled_where_supported
  service: compute
  resource: address
  requirement: Network Endpoint Private Dns Enabled Where Supported
  scope: compute.address.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enable Private DNS for Network Endpoints in GCP Compute
  rationale: Enabling Private DNS for network endpoints enhances security by ensuring that DNS queries are resolved within the private network, reducing exposure to DNS spoofing and man-in-the-middle attacks. This configuration is critical for maintaining data confidentiality and integrity, especially for organizations handling sensitive information. Additionally, it aligns with compliance requirements for data protection under standards like PCI-DSS and HIPAA.
  description: This rule checks if Private DNS is enabled for network endpoints in GCP Compute where supported. Verification involves checking the DNS configuration of network endpoints to ensure they use private DNS zones. To remediate, configure network endpoints to use private DNS by specifying a private DNS policy in the endpoint's settings, ensuring secure and private resolution of DNS queries.
  references:
  - https://cloud.google.com/dns/docs/private-zones
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-security-and-privacy-controls
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.address.public_address_shodan_configured
  service: compute
  resource: address
  requirement: Public Address Shodan Configured
  scope: compute.address.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Public IPs Are Not Indexed by Shodan
  rationale: Exposing public IP addresses to Shodan poses a significant security risk as it enables potential attackers to easily identify and target these resources. This increases the likelihood of unauthorized access, data breaches, and exploitation. Compliance with security standards often requires minimizing exposure of sensitive resources to the internet to protect the organization's assets and reputation.
  description: This rule checks if public IP addresses associated with Google Cloud Compute resources are indexed by Shodan, a search engine for internet-connected devices. Public IPs should be configured to minimize exposure and prevent indexing by Shodan. To verify, ensure that firewall rules restrict access to trusted IP ranges and that unnecessary services are disabled. Remediation involves auditing and adjusting firewall rules, and employing Cloud Armor to filter unwanted traffic.
  references:
  - https://cloud.google.com/vpc/docs/using-firewall-rules
  - https://cloud.google.com/compute/docs/ip-addresses
  - 'CIS GCP Benchmark: 4.1 Ensure that there are no public IPs that are not needed'
  - 'NIST SP 800-53: AC-3 Access Enforcement'
  - 'PCI-DSS Requirement 1.2: Build and Maintain a Secure Network and Systems'
  - https://www.shodan.io/
- rule_id: gcp.compute.autoscaler.autoscaler_configured
  service: compute
  resource: autoscaler
  requirement: Autoscaler Configured
  scope: compute.autoscaler.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Autoscaler is Configured for Compute Instances
  rationale: Properly configured autoscalers optimize resource allocation and cost efficiency while maintaining application performance. Failure to configure autoscalers may lead to resource exhaustion or unnecessary expenses, impacting business operations and violating cost management policies. Inadequate scaling can also result in service interruptions, affecting user experience and compliance with service level agreements.
  description: This rule checks if the autoscaler is properly configured for compute instances by verifying the presence of scaling policies and thresholds. Ensure that scaling policies align with workload demands and are set to automatically adjust resources based on metrics such as CPU utilization or request load. To verify, review the autoscaler settings in the Google Cloud Console under Compute Engine > Autoscaler. Ensure that policies are active and thresholds are set appropriately. Remediation involves configuring or adjusting autoscaling policies to match application performance and cost objectives.
  references:
  - https://cloud.google.com/compute/docs/autoscaler
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/instance-groups/autoscaling-considerations
  - https://cloud.google.com/blog/topics/developers-practitioners/understanding-autoscaling-in-google-cloud
  - https://cloud.google.com/compute/docs/instance-groups/using-autoscaler
- rule_id: gcp.compute.backend_service.balancer_ssl_policy_check_secure_ciphers
  service: compute
  resource: backend_service
  requirement: Balancer SSL Policy Check Secure Ciphers
  scope: compute.backend_service.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Secure Ciphers in Backend Service SSL Policies
  rationale: Using secure ciphers in SSL policies is crucial to protect communication between clients and backend services from interception and attacks like man-in-the-middle. Insecure ciphers can lead to vulnerabilities allowing unauthorized access to sensitive information, impacting confidentiality and compliance with standards such as PCI-DSS and HIPAA.
  description: This rule checks if the SSL policies attached to backend services enforce the use of secure ciphers. A secure SSL policy should exclude deprecated ciphers like MD5, RC4, or anonymous ciphers. To verify, review the SSL policy configuration in the GCP Console under Compute Engine, ensuring compliance with recommended secure cipher suites. Remediation involves updating the SSL policy to include only approved ciphers, which can be done through the GCP Console or using gcloud commands.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/load-balancing/ssl-policies
- rule_id: gcp.compute.backend_service.balancing_deletion_protection_configured
  service: compute
  resource: backend_service
  requirement: Balancing Deletion Protection Configured
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Backend Service Deletion Protection is Configured
  rationale: Deletion protection on backend services prevents accidental or unauthorized deletion, which could lead to service downtime and disrupt business operations. By configuring deletion protection, organizations mitigate the risk of data loss and service interruption, which is crucial for maintaining customer trust and meeting regulatory requirements.
  description: This rule verifies that deletion protection is enabled for Google Cloud Platform compute backend services. Deletion protection ensures that these services cannot be deleted without explicit consent, reducing the risk of accidental disruptions. To check this setting, navigate to the Google Cloud Console, select the backend service, and verify that deletion protection is activated. If not enabled, update the service settings to enable deletion protection.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.backend_service.balancing_desync_mitigation_mode_configured
  service: compute
  resource: backend_service
  requirement: Balancing Desync Mitigation Mode Configured
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Desync Mitigation Mode is Configured for Backend Services
  rationale: Configuring the Desync Mitigation Mode for backend services is critical to prevent desynchronization attacks that can lead to data corruption or unauthorized access. This configuration helps in maintaining consistent data flow and integrity across distributed systems, reducing the risk of downtime and potential breaches. Ensuring this setting aligns with compliance requirements such as PCI-DSS and ISO 27001, which mandate secure configurations for network communications.
  description: This rule checks if the Desync Mitigation Mode is enabled for Google Cloud Platform backend services. Desync Mitigation Mode helps protect against HTTP desync attacks by enforcing strict checks on HTTP/1.x request and response headers. To verify this setting, ensure that the 'desyncMitigationMode' field in the backend service configuration is set to 'ENABLED' or 'STRICT'. If this is not configured, update the backend service settings using the Google Cloud Console or gcloud command-line tool to enable this feature.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/load-balancing/docs/https
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.compute.backend_service.edge_cdn_access_logging_enabled
  service: compute
  resource: backend_service
  requirement: Edge Cdn Access Logging Enabled
  scope: compute.backend_service.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Edge CDN Access Logging for Backend Services
  rationale: Enabling Edge CDN access logging enhances visibility into request patterns and access behaviors, helping to identify potential security threats and performance issues. It also supports compliance with regulations that require logging of access to critical resources, thereby reducing the risk of undetected malicious activity and providing a clear audit trail for forensic investigations.
  description: This rule checks if Edge CDN access logging is enabled for Google Cloud backend services. To verify, ensure that the 'logging' configuration of each backend service includes the 'logConfig' field with 'enable' set to true. Remediation involves updating the backend service configuration to enable logging, which can be done via the GCP Console or command-line interface by setting the appropriate logging parameters.
  references:
  - https://cloud.google.com/load-balancing/docs/https/edge-logging
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-analyzing-http-s-logs
  - https://cloud.google.com/logging/docs/audit
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.backend_service.edge_cdn_cache_key_excludes_sensitive_headers
  service: compute
  resource: backend_service
  requirement: Edge Cdn Cache Key Excludes Sensitive Headers
  scope: compute.backend_service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Exclude Sensitive Headers from Edge CDN Cache Keys
  rationale: Excluding sensitive headers from Edge CDN cache keys is crucial to prevent exposure of sensitive information such as authentication tokens, cookies, or personally identifiable information (PII). This helps mitigate the risk of unauthorized access or data leakage, which can lead to security breaches, loss of customer trust, and non-compliance with data protection regulations like GDPR and HIPAA.
  description: This rule checks if sensitive headers are excluded from Edge CDN cache keys in GCP Backend Services. It ensures that headers containing sensitive data are not used in cache key creation, thereby reducing the risk of sensitive data exposure. To verify, inspect the backend service settings and ensure sensitive headers are not part of the CDN cache key configuration. Remediation involves updating the backend service configuration to explicitly exclude sensitive headers from cache key settings.
  references:
  - https://cloud.google.com/cdn/docs/caching#cache-key
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security
- rule_id: gcp.compute.backend_service.edge_cdn_cache_key_minimal_query_and_cookie_variants
  service: compute
  resource: backend_service
  requirement: Edge Cdn Cache Key Minimal Query And Cookie Variants
  scope: compute.backend_service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Minimal Query and Cookie Variants for CDN Cache Keys
  rationale: This rule helps prevent cache poisoning attacks by ensuring that Google Cloud's CDN cache keys are minimally affected by query strings and cookies. By minimizing these variants, the backend service reduces risk of unauthorized data exposure and ensures consistent content delivery, which is crucial for maintaining data integrity and meeting compliance requirements such as PCI-DSS and GDPR.
  description: This check ensures that the CDN cache key configuration for Google Cloud Backend Services is set to use minimal query and cookie variants. This involves verifying the 'includeHttpHeaders', 'includeNamedCookies', and 'queryStringWhitelist' settings in the backend service configuration. To remediate, adjust these settings to only include necessary cookies and query parameters, or use a whitelist approach to limit exposure. This minimizes cache key variability, enhancing security and performance.
  references:
  - https://cloud.google.com/cdn/docs/caching
  - https://cloud.google.com/architecture/best-practices-for-using-cdn#cache_keys
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Control 4.9
  - PCI-DSS Requirement 6.5 - Develop security guidelines and procedures
  - 'NIST SP 800-53 Rev. 5: SI-10 - Information Input Validation'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.backend_service.edge_cdn_hsts_enabled
  service: compute
  resource: backend_service
  requirement: Edge Cdn Hsts Enabled
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable HSTS for Edge CDN on Backend Services
  rationale: Enabling HTTP Strict Transport Security (HSTS) for Edge CDN on backend services mitigates man-in-the-middle attacks by ensuring browsers interact with the server using a secure HTTPS connection. This configuration is crucial for protecting sensitive data, maintaining user trust, and achieving compliance with data protection regulations such as GDPR and PCI-DSS.
  description: This rule checks if HSTS is enabled for the Edge CDN on GCP Compute backend services. HSTS ensures that all communications between clients and the service are conducted over secure HTTPS connections, preventing downgrade attacks. To verify this setting, check the 'cdnPolicy' configuration of the backend service in the Google Cloud Console or use the gcloud command-line tool. Remediation involves setting 'cdnPolicy.edgeSecurityPolicy' to enforce HSTS.
  references:
  - https://cloud.google.com/cdn/docs/edge-security-policy
  - https://cloud.google.com/security-compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://datatracker.ietf.org/doc/html/rfc6797
  - https://cloud.google.com/architecture/best-practices-for-securing-cloud-cdn
- rule_id: gcp.compute.backend_service.edge_cdn_https_only_viewer_protocol
  service: compute
  resource: backend_service
  requirement: Edge Cdn HTTPS Only Viewer Protocol
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enforce HTTPS for Edge CDN Viewer Protocol on Backend Services
  rationale: Enforcing HTTPS for edge CDN viewer protocols minimizes the risk of data interception and tampering by ensuring that data in transit is encrypted. This is crucial for maintaining user trust and meeting compliance requirements such as PCI-DSS, which mandate secure transmission of sensitive information. Failing to enforce HTTPS could result in data breaches, legal liabilities, and damage to the organization's reputation.
  description: This rule checks whether the 'Edge CDN' uses only HTTPS as the viewer protocol for backend services. To verify, ensure that the 'protocol' field for the backend service is set to 'HTTPS'. To remediate, update the backend service configuration to enforce HTTPS by setting the 'protocol' option appropriately. This ensures that all data between clients and the CDN is encrypted, protecting against potential man-in-the-middle attacks.
  references:
  - https://cloud.google.com/cdn/docs/using-cdn#protocol
  - https://cloud.google.com/security/compliance/pci-dss
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.backend_service.edge_cdn_origin_access_restricted
  service: compute
  resource: backend_service
  requirement: Edge Cdn Origin Access Restricted
  scope: compute.backend_service.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Edge CDN Origin Access for Backend Services
  rationale: Restricting access to the origins of Edge CDN prevents unauthorized entities from bypassing caching layers to access backend services directly. This reduces the risk of data breaches, service disruptions, and unnecessary load on backend systems, aligning with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks if the backend services in GCP Compute are configured to restrict origin access when using Edge CDN. Ensure that origin access is limited to trusted sources by configuring appropriate identity and access management (IAM) policies. Verify that backend services have settings that enforce origin access restrictions and update IAM roles and policies to limit permissions to necessary entities only.
  references:
  - https://cloud.google.com/cdn/docs/overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.backend_service.edge_cdn_signed_urls_or_headers_required_for_private_content
  service: compute
  resource: backend_service
  requirement: Edge Cdn Signed Urls Or Headers Required For Private Content
  scope: compute.backend_service.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Require Signed URLs/Headers for Private CDN Content
  rationale: Ensuring that edge CDN traffic is verified using signed URLs or headers helps protect private content from unauthorized access and reduces the risk of data breaches. This control is crucial for maintaining data confidentiality and integrity, especially for businesses subject to compliance requirements like PCI-DSS or HIPAA, where unauthorized data exposure can lead to significant financial and reputational damage.
  description: This rule checks if Google Cloud's Backend Services within the private networking scope require signed URLs or headers for edge CDN access to private content. To verify, ensure that the 'signedUrlKeyNames' or 'signedUrlCacheMaxAgeSec' fields are set when configuring backend services. Remediation involves updating the backend service configuration to enforce signed URL or header requirements, thereby ensuring that only authenticated requests can access sensitive content.
  references:
  - https://cloud.google.com/cdn/docs/private-content#signed-urls
  - https://cloud.google.com/architecture/best-practices-for-content-delivery-networking
  - 'PCI-DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - https://cloud.google.com/security/compliance
  - 'NIST SP 800-53 AC-3: Access Enforcement'
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.backend_service.edge_cdn_tls_min_1_2_enforced
  service: compute
  resource: backend_service
  requirement: Edge Cdn TLS Min 1 2 Enforced
  scope: compute.backend_service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS 1.2+ for Edge CDN in Backend Services
  rationale: Enforcing a minimum of TLS 1.2 for Edge CDN connections in backend services protects data in transit from interception and tampering by ensuring strong encryption protocols. This is crucial for maintaining data integrity and confidentiality, especially in compliance-driven industries such as finance and healthcare, where robust encryption standards are mandated by regulations like PCI-DSS and HIPAA.
  description: This rule checks whether backend services in Google Cloud's Compute Engine enforce a minimum of TLS 1.2 for connections through the Edge CDN. To verify compliance, ensure that the 'minTlsVersion' attribute in the backend service configuration is set to 'TLS_1_2' or higher in the Google Cloud Console or via the gcloud command-line tool. Remediate non-compliance by updating the backend service settings to enforce TLS 1.2 or higher, thereby enhancing data in transit protection.
  references:
  - https://cloud.google.com/cdn/docs/https
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.compute.backend_service.edge_cdn_valid_trusted_certificate_attached
  service: compute
  resource: backend_service
  requirement: Edge Cdn Valid Trusted Certificate Attached
  scope: compute.backend_service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Valid Trusted Certificates for Edge CDN in Backend Services
  rationale: Attaching a valid trusted certificate to your backend service's edge CDN is critical for ensuring encrypted data transmission over the network. Without a trusted certificate, data may be intercepted by attackers, leading to potential data breaches and non-compliance with regulations such as GDPR and PCI-DSS. This not only affects customer trust but can also result in significant financial and reputational damage.
  description: This rule checks whether a valid trusted certificate is attached to the edge CDN of a GCP backend service. To verify, ensure that your backend service has a certificate issued by a recognized certificate authority (CA) and that it is not expired or self-signed. Remediation involves acquiring a valid certificate from a CA and configuring your backend service to use this certificate for its edge CDN. This ensures that data in transit is encrypted and secure, thereby protecting sensitive information.
  references:
  - https://cloud.google.com/cdn/docs/using-ssl-certificates
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.nist.gov/cyberframework
- rule_id: gcp.compute.backend_service.edge_cdn_waf_web_acl_attached
  service: compute
  resource: backend_service
  requirement: Edge Cdn Waf Web ACL Attached
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge CDN WAF Web ACL is Attached to Backend Service
  rationale: Attaching a Web Application Firewall (WAF) ACL to an Edge CDN backend service helps protect web applications from common threats such as SQL injection, cross-site scripting, and other OWASP top 10 vulnerabilities. This enhances security by inspecting traffic and blocking malicious requests, reducing the risk of data breaches and ensuring compliance with security standards like PCI-DSS and NIST.
  description: This rule checks if a Web ACL is attached to a backend service in GCP's Edge CDN. Without this configuration, backend services are vulnerable to web-based attacks. To verify, inspect the backend service's configuration in the GCP Console or via the gcloud CLI to ensure a WAF Web ACL is associated. Remediation involves configuring or attaching an existing Web ACL to the backend service, which can be done through the Cloud Armor interface in the console.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/architecture/best-practices-for-ddos-resiliency
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.compute.backend_service.logging_enabled_gcp_storage_bucket_server_access_log_logging
  service: compute
  resource: backend_service
  requirement: Logging Enabled Gcp Storage Bucket Server Access Log Logging
  scope: compute.backend_service.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for Backend Service Access to GCP Storage Buckets
  rationale: Enabling server access logging for backend services that interact with GCP Storage Buckets is crucial for monitoring and auditing access patterns. This logging provides visibility into access operations, aiding in the detection of unauthorized access or anomalies. It supports compliance with regulations such as PCI-DSS and HIPAA, which require detailed access logs for sensitive data.
  description: This rule checks if logging is enabled for backend services that access GCP Storage Buckets. To verify, ensure that the 'logConfig' field is configured in the backend service with 'enable' set to true. Remediation involves updating the backend service configuration to include 'logConfig', thereby enabling logging. This ensures audit trails are maintained for all access requests, supporting both security and compliance efforts.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/storage/docs/access-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.compute.backend_service.network_tg_health_checks_tls_where_supported
  service: compute
  resource: backend_service
  requirement: Network Tg Health Checks TLS Where Supported
  scope: compute.backend_service.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS for Health Checks on Network Tg Where Supported
  rationale: Implementing TLS for network target group health checks enhances data integrity and confidentiality by encrypting the health check data in transit. This reduces the risk of man-in-the-middle attacks, ensuring that health checks cannot be intercepted or altered, thus maintaining the reliability and security of backend services. Compliance with regulations such as PCI-DSS and HIPAA often require encryption in transit, making TLS implementation crucial for meeting these standards.
  description: This rule checks if TLS is enforced on network target group health checks where supported, ensuring that sensitive data is encrypted in transit. Verify that the backend services utilizing network target groups have health checks configured with TLS. If not configured, update the health checks to support and enforce TLS by specifying the 'useSsl' attribute to 'true' in the health check configuration. This configuration can be reviewed and modified in the GCP Console or via the gcloud command-line tool.
  references:
  - https://cloud.google.com/load-balancing/docs/health-checks
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/encryption-in-transit/
  - https://cloud.google.com/load-balancing/docs/https/
  - https://cloud.google.com/storage/docs/encryption
- rule_id: gcp.compute.backend_service.network_tg_targets_in_private_subnets
  service: compute
  resource: backend_service
  requirement: Network Tg Targets In Private Subnets
  scope: compute.backend_service.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Backend Service Targets Are in Private Subnets
  rationale: Configuring backend services to target instances within private subnets reduces exposure to potential external threats and unauthorized access. This setup is crucial for protecting sensitive data and maintaining compliance with network security standards such as PCI-DSS and ISO 27001. Failure to properly isolate backend services can lead to data breaches and compromise service integrity.
  description: This rule checks that all network traffic targets for backend services are located within private subnets, preventing direct exposure to the public internet. Verify configurations by ensuring that backend instances are assigned private IPs in VPC subnets. Remediation involves modifying backend services to ensure only private subnets are used, adjusting the network configuration in the GCP console under VPC settings.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/overview/whitepapers
  - https://cloud.google.com/vpc/docs/subnets
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.compute.backend_service.service_https_logging_enabled
  service: compute
  resource: backend_service
  requirement: Service HTTPS Logging Enabled
  scope: compute.backend_service.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable HTTPS Logging for GCP Compute Backend Services
  rationale: Enabling HTTPS logging for backend services in GCP is critical for monitoring and auditing traffic patterns, detecting anomalies, and ensuring compliance with regulatory frameworks such as PCI-DSS and SOC2. Without HTTPS logging, organizations may miss crucial insights into service usage, potentially leading to unmonitored security breaches or non-compliance issues.
  description: This rule checks whether HTTPS logging is enabled for Google Cloud Platform Compute Backend Services. To verify, ensure that the 'logConfig.enable' field is set to true in the backend service's configuration. Remediation involves updating the backend service settings to enable logging, which can be done via the GCP Console, gcloud command-line tool, or Terraform scripts. This setup allows for detailed logging of HTTP(S) requests, facilitating enhanced security monitoring and auditing.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.backend_service.waf_acl_attached
  service: compute
  resource: backend_service
  requirement: Waf ACL Attached
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure WAF ACL is Attached to Backend Services
  rationale: Attaching a Web Application Firewall (WAF) Access Control List (ACL) to backend services helps protect against common web exploits that could compromise data integrity and availability. Without this protection, an application is more vulnerable to attacks such as SQL injection and cross-site scripting (XSS), which can lead to data breaches and service disruptions. Compliance with security standards like PCI-DSS and NIST requires robust application layer protections, which WAFs provide.
  description: This check verifies that a WAF ACL is attached to Google Cloud Platform's backend services. A WAF ACL helps filter and monitor HTTP requests, providing an additional layer of security by allowing or blocking requests based on predefined rules. To verify, access the GCP Console, navigate to the Compute Engine section, and ensure each backend service has an associated WAF ACL. Remediation involves creating or associating a WAF ACL with the backend service if it is not already attached.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/security/compliance/pci-dss
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.compute.disk.cmk_cmek_configured
  service: compute
  resource: disk
  requirement: CMK Cmek Configured
  scope: compute.disk.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure CMK or CMEK Configured for Compute Disks
  rationale: Configuring Customer-Managed Keys (CMK) or Customer-Managed Encryption Keys (CMEK) for Compute Engine disks ensures that your data is encrypted with keys you control. This enhances data security by protecting sensitive information from unauthorized access and meets compliance requirements such as GDPR, HIPAA, and PCI-DSS which mandate strong encryption practices.
  description: This rule checks if Compute Engine disks are configured with CMK or CMEK. Disks not using these keys rely on Google-managed encryption keys, which may not meet certain regulatory or organizational security requirements. To verify, review your disk settings in the GCP Console under 'Encryption' to ensure CMK or CMEK is selected. Remediation involves creating a key in Cloud Key Management Service (KMS) and applying it to your disks.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.compute.disk.csek_status_configured
  service: compute
  resource: disk
  requirement: Csek Status Configured
  scope: compute.disk.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure CMEK is Configured for Compute Disks
  rationale: Configuring Customer-Supplied Encryption Keys (CSEK) for disks helps secure data at rest by allowing organizations to manage and control encryption keys. This mitigates risks associated with unauthorized data access and is essential for meeting compliance requirements like GDPR and HIPAA, where data protection is crucial.
  description: This rule checks if Customer-Supplied Encryption Keys (CSEK) are configured for Google Cloud Compute Engine disks. To verify, inspect the disk configuration for 'diskEncryptionKey' settings that specify a CSEK. Remediation involves updating disk settings to include a CSEK, ensuring the encryption keys used align with organizational security policies. This enhances data security by ensuring encryption keys are managed by the organization.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/docs/security/encryption-at-rest
  - https://cloud.google.com/security/encryption/default-encryption
- rule_id: gcp.compute.disk.data_protection_storage_volume_cmk_cmek_configured
  service: compute
  resource: disk
  requirement: Data Protection Storage Volume CMK Cmek Configured
  scope: compute.disk.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure CMEK Configured for Compute Engine Disks
  rationale: Configuring Customer Managed Encryption Keys (CMEK) for Compute Engine disks enhances control over cryptographic operations and key management policies, thereby reducing the risk of unauthorized access to sensitive data. It also aids in meeting compliance requirements for data protection and privacy regulations such as GDPR and HIPAA, which mandate strong data encryption standards.
  description: This rule checks if Compute Engine disks are configured with Customer Managed Encryption Keys (CMEK) rather than using Google-managed keys. To verify, ensure that the 'kmsKeyName' property is set for each disk resource. If not configured, update the disk configuration to specify a suitable CMEK, leveraging Cloud Key Management Service (KMS) to create and manage cryptographic keys. This configuration provides enhanced security by allowing organizations to manage their own encryption keys.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.disk.data_protection_storage_volume_encryption_at_rest_enabled
  service: compute
  resource: disk
  requirement: Data Protection Storage Volume Encryption At Rest Enabled
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Encryption at Rest for GCP Compute Disks
  rationale: Ensuring data stored on Google Cloud Platform Compute Engine disks is encrypted at rest protects sensitive information from unauthorized access and potential breaches. This is critical for maintaining customer trust, meeting regulatory requirements like GDPR and HIPAA, and mitigating the risk of data exposure in case of hardware theft or unauthorized administrative access.
  description: This rule checks whether encryption at rest is enabled for Google Cloud Platform Compute Engine disks. By default, GCP encrypts all data at rest; however, users can apply their own encryption keys for added security. To verify this, ensure that the Compute Engine disks have the 'encryptionKey' property configured. Remediation involves setting up Customer-Managed Encryption Keys (CMEK) through Google Cloud Key Management Service (KMS) and updating disk configurations to use these keys.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
- rule_id: gcp.compute.disk.data_protection_storage_volume_snapshots_encrypted
  service: compute
  resource: disk
  requirement: Data Protection Storage Volume Snapshots Encrypted
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Snapshots of Compute Disk are Encrypted
  rationale: Encrypting snapshots of Compute Engine disks is crucial to protect sensitive data from unauthorized access and potential exposure. This measure reduces the risk of data breaches, unauthorized data manipulation, and compliance violations with standards such as PCI-DSS, HIPAA, and GDPR. Encryption at rest ensures that even if physical security is compromised, data remains unreadable and secure.
  description: This rule checks whether snapshots of Compute Engine disks are encrypted using customer-managed or Google-managed encryption keys. Unencrypted snapshots can lead to data exposure if accessed by unauthorized users. To verify, ensure that all disk snapshots are created with encryption enabled by configuring the appropriate encryption keys in the Cloud Console or via the gcloud CLI. Remediate by updating existing snapshots to apply encryption and setting organization policies to enforce encryption on all new snapshots.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
- rule_id: gcp.compute.disk.disk_encrypted
  service: compute
  resource: disk
  requirement: Disk Encrypted
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Engine Disks are Encrypted at Rest
  rationale: Encrypting disks at rest protects sensitive data from unauthorized access and potential breaches, particularly if a disk is improperly decommissioned. This is critical for compliance with data protection regulations like GDPR and helps mitigate risks associated with data exposure or loss, ensuring that sensitive information remains secure even if physical security controls are bypassed.
  description: This rule checks whether Compute Engine disks are encrypted using Google-managed encryption keys or customer-managed encryption keys (CMEK). To verify, review the disk's properties in the GCP Console or use the `gcloud compute disks describe` command to check the 'encryption' field. Remediation involves enabling encryption for all disks during creation or updating existing disks with encryption by moving data to a new encrypted disk.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.disk.dr_replication_auth_roles_least_privilege
  service: compute
  resource: disk
  requirement: DR Replication Auth Roles Least Privilege
  scope: compute.disk.replication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for DR Replication Auth Roles
  rationale: Restricting the roles for disaster recovery (DR) replication to the least privilege necessary reduces the risk of unauthorized access and potential data breaches. Excessive permissions can lead to accidental or malicious data manipulation, impacting business continuity and potentially violating compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks that only the minimum necessary permissions are granted for roles involved in DR replication for compute disks. Ensure that roles are strictly defined to avoid excessive permissions that could be misused. Verification involves reviewing IAM policies assigned to resources managing DR replication and removing any permissions that are not essential. Remediation involves customizing roles to limit permissions to only those required for DR replication operations.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/compute/docs/disks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/least-privilege
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
- rule_id: gcp.compute.disk.dr_replication_cross_region_replication_encrypted
  service: compute
  resource: disk
  requirement: DR Replication Cross Region Replication Encrypted
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region Disk Replication is Encrypted
  rationale: Encrypting cross-region replicated disks safeguards sensitive data from unauthorized access and potential data breaches during disaster recovery scenarios. This practice mitigates risks of data exposure when data is transferred across regions and helps meet compliance requirements such as GDPR, which mandates data protection during transit and at rest.
  description: This rule checks that all cross-region replicated disks in GCP are encrypted at rest using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). Verify that the 'diskEncryptionKey' is specified in the disk's configuration. To remediate, enable encryption on disks by setting up CMEK or GMEK in the Compute Engine settings, ensuring that the 'diskEncryptionKey' is correctly configured to protect the data.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance
  - 'CIS GCP Benchmark: Section 4.4 - Ensure that Cloud Storage buckets are encrypted with Customer-Managed Keys (CMKs)'
  - 'NIST SP 800-53: SC-13 Cryptographic Protection'
  - 'PCI-DSS Requirement 3: Protect stored cardholder data'
  - ISO 27001:2013 A.10.1 Cryptographic controls
- rule_id: gcp.compute.disk.dr_replication_encryption_in_transit_tls_required
  service: compute
  resource: disk
  requirement: DR Replication Encryption In Transit TLS Required
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure TLS for DR Replication Encryption In Transit for Disks
  rationale: Encrypting data in transit using TLS during disaster recovery (DR) replication is crucial to protect sensitive information from interception and unauthorized access. This is essential to mitigate the risks of data breaches and ensure compliance with standards like PCI-DSS and HIPAA, which mandate encrypted data transfers. Failure to do so can lead to significant reputational damage and legal penalties.
  description: This rule checks whether Transport Layer Security (TLS) is enabled for encrypting data in transit during DR replication of Compute Engine disks. Ensure that TLS is configured by verifying the encryption settings in the Google Cloud Console or via the gcloud command-line tool. To remediate, update the disk replication settings to enforce TLS encryption when data is transferred between data centers.
  references:
  - https://cloud.google.com/compute/docs/disks/add-persistent-disk#encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://cloud.google.com/security/encryption-in-transit/
- rule_id: gcp.compute.disk.dr_replication_lag_monitoring_alerts_enabled
  service: compute
  resource: disk
  requirement: DR Replication Lag Monitoring Alerts Enabled
  scope: compute.disk.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Monitoring Alerts for DR Replication Lag on Disks
  rationale: Monitoring DR replication lag is crucial for ensuring data consistency and availability in disaster recovery scenarios. Without timely alerts, businesses risk data loss or corruption, which can impact service continuity and violate compliance mandates such as ISO 27001 and SOC2. Proactive monitoring helps mitigate these risks by allowing prompt corrective actions.
  description: This rule checks if alerts are configured for monitoring replication lag of disks in disaster recovery setups on GCP. Ensure that you have set up Cloud Monitoring alerts to notify you of any lag beyond acceptable thresholds. Verify by accessing the Cloud Monitoring dashboard and reviewing alert policies related to disk replication metrics. Remediation involves configuring alert policies to track relevant metrics and setting appropriate thresholds to trigger notifications.
  references:
  - https://cloud.google.com/compute/docs/disks/disaster-recovery
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/monitoring/support/notification-options
- rule_id: gcp.compute.disk.encryption_at_rest_enabled
  service: compute
  resource: disk
  requirement: Encryption At Rest Enabled
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption at Rest is Enabled for Compute Disks
  rationale: Enabling encryption at rest protects sensitive data stored on Compute Engine disks from unauthorized access and potential data breaches. This is critical for maintaining business reputation, preventing financial loss from data theft, and adhering to regulatory standards such as GDPR and HIPAA.
  description: This rule checks if Compute Engine disks have encryption at rest enabled using either Google-managed or customer-managed encryption keys. To verify, check the disk's encryption settings in the GCP Console or via the gcloud CLI. Remediation involves enabling encryption for disks without it, which can be done by configuring the disk to use Google-managed keys or by specifying a customer-managed key in Cloud KMS.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/compute/docs/disks#encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/
- rule_id: gcp.compute.disk.encryption_enabled
  service: compute
  resource: disk
  requirement: Encryption Enabled
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Compute Engine Disks Have Encryption Enabled
  rationale: Enabling encryption for Compute Engine disks protects sensitive data from unauthorized access and data breaches. It is essential for maintaining confidentiality and integrity of data-at-rest, and is often a requirement for regulatory compliance frameworks such as PCI-DSS and HIPAA. Unencrypted disks can expose data to risk if compromised, leading to potential financial loss and reputational damage.
  description: This rule verifies that all Compute Engine disks have encryption enabled, either with Google-managed or customer-supplied encryption keys. To check, navigate to the Google Cloud Console, select 'Compute Engine', and review disk settings for encryption status. Remediation involves configuring disk encryption during disk creation or updating existing disks to use encryption keys. Utilize customer-managed keys for enhanced security control.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/compute/docs/disks
  - https://cloud.google.com/iam/docs/using-cmek
- rule_id: gcp.compute.disk.in_use
  service: compute
  resource: disk
  requirement: In Use
  scope: compute.disk.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure GCP Compute Disks Are Attached to Active Instances
  rationale: Unused disks in GCP incur costs and can lead to potential security risks such as unauthorized access or data leaks. Ensuring disks are in use helps optimize resource allocation and reduces the attack surface by limiting exposure of data stored on unassociated disks. It also aids in maintaining compliance with data protection standards by ensuring data is not inadvertently exposed.
  description: This rule checks that all Google Cloud Platform Compute Engine disks are attached to active instances. Disks that are not in use should be evaluated and either attached to an instance or deleted if no longer needed. This helps in managing costs effectively and reducing security risks. To remediate, attach unattached disks to appropriate instances or delete them if they are redundant.
  references:
  - https://cloud.google.com/compute/docs/disks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances
- rule_id: gcp.compute.disk.network_mtls_required_for_internal_services_where_supported
  service: compute
  resource: disk
  requirement: Network Mtls Required For Internal Services Where Supported
  scope: compute.disk.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce mTLS for Internal Disk Services in Compute
  rationale: Requiring mutual TLS for internal disk services mitigates risks such as unauthorized access and data interception by ensuring that both the client and server authenticate each other. This is crucial for protecting sensitive data and maintaining compliance with standards like PCI-DSS and HIPAA, which mandate strong encryption and authentication mechanisms to safeguard data privacy.
  description: This rule checks that network mTLS is enabled for internal disk services where supported within Google Compute Engine. Verify that all internal services are configured to use mTLS by updating the load balancer settings or service configurations to require client certificates. Remediation involves configuring IAM roles and certificates in the GCP Console or using gcloud commands to enforce mTLS, ensuring secure communication channels.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://cloud.google.com/compute/docs/disks
  - CIS Google Cloud Computing Foundations Benchmark v1.0.0, Section 5.2
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.compute.disk.network_tls_min_1_2_enforced
  service: compute
  resource: disk
  requirement: Network TLS Min 1 2 Enforced
  scope: compute.disk.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Disk Network TLS Min Version is 1.2
  rationale: Enforcing a minimum TLS version of 1.2 for GCP Compute Disk network communication is crucial to protect data in transit from being intercepted or tampered with by attackers. This enhances data integrity and confidentiality, reducing the risk of man-in-the-middle attacks and ensuring compliance with industry standards like PCI-DSS and NIST SP 800-52.
  description: This rule checks that all network communications involving GCP Compute Disks enforce a minimum TLS version of 1.2. To verify, ensure that TLS policies are configured correctly in your network security settings. Remediation involves updating any outdated configurations to specify TLS 1.2 as the minimum version to secure data transfers, particularly for sensitive or regulated information.
  references:
  - https://cloud.google.com/compute/docs/disks
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.disk.public_snapshot
  service: compute
  resource: disk
  requirement: Public Snapshot
  scope: compute.disk.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure Compute Disk Snapshots Are Not Publicly Accessible
  rationale: Publicly accessible disk snapshots can expose sensitive data to unauthorized users, leading to potential data breaches. This can result in financial loss, reputational damage, and non-compliance with regulations such as GDPR and HIPAA. Ensuring snapshots are private mitigates the risk of unauthorized data access and enhances the security posture of your cloud environment.
  description: This rule checks if any Compute Engine disk snapshots are publicly accessible. Public snapshots can be accessed by anyone on the internet, increasing the risk of data exposure. To verify, review the 'isPublic' property of each snapshot in the GCP Console or use gcloud commands. Remediation involves modifying the snapshot permissions to restrict access, ensuring only authorized users can view or restore them.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/identity-security/understanding-google-cloud-security-and-compliance
- rule_id: gcp.compute.disk.snapshot
  service: compute
  resource: disk
  requirement: Snapshot
  scope: compute.disk.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Compute Disk Snapshots are Encrypted
  rationale: Encrypting disk snapshots is crucial to protect sensitive data from unauthorized access, especially in backup and recovery scenarios. Unencrypted snapshots can lead to data breaches, exposing critical business information and potentially violating compliance standards such as GDPR and HIPAA. By enforcing encryption, organizations mitigate risks associated with data theft and ensure data integrity and confidentiality.
  description: This rule checks whether snapshots of Compute Engine disks are encrypted using Customer-Managed Encryption Keys (CMEK) or Google-managed encryption keys. To verify, inspect the encryption status of existing disk snapshots in the GCP Console or via gcloud CLI. Remediation involves enabling encryption by specifying a CMEK during snapshot creation or ensuring current snapshots use Google-managed keys. Regular reviews and updates to encryption keys are recommended to maintain security posture.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots#encrypted_snapshots
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/compute/docs/disks/viewing-and-applying-resource-policies
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.disk.snapshots_encrypted
  service: compute
  resource: disk
  requirement: Snapshots Encrypted
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Snapshots Are Encrypted at Rest
  rationale: Encrypting snapshots at rest is crucial for protecting sensitive data from unauthorized access and potential breaches. Unencrypted snapshots can be exploited if compromised, leading to data leaks and violating compliance with standards like PCI-DSS, HIPAA, and GDPR. Encrypting data at rest mitigates the risk of data exposure and enhances data privacy and security posture.
  description: This rule checks whether snapshots of Compute Engine disks are encrypted using Google-managed or customer-managed encryption keys. Verify that all snapshots are encrypted by default by reviewing the encryption settings in the Google Cloud Console or using the gcloud command-line tool. To remediate, enable encryption for snapshots through the Compute Engine settings, ensuring compliance with encryption standards and enhancing data protection.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis#section-5.4
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.disk.snapshots_not_public
  service: compute
  resource: disk
  requirement: Snapshots Not Public
  scope: compute.disk.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure Compute Disk Snapshots Are Not Publicly Accessible
  rationale: Publicly accessible disk snapshots expose sensitive data to unauthorized users, increasing the risk of data breaches and non-compliance with regulations such as GDPR and PCI-DSS. Unauthorized access could lead to data leaks, financial loss, and reputational damage to the organization.
  description: This rule checks if Google Cloud Platform Compute Disk snapshots are publicly accessible. It ensures that no snapshots are set to have 'allUsers' or 'allAuthenticatedUsers' in their access control lists. To verify, review the IAM policies associated with each snapshot and remove any public access entries. Ensure that only necessary and authorized personnel have access to these snapshots.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.allow_ingress_from_internet_to_all_ports_gcp_api_ga_attached
  service: compute
  resource: firewall
  requirement: Allow Ingress From Internet To All Ports Gcp API Ga Attached
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Ingress from Internet to All Ports on GCP Firewall
  rationale: Allowing unrestricted ingress from the internet to all ports poses significant security risks, including potential unauthorized access, data breaches, and exposure to attacks such as DDoS. This configuration can lead to non-compliance with standards like PCI-DSS and ISO 27001, which require controlled access to protect sensitive data.
  description: This rule checks for firewall rules that allow ingress traffic from the internet on all ports, which can expose resources to unauthorized access. Verify that firewall rules specify limited source IP ranges and only open necessary ports. Remediation involves restricting ingress rules to specific IP addresses and limiting ports to those essential for operations. Use the GCP Console or gcloud CLI to review and update firewall rules accordingly.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.allow_ingress_tcp_port_22_gcp_compute_securitygro_configured
  service: compute
  resource: firewall
  requirement: Allow Ingress Tcp Port 22 Gcp Compute Securitygro Configured
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Ingress TCP Port 22 to Specific IPs in GCP Firewall
  rationale: Allowing unrestricted access to TCP port 22 (SSH) can expose virtual machines to unauthorized access attempts and brute-force attacks, compromising sensitive data. Implementing strict access controls aligns with regulatory requirements such as PCI-DSS and helps mitigate potential security breaches.
  description: This rule checks for GCP firewall configurations that permit ingress traffic on TCP port 22 from any source IP. It is crucial to limit access to known IP addresses to reduce the attack surface. Verify firewall rules and update them to allow SSH access only from trusted IP addresses or ranges. Remediation involves modifying the firewall rule to specify allowed source IPs, ensuring enhanced security while maintaining necessary access.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.firewall.data_warehouse_sg_egress_egress_restricted
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Egress Egress Restricted
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Egress Traffic for Data Warehouse Firewall
  rationale: Unrestricted egress traffic from data warehouse resources can expose sensitive data to unauthorized networks, leading to data breaches and non-compliance with data protection regulations such as GDPR and CCPA. Restricting egress traffic minimizes the attack surface and ensures that only legitimate communication channels are used, reducing the risk of data exfiltration.
  description: This rule checks if egress traffic from the data warehouse firewall is restricted to necessary IP ranges and ports. To verify, ensure that egress rules specify allowed destination IPs and protocols, and block all others by default. Remediation involves updating firewall rules to only allow traffic to known, trusted destinations necessary for business operations, and logging all egress activities for audit purposes.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.data_warehouse_sg_egress_restricted
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Egress Restricted
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Data Warehouse Firewall Egress Traffic
  rationale: Unrestricted egress traffic from data warehouses can lead to data exfiltration and unauthorized access to sensitive data, posing significant business risks. Ensuring egress restrictions aligns with compliance requirements such as PCI-DSS and HIPAA, and mitigates threats like data breaches and insider threats.
  description: This rule checks whether egress traffic from Data Warehouse security groups is appropriately restricted to prevent unauthorized data flows. It requires configuring firewall rules to limit outbound connections to only trusted IP ranges and necessary ports. To verify, review the firewall settings in the GCP Console under VPC Network > Firewall, and ensure egress rules are defined for the data warehouse subnet with least privilege access. Remediation involves updating firewall rules to enforce these restrictions.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.data_warehouse_sg_ingress_no_0_0_0_0
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Ingress No 0 0 0 0
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Disallow Wide Open Ingress to Data Warehouse Firewalls
  rationale: Allowing ingress from 0.0.0.0/0 to data warehouse systems can expose sensitive data to unauthorized access, increasing the risk of data breaches. This is particularly critical for organizations handling regulated data, where compliance with standards like PCI-DSS, HIPAA, or GDPR is necessary to avoid fines and reputational damage.
  description: This rule checks for firewall rules that permit ingress traffic from any IP address (0.0.0.0/0) to data warehouse resources. Such configurations should be restricted to mitigate unauthorized access risks. To remediate, modify the firewall rule to limit ingress to trusted IP ranges or utilize identity-aware proxies. Verification can be done via the Google Cloud Console by reviewing firewall rules under the Compute Engine section.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.data_warehouse_sg_ingress_only_required_ports
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Ingress Only Required Ports
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Data Warehouse Ingress to Required Ports
  rationale: Allowing only necessary ports for ingress to your data warehouse minimizes the attack surface, reducing the likelihood of unauthorized access or data breaches. This is critical for protecting sensitive data and meeting compliance requirements such as PCI-DSS and HIPAA, which mandate strict access controls to safeguard sensitive information.
  description: This rule checks whether ingress traffic to Google Cloud data warehouses is limited to only the required ports. By default, all traffic should be denied except for specific ports used by your applications or services. To verify, review the firewall rule settings to ensure only necessary ports are open, such as port 5432 for PostgreSQL. Remediation involves updating or creating firewall rules to restrict inbound traffic to these specified ports, ensuring all other ingress traffic is denied.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.data_warehouse_sg_no_0_0_0_0_ingress
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg No 0 0 0 0 Ingress
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Ingress Traffic to Data Warehouse Firewalls
  rationale: Allowing ingress from 0.0.0.0/0 exposes the data warehouse to potential unauthorized access, increasing the risk of data breaches. This misconfiguration can lead to non-compliance with data protection regulations such as PCI-DSS and GDPR, which mandate restricted access to sensitive data. Properly scoped firewall rules are crucial to safeguarding data and maintaining customer trust.
  description: This rule checks for firewall configurations allowing ingress traffic from any IP address (0.0.0.0/0) to data warehouses. To verify compliance, review firewall rules in the GCP Console or via the gcloud CLI to ensure that ingress traffic is restricted to specific IPs or ranges. Remediation involves updating firewall rules to specify allowed IP addresses, reducing exposure to only trusted sources.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/best-practices-for-enterprise-organizations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://ec.europa.eu/info/law/law-topic/data-protection_en
- rule_id: gcp.compute.firewall.data_warehouse_sg_only_required_ports_open
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Only Required Ports Open
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Ports for Data Warehouse Firewalls
  rationale: Opening only necessary ports for data warehouse firewalls minimizes the attack surface, reducing the risk of unauthorized access or data breaches. This practice helps protect sensitive data and supports compliance with regulations like PCI-DSS and HIPAA, which require stringent access controls and data protection mechanisms.
  description: This rule checks if firewall rules for data warehouse instances are configured to allow only the necessary ports. Specifically, it ensures that ports such as 5432 for PostgreSQL or 3306 for MySQL are open while unnecessary ports are closed. To verify, review firewall rules in the GCP Console, ensuring only required ports are listed. Remediate by modifying firewall rules to restrict ports to those essential for data warehouse operations.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.network_default_deny_between_tiers
  service: compute
  resource: firewall
  requirement: Network Default Deny Between Tiers
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Default Deny Policy Between Network Tiers
  rationale: Implementing a default deny policy between network tiers minimizes the risk of lateral movement by attackers and helps prevent unauthorized access to sensitive resources. This is critical for maintaining data integrity and confidentiality, as it ensures that only explicitly permitted communication is allowed, aligning with regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that firewall rules enforce a default deny policy between different network tiers within GCP. By default, all inbound and outbound traffic between tiers should be denied, and only specific, necessary communications should be allowed. To verify compliance, review firewall rule settings in the GCP Console or using the gcloud command-line tool. Remediation involves configuring firewall rules to explicitly allow only required traffic while denying all other traffic by default.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-41r1.pdf
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#security_and_compliance
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.compute.firewall.network_exception_approvals_required
  service: compute
  resource: firewall
  requirement: Network Exception Approvals Required
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Firewall Rules Require Network Exception Approvals
  rationale: Requiring approvals for network exceptions minimizes the risk of unauthorized access and potential data breaches. It ensures that firewall rule changes are vetted for security impact, aligning with compliance needs such as PCI-DSS and ISO 27001, which mandate strict access controls and auditing. This practice helps prevent accidental or malicious misconfigurations that could expose sensitive data.
  description: This rule checks that any exceptions to standard network access control policies in GCP firewall rules have documented approvals. Specifically, it ensures that any rule allowing traffic from less secure networks or any 'allow' rules with a broad IP range have been reviewed and approved by a security team. To verify, review the audit logs for evidence of approval and ensure that all firewall changes are tracked in a change management system. Remedy any unapproved rules by conducting a security review and obtaining necessary approvals or removing the rule.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.network_logging_enabled
  service: compute
  resource: firewall
  requirement: Network Logging Enabled
  scope: compute.firewall.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Logging is Enabled for Firewall Rules
  rationale: Enabling network logging on firewall rules is critical for monitoring and auditing network traffic, helping to identify unauthorized access attempts or breaches. This enhances visibility into network operations, supports incident response efforts, and aids in meeting various compliance requirements such as PCI-DSS and SOC2 by providing detailed records of data flow across network boundaries.
  description: This rule checks if network logging is enabled for all firewall rules within your GCP project. Network logging should be enabled to capture logs of connections passing through your firewall, providing crucial data for security analysis and incident response. To remediate, ensure that logging is turned on for each firewall rule by setting the 'enableLogging' field to true. Verification can be performed by reviewing the firewall rule settings in the Google Cloud Console or via the gcloud CLI.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://cloud.google.com/compute/docs/reference/rest/v1/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.network_microseg_endpoint_policies_applied
  service: compute
  resource: firewall
  requirement: Network Microseg Endpoint Policies Applied
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Microsegmentation Policies on GCP Firewall
  rationale: Applying network microsegmentation endpoint policies is crucial for minimizing lateral movement within the network, thereby reducing the attack surface. This enhances the security posture by ensuring that only necessary and explicit communications are allowed between network segments, which is essential for meeting regulatory requirements such as PCI-DSS and HIPAA.
  description: This rule checks for the application of network microsegmentation policies on GCP firewall configurations, ensuring that endpoint policies are properly applied to manage and restrict traffic flows. Verify that the firewall rules are configured to enforce microsegmentation by using network tags or service accounts to define granular access controls. Remediation involves auditing current configurations and applying policies that enforce strict traffic segregation between different network zones.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/architecture/network-security#microsegmentation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-207/final
- rule_id: gcp.compute.firewall.network_microseg_identity_aware_policies_enabled
  service: compute
  resource: firewall
  requirement: Network Microseg Identity Aware Policies Enabled
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enable Identity-Aware Policies for Network Microsegmentation
  rationale: Identity-aware policies enhance security by enforcing access controls based on user identity and context, reducing the risk of unauthorized access to sensitive data. This approach helps in mitigating insider threats and limits the attack surface by ensuring only authenticated and authorized entities can access specific network segments. It also aids in meeting compliance with regulations such as GDPR and HIPAA, which mandate stringent access controls.
  description: This rule checks if identity-aware policies are enabled for microsegmented networks in GCP. These policies should be configured to restrict access based on user identity and contextual attributes. Verification involves reviewing firewall rules to ensure they incorporate identity-based restrictions. To remediate, configure Identity-Aware Proxy (IAP) for your applications and define firewall rules that incorporate identity conditions to control access effectively.
  references:
  - https://cloud.google.com/iap/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/concepts-cspm
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Control 9.1
  - NIST SP 800-53 Rev. 5, AC-3 Access Enforcement
  - https://cloud.google.com/security/infrastructure/design
  - ISO/IEC 27001:2013, A.9.1.2 Access Control Policy
- rule_id: gcp.compute.firewall.network_nacl_egress_restricted
  service: compute
  resource: firewall
  requirement: Network Nacl Egress Restricted
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Egress Traffic with Network ACLs on GCP Firewall
  rationale: Restricting egress traffic in your Google Cloud environment is crucial to prevent data exfiltration and mitigate the risk of unauthorized external communications. Open egress rules can expose your network to potential threats, including data breaches and regulatory non-compliance, affecting business continuity and reputation.
  description: This rule checks that egress rules in GCP firewall configurations are restricted to only necessary IP ranges and ports. To verify, review firewall settings in the Google Cloud Console to ensure no overly permissive egress rules exist. Remediation involves updating firewall rules to limit outbound traffic to specific, trusted IP addresses and ports, aligning with least privilege principles.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.network_nacl_no_allow_all_rules
  service: compute
  resource: firewall
  requirement: Network Nacl No Allow All Rules
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Firewall Rules to Avoid Open Access
  rationale: Allowing open access through firewall rules can expose your network to unauthorized access and potential cyber threats, increasing the risk of data breaches and compromising sensitive information. Adhering to the principle of least privilege and preventing 'allow all' rules is crucial for maintaining security posture and ensuring compliance with regulations like PCI-DSS and ISO 27001.
  description: This rule checks for firewall rules in GCP that permit open access to all IP addresses. It ensures that no firewall rule is overly permissive by allowing traffic from any IP, which can be verified by reviewing the 'sourceRanges' field in firewall configurations. To remediate, replace overly broad source ranges with specific IP addresses or CIDR blocks that correspond to trusted sources, reducing the attack surface.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.compute.firewall.network_no_permit_all
  service: compute
  resource: firewall
  requirement: Network No Permit All
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Firewall Rules from Allowing Open Network Access
  rationale: Allowing unrestricted network access poses significant security risks, including unauthorized access, data breaches, and potential exploitation by attackers. This can lead to financial loss, reputational damage, and legal liabilities, especially if sensitive data is exposed. Ensuring firewall rules do not permit all traffic is crucial for maintaining a secure and compliant cloud environment.
  description: This rule checks for firewall configurations that allow open access (0.0.0.0/0) to networks. It verifies that firewall rules are not set to 'allow' all inbound or outbound traffic without proper restrictions. Remediation involves reviewing firewall rules and limiting access to specific IP ranges, services, or protocols as per business requirements. For verification, review firewall rule settings in the Google Cloud Console under 'VPC Network > Firewall'.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/network-connectivity/docs
  - https://cloud.google.com/architecture/security-foundations/network-architecture
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.compute.firewall.network_no_permit_any_any
  service: compute
  resource: firewall
  requirement: Network No Permit Any Any
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Any-Any Firewall Rules in GCP
  rationale: Allowing 'any-to-any' traffic in firewall rules can expose your network to unauthorized access and potential attacks. This configuration increases the risk of data breaches and non-compliance with security standards such as PCI-DSS and ISO 27001, which mandate strict access controls to protect sensitive data.
  description: This rule checks for firewall rules in GCP that allow unrestricted access to and from any IP addresses. To verify, review the firewall configurations in the Google Cloud Console under VPC network settings and ensure no rules permit '0.0.0.0/0' as both source and destination. Remediation involves updating these rules to specify only the necessary IP ranges and ports, implementing least privilege access principles.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.firewall.network_policies_present
  service: compute
  resource: firewall
  requirement: Network Policies Present
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Policies Are Applied to GCP Firewall Rules
  rationale: Implementing network policies in GCP firewall rules is crucial for defining and enforcing traffic ingress and egress controls. This reduces the risk of unauthorized access, data breaches, and lateral movement within the network. Properly configured network policies help meet compliance with standards such as PCI-DSS and ISO 27001, thereby protecting sensitive data and maintaining customer trust.
  description: This rule checks if network policies are applied to GCP firewall rules, ensuring that only authorized traffic can access network resources. Verify that all firewall rules have associated network policies specifying ingress and egress traffic controls. Remediation involves reviewing each firewall rule and applying appropriate network policies to restrict access based on business and security requirements. Adjust or create firewall rules with network policies using the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/network-security
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.compute.firewall.network_policy_store_encrypted
  service: compute
  resource: firewall
  requirement: Network Policy Store Encrypted
  scope: compute.firewall.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Network Policy Store is Encrypted at Rest
  rationale: Encrypting network policy data at rest protects sensitive configuration data from unauthorized access and potential data breaches. This is crucial for maintaining the integrity and confidentiality of network policies, which can prevent unauthorized network traffic and ensure compliance with data protection regulations like GDPR and CCPA.
  description: This rule checks if the network policy store in GCP's Compute Engine Firewall is encrypted at rest. Verify that encryption mechanisms are enabled for all firewall configurations by examining the firewall's metadata and ensuring that customer-managed encryption keys (CMEK) are utilized where applicable. Remediation involves configuring encryption settings in the GCP Console or using Terraform to enforce CMEK for enhanced security.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/encryption-at-rest/customer-managed-encryption-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.firewall.network_rule_groups_attached
  service: compute
  resource: firewall
  requirement: Network Rule Groups Attached
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Rule Groups Are Attached to Firewalls
  rationale: Attaching network rule groups to firewalls ensures that traffic is managed and monitored according to defined security policies. This reduces the risk of unauthorized access, data breaches, and non-compliance with regulatory requirements by controlling ingress and egress traffic effectively.
  description: This rule checks whether network rule groups are attached to GCP Compute Firewall resources, which are essential for applying consistent and comprehensive access control policies. Network rule groups allow for scalable management of firewall rules, enabling centralized updates and enforcement. To verify, ensure that all firewalls have associated rule groups via the GCP Console or CLI. Remediation involves creating or updating firewall rules to include network rule groups as applicable.
  references:
  - https://cloud.google.com/firewall/docs/network-rule-groups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/compute/docs/networking
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.firewall.network_sg_egress_restricted
  service: compute
  resource: firewall
  requirement: Network Sg Egress Restricted
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Egress Traffic in GCP Firewall Rules
  rationale: Unrestricted egress traffic from GCP firewall rules may lead to data exfiltration, unintentional exposure of internal services, and increased attack surface. By limiting egress traffic, organizations can prevent unauthorized data flow and comply with regulatory mandates such as GDPR and PCI-DSS, which require stringent data protection measures.
  description: This rule evaluates GCP firewall settings to ensure that egress traffic is restricted to necessary destinations. Check for overly permissive egress rules, such as allowing traffic to 0.0.0.0/0. To remediate, configure specific egress rules that only permit traffic to known, trusted IP ranges and ports required for business operations. Regularly review and update these rules to adapt to evolving threats.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.firewall.network_sg_no_0_0_0_0_ingress
  service: compute
  resource: firewall
  requirement: Network Sg No 0 0 0 0 Ingress
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Inbound Traffic from 0.0.0.0/0 in Firewall Rules
  rationale: Allowing inbound traffic from 0.0.0.0/0 exposes your resources to the internet, increasing the risk of unauthorized access, data breaches, and exploitation of vulnerabilities. This configuration can lead to non-compliance with industry standards and regulations like PCI-DSS and HIPAA, which mandate strict access control measures to protect sensitive data.
  description: This rule checks for firewall configurations that permit inbound traffic from 0.0.0.0/0, indicating unrestricted access. Such settings should be avoided unless absolutely necessary. To verify, inspect your GCP Firewall rules in the Cloud Console or via gcloud commands. Remediation involves restricting the source IP ranges to known, trusted networks and applying the principle of least privilege. Consider using identity-aware proxies or VPNs to limit access where appropriate.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.firewall.network_sg_only_required_ports_open
  service: compute
  resource: firewall
  requirement: Network Sg Only Required Ports Open
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Firewall to Open Only Necessary Ports
  rationale: Opening unnecessary ports in network security groups increases the attack surface, potentially allowing unauthorized access to systems and data. This can lead to breaches, data loss, and non-compliance with regulations such as PCI-DSS and HIPAA, which mandate strict access controls.
  description: This rule checks GCP firewall configurations to ensure that only required ports are open, minimizing exposure to potential threats. Firewall rules should be configured to allow traffic only on specific ports needed for business operations. Verification involves reviewing firewall rules in the Google Cloud Console or via gcloud commands and closing any ports not essential for application functionality. Remediation includes updating firewall rules to restrict access and logging rule changes for audit purposes.
  references:
  - https://cloud.google.com/vpc/docs/using-firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.firewall.network_tier_to_tier_policies_defined
  service: compute
  resource: firewall
  requirement: Network Tier To Tier Policies Defined
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Tier Policies Are Defined for Firewall Rules
  rationale: Defining network tier policies for firewall rules in GCP helps ensure that traffic flows are adequately managed between different network tiers, reducing the risk of unauthorized access and potential data breaches. This practice is essential for maintaining robust network segmentation, which mitigates lateral movement by attackers and helps in achieving compliance with standards that require network isolation and access control, such as PCI-DSS and ISO 27001.
  description: This rule checks if network tier-to-tier policies are defined for all firewall rules within GCP's Compute Engine. Proper configuration requires specifying which network tiers (Standard or Premium) can communicate with each other, ensuring traffic is only allowed where explicitly permitted. Verification involves reviewing the firewall rules and ensuring that tier policies are accurately configured. Remediation includes updating firewall configurations to include explicit tier policies, using the GCP Console or gcloud CLI, to prevent unintended traffic flows.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/blog/products/networking/achieve-better-control-over-your-network-with-gcp-firewall-rules
- rule_id: gcp.compute.firewall.networkacl_allow_ingress_any_port_configured
  service: compute
  resource: firewall
  requirement: Networkacl Allow Ingress Any Port Configured
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Ingress Traffic to Specific Ports on GCP Firewall
  rationale: Allowing ingress traffic from any port increases the attack surface and exposes the network to potential threats such as unauthorized access and data breaches. Limiting allowed ports helps in meeting compliance requirements by ensuring only necessary services are accessible, reducing the risk of exploitation.
  description: This rule checks for Google Cloud Platform firewall configurations that permit ingress traffic from any port. It is crucial to restrict this setting to only necessary ports to minimize exposure to potential threats. Verify by reviewing firewall rules in the GCP Console or via gcloud command-line tool. Remediate by updating the firewall rules to allow traffic only on specific, required ports and implementing strict access controls.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#network-security
- rule_id: gcp.compute.firewall.networkacl_unrestricted_ingress_configured
  service: compute
  resource: firewall
  requirement: Networkacl Unrestricted Ingress Configured
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Restrict Unrestricted Ingress in Firewall Rules
  rationale: Allowing unrestricted ingress to your GCP network can expose critical resources to unauthorized access, leading to potential data breaches, service disruptions, and compliance violations. Attackers can exploit open ports to launch attacks or gain unauthorized access, impacting operational integrity and regulatory compliance with frameworks like PCI-DSS and NIST.
  description: This check ensures that GCP firewall rules are not configured with wide-open ingress settings. It assesses firewall rules for overly permissive source IP ranges (e.g., 0.0.0.0/0) that may expose services to the internet without proper access controls. To remediate, review and update these rules to specify more restrictive IP ranges or use identity-aware proxy configurations to limit access to trusted sources.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.compute.firewall.rdp_access_from_the_internet_allowed
  service: compute
  resource: firewall
  requirement: Rdp Access From The Internet Allowed
  scope: compute.firewall.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict RDP Access from the Internet in GCP Firewall
  rationale: Allowing unrestricted RDP access from the internet can expose your virtual machines to unauthorized access and potential attacks such as brute force or exploitation of RDP vulnerabilities. This poses a risk to sensitive data and critical systems, potentially leading to data breaches and compliance violations with standards like PCI-DSS and HIPAA.
  description: This rule checks for firewall rules that allow RDP access (TCP port 3389) from any IP address on the internet. It is recommended to restrict RDP access to specific IP addresses or ranges associated with trusted networks or VPNs. To remediate, modify the firewall rule to limit RDP access to known IP addresses or implement a bastion host to control access securely.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/solutions/best-practices-vpc-design#restricting_network_access
- rule_id: gcp.compute.firewall.rule_no_rdp_internet_access
  service: compute
  resource: firewall
  requirement: Rule No Rdp Internet Access
  scope: compute.firewall.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prohibit RDP Access from the Internet
  rationale: Allowing RDP access from the internet increases the risk of unauthorized access, brute force attacks, and exposure to vulnerabilities. This can lead to data breaches, service disruptions, and non-compliance with regulations such as PCI-DSS and ISO 27001, which require secure access controls and protection of sensitive data.
  description: This rule checks for firewall configurations that permit Remote Desktop Protocol (RDP) traffic from any source IP address, which poses security risks. Ensure that RDP access is restricted to specific IP addresses or ranges, preferably through a VPN or private network. Remediation involves modifying firewall rules to specify only trusted sources for RDP traffic, thereby reducing the attack surface.
  references:
  - https://cloud.google.com/compute/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.firewall.securitygroup_common_ports_restricted
  service: compute
  resource: firewall
  requirement: Securitygroup Common Ports Restricted
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Common Ports in GCP Firewall Rules
  rationale: Open common ports such as 22 (SSH), 80 (HTTP), and 443 (HTTPS) can expose your GCP resources to unauthorized access and potential attacks. Restricting access to these ports reduces the risk of exploitation from malicious actors, ensuring that only legitimate traffic reaches your services. This is crucial for maintaining compliance with security standards like NIST and ISO 27001 that require stringent access controls.
  description: This rule checks that GCP firewall rules are configured to restrict access to common ports to only trusted IP ranges or specific service accounts. It ensures that open ports are not accessible from the internet at large, minimizing attack surfaces. Verify by reviewing firewall rule settings in the GCP Console or using gcloud commands. Remediation involves adjusting firewall rules to limit access to these ports, ideally using private IP ranges or VPC Service Controls.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.firewall.securitygroup_default_restrict_traffic_configured
  service: compute
  resource: firewall
  requirement: Securitygroup Default Restrict Traffic Configured
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Default Firewall Rules in GCP Compute
  rationale: Default firewall rules in GCP can expose your network to unauthorized access, potentially leading to data breaches or service disruptions. By restricting these rules, organizations can minimize attack surfaces, comply with best practices, and adhere to regulatory standards such as PCI-DSS and ISO 27001.
  description: This rule checks for the existence and configuration of default firewall rules in GCP Compute that allow unrestricted traffic. It ensures these rules are modified to restrict access only to necessary services and IP ranges. To verify, review the 'default-allow' rules in your GCP Firewall settings and update them to use restrictive source ranges and protocols. Remediation involves adjusting the 'default' network's firewall settings to limit exposure.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/vpc/docs/firewall-rules-best-practices
- rule_id: gcp.compute.firewall.securitygroup_default_restricted
  service: compute
  resource: firewall
  requirement: Securitygroup Default Restricted
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Default Security Group Access
  rationale: Unrestricted firewall rules in the default security group can expose GCP resources to unauthorized access, leading to potential data breaches and service disruptions. Organizations often underestimate the risk posed by default configurations, which can be exploited by attackers to gain unauthorized entry into the network. Adhering to security best practices and compliance standards, such as NIST and PCI-DSS, requires limiting network access to only necessary services.
  description: This rule checks that the default security group in GCP Compute Engine does not allow overly permissive ingress and egress traffic, particularly from the internet. Firewall rules should be configured to deny non-essential traffic and limit access to specific IP addresses or ranges as needed. To remediate, review and modify the default firewall rules to ensure they align with the principle of least privilege, allowing only required traffic through. Verification can be done by inspecting the firewall rule configurations in the Google Cloud Console or via gcloud CLI.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#firewall
- rule_id: gcp.compute.firewall.securitygroup_rdp_restricted
  service: compute
  resource: firewall
  requirement: Securitygroup Rdp Restricted
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict RDP Access in GCP Firewall Rules
  rationale: Unrestricted RDP access over the internet can expose your instances to unauthorized access, leading to potential data breaches and system compromise. Limiting RDP access aligns with best practices for minimizing attack vectors and meets compliance requirements for secure network access control.
  description: This rule verifies that GCP firewall rules do not allow unrestricted RDP (TCP port 3389) access from the internet. It checks for firewall rules permitting `0.0.0.0/0` as a source for RDP. To remediate, configure the firewall rules to allow RDP access only from specific IP addresses or ranges. This can be done by updating source ranges in the GCP Console or using gcloud commands.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.compute.firewall.securitygroup_ssh_restricted
  service: compute
  resource: firewall
  requirement: Securitygroup Ssh Restricted
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict SSH Access in GCP Firewall Rules
  rationale: Unrestricted SSH access can expose your infrastructure to unauthorized access and potential attacks, leading to data breaches and service disruptions. By limiting SSH access to specific IP ranges or using identity-aware proxy, you reduce attack vectors and comply with best practices such as the CIS GCP Benchmark, NIST, and PCI-DSS requirements.
  description: This check ensures that SSH access (port 22) is restricted in GCP firewall rules to only allow trusted IP addresses or ranges. Verify that firewall rules do not permit 0.0.0.0/0 for SSH and implement identity-aware proxy where possible. Remediation involves updating firewall rules to specify trusted IP addresses and auditing rules regularly to ensure compliance.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.ssh_access_from_the_internet_allowed
  service: compute
  resource: firewall
  requirement: Ssh Access From The Internet Allowed
  scope: compute.firewall.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: SSH Access from Internet Not Restricted
  rationale: Allowing SSH access from the internet exposes your VM instances to potential unauthorized access and brute force attacks, increasing the risk of data breaches and service disruption. Restricting SSH access to specific IP addresses mitigates these risks and helps maintain compliance with security standards such as PCI-DSS and ISO 27001.
  description: This rule checks for firewall rules that permit SSH access (TCP port 22) from any IP address on the internet. To minimize security risks, SSH access should be restricted to specific IP ranges. Verify by reviewing firewall rules with open access and update them to allow connections only from trusted IP addresses or networks. Consider using Identity-Aware Proxy or VPN for more secure access.
  references:
  - https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/securing-ssh
- rule_id: gcp.compute.firewall.ssh_internet_restriction_configured
  service: compute
  resource: firewall
  requirement: Ssh Internet Restriction Configured
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict SSH Access from the Internet
  rationale: Allowing SSH access from the internet increases the risk of unauthorized access and potential exploits such as brute force attacks. Implementing restrictions helps protect sensitive data and resources, aligning with security best practices and compliance requirements like PCI-DSS and SOC2.
  description: This rule checks for firewall rules that allow SSH (port 22) access from any IP address. To enhance security, configure firewall rules to restrict SSH access to specific IP ranges or use identity-based access controls. Verify by reviewing firewall configurations in the GCP Console or using gcloud commands, and update rules to block 0.0.0.0/0 or to allow specific trusted IP addresses only.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org
  - https://www.sans.org/reading-room/whitepapers/cloud/securing-cloud-infrastructure-best-practices-38997
- rule_id: gcp.compute.forwarding_rule.network_listener_cipher_policy_secure
  service: compute
  resource: forwarding_rule
  requirement: Network Listener Cipher Policy Secure
  scope: compute.forwarding_rule.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Secure Cipher Policies for Network Listeners
  rationale: Using insecure cipher policies for network listeners can expose sensitive data to interception and tampering, leading to data breaches and non-compliance with security standards. Secure cipher configurations are crucial to protect data in transit, maintain customer trust, and meet regulatory requirements such as PCI-DSS and NIST guidelines.
  description: This rule checks that all forwarding rules in the GCP Compute service use secure cipher policies for network listeners. Insecure ciphers can be vulnerable to attacks like BEAST, POODLE, or Logjam. To verify, inspect the TLS policies associated with forwarding rules and ensure they only support strong, secure ciphers. Remediation involves updating the cipher policies to exclude weak ciphers and protocols, aligning with the latest security standards.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.forwarding_rule.network_listener_tls_min_1_2
  service: compute
  resource: forwarding_rule
  requirement: Network Listener TLS Min 1 2
  scope: compute.forwarding_rule.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS 1.2+ for Network Listener on Forwarding Rules
  rationale: Utilizing TLS version 1.2 or higher for network listeners on forwarding rules is critical to safeguard data in transit against interception and tampering. Older TLS versions are vulnerable to several known exploits that could compromise data integrity and confidentiality. This configuration aids in meeting compliance obligations under standards like PCI-DSS, which require strong encryption protocols.
  description: This check ensures that all forwarding rules in Google Cloud Compute Engine enforce a minimum TLS version of 1.2 for network listeners. Verify this by inspecting the 'minTlsVersion' attribute of each forwarding rule's associated HTTPS or SSL proxy. If the version is below 1.2, update the configuration via the Google Cloud Console or gcloud CLI to enforce TLS 1.2 or higher. This helps prevent downgrade attacks and ensures secure data transmission.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.health_check.dns_alerts_configured
  service: compute
  resource: health_check
  requirement: Dns Alerts Configured
  scope: compute.health_check.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure DNS Alerts Configured for Compute Health Checks
  rationale: Configuring DNS alerts for health checks is crucial for timely detection and response to potential disruptions in service availability. Without these alerts, businesses risk prolonged downtime, impacting customer satisfaction and potentially leading to financial losses. Additionally, this practice supports compliance with industry standards that mandate monitoring and alerting for critical infrastructure.
  description: This rule checks if DNS-based alerts are configured for Compute Engine health checks. Properly configured alerts ensure that any issues with DNS resolution impacting health checks are promptly notified to the relevant teams. To verify, ensure that alert policies are configured in Cloud Monitoring with conditions based on DNS health check metrics. Remediation involves setting up alert policies that trigger notifications when DNS resolution issues are detected for health checks.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/compute/docs/load-balancing/health-checks
  - https://cloud.google.com/monitoring/docs/alerting
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.health_check.dns_https_or_tls_used
  service: compute
  resource: health_check
  requirement: Dns HTTPS Or TLS Used
  scope: compute.health_check.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Health Checks Use DNS, HTTPS, or TLS
  rationale: Using DNS, HTTPS, or TLS for health checks is crucial to protect data in transit from interception or tampering, ensuring the reliability and security of services. Failure to use encrypted protocols can expose sensitive data to unauthorized access, leading to data breaches and non-compliance with regulations such as GDPR and HIPAA.
  description: This rule checks if Google Cloud Platform health checks are configured to use DNS, HTTPS, or TLS protocols, ensuring that data in transit is encrypted. Verify that health checks are not set to use unencrypted protocols such as HTTP. To remediate, modify health check configurations to use HTTPS or TLS by navigating to the Cloud Console, selecting the appropriate health check, and updating the protocol settings.
  references:
  - https://cloud.google.com/load-balancing/docs/health-checks
  - https://cloud.google.com/security/compliance/cis-gcp-1-0-0
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/itl/applied-cybersecurity/nist-special-publication-800-53
- rule_id: gcp.compute.health_check.dns_no_plaintext_credentials_in_url
  service: compute
  resource: health_check
  requirement: Dns No Plaintext Credentials In Url
  scope: compute.health_check.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Avoid Plaintext Credentials in Health Check URLs
  rationale: Including plaintext credentials in DNS health check URLs can lead to unauthorized access if intercepted, compromising system integrity and exposing sensitive data. This practice increases the risk of credential leakage, potentially violating compliance requirements such as ISO 27001 and PCI-DSS, and can result in severe business impacts including data breaches and financial loss.
  description: This rule checks that no plaintext credentials are embedded in DNS health check URLs within GCP Compute Engine configurations. Review and ensure that health check configurations do not include credentials in the URL or switch to using secure methods like OAuth tokens or service accounts. Remediation involves updating the health check settings and ensuring credentials are managed securely through GCP's Secret Manager or environment variables.
  references:
  - https://cloud.google.com/compute/docs/load-balancing/health-checks
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.compute.image.approved_image_allowlist_enforced
  service: compute
  resource: image
  requirement: Approved Image Allowlist Enforced
  scope: compute.image.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enforce Approved Image Allowlist for Compute Images
  rationale: Using unapproved images on GCP can lead to potential security vulnerabilities, compliance violations, and operational inefficiencies. Enforcing an approved image allowlist helps mitigate risks by ensuring that only vetted and compliant images are used, thus reducing the attack surface and aligning with industry standards and best practices.
  description: This rule verifies that all compute instances are launched using images from a predefined allowlist. It checks for the enforcement of image allowlists to prevent unauthorized or insecure images from being used. To remediate, implement IAM policies that restrict image creation and selection to approved images only. Regularly review and update the allowlist to include necessary images that comply with security and operational guidelines.
  references:
  - https://cloud.google.com/compute/docs/images
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/compute/docs/instances/image-management-best-practices
- rule_id: gcp.compute.image.encrypted_cmek
  service: compute
  resource: image
  requirement: Encrypted Cmek
  scope: compute.image.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Images Use Customer-Managed Encryption Keys
  rationale: Using Customer-Managed Encryption Keys (CMEK) to encrypt compute images provides greater control over data protection and access management, mitigating risks of unauthorized access and data breaches. This practice is crucial for meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strong encryption measures to protect sensitive data at rest.
  description: This rule checks whether Google Compute Engine images are encrypted with Customer-Managed Encryption Keys (CMEK). To verify, ensure the image's 'encryptionKey' parameter is set to a valid CMEK key. Remediation involves creating or using an existing Cloud Key Management Service (KMS) key and specifying it during the image creation or update process. This enhances security by allowing key rotation and revocation, thus maintaining strict data governance.
  references:
  - https://cloud.google.com/compute/docs/images
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.image.image_signed_and_verified
  service: compute
  resource: image
  requirement: Image Signed And Verified
  scope: compute.image.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Image is Signed and Verified
  rationale: Unsigned or unverified images pose security risks as they may contain malicious code, leading to compromised instances and data breaches. Ensuring images are signed and verified enhances trust and integrity in your compute environment, aligning with compliance frameworks that mandate secure software delivery practices.
  description: This rule checks that compute images used in the environment are cryptographically signed and verified. Images should be signed using a trusted certificate authority to ensure authenticity and integrity. Administrators can verify image signatures via GCP's Container Analysis API. Remediation involves signing images with a trusted key and configuring GCP to verify this signature before deployment.
  references:
  - https://cloud.google.com/container-registry/docs/using-container-analysis
  - https://cloud.google.com/security/best-practices-for-enterprise-organizations#implement_image_signing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.image.not_publicly_shared
  service: compute
  resource: image
  requirement: Not Publicly Shared
  scope: compute.image.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Access to GCP Compute Images
  rationale: Publicly shared compute images can expose sensitive data and lead to unauthorized access or resource misuse. Ensuring images are not publicly accessible reduces the risk of data breaches and aligns with compliance requirements such as PCI-DSS and ISO 27001, which mandate controlled access to sensitive resources.
  description: This rule checks if any Google Cloud Platform compute images are publicly accessible. To verify, examine the IAM policy bindings for each image to ensure 'allUsers' or 'allAuthenticatedUsers' are not granted 'roles/compute.imageUser'. Remediation involves updating the IAM policies to restrict access to only necessary users or groups, thereby preventing potential unauthorized access.
  references:
  - https://cloud.google.com/compute/docs/images
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.compute.image.vuln_scanned_no_critical
  service: compute
  resource: image
  requirement: Vuln Scanned No Critical
  scope: compute.image.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Images Are Scanned and Free of Critical Vulnerabilities
  rationale: Conducting vulnerability scans on images helps prevent the deployment of workloads with critical security flaws that could be exploited by attackers, thereby reducing the risk of data breaches and service disruptions. This practice is crucial for maintaining a secure cloud environment and meeting compliance requirements such as PCI-DSS and ISO 27001, which mandate vulnerability management and secure configurations.
  description: This rule checks that all Compute Engine images have undergone vulnerability scanning and contain no critical vulnerabilities. To verify, ensure that images in use are regularly scanned using tools like Google Cloud's Container Analysis or third-party security solutions. Remediation involves updating or patching images to address any critical vulnerabilities found, or replacing them with secure, compliant versions.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-container-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/images
  - https://cloud.google.com/container-registry/docs/container-analysis
- rule_id: gcp.compute.instance.backup_enabled
  service: compute
  resource: instance
  requirement: Backup Enabled
  scope: compute.instance.backup_recovery
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Backup is Enabled for Compute Instances
  rationale: Enabling backups for Compute instances is crucial for data resilience and business continuity. In the event of accidental deletion, corruption, or a security breach, having backups ensures that critical data can be restored, minimizing downtime and potential financial losses. Moreover, backups help meet regulatory compliance requirements, such as those specified by PCI-DSS and ISO 27001, which mandate data protection and recovery strategies.
  description: This rule checks that Compute Engine instances have backup mechanisms, such as snapshots or persistent disk backup policies, properly configured. To verify, ensure that instance disks have scheduled snapshots or are part of a managed backup plan. Remediation involves setting up Cloud Scheduler to automate disk snapshots or using Google Cloud's Backup and DR service to configure a backup policy that aligns with your RPO and RTO objectives.
  references:
  - https://cloud.google.com/compute/docs/disks/backup-and-restore
  - https://cloud.google.com/backup-and-disaster-recovery/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instance-templates
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
- rule_id: gcp.compute.instance.balancer_health_check_configured
  service: compute
  resource: instance
  requirement: Balancer Health Check Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Load Balancer Health Check is Configured for Instances
  rationale: Configuring a health check for instances behind a load balancer ensures that traffic is only routed to healthy instances, improving the availability and reliability of applications. Without proper health checks, there is a risk of sending traffic to unresponsive or malfunctioning instances, leading to application downtime and potential business disruptions. This is crucial for maintaining service level agreements and meeting compliance standards regarding uptime and reliability.
  description: This rule checks whether health checks are configured for instances behind Google Cloud Load Balancers. Health checks monitor the status of instances, ensuring that only healthy instances receive traffic. To verify, inspect the load balancer's configuration in the Google Cloud Console or use the gcloud CLI to confirm health checks are set. Remediation involves navigating to the Load Balancer settings and ensuring health checks are properly defined for each instance group attached to the load balancer.
  references:
  - https://cloud.google.com/load-balancing/docs/health-check-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/load-balancing/docs/backend-service
- rule_id: gcp.compute.instance.balancing_logging_enabled
  service: compute
  resource: instance
  requirement: Balancing Logging Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Load Balancer Logging for Compute Instances
  rationale: Enabling logging for load-balanced compute instances in GCP provides critical insights into traffic patterns and potential security threats. Without logging, anomalous activities and unauthorized access attempts may go undetected, increasing the risk of data breaches and non-compliance with regulatory standards such as PCI-DSS and SOC2. Logging aids in forensic analysis, operational troubleshooting, and ensures adherence to audit requirements.
  description: This rule checks if load balancing logging is enabled for compute instances. To verify, ensure that the HTTP(S) load balancer associated with the compute instance has logging configured in the Google Cloud Console under 'Network services' > 'Load balancing'. Remediation involves enabling logging at the load balancer level, specifying a Cloud Storage bucket for logs. This process includes setting up IAM permissions to allow the load balancer to write logs to the specified bucket.
  references:
  - https://cloud.google.com/load-balancing/docs/https/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/resources/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.compute.instance.block_project_wide_ssh_keys_disabled
  service: compute
  resource: instance
  requirement: Block Project Wide Ssh Keys Disabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Block Project-Wide SSH Keys Enabled for Instances
  rationale: Disabling project-wide SSH keys prevents unauthorized access to instances by limiting SSH key usage to instance-specific keys only. This reduces the risk of unauthorized access due to compromised keys that might be used across multiple instances, thus enhancing security posture. Ensuring this setting aligns with compliance requirements like PCI-DSS and HIPAA, which mandate strict access controls.
  description: This rule checks whether the 'blockProjectWideSshKeys' setting is enabled for GCP Compute Instances. Enabling this setting ensures that only instance-specific SSH keys are allowed, preventing the use of project-wide keys that could lead to unintended access. To verify, inspect the instance's metadata and ensure the 'blockProjectWideSshKeys' attribute is set to true. Remediation involves updating the instance metadata to enable this setting via the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/compute/docs/oslogin
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/connecting-advanced#project-wide-keys
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 4.3
  - PCI-DSS Requirement 7.1.2
  - ISO 27001 Annex A.9.4.2
- rule_id: gcp.compute.instance.confidential_computing_enabled
  service: compute
  resource: instance
  requirement: Confidential Computing Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Confidential Computing for Compute Instances
  rationale: Enabling Confidential Computing on GCP Compute Instances enhances data protection by encrypting data in use, reducing the risk of unauthorized access to sensitive information during processing. This is crucial for meeting regulatory compliance and safeguarding intellectual property from advanced threats and insider attacks.
  description: This rule checks if Confidential Computing is enabled for GCP Compute Instances, which ensures that data remains encrypted in memory during processing using secure enclaves. To verify, ensure that instances are configured with the Confidential VM option enabled. Remediation involves modifying instance settings to activate Confidential Computing, which can be done via the GCP Console or the CLI.
  references:
  - https://cloud.google.com/confidential-computing/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/nist-special-publication-800-53-revision-5
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/create-start-confidential-vm
- rule_id: gcp.compute.instance.default_service_account_full_access_configured
  service: compute
  resource: instance
  requirement: Default Service Account Full Access Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Avoid Default Service Account Full Access on Compute Instances
  rationale: Using the default service account with full access on compute instances can lead to over-privileged configurations, increasing the risk of unauthorized access and potential data breaches. This can compromise the principle of least privilege, leading to non-compliance with security standards such as CIS, NIST, and ISO 27001, which emphasize minimal access and role-based permissions.
  description: This rule checks whether GCP compute instances are configured to use the default service account with full access. Such a configuration can unintentionally grant more permissions than necessary, making it critical to customize service accounts with the least privilege principle in mind. To remediate, create a custom service account with only the necessary permissions and update the compute instance to use this account instead of the default. Verification can be done through the Google Cloud Console or using the gcloud command-line tool.
  references:
  - https://cloud.google.com/compute/docs/access/service-accounts
  - https://cloud.google.com/iam/docs/service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.compute.instance.default_service_account_in_use_with_full_api_access
  service: compute
  resource: instance
  requirement: Default Service Account In Use With Full API Access
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Avoid Default Service Account with Full API Access in Instances
  rationale: Using the default service account with full API access increases the risk of privilege escalation and unauthorized data access if the account is compromised. It can lead to extensive exposure of cloud resources and sensitive data, undermining compliance with security frameworks and increasing the attack surface for malicious actors.
  description: This rule checks if any Compute Engine instances are configured to use the default service account with full API access. Best practices recommend creating and using custom service accounts with the principle of least privilege applied. To verify, inspect the service account settings of each instance in the GCP Console or via the gcloud CLI, and adjust permissions as necessary by assigning a custom service account with only the necessary IAM roles.
  references:
  - https://cloud.google.com/compute/docs/access/service-accounts#default_service_account
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#service_accounts
- rule_id: gcp.compute.instance.detailed_monitoring_enabled
  service: compute
  resource: instance
  requirement: Detailed Monitoring Enabled
  scope: compute.instance.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Detailed Monitoring on Compute Instances
  rationale: Enabling detailed monitoring on compute instances allows for better visibility into system performance, enabling proactive identification of potential issues. This is crucial for maintaining system reliability and meeting compliance requirements for performance monitoring in frameworks like ISO 27001. Without detailed monitoring, businesses risk undetected performance degradation and potential loss of service availability.
  description: This rule checks whether detailed monitoring is enabled on Google Compute Engine instances. Detailed monitoring provides additional metrics that are crucial for performance analysis and capacity planning. To verify this setting, ensure that the 'monitoring' service is enabled on each instance. Remediation involves enabling the 'Stackdriver Monitoring' agent on the instance, which can be done via the Google Cloud Console or using gcloud commands.
  references:
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/compute/docs/instances
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Section 5.2
  - ISO 27001:2013 - A.12.1.3 Capacity management
  - https://cloud.google.com/monitoring/agent
  - NIST SP 800-53 Rev. 5 - CA-7 Continuous Monitoring
- rule_id: gcp.compute.instance.distributions_logging_enabled
  service: compute
  resource: instance
  requirement: Distributions Logging Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Compute Instances Have Distributions Logging Enabled
  rationale: Enabling distributions logging for compute instances is crucial for monitoring and analyzing the distribution of network traffic and resource usage, which helps in identifying potential security threats and performance bottlenecks. It also supports compliance with regulatory requirements by ensuring that all access and activity logs are captured and reviewed, reducing the risk of undetected breaches.
  description: This rule checks whether distribution logging is enabled for all compute instances, which requires configuring the appropriate logging settings in the GCP Console or using gcloud commands. To enable this, navigate to the 'Logging' section under the instance's settings and ensure that logging is activated. This setup allows for comprehensive visibility into the instance's operations and network traffic, facilitating timely threat detection and incident response.
  references:
  - https://cloud.google.com/logging/docs/setup/manage-logs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/compute/docs/logging
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.instance.dr_testing_execution_roles_least_privilege
  service: compute
  resource: instance
  requirement: DR Testing Execution Roles Least Privilege
  scope: compute.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for DR Testing Execution on Compute Instances
  rationale: Applying the principle of least privilege for disaster recovery (DR) testing roles minimizes the risk of unauthorized access, reducing potential data breaches and disruptions. This is crucial for maintaining business continuity and protecting sensitive data, ensuring compliance with regulations such as NIST and PCI-DSS, which mandate controlled access to critical infrastructure.
  description: This rule checks that roles assigned for DR testing on Compute instances are granted only the permissions necessary to perform their specific tasks, avoiding excessive privileges. Verify that IAM roles do not include permissions beyond what is strictly required for DR testing. To remediate, audit existing roles and adjust permissions to align with the principle of least privilege, using predefined roles or custom roles tailored to specific DR tasks.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/compute/docs/access/iam
  - CIS Google Cloud Platform Foundation Benchmark 1.1.0 - 1.6
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.compute.instance.dr_testing_logs_enabled
  service: compute
  resource: instance
  requirement: DR Testing Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable DR Testing Logs for Compute Instances
  rationale: Enabling disaster recovery (DR) testing logs for compute instances is crucial for ensuring that all DR test activities are recorded and auditable. This helps organizations identify potential issues in their DR plans, improve system resilience, and meet compliance requirements such as PCI-DSS and ISO 27001, which mandate logging and monitoring of DR activities.
  description: This rule checks if logging is enabled for disaster recovery testing activities on GCP Compute Engine instances. To verify, ensure that Stackdriver Logging is configured to capture DR test logs by setting up a log sink for relevant projects. Remediation involves creating a log sink in the GCP Console or via gcloud CLI that targets DR testing activities, ensuring that all DR tests are logged and stored for future analysis.
  references:
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/compute/docs/instances
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/dr-scenarios-planning-guide
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.compute.instance.dr_testing_results_storage_encrypted_and_private
  service: compute
  resource: instance
  requirement: DR Testing Results Storage Encrypted And Private
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DR Test Results Are Encrypted and Access is Restricted
  rationale: Encrypting and restricting access to DR testing results protects sensitive data from unauthorized access and potential breaches. This is crucial for maintaining data integrity and confidentiality, especially as these results may contain sensitive information critical for business continuity plans. Non-compliance with encryption standards can lead to significant financial penalties and damage to reputation.
  description: This rule checks that storage solutions used for Disaster Recovery (DR) testing results within Compute Engine instances are encrypted and have access controls in place. Verify that disks are encrypted using Customer-Managed Encryption Keys (CMEK) and that IAM policies restrict access to authorized personnel only. Remediation involves enabling encryption on all relevant storage resources and reviewing IAM policies to ensure least privilege access.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis#control_4.1
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.instance.encryption_with_csek_enabled
  service: compute
  resource: instance
  requirement: Encryption With Csek Enabled
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enforce Customer-Supplied Encryption Keys for VM Disks
  rationale: Utilizing customer-supplied encryption keys (CSEK) ensures that the data remains under the customer's control, even if Google Cloud Platform experiences a breach. This approach aligns with stringent compliance requirements such as GDPR and PCI-DSS, which mandate robust data protection measures. Without CSEK, sensitive data may be at risk of unauthorized access, leading to significant financial and reputational damage.
  description: This rule checks whether Google Compute Engine VM instances are configured to use customer-supplied encryption keys (CSEK) for disk encryption. To verify, check that the disks associated with the instances are encrypted with keys managed by the customer rather than Google. Remediation involves updating the disk encryption settings to use CSEK, which can be done via the GCP Console, CLI, or API by specifying the CSEK during instance creation or disk attachment.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.instance.env_vars_secret_configured
  service: compute
  resource: instance
  requirement: Env Vars Secret Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Secrets in Environment Variables are Securely Configured
  rationale: Storing sensitive data such as API keys, database credentials, or other secrets in environment variables without proper security measures can lead to unauthorized access and data breaches. This practice can expose critical business operations to cyber threats and compromise compliance with standards like PCI-DSS and HIPAA, which mandate strict controls over sensitive information handling.
  description: This rule checks if sensitive information is stored securely within the environment variables of Google Compute Engine (GCE) instances. It verifies that secrets are encrypted and managed through GCP Secret Manager rather than being stored in plaintext. To remediate, ensure that all secrets are moved to Secret Manager, and access them programmatically within your application using IAM roles and permissions. Follow the principle of least privilege to limit access to these secrets.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-security-rule/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.instance.flow_logs_enabled
  service: compute
  resource: instance
  requirement: Flow Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure VPC Flow Logs are Enabled for Compute Instances
  rationale: Enabling VPC Flow Logs for Google Cloud Compute Instances is crucial for monitoring network traffic, diagnosing network issues, and ensuring compliance with security policies. It provides visibility into traffic patterns and helps detect anomalies, which could indicate potential security threats or misuse. This logging capability supports incident response and forensic investigations, aligning with regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule verifies that VPC Flow Logs are enabled for each Google Cloud Compute Instance. Flow Logs capture IP traffic information flowing to and from network interfaces, which can be enabled by configuring the subnet containing the instance. To verify, access the Google Cloud Console, navigate to the VPC network settings, and ensure Flow Logs are set to 'On' for the respective subnets. Remediation involves enabling Flow Logs in the subnet configuration, specifying log aggregation options such as sampling rate and log format.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/blog/products/gcp/using-vpc-flow-logs-and-bigquery-for-network-analytics
- rule_id: gcp.compute.instance.gke_worker_nodes_firewall_restriction
  service: compute
  resource: instance
  requirement: Gke Worker Nodes Firewall Restriction
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict GKE Worker Nodes with Specific Firewall Rules
  rationale: Unrestricted network access to GKE worker nodes can expose them to potential threats such as unauthorized access, data breaches, and Denial-of-Service (DoS) attacks. Implementing specific firewall rules minimizes the attack surface by allowing only necessary traffic, thereby enhancing the security posture and ensuring compliance with standards like PCI-DSS and ISO 27001.
  description: This rule checks if GKE worker nodes have restrictive firewall rules that limit inbound and outbound traffic to only what is necessary for operations. Verifying this involves inspecting firewall configurations associated with the nodes to ensure they adhere to the principle of least privilege. Remediation includes defining and applying specific firewall rules that only allow required traffic, such as SSH from trusted IPs and inter-node communications, while blocking all other traffic.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_access_to_the_control_plane
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc/docs/firewalls
- rule_id: gcp.compute.instance.group_autoscaling
  service: compute
  resource: instance
  requirement: Group Autoscaling
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instance Groups Use Autoscaling
  rationale: Autoscaling optimizes resource usage and cost by automatically adjusting the number of VM instances in a group based on demand. Not using autoscaling can lead to over-provisioning, increased costs, and potential performance issues during periods of high demand, affecting business continuity and customer satisfaction.
  description: This rule checks if Compute Engine instance groups have autoscaling enabled. Autoscaling must be configured to automatically adjust the number of VM instances in response to workload demands. To verify, check the instance group settings in the Google Cloud Console under 'Compute Engine' > 'Instance groups'. If autoscaling is not enabled, configure it by setting up an autoscaling policy that suits your workload patterns. Consider factors such as target CPU utilization and custom metrics.
  references:
  - https://cloud.google.com/compute/docs/autoscaler
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-operating-your-cloud-environment
- rule_id: gcp.compute.instance.host_sharing_restricted
  service: compute
  resource: instance
  requirement: Host Sharing Restricted
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict Shared Host Usage for Compute Instances
  rationale: Restricting host sharing for compute instances minimizes the risk of cross-tenant data leakage and potential side-channel attacks. It enhances data protection and isolation, which is critical for maintaining confidentiality and integrity of sensitive workloads. This practice is particularly important for compliance with data protection regulations such as GDPR and HIPAA, which mandate strict data isolation controls.
  description: This rule checks whether Google Cloud Compute Instances are configured to restrict host sharing by using sole-tenant nodes. Sole-tenant nodes ensure that your instances do not share physical hardware with instances from other projects, reducing the risk of resource contention and vulnerabilities. To verify this, review your instance settings in the GCP Console or via the gcloud CLI and configure instances to utilize sole-tenant nodes if necessary. Remediation involves updating instance settings to select a specific node group dedicated to your project's instances.
  references:
  - https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/instances/
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.compute.instance.https_load_balancer_configured
  service: compute
  resource: instance
  requirement: HTTPS Load Balancer Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure HTTPS Load Balancer Configured for Compute Instances
  rationale: Configuring an HTTPS Load Balancer for compute instances ensures that data transmitted over the network is encrypted, protecting against interception and man-in-the-middle attacks. This is crucial for maintaining data confidentiality and integrity, especially for sensitive information. Additionally, using HTTPS aligns with compliance requirements such as PCI-DSS and HIPAA, reducing legal and financial risks.
  description: This rule verifies that compute instances are behind an HTTPS Load Balancer, which ensures all traffic is encrypted using TLS. To check this configuration, ensure that the target instances are part of a backend service linked to an HTTPS forwarding rule. Remediation involves setting up an HTTPS Load Balancer with a valid SSL certificate and associating it with the relevant backend services and instances. This configuration not only secures data in transit but also improves application availability and scalability.
  references:
  - https://cloud.google.com/load-balancing/docs/https
  - https://cloud.google.com/security/compliance/pci-dss
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/security-foundations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.compute.instance.instance_auto_placement_controlled
  service: compute
  resource: instance
  requirement: Instance Auto Placement Controlled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Control Instance Auto-Placement for Managed Deployments
  rationale: Controlling the auto-placement of compute instances is vital to ensure workloads are deployed in specific zones or regions, which can help meet data residency requirements, optimize resource utilization, and reduce latency. Uncontrolled placement may lead to compliance violations and inefficient resource allocation, impacting performance and incurring unnecessary costs.
  description: This rule checks that Google Cloud Compute Engine instances are configured with controlled auto-placement settings. Specifically, instances should have constraints set to deploy in designated zones or regions based on organizational policies. Verify by reviewing instance configurations in the Google Cloud Console or using gcloud commands. Remediation involves setting the desired zone or region during instance creation or updating existing instances to adhere to placement policies.
  references:
  - https://cloud.google.com/compute/docs/instances/creating-instance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - CIS Google Cloud Platform Foundation Benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/compute/docs/regions-zones
- rule_id: gcp.compute.instance.instance_multi_az
  service: compute
  resource: instance
  requirement: Instance Multi Az
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances are Configured for Multi-Region Availability
  rationale: Deploying compute instances across multiple zones enhances fault tolerance and business continuity by mitigating the risk of a single-zone failure. This configuration is crucial for maintaining service availability and meeting compliance with standards that require high availability and disaster recovery solutions.
  description: This rule checks if compute instances are configured to run in multiple zones within a region, ensuring redundancy and reliability. Instances should be part of managed instance groups with multi-zonal configurations. Verify this by reviewing instance group settings in the GCP Console or via CLI, ensuring that the 'distributionPolicy.zones' setting includes multiple zones. Remediation involves updating instance groups to include multiple zones or creating new multi-zonal configurations.
  references:
  - https://cloud.google.com/compute/docs/regions-zones
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/best-practices-for-building-fault-tolerant-applications
- rule_id: gcp.compute.instance.instance_older_than_specific_days
  service: compute
  resource: instance
  requirement: Instance Older Than Specific Days
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Identify and Manage Aging Compute Instances
  rationale: Compute instances that have been running longer than necessary may be using outdated software, posing security risks due to unpatched vulnerabilities. This can lead to increased exposure to exploitation and non-compliance with security policies that mandate regular instance updates. Regularly reviewing and updating such instances ensures alignment with security best practices and regulatory requirements.
  description: This rule checks for Google Cloud Compute instances that have been operational beyond a specified number of days. Instances that exceed this threshold should be reviewed for software updates, configuration changes, or decommissioning if no longer needed. Administrators can use the Google Cloud Console or CLI to list instances, assess their creation dates, and take appropriate remediation steps such as updating the instance or replacing it with a newer, patched version.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-securing-your-instances
- rule_id: gcp.compute.instance.inventory_api_enabled
  service: compute
  resource: instance
  requirement: Inventory API Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Inventory API is Enabled on Compute Instances
  rationale: Enabling the Inventory API on compute instances allows for better asset management and monitoring, helping to identify and mitigate vulnerabilities. This is crucial for maintaining compliance with industry standards like ISO 27001 and ensuring that only approved software is installed, reducing the risk of unauthorized or outdated configurations.
  description: This rule checks if the Inventory API is enabled on Google Compute Engine instances, which is essential for collecting data about installed packages and OS configurations. To verify this setting, access the Compute Engine API in the GCP Console and ensure the Inventory API is activated. If it is not enabled, navigate to the API & Services section, find the Inventory API, and activate it to ensure comprehensive instance monitoring and compliance with security policies.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/compute/docs/instances/viewing-managing-inventory
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.cisecurity.org/cis-benchmarks/
  - https://cloud.google.com/security/overview/whitepapers
- rule_id: gcp.compute.instance.ip_forwarding_configured
  service: compute
  resource: instance
  requirement: Ip Forwarding Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure IP Forwarding is Properly Configured for Instances
  rationale: Improper configuration of IP forwarding on instances can lead to unauthorized data transit, making the network susceptible to man-in-the-middle attacks and data exfiltration. Ensuring IP forwarding is correctly set aligns with security best practices and helps maintain compliance with industry standards such as PCI-DSS and NIST, which require stringent control over network traffic.
  description: This check verifies whether IP forwarding is enabled on Google Compute Engine instances, which may be necessary for specific network configurations such as NAT or VPN gateways. However, it should be disabled for instances not intended to forward packets to prevent unauthorized routing. To verify, review the 'canIpForward' setting in the instance configuration. Remediation involves disabling IP forwarding for non-gateway instances by setting 'canIpForward' to false in the instance's properties.
  references:
  - https://cloud.google.com/compute/docs/troubleshooting/general-tips#ipforwarding
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/create-start-instance
- rule_id: gcp.compute.instance.ip_forwarding_is_enabled
  service: compute
  resource: instance
  requirement: Ip Forwarding Is Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable IP Forwarding for Compute Instances
  rationale: IP forwarding is crucial for routing network traffic beyond the local instance, enabling it for specific instances can enhance network management but poses a security risk if enabled unnecessarily. Improper configuration can expose the instance to unauthorized access, leading to data breaches or network misuse. Compliance with network security standards requires careful control of IP forwarding settings.
  description: This rule checks if IP forwarding is enabled on Google Compute Engine instances, which should only be activated for instances acting as gateways or routers. Verification involves reviewing instance configurations to ensure IP forwarding is only enabled where necessary. Remediation includes disabling IP forwarding on instances where it is not required, thereby reducing attack surfaces and improving security posture.
  references:
  - https://cloud.google.com/compute/docs/networking
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.key_pair_disable_unused_keys
  service: compute
  resource: instance
  requirement: Key Pair Disable Unused Keys
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Disable Unused SSH Keys on Compute Instances
  rationale: Unused SSH keys on compute instances pose a significant security risk as they may allow unauthorized access if compromised. This is particularly critical in environments where key rotation and user deactivation are not consistently managed. Disabling unused keys reduces the attack surface and helps maintain compliance with security standards such as CIS benchmarks and regulatory frameworks like PCI-DSS and ISO 27001.
  description: This rule checks for and disables any SSH keys on GCP Compute Engine instances that have not been used for a predefined period, indicating potential obsolescence or abandonment. To verify, review the instance metadata for SSH keys and cross-reference with access logs to determine usage patterns. Remediation involves removing or disabling keys that have not been accessed within the organization's key management policy timeframe. This can be automated using IAM policies and regular audits of access logs.
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.compute.instance.key_pair_max_age_days
  service: compute
  resource: instance
  requirement: Key Pair Max Age Days
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Key Pair Age for Compute Instances is Limited
  rationale: Limiting the age of key pairs for compute instances is crucial to reduce the risk of key compromise. Older keys are more susceptible to being discovered and misused by attackers, potentially leading to unauthorized access. Regularly rotating key pairs aligns with compliance requirements and mitigates threats such as brute force attacks and credential theft.
  description: This rule checks that the key pairs used for Google Compute Engine instances do not exceed a specified maximum age, typically set to 90 days. To verify, audit the SSH key metadata associated with instances and ensure they are rotated within the defined period. Remediation involves updating the key pairs by generating new keys and replacing the old ones, ensuring minimal disruption and maintaining access control integrity.
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://cloud.google.com/security-compliance/cis-gcp-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
- rule_id: gcp.compute.instance.loadbalancer_logging_enabled
  service: compute
  resource: instance
  requirement: Loadbalancer Logging Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Load Balancer Logging is Enabled for Compute Instances
  rationale: Enabling load balancer logging for compute instances is crucial for maintaining a comprehensive audit trail of network activity, which aids in identifying and mitigating potential security threats. This capability supports forensic analysis, performance monitoring, and can help meet compliance requirements related to data protection and incident response. Without logging, organizations might miss critical insights into traffic patterns and potential anomalies, posing a risk to both security and operational efficiency.
  description: This rule verifies that load balancer logging is enabled for all Google Cloud compute instances, ensuring that all incoming and outgoing network requests are logged. To verify, navigate to the Google Cloud Console, select 'Network services', then 'Load balancing', and check that 'Enable logging' is turned on for each load balancer associated with your instances. If not enabled, configure each load balancer's logging settings to ensure all traffic is logged and stored in Cloud Logging for review and analysis.
  references:
  - https://cloud.google.com/load-balancing/docs/audit-logs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.instance.managed_by_os_config
  service: compute
  resource: instance
  requirement: Managed By OS Config
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances are Managed by OS Config
  rationale: Managing compute instances with OS Config ensures centralized control over configuration, compliance, and updates, reducing the risk of security vulnerabilities due to misconfiguration or outdated software. It provides a consistent approach to enforcing security policies and can help meet compliance requirements by ensuring instances are aligned with organizational security standards.
  description: This rule checks if GCP Compute Engine instances are managed by OS Config, which facilitates automated patch management, configuration compliance, and software management. To verify, ensure that the OS Config agent is installed and configured on each instance. Remediation involves installing the OS Config agent and enabling the necessary API permissions for the project to allow management through OS Config.
  references:
  - https://cloud.google.com/compute/docs/os-config-management
  - https://cloud.google.com/compute/docs/instances/os-config
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/osconfig-agent
- rule_id: gcp.compute.instance.managed_by_ssm
  service: compute
  resource: instance
  requirement: Managed By Ssm
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances Managed by Systems Manager
  rationale: Managing compute instances with a centralized service like AWS Systems Manager (SSM) enhances operational efficiency and security by enabling automated patching, configuration management, and resource monitoring. This mitigates risks associated with manual instance management, reducing the likelihood of configuration drift and non-compliance with security policies. It also aids in achieving compliance with frameworks that require demonstrable controls over compute resources.
  description: This rule checks if Google Cloud compute instances are managed by AWS Systems Manager, ensuring that they are configured for automated management tasks. Verify that the SSM agent is installed and properly configured on each instance, and that instances are registered with the Systems Manager service. To remediate, install the SSM agent on unmanaged instances and configure them to communicate with the Systems Manager, allowing for automated updates and configuration management.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.metadata_block_project_ssh_keys_enabled
  service: compute
  resource: instance
  requirement: Metadata Block Project Ssh Keys Enabled
  scope: compute.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Instance Metadata Blocks Project SSH Keys
  rationale: Allowing project-wide SSH keys can lead to unauthorized access if those keys are compromised, posing a significant risk to data integrity and confidentiality. Disabling project SSH keys at the instance level minimizes the attack surface and aligns with the principle of least privilege, thus enhancing the security of sensitive data and meeting compliance requirements such as PCI-DSS and HIPAA.
  description: 'This rule checks that the metadata key ''block-project-ssh-keys'' is set to true for each GCP compute instance, ensuring that project-wide SSH keys are not used. This configuration prevents users with project-level access from SSH-ing into instances, thus limiting access to those explicitly granted instance-level credentials. To verify this setting, review the instance metadata in the GCP Console or use the gcloud command-line tool. Remediation involves updating the instance metadata to include ''block-project-ssh-keys'': ''true''.'
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#block-project-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/instances/metadata-protection
- rule_id: gcp.compute.instance.metadata_concealment_enabled
  service: compute
  resource: instance
  requirement: Metadata Concealment Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Metadata Concealment for Compute Instances
  rationale: Enabling metadata concealment for compute instances is crucial to prevent unauthorized access to sensitive data such as service account tokens and instance metadata, which can be exploited by attackers to escalate privileges or perform lateral movements. This configuration helps mitigate risks associated with data exposure and meets compliance requirements outlined in frameworks like NIST SP 800-53 and ISO 27001, ensuring robust protection against metadata-related vulnerabilities.
  description: This rule checks if metadata concealment is enabled for GCP Compute Engine instances, which prevents direct access to instance metadata from within the virtual machine. To verify, inspect the instance's metadata server configuration and ensure the 'enable-oslogin' metadata key is set to 'true'. Remediation involves configuring the instance's metadata server to enable concealment, which can be done via the Google Cloud Console, CLI, or API by updating instance settings to enable 'enable-oslogin'.
  references:
  - https://cloud.google.com/compute/docs/instances/access-advanced-configurations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/metadata/overview
  - https://cloud.google.com/compute/docs/security
- rule_id: gcp.compute.instance.network_alerts_for_anomalies_configured
  service: compute
  resource: instance
  requirement: Network Alerts For Anomalies Configured
  scope: compute.instance.network_security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Enable Network Anomaly Alerts on GCP Compute Instances
  rationale: Configuring network alerts for anomalies on GCP Compute instances is critical for timely detection and response to potential security incidents such as unauthorized access or data exfiltration. Network anomalies can indicate compromised instances, leading to data breaches, financial loss, and non-compliance with regulations like PCI-DSS and HIPAA. Proactively monitoring network activity helps mitigate these risks and ensures business continuity.
  description: This rule checks whether network anomaly alerts are configured for GCP Compute instances. It ensures that Stackdriver Monitoring and Logging are set up to detect unusual network patterns, such as atypical traffic spikes or unexpected outbound connections. To verify, ensure that alerting policies are active in Stackdriver with conditions set for anomaly detection based on historical data. Remediation involves configuring Stackdriver to monitor network metrics and setting up alerts that notify security teams of any detected anomalies.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.network_eni_security_groups_present
  service: compute
  resource: instance
  requirement: Network Eni Security Groups Present
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network ENI Security Groups Are Configured for Instances
  rationale: Configuring security groups for network interfaces ensures that only authorized traffic can reach your instances, reducing the risk of unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, and for complying with regulations such as GDPR and PCI-DSS, which mandate strict access controls.
  description: This rule checks if each network interface (ENI) associated with a GCP Compute Engine instance has an appropriate security group configured. Security groups act as virtual firewalls, controlling inbound and outbound traffic to instances. To verify, inspect the instance's network interface settings in the GCP Console and ensure security groups are applied. Remediation involves assigning appropriate security groups to network interfaces to enforce the principle of least privilege.
  references:
  - https://cloud.google.com/vpc/docs/using-firewalls
  - CIS Google Cloud Platform Foundation Benchmark v1.0.0
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.instance.network_eni_security_groups_restrictive
  service: compute
  resource: instance
  requirement: Network Eni Security Groups Restrictive
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network ENI Security Groups are Restrictive
  rationale: Restrictive security group configurations minimize the attack surface by limiting unnecessary access to instances, reducing the risk of unauthorized access or data breaches. Properly configured security groups help comply with regulations such as PCI-DSS and HIPAA, which mandate robust network access controls.
  description: This rule checks that security groups associated with network interfaces (ENIs) of compute instances enforce restrictive ingress and egress rules. Specifically, it requires that only necessary ports and IP addresses are allowed. To verify, review security group rules for overly permissive settings, such as open ports to any IP. Remediation involves updating security group rules to follow the principle of least privilege, ensuring only trusted IPs can access necessary services.
  references:
  - https://cloud.google.com/vpc/docs/using-firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.instance.network_flow_logs_enabled
  service: compute
  resource: instance
  requirement: Network Flow Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Network Flow Logs for Compute Instances
  rationale: Enabling network flow logs for compute instances is crucial for monitoring and analyzing traffic patterns, which helps in identifying unusual activities that might indicate security threats. It also aids in meeting compliance requirements by providing visibility into network operations, supporting audits, and forensic investigations.
  description: This rule checks if network flow logging is enabled for Google Cloud Compute instances. Network flow logs capture metadata about the IP traffic going to and from VM instances, which is vital for security analysis and operational oversight. To enable, configure VPC Flow Logs for the relevant subnets in each VPC network. This can be done via the Google Cloud Console or gcloud CLI by setting the flow logs configuration to 'enabled'.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.no_public_ip
  service: compute
  resource: instance
  requirement: No Public Ip
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure Instances Do Not Have Public IP Addresses
  rationale: Exposing instances to the public internet significantly increases the risk of unauthorized access, data breaches, and exploitation by attackers. Instances with public IPs are more susceptible to attacks such as brute force, DDoS, and unauthorized data extraction. Ensuring instances remain private helps comply with regulations like PCI-DSS and HIPAA, which mandate strict control over network access to sensitive data.
  description: This rule checks if any compute instance is assigned a public IP address. Instances should be configured to use private IPs and access the internet through secure channels like Cloud NAT or VPN. To verify, review instance network interfaces for public IP associations. Remediation involves removing public IPs and utilizing VPNs or private connectivity options for necessary internet access.
  references:
  - https://cloud.google.com/compute/docs/instances/instance-config#public_ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/nat/docs/overview
  - https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.instance.older_than_specific_days
  service: compute
  resource: instance
  requirement: Older Than Specific Days
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances Are Not Older Than Specific Days
  rationale: Regularly updating and replacing older compute instances helps maintain security by ensuring that instances benefit from the latest security patches and updates. Older instances may have vulnerabilities that could be exploited by attackers, potentially leading to unauthorized access or data breaches. Maintaining a policy on instance age can also support compliance with standards that require up-to-date security measures.
  description: This rule checks for Google Cloud compute instances that have been running longer than a specified number of days, which could indicate outdated software or unpatched vulnerabilities. To verify compliance, review the creation date of instances and compare against the organizationâ€™s policy for instance lifecycle management. Instances identified as older than the specified threshold should be evaluated for updating or decommissioning. Implementing automated lifecycle management tools can assist in mitigating risks associated with outdated instances.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_services
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/compute/docs/instances/instance-life-cycle
- rule_id: gcp.compute.instance.os_config_compliance_configured
  service: compute
  resource: instance
  requirement: OS Config Compliance Configured
  scope: compute.instance.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure OS Config Compliance on GCP Compute Instances
  rationale: Configuring OS Config Compliance on instances is vital to maintaining a secure and up-to-date cloud environment. It helps mitigate security risks by ensuring that operating systems are patched, configurations are compliant with security policies, and any deviation is promptly addressed. This is crucial for meeting regulatory requirements like PCI-DSS and SOC2, which mandate regular updates and configuration management.
  description: This rule checks if OS Config Compliance is configured on GCP Compute Instances, which involves setting up the OS Config agent to manage patch compliance, configuration compliance, and software inventory. To verify, ensure the OS Config API is enabled and the agent is installed and running on each instance. Remediation involves enabling the OS Config API, installing the agent, and configuring it to report compliance status. This ensures that instances adhere to organizational security policies and compliance frameworks.
  references:
  - https://cloud.google.com/compute/docs/osconfig
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances
  - https://www.pcisecuritystandards.org/
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
- rule_id: gcp.compute.instance.paas_disk_encrypted
  service: compute
  resource: instance
  requirement: Paas Disk Encrypted
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Engine Disks Are Encrypted
  rationale: Encrypting disks at rest is critical to protect sensitive data from unauthorized access and potential data breaches. It mitigates risks associated with data exposure if the disk is compromised, aligning with compliance requirements such as PCI-DSS and HIPAA that mandate encryption of sensitive information.
  description: This rule verifies that all persistent disks attached to Google Compute Engine instances have encryption enabled. By default, GCP encrypts data at rest using Google-managed keys, but customers can use Customer-Managed Encryption Keys (CMEK) for additional control. To ensure compliance, verify that CMEK is applied where necessary and review the encryption status of disks through the GCP Console or CLI. If encryption is not enabled, configure CMEK by creating a key in Cloud KMS and associating it with the disks.
  references:
  - https://cloud.google.com/compute/docs/disks#encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.compute.instance.paas_no_public_ip_unless_required
  service: compute
  resource: instance
  requirement: Paas No Public Ip Unless Required
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Prevent Unnecessary Public IPs on PaaS Compute Instances
  rationale: Public IPs on PaaS compute instances increase the risk of unauthorized access and potential data breaches, exposing critical workloads to the internet. This can lead to compliance violations with regulations such as PCI-DSS and HIPAA, and increase the attack surface for cyber threats. Ensuring instances do not have public IPs unless necessary mitigates these risks and supports secure architecture practices.
  description: This rule checks for PaaS compute instances assigned with public IP addresses without explicit justification. Instances should be configured to use internal IPs unless access from the internet is essential and properly secured. To remediate, review network settings in the Google Cloud Console or via the CLI, removing public IPs where not needed, and consider using Cloud NAT for outbound internet access without exposing instances directly.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/nat/docs/overview
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://www.hipaajournal.com/hipaa-compliance-google-cloud-platform/
- rule_id: gcp.compute.instance.paas_ssh_password_auth_disabled
  service: compute
  resource: instance
  requirement: Paas Ssh Password Auth Disabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable SSH Password Authentication on Compute Instances
  rationale: Disabling SSH password authentication reduces the risk of unauthorized access by preventing brute force attacks that exploit weak passwords. This practice enhances security by enforcing key-based authentication, which is significantly more secure and aligns with compliance requirements like PCI-DSS and ISO 27001, protecting sensitive data and maintaining customer trust.
  description: This rule checks whether SSH password authentication is disabled on Google Compute Engine instances. To verify, examine the SSH configuration file to ensure 'PasswordAuthentication no' is set. Remediation involves updating the SSH configuration and restarting the SSH service to apply changes, thereby enforcing the use of SSH keys for authentication. This step is crucial for minimizing attack vectors on cloud resources.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-to-instance
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/compute/docs/instances/setting-instance-access
  - https://csrc.nist.gov/publications/detail/sp/800-123/final
- rule_id: gcp.compute.instance.patch_compliance_configured
  service: compute
  resource: instance
  requirement: Patch Compliance Configured
  scope: compute.instance.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Compliance Configured for Compute Instances
  rationale: Unpatched virtual machine instances can expose your cloud environment to security vulnerabilities, leading to potential data breaches, service disruptions, or unauthorized access. This could result in significant business impact, including financial losses and damage to reputation. Ensuring patch compliance is also a critical requirement for meeting various regulatory standards such as PCI-DSS and ISO 27001.
  description: This rule checks if patch compliance is configured for GCP Compute Engine instances. It verifies that instances are set to automatically apply security patches, ensuring they are protected against known vulnerabilities. To configure patch compliance, enable 'OS patch management' in the Google Cloud Console and ensure that the 'Automatic patching' option is set to 'Enabled'. Remediation involves auditing your instance configurations and adjusting them to meet compliance requirements.
  references:
  - https://cloud.google.com/compute/docs/instances/create-start-instance
  - https://cloud.google.com/compute/docs/instance-templates#os-patch-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security-privacy/compliance/
  - https://cloud.google.com/compute/docs/instance-groups/autohealing-instances
- rule_id: gcp.compute.instance.profile_attached
  service: compute
  resource: instance
  requirement: Profile Attached
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Verify Compute Instances Have Service Accounts Attached
  rationale: Attaching a service account to a Compute Engine instance is essential for controlling permissions and access to other Google Cloud services. Without a service account, instances may operate with excessive privileges or lack necessary access, increasing the risk of unauthorized actions or service disruptions. Adhering to this practice supports compliance with security frameworks by ensuring least privilege access.
  description: This rule checks whether all Compute Engine instances have a service account profile attached. Instances without a service account lack the ability to access Google Cloud APIs securely, potentially leading to security vulnerabilities. To verify, inspect the 'serviceAccounts' property of the instance configuration. If missing, attach an appropriate service account with minimal permissions required for the instance's operations. This can be done via the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances
  - https://cloud.google.com/iam/docs/service-accounts
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, 5.1
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.project_os_login_enabled
  service: compute
  resource: instance
  requirement: Project OS Login Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure OS Login is Enabled for Compute Instances
  rationale: Enabling OS Login enhances security by centralizing identity management and providing consistent access controls across instances. This reduces the risk of unauthorized access and simplifies compliance with regulatory frameworks such as PCI-DSS and ISO 27001, which require robust identity and access management practices. It also mitigates the threat of compromised SSH keys by eliminating the need for local key management.
  description: 'This rule checks whether OS Login is enabled at the project level for Google Compute Engine instances. OS Login uses IAM roles to manage SSH access, replacing traditional SSH keys with Google-managed keys, thereby improving security and auditability. To verify, check if the metadata ''enable-oslogin'' is set to ''TRUE'' at the project level. Remediation involves enabling OS Login via the Google Cloud Console or using the gcloud command-line tool: `gcloud compute project-info add-metadata --metadata enable-oslogin=TRUE`.'
  references:
  - https://cloud.google.com/compute/docs/oslogin
  - https://cloud.google.com/compute/docs/instances/enable-oslogin
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.compute.instance.project_source_repo_url_no_sensitive_credentials_g_variables
  service: compute
  resource: instance
  requirement: Project Source Repo Url No Sensitive Credentials G Variables
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure No Sensitive Credentials in GCP Project Source Repo URLs
  rationale: Embedding sensitive credentials in source repository URLs can lead to unauthorized access and data breaches, significantly impacting business operations and client trust. Such exposure can also result in non-compliance with regulatory frameworks like PCI-DSS, HIPAA, and SOC2, leading to potential legal penalties and reputational damage.
  description: This rule checks for the presence of sensitive credentials within environment variables that reference project source repository URLs in GCP Compute Engine instances. It ensures that no sensitive information, such as API keys or passwords, is included in URLs. To verify, review instance metadata and environment variables, and remove any credentials embedded in URLs. Instead, use IAM roles and service accounts for secure authentication.
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/architecture/best-practices-for-using-service-accounts
- rule_id: gcp.compute.instance.protection
  service: compute
  resource: instance
  requirement: Protection
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances Have Shielded VM Enabled
  rationale: Enabling Shielded VMs helps protect your instances from rootkits, bootkits, and other persistent threats by verifying the integrity of the VM's boot process. Without this protection, instances are vulnerable to advanced attacks that can lead to data breaches, compliance violations, and operational disruptions.
  description: This rule checks that all Google Compute Engine instances have Shielded VM features enabled. Shielded VMs provide verifiable integrity of the boot process and help prevent and detect unauthorized changes. To verify, ensure 'vulnerabilityProtection' is enabled in the instance's configuration. Remediate by configuring existing instances to use Shielded VM features via the GCP Console or CLI, and configure new instances with Shielded VM settings from the start.
  references:
  - https://cloud.google.com/shielded-vm/docs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/compute/docs/instances/enable-shielded-vm
- rule_id: gcp.compute.instance.public_ip
  service: compute
  resource: instance
  requirement: Public Ip
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure Compute Instances Do Not Have Public IP Addresses
  rationale: Exposing compute instances to the public internet can significantly increase the attack surface, making them vulnerable to unauthorized access, data breaches, and distributed denial-of-service (DDoS) attacks. This exposure can lead to compliance violations under standards such as PCI-DSS and GDPR, which require stringent data protection measures. Limiting public access helps safeguard sensitive data and maintain service availability.
  description: This rule checks for Google Cloud Compute instances configured with public IP addresses. Instances with public IPs are accessible from the internet, posing potential security risks. To verify, inspect the network interface configurations for each instance and ensure no public IPs are assigned. Remediation involves either removing the public IP or replacing it with a private IP and using a VPN or Cloud NAT for necessary external communications.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-advanced#no-public-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/nat/docs/overview
  - https://cloud.google.com/vpc/docs/using-vpn
- rule_id: gcp.compute.instance.resilience_dr_approvals_required_for_changes
  service: compute
  resource: instance
  requirement: Resilience DR Approvals Required For Changes
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure DR Approval for Compute Instance Changes
  rationale: Requiring Disaster Recovery (DR) approvals for changes to compute instances ensures that modifications are assessed for their impact on system resilience and business continuity. This process minimizes the risk of unauthorized changes that could lead to downtime or data loss, which is critical for maintaining service availability and meeting compliance requirements such as ISO 27001 and SOC 2.
  description: This rule verifies that any configuration changes to GCP compute instances, such as network settings or instance types, require formal approval from designated DR personnel. Organizations should implement a change management process where all changes are logged and approved by relevant stakeholders. To remediate, configure a Cloud Function triggered by Cloud Audit Logs that enforces approval workflows for instance modifications.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://cloud.google.com/architecture/dr-scenarios-planning-guide
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.resilience_dr_change_audit_logging_enabled
  service: compute
  resource: instance
  requirement: Resilience DR Change Audit Logging Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Audit Logging for DR Changes on Compute Instances
  rationale: Enabling audit logging for disaster recovery (DR) changes on compute instances is crucial to track and understand changes that could impact system resilience. Without logging, unauthorized or accidental modifications may go unnoticed, potentially leading to increased downtime or data loss. This practice supports compliance with regulations such as GDPR and assists in forensic investigations.
  description: This rule checks whether audit logging is enabled for changes related to disaster recovery on GCP Compute Engine instances. To verify, ensure that the necessary audit logs are enabled in Google Cloud's IAM & Admin under Audit Logs for Compute Engine. Remediation involves configuring the logging settings to include 'ADMIN_READ' and 'DATA_WRITE' log types for the 'compute.googleapis.com' service, thus capturing relevant DR configuration changes.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/audit-logging
  - https://cloud.google.com/iam/docs/auditing
- rule_id: gcp.compute.instance.resilience_dr_logs_enabled
  service: compute
  resource: instance
  requirement: Resilience DR Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Compute Instance Resilience DR Logs are Enabled
  rationale: Enabling resilience disaster recovery (DR) logging for compute instances is crucial for maintaining business continuity during unforeseen events. These logs provide critical insights into system performance and data recovery processes, helping to mitigate risks such as data loss or extended downtime. Additionally, they support compliance with industry regulations that require logging and monitoring of disaster recovery activities.
  description: This rule checks if resilience disaster recovery logs are enabled on GCP Compute Engine instances. To verify, ensure that the Compute Engine instances are configured to send logs to Google Cloud Logging by enabling the 'logs' field in the VM instance's metadata or by using the --logging flag during instance creation. Remediation involves updating instance configurations to include logging settings that capture DR activities, which can be achieved through the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/logging/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/logging
  - https://cloud.google.com/architecture/dr-scenarios
- rule_id: gcp.compute.instance.resilience_dr_network_private_only
  service: compute
  resource: instance
  requirement: Resilience DR Network Private Only
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Compute Instances Use Private Networks for DR Resilience
  rationale: Limiting compute instance network interfaces to private networks enhances security by reducing exposure to external threats, thereby supporting business continuity through resilient disaster recovery (DR) strategies. This practice mitigates risks such as data breaches and unauthorized access, and aligns with compliance requirements that mandate controlled and limited network access.
  description: This rule checks that all Google Cloud Compute instances configured for disaster recovery (DR) are connected only to private networks. Instances should not have public IP addresses to avoid exposure to the internet. To verify, inspect the network interfaces of your instances to confirm they are attached only to private subnets. Remediation involves configuring instances without public IPs and ensuring they are associated with VPC networks configured for private connectivity.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#networkinterfaces
  - https://cloud.google.com/security/compliance/cis#section_4.0
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/vpc/docs/vpc
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.resilience_dr_roles_least_privilege
  service: compute
  resource: instance
  requirement: Resilience DR Roles Least Privilege
  scope: compute.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Compute Instance DR Roles
  rationale: Implementing least privilege for Disaster Recovery (DR) roles on GCP Compute Instances minimizes the risk of unauthorized access and data breaches. Overprivileged accounts can lead to misuse or exploitation during an incident, increasing the potential for data loss and non-compliance with regulations such as PCI-DSS and ISO 27001. Least privilege helps ensure that only necessary permissions are granted, supporting both security and compliance objectives.
  description: This rule checks if DR roles associated with compute instances are granted only the permissions essential for their function, avoiding excessive privileges. Verify role assignments by reviewing IAM policies and ensure that roles like 'Compute Instance Admin' are not overly broad. Remediation involves auditing roles with 'gcloud' commands and refining IAM policies to adhere strictly to the principle of least privilege, assigning custom roles if necessary.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/compute/docs/access/
- rule_id: gcp.compute.instance.resilience_dr_source_agent_to_service_tls_required
  service: compute
  resource: instance
  requirement: Resilience DR Source Agent To Service TLS Required
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enforce TLS for Resilience DR Source Agent Communications
  rationale: Requiring TLS for communications between the Resilience DR source agent and service ensures data integrity and confidentiality during transmission. Without TLS, sensitive data could be exposed to interception or tampering, posing a risk to operational continuity and potentially leading to non-compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks if TLS encryption is enforced for communications between the Resilience DR source agent and its corresponding service on GCP instances. To verify, ensure that all network traffic between these components uses TLS by configuring the agent and service endpoints to support HTTPS connections. Remediation involves updating the instance configurations and network policies to mandate TLS usage for these communications.
  references:
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/compute/docs/instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-95.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.resilience_dr_source_encryption_at_rest_cmek_where_supported
  service: compute
  resource: instance
  requirement: Resilience DR Source Encryption At Rest Cmek Where Supported
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure CMEK for Resilience DR Source Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encrypting data at rest in Compute Engine enhances security by giving you control over key management. This reduces the risk of unauthorized access and data breaches, and helps meet compliance requirements such as GDPR and HIPAA by ensuring data is protected with user-managed keys.
  description: This rule checks if Compute Engine instances used as sources for disaster recovery (DR) have their data encrypted at rest using CMEK where supported. Verify by ensuring the encryption key settings of the instances are configured to use user-managed keys. Remediate by updating the instance settings to specify a CMEK in the relevant Google Cloud Key Management Service (KMS) key ring.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.instance.resilience_dr_source_min_telemtry_required_no_pii
  service: compute
  resource: instance
  requirement: Resilience DR Source Min Telemtry Required No Pii
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Non-PII Telemetry for Compute Instance Resilience
  rationale: Ensuring that telemetry data from compute instances used for disaster recovery (DR) is devoid of personally identifiable information (PII) mitigates privacy risks and data breaches. This practice supports compliance with privacy regulations such as GDPR, enhances data protection, and reduces the risk of sensitive data exposure in DR scenarios.
  description: This rule verifies that compute instances configured for resilience and disaster recovery in GCP do not include PII in their telemetry data. It checks the telemetry configuration to ensure data minimization principles are followed. Administrators should review and configure telemetry settings to exclude PII, using GCP's logging and monitoring services to filter and control data flow. Remediation involves auditing telemetry configurations and applying filters or exclusions to sensitive data fields.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/monitoring/docs
- rule_id: gcp.compute.instance.resilience_dr_storage_encrypted_and_private
  service: compute
  resource: instance
  requirement: Resilience DR Storage Encrypted And Private
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Instance DR Storage is Encrypted and Private
  rationale: Encryption of data at rest is crucial to protect sensitive information from unauthorized access, especially in disaster recovery scenarios. Unencrypted or publicly accessible storage can lead to data breaches, compromising confidentiality and integrity. Compliance with standards such as PCI-DSS and HIPAA often mandates encryption to safeguard data and maintain trust.
  description: This rule checks that all disaster recovery storage associated with compute instances in GCP is encrypted and not publicly accessible. Ensure that Compute Engine instances use customer-managed encryption keys (CMEK) or Google-managed keys for all attached disks and that these disks are not shared with public IPs or unauthorized networks. Remediation involves configuring disk encryption settings in the GCP Console or using the gcloud CLI, and setting firewall rules to restrict access.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/
- rule_id: gcp.compute.instance.resilience_private_network_only
  service: compute
  resource: instance
  requirement: Resilience Private Network Only
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Compute Instances Use Private Networks for Resilience
  rationale: Restricting compute instances to private networks reduces exposure to potential external threats and minimizes attack surfaces. This isolation is crucial for protecting sensitive data and maintaining service availability, particularly for systems handling critical workloads or sensitive information. Compliance with regulatory standards often mandates network segmentation to safeguard data integrity and confidentiality.
  description: This rule verifies that all Google Compute Engine instances are configured to operate solely on private networks. Instances should not have external IP addresses, ensuring they are not accessible from the internet directly. To remediate, review your instances' network interface settings and ensure they are aligned with private network configurations. Utilize VPC firewall rules to further control instance access and maintain strong network segmentation.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/compute/docs/ip-addresses#networkaddresses
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.instance.resilience_security_groups_restrictive
  service: compute
  resource: instance
  requirement: Resilience Security Groups Restrictive
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Inbound Rules for Compute Instance Network Security
  rationale: Restricting inbound network access to compute instances minimizes the attack surface, reducing the risk of unauthorized access and potential data breaches. This is crucial for maintaining compliance with security frameworks such as PCI-DSS and HIPAA, which mandate strict access controls to protect sensitive data. By ensuring minimal and necessary network paths are open, organizations can significantly lower the likelihood of exploitation through network-based attacks.
  description: This rule checks compute instances for overly permissive network security groups, specifically those with wide-ranging inbound rules. A restrictive security group should limit access to only necessary ports and IP ranges. To verify, review the security group's inbound rules and ensure they align with the principle of least privilege, allowing only required traffic. Remediation involves adjusting security group rules to close unnecessary ports and limit IP ranges to trusted sources.
  references:
  - https://cloud.google.com/vpc/docs/using-firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.resilience_volumes_encrypted
  service: compute
  resource: instance
  requirement: Resilience Volumes Encrypted
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Instance Resilience Volumes are Encrypted
  rationale: Encrypting resilience volumes on Compute Instances is crucial for protecting sensitive data at rest from unauthorized access and potential breaches. Failure to encrypt could lead to data exposure in case of media theft or improper disposal, and may result in non-compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks whether resilience volumes attached to GCP Compute Instances are encrypted. Encryption should be enabled by default using Google-managed keys or customer-managed keys for enhanced control. Verify encryption status through the GCP Console or gcloud CLI. To remediate, update the instance's volume settings to ensure encryption is enabled, which can be configured at instance creation or by modifying existing volumes.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-111/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.instance.restrict_public_access
  service: compute
  resource: instance
  requirement: Restrict Public Access
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Restrict Public IPs on Compute Instances
  rationale: Allowing public IP access on compute instances increases the risk of unauthorized access, exposing sensitive applications and data to potential attackers, which can lead to data breaches and compliance violations. Restricting public access helps protect against common attack vectors such as brute force attacks and reduces the attack surface, aligning with compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks whether Google Cloud Compute instances have external IP addresses assigned, which can be accessed over the internet. Instances should be configured without public IPs, instead utilizing private IPs and appropriate network configurations like VPNs or Bastion Hosts for secure access. To verify, review instance settings in the GCP Console or use the gcloud command-line tool. Remediation involves removing the external IP and ensuring access through secure internal networks.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-to-instance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#security
  - https://cloud.google.com/vpc/docs/best-practices
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.serial_port_access_disabled
  service: compute
  resource: instance
  requirement: Serial Port Access Disabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable Serial Port Access on Compute Instances
  rationale: Enabling serial port access on virtual machines can expose sensitive information and increase the risk of unauthorized access, as attackers can exploit this to gain insights into system operations or bypass security controls. Disabling this feature helps in adhering to security best practices and compliance requirements, such as PCI-DSS and ISO 27001, which emphasize minimizing unnecessary exposure of system interfaces.
  description: This rule checks whether serial port access is disabled on Google Compute Engine instances. Serial port access should be restricted as it can be used to expose sensitive data and configuration details. To verify, ensure that 'enable-serial-port' is set to 'false' in the instance metadata. Remediation involves updating the instance settings via the Google Cloud Console or gcloud command-line tool to set 'enable-serial-port' to 'false'.
  references:
  - https://cloud.google.com/compute/docs/instances/interacting-with-serial-console
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.serial_ports_in_use
  service: compute
  resource: instance
  requirement: Serial Ports In Use
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Serial Ports Are Disabled for Compute Instances
  rationale: Enabling serial ports on Google Cloud Compute instances can expose sensitive information and create potential entry points for unauthorized access. This setting should be disabled to mitigate risks associated with data leaks and unauthorized configuration changes, which can lead to compliance violations with standards such as PCI-DSS and ISO 27001.
  description: This rule checks for the presence of enabled serial ports on GCP Compute Engine instances. To ensure secure configuration, serial ports should be disabled unless explicitly required for troubleshooting purposes. Users can verify this setting by checking the metadata of each instance and can disable serial ports by setting the 'enable-serial-port' metadata key to 'false'. Remediation involves updating the instance metadata to disable serial ports across all instances.
  references:
  - https://cloud.google.com/compute/docs/instances/interacting-with-serial-console
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.service_account_non_default_configured
  service: compute
  resource: instance
  requirement: Service Account Non Default Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Custom Service Accounts for Compute Instances
  rationale: Assigning non-default service accounts to compute instances limits permissions to only those necessary for the instance to function, reducing the risk of privilege escalation and lateral movement within the cloud environment. This practice helps to minimize the attack surface and supports compliance with least privilege access principles, which are vital for meeting regulatory requirements such as PCI-DSS and SOC2.
  description: This rule checks if Google Cloud Compute Engine instances are configured with a non-default service account. By default, Compute Engine instances use the default service account, which often has more permissions than necessary. To verify, inspect the instance metadata and ensure a custom service account is specified. Remediation involves creating a custom service account with the minimal required IAM roles and updating the instance configuration to use this account.
  references:
  - https://cloud.google.com/compute/docs/access/service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloudsecurityalliance.org/guidance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/cyberframework
  - https://www.isaca.org/resources/cobit
- rule_id: gcp.compute.instance.shielded_vm_compliance_configured
  service: compute
  resource: instance
  requirement: Shielded VM Compliance Configured
  scope: compute.instance.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Shielded VM Compliance for Compute Instances
  rationale: Shielded VMs protect against rootkits and bootkits by ensuring instances boot with verified images, thus minimizing the risk of unauthorized code execution. This is crucial for maintaining data integrity and compliance with security standards such as PCI-DSS and HIPAA, particularly in environments handling sensitive information.
  description: This rule verifies that Compute Engine instances have Shielded VM features enabled, including Secure Boot, vTPM, and Integrity Monitoring. To check compliance, review the instance configurations in the GCP Console or via gcloud CLI, ensuring these features are activated. Remediation involves enabling Shielded VM options under the instance's security settings to enhance threat resistance.
  references:
  - https://cloud.google.com/shielded-vm
  - https://cloud.google.com/compute/docs/instances/verifying-shielded-vm
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.compute.instance.shielded_vm_enabled
  service: compute
  resource: instance
  requirement: Shielded VM Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Shielded VM is Enabled for Compute Instances
  rationale: Shielded VMs provide verifiable integrity of virtual machines, protecting against rootkits and bootkits, which can compromise the integrity of your instances. Enabling Shielded VM safeguards against unauthorized alterations and enhances compliance with security frameworks like PCI-DSS and ISO 27001, mitigating risks of data breaches and unauthorized access.
  description: This rule checks if Shielded VM features are enabled on Google Compute Engine instances, ensuring that vTPM, integrity monitoring, and secure boot are active. To verify, navigate to the VM instance settings in the GCP console and confirm that Shielded VM options are enabled. Remediation involves editing the instance settings and enabling these features, or recreating the instance with Shielded VM enabled.
  references:
  - https://cloud.google.com/shielded-vm
  - https://cloud.google.com/compute/docs/instances/verifying-shielded-vm
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.compute.instance.spot_instance_instance_profile_least_privilege
  service: compute
  resource: instance
  requirement: Spot Instance Instance Profile Least Privilege
  scope: compute.instance.least_privilege
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Spot Instance Profiles Use Least Privilege
  rationale: Applying the principle of least privilege to Spot Instance profiles minimizes the risk of unauthorized access and potential data breaches. Incorrectly configured instance profiles can lead to privilege escalation, allowing attackers to exploit permissions not needed for the instance's operation. This practice is crucial for maintaining a secure and compliant cloud environment, especially under regulatory frameworks like PCI-DSS and ISO 27001.
  description: This rule checks if Spot Instances are assigned instance profiles with excessive permissions beyond what is necessary for their operation. Ensure that IAM roles linked to Spot Instances only include permissions essential for their specific task. Regularly review and update IAM policies to align with current workload requirements and remove any superfluous permissions. Remediation involves adjusting the IAM roles assigned to Spot Instances to enforce the least privilege principle.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.spot_instance_no_public_ip_assigned
  service: compute
  resource: instance
  requirement: Spot Instance No Public Ip Assigned
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Spot Instances Must Not Have Public IPs Assigned
  rationale: Assigning public IPs to spot instances increases the attack surface, exposing these ephemeral resources to potential unauthorized access and attacks. This can lead to data breaches, unauthorized access to sensitive workloads, and service disruptions. Complying with security frameworks like NIST and PCI-DSS necessitates minimizing public exposure of cloud resources to protect against potential threats.
  description: This rule checks if any spot instances in your GCP environment have public IP addresses assigned. Spot instances are usually short-lived and should be tightly controlled to prevent exposure to the internet. Verification involves ensuring that spot instances do not have 'networkInterfaces.accessConfigs' configured. Remediation requires removing any public IPs assigned to spot instances by updating the network interface settings to restrict to private IPs only.
  references:
  - https://cloud.google.com/compute/docs/instances/spot
  - https://cloud.google.com/vpc/docs/compute-instances#no-external-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.compute.instance.spot_instance_uses_approved_launch_template
  service: compute
  resource: instance
  requirement: Spot Instance Uses Approved Launch Template
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Spot Instances Use Approved Launch Templates
  rationale: Using approved launch templates for Spot Instances ensures that instances are created with consistent and secure configurations. It mitigates risks associated with misconfigurations which could lead to vulnerabilities, data exposure, or service disruptions. This is particularly important for maintaining compliance with security standards and reducing the operational costs associated with manual configuration errors.
  description: This rule checks that all Spot Instances in your GCP environment are launched using pre-approved launch templates. These templates should include security settings such as firewall rules, IAM roles, and instance metadata settings. To verify, review instance configurations in the GCP Console or use the gcloud CLI to ensure templates are used consistently. If discrepancies are found, update your launch processes to enforce template usage and document any exceptions.
  references:
  - https://cloud.google.com/compute/docs/instances/spot
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 3.1
  - NIST SP 800-53 Rev. 5
  - ISO/IEC 27001:2013
- rule_id: gcp.compute.instance.ssh_key_authentication
  service: compute
  resource: instance
  requirement: Ssh Key Authentication
  scope: compute.instance.authentication
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Enforce SSH Key Authentication on Compute Instances
  rationale: SSH key authentication enhances security by eliminating password-based logins, reducing the risk of brute force attacks and unauthorized access. It is crucial for maintaining the confidentiality and integrity of sensitive data processed by compute instances, and is often a requirement for compliance with standards like PCI-DSS and ISO 27001.
  description: This rule checks if SSH key authentication is enforced on Google Compute Engine instances. Instances should be configured to accept only SSH keys instead of passwords for remote access. To verify, ensure the 'Block project-wide SSH keys' setting is disabled, and instance-specific SSH keys are configured. Remediation involves updating instance metadata to include only authorized public SSH keys and disabling password authentication in the SSH daemon configuration.
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.ssm_enabled
  service: compute
  resource: instance
  requirement: Ssm Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure SSM Agent is Enabled on GCP Compute Instances
  rationale: Enabling the SSM (Systems Manager) agent on GCP Compute instances enhances security by allowing administrators to manage and automate configurations securely without direct SSH access. This reduces the attack surface by limiting open ports and direct access, mitigating risks such as unauthorized access and potential data breaches. It also ensures compliance with security policies requiring centralized management and monitoring of virtual machines.
  description: This rule checks if the SSM agent is installed and running on GCP Compute instances. Verify that the agent is enabled by reviewing instance metadata or using gcloud command-line tools. If not enabled, install the SSM agent using the appropriate package manager for the instance's operating system and ensure it is configured to start on boot. This can be automated across instances using startup scripts or configuration management tools.
  references:
  - https://cloud.google.com/compute/docs/instances/access-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/automating-secure-infrastructure
- rule_id: gcp.compute.instance.ssm_managed
  service: compute
  resource: instance
  requirement: Ssm Managed
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances are SSM Managed
  rationale: Configuring instances to be SSM managed enhances security by allowing centralized management and automation of patching, configuration, and compliance policies, reducing the risk of vulnerabilities and misconfigurations. This is critical for maintaining the integrity and security of workloads and ensuring compliance with industry standards.
  description: This rule checks if Google Compute Engine instances are managed by AWS Systems Manager (SSM), which allows for automated patching, monitoring, and configuration management. To verify, ensure that the SSM agent is installed and configured correctly on each instance. Remediation involves installing the SSM agent on instances and configuring the necessary IAM roles for SSM access, facilitating efficient management and security compliance.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/compute/docs/storing-retrieving-metadata
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.stopped_instance_configured
  service: compute
  resource: instance
  requirement: Stopped Instance Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Stopped Instances Are Properly Configured
  rationale: Unconfigured stopped instances may lead to unexpected charges and security risks if accidentally started with insecure settings. Ensuring that stopped instances are configured with necessary security measures helps prevent unauthorized access and data leakage, aligning with compliance requirements and safeguarding organizational assets.
  description: This rule checks that all stopped Google Compute Engine instances have security configurations such as firewall rules, IAM policies, and network settings appropriately set. Administrators should verify that no critical data is stored on local disks and that instances have minimal access permissions. Remediating involves reviewing and updating instance configurations to meet security policies before they are started again.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/instance-life-cycle
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc/docs/firewalls
- rule_id: gcp.compute.instance.subnet_flow_logs_enabled
  service: compute
  resource: instance
  requirement: Subnet Flow Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Subnet Flow Logs are Enabled for Compute Instances
  rationale: Enabling subnet flow logs provides visibility into the network traffic entering and leaving your compute instances, which is crucial for detecting anomalous activities and unauthorized access attempts. This visibility helps in identifying security threats and maintaining compliance with regulations that require detailed logging and monitoring of network activities.
  description: This rule checks whether subnet flow logs are enabled for Google Cloud Compute instances. Subnet flow logs capture information about network traffic, which is essential for auditing and troubleshooting. To verify, navigate to the VPC network in the Google Cloud Console and ensure that flow logs are enabled for each subnet used by compute instances. Remediation involves modifying the subnet settings to enable flow logs, providing crucial data for network analysis and security auditing.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.compute.instance.url_authentication
  service: compute
  resource: instance
  requirement: Url Authentication
  scope: compute.instance.authentication
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Enable Secure URL Authentication for GCP Compute Instances
  rationale: Enabling secure URL authentication on GCP Compute Instances mitigates the risk of unauthorized access to sensitive resources. It prevents potential data breaches and unauthorized usage, ensuring compliance with regulatory standards such as PCI-DSS and HIPAA. Failing to secure URL endpoints can lead to exposure of critical services, impacting business operations and customer trust.
  description: This rule checks that GCP Compute Instances have URL authentication mechanisms properly configured. It ensures that only authorized users and applications can access instance endpoints via secure URLs. Verification involves reviewing instance metadata and network configuration settings to confirm authentication is enabled. Remediation includes configuring IAM policies to restrict access and implementing OAuth 2.0 or signed URLs for additional security.
  references:
  - https://cloud.google.com/compute/docs/security
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-in-the-cloud/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.compute.instance.vm_data_volumes_encrypted_cmek
  service: compute
  resource: instance
  requirement: VM Data Volumes Encrypted Cmek
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure VM Data Volumes Use CMEK for Encryption
  rationale: Encrypting VM data volumes with Customer-Managed Encryption Keys (CMEK) enhances data protection by allowing organizations to control key rotation, revocation, and management. This mitigates risks such as unauthorized access and data breaches, which can have significant financial and reputational impacts. Compliance with regulatory frameworks like PCI-DSS and HIPAA often requires strict encryption controls.
  description: This rule checks if Google Cloud Platform Compute Engine VM instances use Customer-Managed Encryption Keys for encrypting attached data volumes. To verify, review the 'properties.disks' field in the VM instance configurations for the 'kmsKeyName' attribute. If it's absent, update the instance configuration to include a CMEK. Remediation involves specifying a KMS key in the VM instance settings, ensuring that data is encrypted using organization-controlled keys.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.compute.instance.vm_imds_hardened
  service: compute
  resource: instance
  requirement: VM Imds Hardened
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Harden Metadata Server Access on VM Instances
  rationale: Properly securing access to the VM Instance Metadata Server (IMDS) mitigates risks of unauthorized access to sensitive metadata, which could lead to privilege escalation or data exfiltration. This is crucial for maintaining the integrity and confidentiality of workloads running in Google Cloud, and aligns with regulatory requirements like PCI-DSS and ISO 27001.
  description: This rule checks that VM instances have restricted access to the metadata server by utilizing GCP firewall rules or configuring the metadata concealment feature. To verify, ensure that the metadata server is only accessible from trusted sources or has been disabled where not needed. Remediation involves setting up network tags and firewall rules that block access to the metadata server from unauthorized sources, or using GCP's metadata concealment feature to limit exposure.
  references:
  - https://cloud.google.com/compute/docs/storing-retrieving-metadata
  - https://cloud.google.com/compute/docs/security-best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.compute.instance.vm_no_public_ip_assigned
  service: compute
  resource: instance
  requirement: VM No Public Ip Assigned
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure VM Instances Have No Public IP Assigned
  rationale: Public IP addresses on VM instances expose them to the internet, increasing the risk of unauthorized access, DDoS attacks, and data breaches. This can lead to loss of sensitive data, service disruption, and non-compliance with regulations such as PCI-DSS and GDPR, which mandate secure handling of data.
  description: This rule checks whether any VM instances have public IP addresses assigned, which should be avoided to reduce exposure to external attacks. Ensure that VM instances are only accessible through secure internal networks or via a VPN. To remediate, configure instances to use private IPs and access them through a bastion host or secure gateway. You can verify this setting in the Google Cloud Console under the instance's network interface settings.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#ephemeraladdress
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://cloud.google.com/architecture/best-practices-vpc-design
- rule_id: gcp.compute.instance.vm_root_volume_encrypted_cmek
  service: compute
  resource: instance
  requirement: VM Root Volume Encrypted Cmek
  scope: compute.instance.encryption
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure VM Root Volume is Encrypted with Customer-Managed Keys
  rationale: Encrypting the VM root volume with Customer-Managed Encryption Keys (CMEK) provides enhanced data security by allowing customers to control and manage the encryption keys. This is crucial for protecting sensitive data and ensuring compliance with regulatory frameworks such as PCI-DSS and HIPAA, which require strong encryption controls. Without CMEK, organizations face increased risk of unauthorized access and potential data breaches.
  description: This rule checks if the VM root volume is encrypted using Customer-Managed Encryption Keys on Google Cloud Platform. To verify, ensure that CMEK is enabled for the disk encryption settings during the creation of the VM instance. Remediation involves setting up a Cloud Key Management Service (KMS) key and applying it to the VM root volume. This ensures that encryption keys are under customer control, providing better security and compliance with industry standards.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/compute/docs/disks/add-persistent-disk#encrypted
  - https://cloud.google.com/compute/docs/disks/viewing-and-applying-encryption-keys
- rule_id: gcp.compute.instance.vm_secure_boot_enabled
  service: compute
  resource: instance
  requirement: VM Secure Boot Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure VM Instances Have Secure Boot Enabled
  rationale: Enabling Secure Boot on VM instances minimizes the risk of unauthorized code execution during the boot process, protecting against rootkits and low-level malware. This is crucial for maintaining the integrity and trustworthiness of the system's boot sequence, thus ensuring compliance with security standards like NIST and ISO 27001, which require strong boot-time security controls.
  description: 'This rule checks if Secure Boot is enabled for Google Compute Engine VM instances. Secure Boot is a feature that helps prevent malicious software and unauthorized operating systems from loading during the system start-up process. To verify, ensure the VM instance''s boot configuration includes ''secureBoot: true''. If not enabled, modify the instance configuration by setting the secure boot option in the Shielded VM settings. This action enhances the security posture by enforcing trusted boot paths.'
  references:
  - https://cloud.google.com/compute/docs/instances/enable-secure-boot
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/shielded-cloud
- rule_id: gcp.compute.instance.vm_security_group_inbound_restricted
  service: compute
  resource: instance
  requirement: VM Security Group Inbound Restricted
  scope: compute.instance.network_security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict Inbound Traffic to VM Instances
  rationale: Restricting inbound traffic to VM instances reduces the attack surface, preventing unauthorized access and potential breaches. This control is crucial for safeguarding sensitive data and maintaining compliance with security frameworks such as PCI-DSS and HIPAA, which mandate strict network access controls.
  description: This rule checks that security groups associated with VM instances in Google Cloud Platform have restrictive inbound rules, allowing only necessary traffic. It verifies that no overly permissive rules (e.g., 0.0.0.0/0 allowing all IPs) exist, especially for sensitive ports like SSH (22) or RDP (3389). Remediation involves reviewing and updating firewall rules to specify only trusted IP ranges for inbound access, utilizing identity-aware proxies or VPNs where feasible.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/identity-aware-proxy/docs/concepts-overview
- rule_id: gcp.compute.instance.vm_serial_console_access_restricted
  service: compute
  resource: instance
  requirement: VM Serial Console Access Restricted
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict VM Serial Console Access on Compute Instances
  rationale: Restricting VM serial console access is crucial to prevent unauthorized users from accessing sensitive information or executing commands on virtual machines. Unrestricted access could lead to potential data breaches, unauthorized configurations, and compromise of critical workloads, impacting business continuity and compliance with standards like ISO 27001 and PCI-DSS.
  description: This rule checks if serial console access is disabled for GCP Compute instances. By default, the VM serial console should be disabled as it allows users to connect to the VM even if they do not have SSH access. To verify, ensure the 'enable-serial-port' metadata is set to false on instances. Remediation involves updating the instance metadata to disable serial port access, which can be done via the GCP Console, CLI, or API.
  references:
  - https://cloud.google.com/compute/docs/instances/interacting-with-serial-console
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.compute.instance.vm_ssh_key_based_auth_required
  service: compute
  resource: instance
  requirement: VM Ssh Key Based Auth Required
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Enforce SSH Key-Based Authentication for GCP VMs
  rationale: Relying solely on password-based authentication for VM instances can expose systems to brute force attacks, leading to unauthorized access and potential data breaches. Enforcing SSH key-based authentication mitigates these risks by requiring cryptographic keys that are significantly more secure and align with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks if SSH key-based authentication is enforced on all GCP VM instances. SSH keys provide a more secure method of accessing instances by using asymmetric encryption keys instead of passwords. To ensure compliance, administrators should disable password authentication and configure instances to accept SSH keys only. This can be verified using the GCP Console or CLI by checking the instance metadata settings and ensuring 'block-project-ssh-keys' is not set, and the 'ssh-keys' metadata entry is configured.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-advanced
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.vm_ssh_password_auth_disabled
  service: compute
  resource: instance
  requirement: VM Ssh Password Auth Disabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable SSH Password Authentication for VM Instances
  rationale: Disabling SSH password authentication reduces the risk of brute-force attacks and unauthorized access, as passwords can be easily guessed or stolen. This practice aligns with security best practices and compliance requirements by enforcing the use of stronger authentication methods like SSH keys, thereby enhancing the security posture of cloud workloads.
  description: This rule checks if SSH password authentication is disabled on Google Cloud VM instances. Enabling SSH key-based authentication and disabling password-based access helps secure the VM from unauthorized logins. Verify the 'PasswordAuthentication' setting in the SSH configuration file is set to 'no'. Remediation involves updating the SSH configuration and restarting the SSH service on the VM instance.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-advanced#disable-ssh-password
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.compute.instance.vm_user_data_no_secrets
  service: compute
  resource: instance
  requirement: VM User Data No Secrets
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure VM User Data Does Not Contain Secrets
  rationale: Storing secrets such as API keys, passwords, or private keys in VM user data poses significant security risks. Unauthorized access to these secrets can lead to data breaches, financial loss, or compromised services. Regulatory compliance frameworks like PCI-DSS and HIPAA mandate secure handling of sensitive information, making it crucial to avoid embedding secrets in VM configurations.
  description: This rule checks that VM user data does not contain any sensitive information such as secrets or credentials. User data is often used for instance initialization scripts, but it should never include confidential data. Verify user data configurations through the GCP Console or CLI and ensure encryption and secret management solutions like Google Secret Manager are used for sensitive data. Remediation involves auditing user data and removing any embedded secrets, replacing them with secure reference calls to managed secret services.
  references:
  - https://cloud.google.com/compute/docs/metadata/overview
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.vm_vtpm_enabled
  service: compute
  resource: instance
  requirement: VM Vtpm Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure VM Instances have vTPM Enabled
  rationale: Enabling the virtual Trusted Platform Module (vTPM) on VM instances enhances the security by providing hardware-based cryptographic operations. This helps protect sensitive data and application workloads from unauthorized access and tampering, reducing the risk of data breaches. Compliance with security standards like PCI-DSS and HIPAA often require such measures to ensure data confidentiality and integrity.
  description: This rule checks whether vTPM is enabled for Google Cloud VM instances. The vTPM feature helps in securing VM instances by providing a hardware root of trust. To verify, navigate to the VM instance settings in the Google Cloud Console and ensure vTPM is enabled under the 'Shielded VM' options. Remediation involves activating vTPM on existing instances or configuring it during the creation of new instances.
  references:
  - https://cloud.google.com/shielded-vm/docs/shielded-vm-concepts
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.vpc_subnet_flow_logs_compliance_configured
  service: compute
  resource: instance
  requirement: VPC Subnet Flow Logs Compliance Configured
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure VPC Subnet Flow Logs Are Enabled for Instances
  rationale: Enabling VPC subnet flow logs is crucial for monitoring network traffic, detecting anomalous behavior, and fulfilling audit requirements. It helps in identifying potential security threats, such as data exfiltration or unauthorized access patterns, thereby reducing the risk of data breaches. Compliance with regulatory frameworks often mandates detailed logging for forensic analysis and reporting purposes.
  description: This rule checks if flow logs are enabled for VPC subnets associated with instances in your GCP environment. VPC flow logs capture information about the ingress and egress IP traffic, which can be used for performance analysis and security insights. To verify, ensure that the 'Enable flow logs' option is active on your VPC subnets. Remediation involves navigating to the VPC network details in the GCP Console, selecting 'Edit' for the subnet, and enabling flow logs, specifying the desired log format and sampling rate.
  references:
  - https://cloud.google.com/vpc/docs/flow-logs
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/logging/docs/reference
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance_group.instance_profile_least_privilege
  service: compute
  resource: instance_group
  requirement: Instance Profile Least Privilege
  scope: compute.instance_group.least_privilege
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Enforce Least Privilege for Compute Instance Profiles
  rationale: Implementing least privilege for instance profiles minimizes the risk of unauthorized access and potential data breaches by ensuring that compute resources have only the permissions necessary for their function. This approach reduces the attack surface and helps comply with regulatory standards like PCI-DSS and ISO 27001, which mandate access controls to protect sensitive information.
  description: This rule verifies that GCP Compute Instance Groups are configured with the minimum permissions necessary for their intended purpose. It checks for overly permissive roles attached to instance profiles and suggests replacing them with custom roles that grant only essential permissions. Remediation involves auditing instance roles and adjusting IAM policies to align with the principle of least privilege, using the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/creating-custom-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/compute/docs/access/iam
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
- rule_id: gcp.compute.instance_group.no_public_ip_assigned
  service: compute
  resource: instance_group
  requirement: No Public Ip Assigned
  scope: compute.instance_group.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Prevent Public IP Assignment to GCP Instance Groups
  rationale: Assigning public IPs to instance groups can expose your infrastructure to the internet, increasing the risk of unauthorized access and potential data breaches. Public IPs can be exploited by attackers to perform reconnaissance or launch attacks on your instances, leading to potential business disruptions, data loss, and non-compliance with regulations such as PCI-DSS and SOC2 that mandate secure access controls.
  description: This rule checks that no public IP addresses are assigned to any instances within a GCP instance group. Instances with public IPs should be reconfigured to use private IPs to leverage Google's VPC for secure communication. To verify, review the instance group's network interface settings in the Google Cloud Console or via the gcloud CLI for IP address configurations. Remediation involves removing public IP assignments and setting up a Cloud NAT or VPN for secure external communications.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#assigning_ip_addresses
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/vpc/docs/vpc#vpc_networks_and_subnets
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance_group.uses_approved_launch_template
  service: compute
  resource: instance_group
  requirement: Uses Approved Launch Template
  scope: compute.instance_group.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Instance Groups Use Approved Launch Templates
  rationale: Using approved launch templates for instance groups ensures that all instances adhere to organizational security standards, minimizing the risk of misconfigurations that could lead to vulnerabilities. This practice helps maintain consistency across deployments, reducing the attack surface and supporting compliance with regulations like PCI-DSS, which require stringent control over system configurations.
  description: This rule checks if all instance groups within your GCP environment utilize launch templates that have been approved by your organization's security team. Approved launch templates contain predefined configurations that meet security policies, such as specific network settings, firewall rules, and instance metadata configurations. To verify compliance, review the launch template IDs associated with your instance groups and compare them against the list of approved templates. If discrepancies are found, update the instance group to use an approved launch template, ensuring it matches security requirements.
  references:
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/compute/docs/instance-groups/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/instances/instance-life-cycle
- rule_id: gcp.compute.instance_group.volumes_encrypted
  service: compute
  resource: instance_group
  requirement: Volumes Encrypted
  scope: compute.instance_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Instance Group Volumes are Encrypted
  rationale: Encrypting volumes protects sensitive data from unauthorized access and potential breaches by ensuring that data at rest is unreadable without the proper decryption keys. This is crucial for meeting regulatory compliance requirements like GDPR and HIPAA, which mandate data protection standards to safeguard personal and sensitive information.
  description: This rule checks whether the persistent disks attached to instance groups in GCP Compute are encrypted using Customer-Managed Encryption Keys (CMEK) or Google-managed encryption keys. To verify, review the encryption settings of each disk in the instance group via the Google Cloud Console or gcloud CLI. If disks are not encrypted, configure encryption under 'Disks' by selecting an encryption key. This can be done during disk creation or by modifying existing disks.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/publications/nist-special-publication-800-53-revision-5-security-and-privacy-controls
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/docs/security-overview
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.instance_template.dr_artifacts_encrypted_and_private
  service: compute
  resource: instance_template
  requirement: DR Artifacts Encrypted And Private
  scope: compute.instance_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DR Artifacts in Instance Templates Are Encrypted and Private
  rationale: Encrypting disaster recovery (DR) artifacts in instance templates protects sensitive data from unauthorized access and exposure. Without encryption and privacy controls, there is a heightened risk of data breaches, which can lead to financial loss and damage to reputation. Additionally, compliance with regulations like GDPR and CCPA requires robust data protection measures, including encryption at rest.
  description: This rule checks that all DR artifacts associated with GCP Compute Engine instance templates are encrypted using Customer-Managed Encryption Keys (CMEK) and are not publicly accessible. Verify that the instance templates specify CMEK for disk encryption. Remediation involves updating the instance template to include CMEK and ensuring appropriate IAM roles are applied to restrict access. This helps maintain data confidentiality and integrity.
  references:
  - https://cloud.google.com/compute/docs/instances/disks#encrypted_disks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance_template.dr_change_audit_logging_enabled
  service: compute
  resource: instance_template
  requirement: DR Change Audit Logging Enabled
  scope: compute.instance_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DR Change Audit Logging is Enabled for Instance Templates
  rationale: Enabling audit logging for disaster recovery (DR) changes in instance templates is crucial to track any unauthorized or inappropriate modifications. This helps in maintaining the integrity of DR plans, preventing data loss, and ensuring business continuity. It also aids in meeting compliance requirements by providing a verifiable trail of changes.
  description: This rule checks if audit logging is enabled for changes made to instance templates, specifically those related to disaster recovery configurations. To verify, ensure that the 'enableLogging' attribute is set to true in the instance template's configuration. If logging is not enabled, update the instance template settings to include logging for all change activities. This helps in auditing access and modifications to DR settings.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/compute/docs/instance-templates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://hipaa.jotform.com/resources/hipaa-compliance-guide/
- rule_id: gcp.compute.instance_template.dr_execution_roles_least_privilege
  service: compute
  resource: instance_template
  requirement: DR Execution Roles Least Privilege
  scope: compute.instance_template.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege on DR Execution Roles
  rationale: Implementing least privilege for Disaster Recovery (DR) execution roles is critical to prevent unauthorized access and potential misuse of compute resources. Over-permissioned roles increase the risk of insider threats and exposure to external attacks, potentially leading to data breaches and service disruptions. Compliance with standards such as NIST SP 800-53 and ISO 27001 mandates stringent access controls to safeguard sensitive data and maintain operational integrity.
  description: This rule checks if instance templates are configured with the minimal necessary IAM roles required for DR operations. Ensure that roles assigned to instance templates do not exceed the permissions necessary for their intended function. Verify configurations through IAM policy review and use GCP tools to audit permissions. Remediation involves adjusting IAM policies to remove unnecessary roles while validating functionality remains intact.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/resource-manager/docs/access-control-org
- rule_id: gcp.compute.instance_template.launch_template_imds_hardened
  service: compute
  resource: instance_template
  requirement: Launch Template Imds Hardened
  scope: compute.instance_template.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Harden IMDS Access in Compute Instance Templates
  rationale: Ensuring that the Instance Metadata Service (IMDS) is properly configured in GCP Compute instance templates is crucial for preventing unauthorized access to sensitive metadata, which could lead to potential data breaches. By hardening IMDS access, organizations can mitigate risks such as metadata exposure and privilege escalation, thus enhancing their security posture and aligning with compliance requirements like SOC2 and ISO 27001.
  description: This rule checks if the Instance Metadata Service (IMDS) access is properly restricted in GCP Compute instance templates. It verifies that the 'metadataFlavor' header is set to 'Google' and metadata server requests are secured to prevent unauthorized access. To remediate, ensure that the instance template's metadata options are configured with 'enable-oslogin' set to 'TRUE' and 'block-project-ssh-keys' set to 'TRUE'. Regular audits and updates to these settings are recommended to maintain security.
  references:
  - https://cloud.google.com/compute/docs/instances/accessing-instance-metadata
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance_template.launch_template_instance_profile_least_privilege
  service: compute
  resource: instance_template
  requirement: Launch Template Instance Profile Least Privilege
  scope: compute.instance_template.least_privilege
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Instance Templates Use Least Privilege Profiles
  rationale: Ensuring instance templates use the least privilege principle minimizes the attack surface by restricting unnecessary permissions, reducing the risk of exploitation in case of a compromised instance. This approach mitigates potential data breaches and aligns with compliance frameworks that require stringent access controls to protect sensitive information.
  description: This rule checks that instance templates in GCP do not associate with overly permissive IAM roles. It verifies that the service accounts tied to instance templates have the minimum required permissions. To remediate, review the permissions granted through IAM roles and adjust them to ensure only essential permissions are assigned. This involves using custom roles tailored to the specific needs of applications running on the instances.
  references:
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.compute.instance_template.launch_template_no_public_ip_default
  service: compute
  resource: instance_template
  requirement: Launch Template No Public Ip Default
  scope: compute.instance_template.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure Instance Templates Do Not Use Public IP by Default
  rationale: Public IP addresses expose instances directly to the internet, increasing the risk of unauthorized access, data breaches, and other security threats. Limiting public IP usage helps protect sensitive workloads and aligns with compliance requirements such as CIS Benchmarks, which advocate for minimizing public exposure to reduce attack surfaces.
  description: This rule checks that instance templates do not have a public IP address set as default upon launch. Verify by inspecting the 'networkInterfaces' configuration in the instance template to ensure 'accessConfigs' is not set with a public IP. To remediate, modify the instance template to use private IP addresses only, and utilize Cloud NAT or VPNs for external connectivity if needed.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#types
  - https://cloud.google.com/compute/docs/instance-templates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/nat/docs/overview
  - https://cloud.google.com/vpn/docs/concepts/overview
- rule_id: gcp.compute.instance_template.launch_template_root_volume_encrypted_by_default
  service: compute
  resource: instance_template
  requirement: Launch Template Root Volume Encrypted By Default
  scope: compute.instance_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: critical
  title: Ensure Root Volume Encryption in Instance Templates
  rationale: Encrypting the root volume of instance templates by default is crucial for protecting sensitive data at rest. Without encryption, unauthorized access to storage media could lead to data breaches, violating compliance with standards like PCI-DSS and HIPAA, and potentially resulting in financial and reputational damage.
  description: This rule checks whether the root volumes in GCP instance templates are encrypted by default. It ensures that all new instances launched from these templates have their root volumes encrypted using Google-managed or customer-managed encryption keys. To verify, review the instance template settings for disk encryption properties. Remediation involves enabling encryption for disks in the instance template settings via the GCP Console or CLI.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/encryption-at-rest/
- rule_id: gcp.compute.instance_template.launch_template_security_groups_restrictive
  service: compute
  resource: instance_template
  requirement: Launch Template Security Groups Restrictive
  scope: compute.instance_template.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Launch Templates Use Restrictive Security Groups
  rationale: Restrictive security groups in launch templates minimize exposure to unauthorized access by explicitly defining allowed traffic. This reduces the risk of unauthorized data access and potential breaches, aligning with compliance mandates such as PCI-DSS and NIST that require strict access controls around sensitive data handling.
  description: This rule checks if GCP instance templates are configured with security groups that limit inbound and outbound traffic to only necessary protocols and ports. It examines the security group configurations associated with launch templates and flags those that allow overly permissive access. To remediate, restrict security group rules to the minimum necessary, ensuring only trusted IP addresses and essential services are permitted.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/pci-dss
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.compute.instance_template.launch_template_user_data_no_secrets
  service: compute
  resource: instance_template
  requirement: Launch Template User Data No Secrets
  scope: compute.instance_template.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure No Secrets in Instance Template User Data
  rationale: Storing secrets such as passwords, API keys, or sensitive configuration data in instance templates can lead to unauthorized access and data breaches. This poses significant security risks, including potential exposure of sensitive data and violation of compliance standards like PCI-DSS and HIPAA. Protecting these secrets is crucial to maintaining the integrity and confidentiality of your systems.
  description: This rule checks that sensitive data is not included in the user data field of GCP compute instance templates. Storing secrets in this field can lead to exposure as the user data is accessible to any user with read access to the instance template. To remediate, use secret management tools like Google Secret Manager to securely store and access sensitive data, and ensure instance templates are configured without embedding secrets directly.
  references:
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/secret-manager/docs
  - 'CIS GCP Benchmark: 1.0.0 - Ensure Instance Templates Do Not Contain Sensitive Data'
  - 'NIST SP 800-53: AC-6 Least Privilege'
  - 'PCI-DSS Requirement 3: Protect Stored Cardholder Data'
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance_template.network_change_audit_logging_enabled
  service: compute
  resource: instance_template
  requirement: Network Change Audit Logging Enabled
  scope: compute.instance_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Network Change Audit Logging for Instance Templates
  rationale: Enabling network change audit logging for instance templates is crucial for maintaining visibility over modifications that could impact your cloud infrastructure. Without these logs, unauthorized or erroneous changes could go undetected, leading to potential exposure of resources, disruptions in service, and non-compliance with regulatory standards such as PCI-DSS and SOC 2. Monitoring network changes helps in quickly identifying and responding to suspicious activities, thereby reducing the risk of data breaches and ensuring operational integrity.
  description: This rule checks whether audit logging for network changes is enabled on Google Cloud instance templates. Specifically, it verifies that audit logs are configured to capture events related to network alterations such as updates to firewall rules or changes to network configurations. To ensure compliance, navigate to the 'Audit Logs' section in the Google Cloud Console, select the appropriate service and resource, and enable logging for 'Admin Read' and 'Admin Write' activities. This can be automated using Terraform or gcloud CLI commands. Regularly review these logs for unauthorized access or anomalies.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/compute/docs/instance-templates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance_template.network_remediation_roles_least_privilege
  service: compute
  resource: instance_template
  requirement: Network Remediation Roles Least Privilege
  scope: compute.instance_template.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Network Roles in Instance Templates
  rationale: Assigning least privilege roles for network remediation in instance templates minimizes the risk of unauthorized access and data breaches. It reduces the attack surface by ensuring that users and services have only the necessary permissions, aligning with compliance requirements such as least privilege principles in NIST and ISO 27001. This approach mitigates the potential impact of compromised credentials or insider threats, protecting sensitive network configurations and data.
  description: This rule checks that network remediation roles assigned to instance templates do not exceed the minimum permissions required. Review and adjust IAM policies to ensure roles are designed according to the principle of least privilege. Verify that no overly permissive roles are attached to instance templates, and modify them to limit access to only the actions necessary for remediation tasks. Regular audits and policy reviews should be conducted to maintain compliance and security posture.
  references:
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.compute.network.acl_unused
  service: compute
  resource: network
  requirement: ACL Unused
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Identify and Remove Unused Network ACLs in GCP
  rationale: Unused Network ACLs in GCP can lead to potential security risks by allowing outdated or unnecessary access configurations to persist in the environment. This increases the attack surface and can inadvertently expose resources to unauthorized access. Regularly reviewing and removing unused ACLs is crucial for maintaining a secure and compliant cloud environment by ensuring that only necessary access permissions are in place.
  description: This rule checks for Network ACLs in GCP that are configured but not actively used by any resources. Unused ACLs should be reviewed and removed to reduce complexity and potential misconfigurations. Verification involves checking the association of each ACL with network resources such as subnets or VMs. Remediation involves deleting the ACLs that are not in use, ensuring that no active resources depend on them, and documenting any changes made for audit purposes.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.network.check_default_absence
  service: compute
  resource: network
  requirement: Check Default Absence
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Default Network Is Not Present in GCP Projects
  rationale: The default network in GCP is automatically created with permissive firewall rules and is not tailored to specific security needs, posing a potential security risk. Organizations may inadvertently use it, leading to exposure to unauthorized access and data breaches. Removing it helps align with security best practices and compliance requirements, reducing the attack surface.
  description: This rule checks for the presence of the default network in your GCP projects. By default, GCP creates a network with open firewall rules that may not adhere to your organization's security policies. To enhance security, delete the default network if it exists and create custom networks with specific firewall rules that align with your security posture. Verify by using the GCP Console or gcloud CLI to list available networks and ensure the default is absent. Remediate by deleting the default network via the console or `gcloud compute networks delete default` command.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/vpc/docs/configure-firewall-rules
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.network.default_in_use
  service: compute
  resource: network
  requirement: Default In Use
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Disable Default Network Usage in GCP
  rationale: Using the default network in GCP can lead to security risks as it is automatically created with preconfigured firewall rules that allow broad access. This may expose your resources to unwanted traffic, increasing the attack surface and potential for unauthorized access. Ensuring custom network configurations helps meet compliance requirements and reduce vulnerabilities.
  description: This rule checks if the default network is in use within your Google Cloud Platform project. The default network comes with pre-established firewall rules that may not adhere to the principle of least privilege, posing a security risk. To mitigate this, create custom networks with specific firewall rules tailored to your application's security needs. Verification involves checking the network configurations and ensuring no resources are deployed on the default network. Remediation includes migrating existing resources to custom networks and deleting the default network if not needed.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/solutions/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.network.dns_logging_enabled
  service: compute
  resource: network
  requirement: Dns Logging Enabled
  scope: compute.network.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DNS Logging is Enabled for GCP Networks
  rationale: Enabling DNS logging on Google Cloud Platform networks allows for comprehensive visibility into DNS queries, which is crucial for detecting and responding to potential security threats such as data exfiltration or network attacks. It helps in auditing and monitoring network activities for compliance with regulations like GDPR and PCI DSS, which mandate detailed logging for data protection and integrity.
  description: This rule checks if DNS logging is enabled for networks within Google Cloud Platform. DNS logs provide valuable insights into network traffic, helping identify unusual patterns or unauthorized access. To verify, review the network's logging configuration in the Google Cloud Console or use the gcloud command-line tool. Remediation involves enabling DNS logging in the network settings under the 'Logging' section, ensuring that logs are sent to a designated Cloud Logging project for analysis and storage.
  references:
  - https://cloud.google.com/dns/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf
- rule_id: gcp.compute.network.legacy_configured
  service: compute
  resource: network
  requirement: Legacy Configured
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Avoid Using Legacy Networks in GCP Compute
  rationale: Legacy networks in GCP lack granular control over network security policies, which increases the risk of unauthorized access and data breaches. Utilizing modern network configurations enhances the ability to implement tighter access controls and comply with regulatory requirements such as PCI-DSS and HIPAA. Failure to update legacy networks could lead to security vulnerabilities and non-compliance with industry standards.
  description: This rule checks for the presence of legacy networks configured within GCP Compute environments. Legacy networks do not support subnetting or modern firewall rules, which are critical for fine-grained access control. To verify, navigate to the VPC Networks section in the GCP Console and ensure no networks are labeled as 'legacy'. Remediation involves migrating legacy networks to VPC networks, which offer improved security features and manageability.
  references:
  - https://cloud.google.com/vpc/docs/using-vpc
  - https://landing.google.com/securebydesign/
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/document_library
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.compute.network.network_automated_isolation_supported
  service: compute
  resource: network
  requirement: Network Automated Isolation Supported
  scope: compute.network.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Automated Isolation is Enabled in GCP
  rationale: Automated network isolation helps contain security incidents and limits unauthorized access. It mitigates risks associated with compromised resources by automatically isolating affected networks. This approach reduces the potential impact on business operations and supports compliance with security frameworks that require incident containment strategies.
  description: This rule verifies if automated isolation is supported and enabled within Google Cloud's network settings. It checks for configurations like VPC Service Controls that facilitate automated isolation of resources based on predefined conditions. Ensure you have set up appropriate VPC Service Control boundaries and enabled audit logging for network activities to monitor and respond to suspicious behaviors. Remediation involves configuring VPC Service Controls and enabling network policies that support automated isolation.
  references:
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://cloud.google.com/security/best-practices/identity-access-management
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 5.1
  - 'NIST SP 800-53: SI-4 Information System Monitoring'
  - 'PCI-DSS Requirement 11: Regularly test security systems and processes'
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.network.network_quarantine_network_defined
  service: compute
  resource: network
  requirement: Network Quarantine Network Defined
  scope: compute.network.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Define Quarantine Network for Isolating Untrusted Resources
  rationale: Defining a quarantine network is crucial to isolate potentially malicious or untrusted assets, preventing lateral movement and reducing the risk of data breaches. This practice mitigates the impact of compromised resources by containing threats and preventing them from affecting other parts of the network. It is an essential component for adhering to regulatory frameworks that mandate network segmentation and containment strategies.
  description: This rule checks if a designated quarantine network is defined and utilized within your GCP environment to isolate untrusted or compromised resources. Ensure that a virtual network with strict ingress and egress rules is configured to limit communication with other network segments. To verify, review your VPC network configurations and ensure a network labeled 'quarantine' or equivalent exists, with appropriate firewall rules. Remediation involves creating a new VPC network specifically for quarantine purposes and configuring firewall rules to restrict traffic.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/network-connectivity/docs
- rule_id: gcp.compute.network.not_legacy
  service: compute
  resource: network
  requirement: Not Legacy
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Avoid Use of Legacy Networks in GCP
  rationale: Legacy networks in GCP lack the advanced features and controls of VPC networks, potentially exposing systems to security risks such as unauthorized access and limited traffic management capabilities. Using non-legacy networks supports business continuity by providing enhanced scalability, better integration with modern services, and compliance with industry standards, reducing the risk of non-compliance with regulations such as PCI-DSS and ISO 27001.
  description: This check verifies that no legacy networks are in use within GCP projects. Legacy networks are identified by their flat global IP address space, which lacks the segmentation capabilities of Virtual Private Cloud (VPC) networks. To ensure compliance, administrators should migrate any legacy networks to VPC networks, which offer subnet-level segmentation, custom routes, and firewall rules. Verification can be done through the GCP Console or CLI by listing all networks and ensuring 'VPC' is used. Remediation involves creating VPC networks and migrating resources from legacy networks.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/network-connectivity/docs/overview
- rule_id: gcp.compute.network.subnet_flow_logs_compliance_configured
  service: compute
  resource: network
  requirement: Subnet Flow Logs Compliance Configured
  scope: compute.network.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Subnet Flow Logs Are Configured in GCP
  rationale: Enabling subnet flow logs helps capture IP traffic information, which is crucial for monitoring network activity, diagnosing network issues, and detecting potential security incidents. Without flow logs, organizations may have blind spots in their network security posture, increasing the risk of undetected anomalies and non-compliance with regulatory standards such as PCI-DSS and SOC 2.
  description: This rule verifies that subnet flow logs are enabled for each Google Cloud Platform VPC network. To ensure compliance, navigate to the 'VPC network' section in the GCP Console, select the desired subnet, and verify that flow logs are set to 'On'. If not enabled, adjust the settings accordingly to capture and analyze network data for better security monitoring and incident response capabilities.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.network.vpc_dns_hostnames_and_support_configured
  service: compute
  resource: network
  requirement: VPC Dns Hostnames And Support Configured
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure VPC DNS Hostnames and Support are Properly Configured
  rationale: Configuring VPC DNS hostnames and support is crucial for smooth internal DNS resolution, which enhances network efficiency and security. Without proper configuration, instances may face difficulties in resolving internal hostnames, leading to potential service disruptions. This setting is also vital for meeting certain regulatory requirements that mandate proper network configuration and access controls.
  description: This rule checks whether VPC networks have DNS hostnames and support enabled, facilitating internal DNS resolution for instances. Verify this configuration by navigating to the 'VPC Network' settings in the Google Cloud Console and ensuring that both 'DNS Hostnames' and 'DNS Resolution' are enabled. Remediate by enabling these options to ensure seamless connectivity and compliance with network security best practices.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/vpc/docs/configure-dns
- rule_id: gcp.compute.network.vpc_flow_logs_enabled
  service: compute
  resource: network
  requirement: VPC Flow Logs Enabled
  scope: compute.network.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enable VPC Flow Logs for Enhanced Network Monitoring
  rationale: Enabling VPC Flow Logs provides detailed visibility into network traffic, helping to detect anomalies and potential security threats. This is crucial for identifying unauthorized access attempts and ensuring data integrity, thereby reducing the risk of data breaches and supporting forensic investigations. Additionally, it aids in meeting compliance requirements by providing necessary audit trails for regulatory standards.
  description: This rule checks if VPC Flow Logs are enabled for all networks in GCP. VPC Flow Logs capture information about the IP traffic going to and from network interfaces within a VPC, allowing for detailed analysis of network flows. To verify, ensure that VPC Flow Logs are configured in the Google Cloud Console under the VPC network settings. Remediation involves enabling Flow Logs for each subnet, specifying the desired aggregation interval, sampling rate, and logging metadata. This setup enhances the ability to monitor network activity effectively.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.network.vpc_route_tables_no_unintended_internet_paths
  service: compute
  resource: network
  requirement: VPC Route Tables No Unintended Internet Paths
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Prevent Unintended Internet Paths in VPC Route Tables
  rationale: Unintended internet exposure from misconfigured VPC route tables can lead to unauthorized access, data breaches, and potential compliance violations. Ensuring that only intended internet paths are configured reduces the risk of exposing sensitive resources, aligning with best practices for network security and meeting regulatory requirements.
  description: This rule checks for unintended internet paths in VPC route tables, specifically looking for default routes that may inadvertently expose internal resources to the internet. To verify, review route tables for any 0.0.0.0/0 routes and ensure they direct traffic only through secure gateways like Cloud NAT or specific VPN connections. Remediation involves removing or correcting any routes that allow unintended internet access, ensuring traffic is routed through secure, intended paths.
  references:
  - https://cloud.google.com/vpc/docs/routes
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.3
  - 'NIST SP 800-53 Rev. 5: AC-3 Access Enforcement'
  - 'PCI DSS v3.2.1: Requirement 1.3.2'
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/network-connectivity/docs
- rule_id: gcp.compute.packet_mirroring.network_alert_destinations_configured
  service: compute
  resource: packet_mirroring
  requirement: Network Alert Destinations Configured
  scope: compute.packet_mirroring.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Alert Destinations for Packet Mirroring are Configured
  rationale: Proper configuration of network alert destinations for packet mirroring is essential to ensure that traffic anomalies and potential security threats are detected and addressed promptly. Without configured alert destinations, critical security events may go unnoticed, leading to data breaches or service disruptions. Compliance with standards such as PCI-DSS and NIST requires proper monitoring and alerting mechanisms to safeguard sensitive information.
  description: This rule checks if network alert destinations are configured for Packet Mirroring in GCP Compute. It ensures that mirrored traffic is monitored and that alerts are sent to designated destinations, such as Security Operations Centers (SOCs), for analysis. To verify, ensure alert destinations are set in the Packet Mirroring configuration under the 'MirrorConfig' settings. Remediation involves specifying valid alert destinations in the Packet Mirroring policy to ensure all mirrored traffic is properly monitored.
  references:
  - https://cloud.google.com/vpc/docs/packet-mirroring
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.packet_mirroring.network_ids_ips_enabled_where_supported
  service: compute
  resource: packet_mirroring
  requirement: Network Ids Ips Enabled Where Supported
  scope: compute.packet_mirroring.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enable Network IDs and IPs in Packet Mirroring
  rationale: Enabling network IDs and IPs in packet mirroring is crucial for ensuring that mirrored traffic is accurately tagged and can be properly analyzed. Without this, there is a risk of incomplete traffic analysis, which can lead to undetected anomalies or breaches. This is particularly important for organizations that must comply with security standards such as NIST, PCI-DSS, or HIPAA, where network monitoring and incident detection are critical components.
  description: This rule checks that network IDs and IPs are enabled for packet mirroring configurations where supported. To verify, access the Google Cloud Console, navigate to the Compute Engine section, and review packet mirroring configurations to ensure that network IDs and IPs are included. Remediation involves updating the packet mirroring settings to include these identifiers, ensuring comprehensive traffic analysis and improved incident response capabilities.
  references:
  - https://cloud.google.com/vpc/docs/packet-mirroring
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.project.os_login_compliance_configured
  service: compute
  resource: project
  requirement: OS Login Compliance Configured
  scope: compute.project.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure OS Login is Enabled for Project Compliance
  rationale: Enabling OS Login enhances security by centralizing user management via IAM, reducing the risk of unauthorized access through SSH keys. It aids in compliance with security frameworks by ensuring consistent access control and audit logging across instances, providing traceability and accountability for access activities.
  description: This rule checks if OS Login is configured for all VM instances in the project. OS Login enforces IAM-based user and SSH key management, improving security and simplifying access control. To verify, ensure 'enable-oslogin' metadata is set to 'true' at the project level. Remediation involves updating project metadata to include 'enable-oslogin=true' and verifying IAM roles and permissions for users.
  references:
  - https://cloud.google.com/compute/docs/oslogin
  - https://cloud.google.com/compute/docs/instances/managing-instance-access
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/docs/security/compliance/nist
  - https://cloud.google.com/docs/security/compliance/iso-27001
  - https://cloud.google.com/docs/security/compliance/soc-2
- rule_id: gcp.compute.reservation.cost_approval_workflow_required
  service: compute
  resource: reservation
  requirement: Cost Approval Workflow Required
  scope: compute.reservation.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Mandatory Cost Approval for Compute Reservations
  rationale: Implementing a cost approval workflow for compute reservations ensures that financial expenditures are monitored and validated by appropriate stakeholders, reducing the risk of unexpected charges and budget overruns. This is essential for organizations to maintain financial accountability and align with budgetary constraints, particularly in environments with multiple users and teams. It also aids in detecting and preventing unauthorized or frivolous resource allocations that could lead to financial waste.
  description: This rule checks if a cost approval workflow is in place before compute reservations are finalized in GCP. It requires configurations in Google Cloud's billing settings to enforce an approval process, such as using workflows to route reservation requests to financial controllers or managers for review. To verify, audit the billing account settings to ensure an approval policy exists. Remediation involves setting up a workflow using Google Cloud Functions and Cloud Pub/Sub to trigger notifications and approvals based on reservation requests.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/using-workload-identity-federation
  - https://cloud.google.com/functions/docs/calling/pubsub
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0 - Section 4.10
  - NIST SP 800-53 Rev. 5 - CM-5 Access Restrictions for Change
  - https://cloud.google.com/architecture/cost-management
- rule_id: gcp.compute.reservation.cost_billing_admins_mfa_required
  service: compute
  resource: reservation
  requirement: Cost Billing Admins MFA Required
  scope: compute.reservation.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce MFA for Cost Billing Admins on Compute Reservations
  rationale: Requiring multi-factor authentication (MFA) for Cost Billing Admins mitigates the risk of unauthorized access to financial resources and sensitive configurations. Without MFA, attackers with stolen credentials can access and manipulate billing data, leading to financial loss and compliance violations under frameworks like PCI-DSS and SOC2.
  description: This rule checks if Cost Billing Admins managing compute reservations have MFA enabled. Ensure that all accounts with billing access are secured with two-step verification by navigating to 'IAM & Admin' > 'IAM', selecting the relevant account, and enabling MFA. Remediation involves configuring MFA in the Google Account settings and enforcing it through 'Access Context Manager' policies.
  references:
  - https://cloud.google.com/identity-platform/docs/mfa-overview
  - https://cloud.google.com/iam/docs/best-practices-for-enterprise-organizations
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.compute.reservation.cost_purchase_permissions_restricted
  service: compute
  resource: reservation
  requirement: Cost Purchase Permissions Restricted
  scope: compute.reservation.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict Cost Purchase Permissions for Reservations
  rationale: Restricting permissions for cost purchases on reservations mitigates the risk of unauthorized or inadvertent financial commitments, which can lead to unexpected cost overruns. It ensures that only designated personnel can make such purchases, aligning with financial governance and compliance requirements like SOC2 and ISO 27001.
  description: This check verifies that only authorized users or roles have permissions to purchase committed use discounts on Compute Engine reservations. To ensure compliance, review IAM policies to confirm that the 'roles/compute.reservationAdmin' role is only granted to trusted accounts. Remediation involves auditing IAM policies and revoking permissions from unauthorized users, thereby preventing unintended financial liabilities.
  references:
  - https://cloud.google.com/compute/docs/instances/reserving-zonal-resources
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.resource_policy.snapshot_schedule_configured
  service: compute
  resource: resource_policy
  requirement: Snapshot Schedule Configured
  scope: compute.resource_policy.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Snapshot Schedule is Configured for Compute Resource Policies
  rationale: Configuring snapshot schedules for resource policies is crucial for maintaining data integrity and availability in GCP environments. It helps mitigate risks associated with data loss due to accidental deletion or system failures, ensuring business continuity and compliance with industry standards like ISO 27001 and SOC2.
  description: This rule checks whether a snapshot schedule is configured for each resource policy in Google Cloud's Compute service. Without a snapshot schedule, data may not be regularly backed up, leading to potential data loss. To verify, review the resource policy settings in the GCP Console or via the gcloud command-line tool. Remediation involves creating or updating resource policies to include regular snapshot schedules, ensuring backups are taken at appropriate intervals.
  references:
  - https://cloud.google.com/compute/docs/disks/scheduled-snapshots
  - https://cloud.google.com/architecture/disaster-recovery-planning-guide
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.compute.route.network_table_no_0_0_0_0_from_private_subnets
  service: compute
  resource: route
  requirement: Network Table No 0 0 0 0 From Private Subnets
  scope: compute.route.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Prevent 0.0.0.0/0 Routes from Private Subnets
  rationale: Allowing routes to 0.0.0.0/0 from private subnets can expose sensitive data and resources to the entire internet, increasing the risk of unauthorized access and data breaches. This configuration could lead to non-compliance with standards like PCI-DSS and HIPAA, which require stringent access controls for sensitive data.
  description: This rule checks for routes in Google Cloud Platform that allow traffic from private subnets to be directed to 0.0.0.0/0, which represents the entire internet. Ensure that your routes are configured to restrict such access unless explicitly required and securely managed. To remediate, audit your routing tables to ensure that no route from a private subnet is set to 0.0.0.0/0, and adjust the routes to ensure they target only necessary and secure destinations.
  references:
  - https://cloud.google.com/vpc/docs/routes
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.route.network_table_vpc_endpoints_used_for_saas_where_supported
  service: compute
  resource: route
  requirement: Network Table VPC Endpoints Used For Saas Where Supported
  scope: compute.route.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Use VPC Endpoints for SaaS in Network Routes
  rationale: Using VPC endpoints for SaaS applications reduces exposure to the public internet, minimizing the risk of data interception and unauthorized access. This is crucial for protecting sensitive data and maintaining compliance with regulations such as GDPR and HIPAA, which require stringent data protection controls.
  description: This rule checks whether VPC endpoints are utilized in routing tables for SaaS applications where supported. Ensuring that traffic to SaaS applications is routed through VPC endpoints can enhance security by keeping traffic within the managed Google Cloud network. To verify, inspect the routing tables for routes pointing to VPC endpoints for SaaS, ensuring they are correctly configured. Remediation includes setting up VPC endpoints and updating routing tables to leverage these endpoints for SaaS traffic.
  references:
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/privacy
- rule_id: gcp.compute.security_policy.balancing_waf_acl_attached_configured
  service: compute
  resource: security_policy
  requirement: Balancing Waf ACL Attached Configured
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure WAF ACLs are Attached and Configured on Load Balancers
  rationale: Proper configuration of Web Application Firewall (WAF) Access Control Lists (ACLs) on load balancers is crucial to protect web applications from common threats such as SQL injection and cross-site scripting. Failing to configure these ACLs can lead to security vulnerabilities, exposing sensitive data and potentially violating compliance requirements such as PCI-DSS and GDPR.
  description: This rule checks whether a WAF ACL is both attached and configured on Google Cloud Load Balancers within the security policies. It verifies the presence of security policies that include WAF rules tailored to the application's threat model. Remediation involves reviewing and updating security policies to include appropriate WAF ACLs, ensuring they are actively blocking or mitigating known vulnerabilities.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/armor/docs/rules
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.security_policy.edge_waf_block_actions_not_count_only
  service: compute
  resource: security_policy
  requirement: Edge Waf Block Actions Not Count Only
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Edge WAF Policies Enforce Block Actions
  rationale: Implementing block actions in your Edge WAF policies is crucial for actively preventing malicious traffic from reaching your applications. While 'count' actions allow monitoring and analysis of potential threats, they do not stop attacks in real-time, increasing the risk of data breaches and service disruptions. Organizations must enforce block actions to comply with security standards and mitigate threats effectively.
  description: This rule checks that security policies in Google Cloud Platform's Edge WAF are configured to enforce block actions, rather than just logging or counting suspicious activity. Without active blocking, potential threats are only monitored, which does not prevent them from causing harm. To verify, check the security policies in your GCP account to ensure that they include rules with action set to 'block'. If only 'count' actions are present, update the policy to enforce blocking for critical threat vectors. This ensures compliance with security best practices and reduces the risk of successful attacks.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/armor/docs/rules-language-reference
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.security_policy.edge_waf_ip_rate_limit_rules_configured_where_supported
  service: compute
  resource: security_policy
  requirement: Edge Waf Ip Rate Limit Rules Configured Where Supported
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Edge WAF IP Rate Limit Rules Configured
  rationale: Configuring IP rate limit rules on Edge WAF helps mitigate denial-of-service attacks and limits the potential for abuse from malicious actors. This is crucial for maintaining availability and ensuring that legitimate traffic is not disrupted, thereby supporting business continuity and compliance with security standards that require protection against such threats.
  description: This rule checks whether IP rate limit rules are configured on GCP Edge WAF within security policies where supported. To verify, inspect your security policy settings in the GCP Console or via gcloud commands to ensure rate limiting is enabled. If not configured, establish rate limits based on expected traffic patterns to prevent abuse. Remediation involves modifying security policies to include rate limiting rules, which can be done through the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/load-balancing/docs/using-firewall-rules
  - https://cloud.google.com/security-command-center/docs/concepts-cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security
  - https://cloud.google.com/armor/docs/configure-security-policies
- rule_id: gcp.compute.security_policy.edge_waf_logging_enabled
  service: compute
  resource: security_policy
  requirement: Edge Waf Logging Enabled
  scope: compute.security_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Edge WAF Logging for Security Policies
  rationale: Enabling Edge WAF logging is crucial for detecting and investigating potential security incidents, as it provides visibility into web application attacks targeted at your infrastructure. Without logging, threat actors could exploit vulnerabilities undetected, leading to data breaches, service disruptions, or non-compliance with regulations like PCI-DSS and ISO 27001 which mandate audit trails.
  description: This rule checks if logging is enabled for Edge WAF within your security policies on GCP. To verify, ensure that 'enableLogging' is set to true in the security policy configuration. If logging is not enabled, configure your WAF settings to log all traffic, providing critical data for analysis. This can be done via the GCP Console under 'Security Policies' or using the gcloud command line tool. Enabling logging facilitates proactive monitoring and response to security threats.
  references:
  - https://cloud.google.com/armor/docs/security-policy-concepts#logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/armor/docs/configure-security-policies
- rule_id: gcp.compute.security_policy.edge_waf_managed_rule_sets_enabled
  service: compute
  resource: security_policy
  requirement: Edge Waf Managed Rule Sets Enabled
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Edge WAF Managed Rule Sets on Security Policies
  rationale: Enabling Edge WAF Managed Rule Sets mitigates security risks by providing automatic protection against common web vulnerabilities such as SQL injection and cross-site scripting (XSS). This enhances the security posture of applications by reducing the risk of data breaches and downtime caused by malicious attacks. It also aids in meeting compliance requirements like PCI-DSS by ensuring that web applications are protected against known threats.
  description: This rule checks if Edge WAF Managed Rule Sets are enabled on Google Cloud Platform security policies. Managed rule sets provide predefined rules that detect and block malicious web traffic, which can be crucial for safeguarding applications from a wide range of attacks. To verify, inspect the security policies in the GCP console under 'Network Security' and ensure that managed rule sets are applied. If not enabled, configure the security policy to include Edge WAF Managed Rule Sets to enhance protection.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/security-compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/armor/docs/managed-protection
  - https://www.nist.gov/cyberframework
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.security_policy.edge_waf_web_acl_attached_to_cdn
  service: compute
  resource: security_policy
  requirement: Edge Waf Web ACL Attached To Cdn
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Edge WAF Web ACL is Attached to GCP CDN
  rationale: Attaching an Edge WAF Web ACL to a CDN in GCP helps mitigate web-based attacks such as SQL injection and cross-site scripting, reducing the risk of data breaches and service disruptions. This is critical for maintaining the integrity and availability of web applications and ensuring compliance with industry standards like PCI-DSS and ISO 27001.
  description: This rule checks if an Edge WAF Web ACL is attached to a Google Cloud CDN, ensuring incoming traffic is filtered through configured security policies. Verify by checking the security policy configuration in the Cloud Console or via gcloud CLI. If missing, attach an appropriate Web ACL to your CDN distribution to filter out malicious requests. Remediation involves creating or selecting a Web ACL with rules tailored to your application's security needs and associating it with your CDN.
  references:
  - https://cloud.google.com/cdn/docs/security
  - https://cloud.google.com/armor/docs/security-policy-concepts
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/armor/docs/rule-tuning
- rule_id: gcp.compute.security_policy.network_anomaly_destinations_encrypted
  service: compute
  resource: security_policy
  requirement: Network Anomaly Destinations Encrypted
  scope: compute.security_policy.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Network Anomaly Destinations Use Encrypted Channels
  rationale: Encrypting network anomaly destinations protects against unauthorized data interception and tampering, which can lead to data breaches and compliance violations. This encryption is critical for maintaining the confidentiality and integrity of data in transit, especially for organizations handling sensitive or regulated information.
  description: This rule verifies that all network anomaly destinations defined in GCP Compute Engine security policies use encrypted channels such as TLS for data transmission. To ensure compliance, review security policy configurations to confirm that all destinations are set to accept only secure, encrypted connections. Remediation involves updating security policies to enforce encryption and confirming that all endpoints support secure protocols.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/security/encryption/default-encryption
- rule_id: gcp.compute.security_policy.network_anomaly_detectors_enabled
  service: compute
  resource: security_policy
  requirement: Network Anomaly Detectors Enabled
  scope: compute.security_policy.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Network Anomaly Detectors in Security Policies
  rationale: Network anomaly detectors are essential for identifying unusual patterns that could indicate potential security threats such as DDoS attacks or unauthorized access attempts. Enabling these detectors helps organizations to proactively mitigate risks, comply with regulatory standards like PCI-DSS and ISO 27001, and protect sensitive data from breaches.
  description: This rule checks whether network anomaly detectors are enabled in GCP security policies for Compute Engine. To verify, access the Google Cloud Console, navigate to 'Security Policies' under the 'Compute Engine' service, and ensure that anomaly detection is activated. Remediation involves enabling this feature in the security policy settings to enhance threat detection and response capabilities.
  references:
  - https://cloud.google.com/armor/docs/security-policy-concepts
  - https://cloud.google.com/security-command-center/docs/how-to-anomalies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.snapshot.dr_backup_destination_private_only
  service: compute
  resource: snapshot
  requirement: DR Backup Destination Private Only
  scope: compute.snapshot.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Snapshots for DR Are Stored in Private Buckets
  rationale: Storing backup snapshots in private destinations minimizes the risk of unauthorized access and data breaches, which can lead to significant financial and reputational damage. It ensures compliance with data protection regulations by preventing public exposure of sensitive data stored in backups. This practice supports business continuity by securing critical recovery data.
  description: This rule checks whether Compute Engine snapshots used for disaster recovery (DR) are stored in Cloud Storage buckets with private access settings. Public access to these buckets should be disabled to protect sensitive backup data. To verify, review the bucket's IAM policy and ensure 'allUsers' or 'allAuthenticatedUsers' are not granted access. Remediation involves updating the bucket's permissions to restrict access to specific service accounts or users who require it.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.snapshot.dr_backup_encrypted_at_rest_cmek
  service: compute
  resource: snapshot
  requirement: DR Backup Encrypted At Rest Cmek
  scope: compute.snapshot.encryption
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Snapshots Are Encrypted with Customer-Managed Keys
  rationale: Encrypting snapshots using Customer-Managed Encryption Keys (CMEK) enhances data confidentiality and control, reducing the risk of unauthorized access during a disaster recovery scenario. This approach aligns with compliance standards that require strong encryption methods and empowers organizations to manage encryption keys, crucial for sectors handling sensitive data.
  description: This rule verifies that all Compute Engine snapshots are encrypted at rest using Customer-Managed Encryption Keys (CMEK). By default, Google Cloud encrypts data with Google-managed keys, but using CMEK provides greater control and allows for key rotation and revocation policies. To comply, ensure that snapshot configurations specify CMEK, which can be verified via the GCP Console or gcloud CLI. Remediation involves updating snapshot settings to refer to the appropriate CMEK.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
- rule_id: gcp.compute.snapshot.dr_backup_immutable_or_worm_enabled_where_supported
  service: compute
  resource: snapshot
  requirement: DR Backup Immutable Or Worm Enabled Where Supported
  scope: compute.snapshot.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Snapshots Have Immutable or WORM Backups Enabled
  rationale: Enabling immutable or Write Once Read Many (WORM) backups for snapshots prevents accidental or malicious data modification. This is crucial for maintaining data integrity and ensuring that disaster recovery processes can rely on untampered backups. It also supports compliance with regulations that require data retention and protection against unauthorized changes.
  description: This rule verifies that snapshots in GCP have immutable or WORM backup settings enabled where supported. Enabling these settings ensures that once data is written, it cannot be altered, providing a secure backup solution. Administrators should configure snapshot settings in the GCP Console or via the API to enable immutability where available. Regular audits and configuration checks should be conducted to ensure continuous compliance.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.snapshot.dr_backup_schedule_defined_min_frequency
  service: compute
  resource: snapshot
  requirement: DR Backup Schedule Defined Min Frequency
  scope: compute.snapshot.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Min Frequency for Compute Snapshot DR Backups
  rationale: Defining a minimum frequency for disaster recovery (DR) backups of snapshots is critical to minimize data loss and ensure data availability in the event of a failure. Regular backups enhance resilience by allowing businesses to quickly restore operations, thereby reducing downtime and mitigating financial and reputational damage. Compliance with industry regulations often mandates regular backup schedules to safeguard sensitive data against loss due to disasters.
  description: This rule checks if a minimum frequency for DR backup schedules is defined for Google Cloud Compute snapshots. It ensures that snapshots are regularly backed up according to a predefined schedule, which is crucial for effective disaster recovery. To verify, review the configuration of snapshot schedules in the Google Cloud Console or via gcloud CLI, ensuring that they meet the minimum frequency requirements. If not configured, establish a backup schedule that aligns with your organizationâ€™s data recovery and business continuity objectives.
  references:
  - https://cloud.google.com/compute/docs/disks/scheduled-snapshots
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/disaster-recovery
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.snapshot.dr_restore_cross_account_restore_blocked_by_policy
  service: compute
  resource: snapshot
  requirement: DR Restore Cross Account Restore Blocked By Policy
  scope: compute.snapshot.policy_management
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Prevent Cross-Account Restore for Compute Snapshots
  rationale: Preventing cross-account restores of snapshots is crucial to minimizing unauthorized access and potential data breaches. By enforcing policies that restrict such actions, organizations can ensure that sensitive data remains within the intended account boundaries, reducing the risk of data leakage and aiding in compliance with data protection regulations.
  description: This rule checks for policies that block cross-account restoration of Compute Engine snapshots, ensuring that snapshots cannot be restored in unauthorized accounts. To verify compliance, ensure that IAM policies and organization policies are configured to restrict snapshot sharing and restoration to trusted accounts only. Remediation involves reviewing and updating policy configurations in the GCP Console, ensuring proper permissions are set to prevent unauthorized data access.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.snapshot.dr_restore_from_encrypted_backups_only
  service: compute
  resource: snapshot
  requirement: DR Restore From Encrypted Backups Only
  scope: compute.snapshot.encryption
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Snapshots are Restored from Encrypted Backups Only
  rationale: Restoring from encrypted backups ensures data confidentiality and integrity, mitigating the risk of data breaches during disaster recovery. This practice protects sensitive information from unauthorized access, and aligns with compliance requirements such as GDPR and PCI-DSS, which mandate strong encryption for data at rest.
  description: This rule verifies that all Compute Engine snapshots used for disaster recovery are created with encryption. To ensure compliance, check that snapshots are encrypted using either Google-managed or customer-supplied encryption keys. To remediate, enable encryption on existing snapshots by creating new encrypted snapshots from the original disks, and ensure all future snapshots are encrypted by default.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.snapshot.dr_restore_logs_and_artifacts_encrypted
  service: compute
  resource: snapshot
  requirement: DR Restore Logs And Artifacts Encrypted
  scope: compute.snapshot.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Snapshots Encryption for DR Logs & Artifacts
  rationale: Encrypting snapshots used for disaster recovery (DR) ensures that sensitive data remains protected even in backup and recovery scenarios, reducing the risk of unauthorized data access. This practice is crucial for meeting compliance requirements such as PCI-DSS and HIPAA, which mandate encryption of data at rest to protect customer data and maintain privacy standards.
  description: This rule verifies that all Compute Engine snapshots, particularly those containing disaster recovery logs and artifacts, are encrypted using Google-managed or customer-managed encryption keys. To comply, navigate to the GCP Console, select 'Snapshots', and ensure the 'Encryption' field indicates the use of a valid key. Remediation involves configuring existing snapshots to use encryption keys and ensuring new snapshots are automatically encrypted.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/security/encryption/default-encryption
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.snapshot.dr_restore_roles_least_privilege
  service: compute
  resource: snapshot
  requirement: DR Restore Roles Least Privilege
  scope: compute.snapshot.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Snapshot Restoration Roles
  rationale: Assigning the least privilege necessary for snapshot restoration reduces the risk of unauthorized access to sensitive data and potential data loss. Over-privileged roles can lead to accidental or malicious data exposure, violating compliance with regulations like GDPR and HIPAA, and increasing the attack surface of your cloud infrastructure.
  description: This rule checks if roles assigned for snapshot restoration have the minimum permissions required. Ensure that only essential personnel have the ability to restore snapshots by using custom roles or predefined roles with limited permissions. Regularly audit IAM policies to verify compliance with the principle of least privilege and remove excessive permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-custom-roles
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.compute.snapshot.public_access_blocked
  service: compute
  resource: snapshot
  requirement: Public Access Blocked
  scope: compute.snapshot.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Public Access is Disabled for Compute Snapshots
  rationale: Snapshots containing system and user data, if exposed publicly, can lead to unauthorized data access, potentially resulting in data breaches, non-compliance with data protection regulations, and reputational damage. Securing snapshots is essential to prevent malicious actors from exploiting sensitive information and to maintain the integrity and confidentiality of critical business data.
  description: This rule checks for Google Cloud Compute snapshots that are configured with public access permissions. Snapshots should not be publicly accessible as they may contain sensitive information. To verify, review the snapshot's IAM policies and ensure that no public users (i.e., 'allUsers' or 'allAuthenticatedUsers') have access. Remediation involves updating the IAM policy to restrict access to only necessary and authorized accounts or groups.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0 - 5.3
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 5 CM-8
  - PCI-DSS Requirement 1.2.1
- rule_id: gcp.compute.snapshot.public_snapshot_access_restricted
  service: compute
  resource: snapshot
  requirement: Public Snapshot Access Restricted
  scope: compute.snapshot.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Restrict Public Access to Compute Snapshots
  rationale: Allowing public access to Compute Engine snapshots exposes sensitive data, leading to potential data breaches and unauthorized data manipulation. It violates compliance requirements such as PCI-DSS and HIPAA, which mandate stringent access controls. Restricting access minimizes the risk of data leakage and ensures data integrity essential for business continuity.
  description: This rule checks for any Google Cloud Compute snapshots with public access permissions. Snapshots should be configured with private access, employing Identity and Access Management (IAM) policies to limit visibility and modifications to authorized users only. Remediation involves reviewing and modifying IAM policies to remove 'allUsers' or 'allAuthenticatedUsers' roles from snapshot resources. Use the Google Cloud Console or gcloud CLI to adjust permissions and confirm settings.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.compute.snapshot.rdp_port_3389_blocked
  service: compute
  resource: snapshot
  requirement: Rdp Port 3389 Blocked
  scope: compute.snapshot.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure RDP Port 3389 is Blocked on Compute Snapshots
  rationale: Exposing RDP port 3389 can lead to unauthorized access and potential data breaches, especially if sensitive information is stored on instances. Blocking this port mitigates the risk of brute force attacks and unauthorized access, aligning with compliance and governance standards that require minimizing unnecessary exposure of services.
  description: This check ensures that RDP port 3389 is not open on any snapshots of compute instances. Snapshots should be configured to block inbound traffic on port 3389 to prevent unauthorized remote desktop access. Verify by reviewing firewall rules associated with the snapshots and adjust the rules to deny access on this port. Remediation involves updating firewall settings to explicitly block port 3389 for all instances derived from these snapshots.
  references:
  - https://cloud.google.com/compute/docs/firewalls
  - https://www.cisecurity.org/cis-benchmarks/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/instances/windows/connecting-rdp
- rule_id: gcp.compute.ssl_policy.balancing_insecure_ssl_ciphers_configured
  service: compute
  resource: ssl_policy
  requirement: Balancing Insecure SSL Ciphers Configured
  scope: compute.ssl_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Configure Secure SSL Ciphers in SSL Policies
  rationale: Using insecure SSL ciphers can expose your application to vulnerabilities such as man-in-the-middle attacks, compromising data confidentiality and integrity. Organizations must adhere to regulatory standards, like PCI-DSS and ISO 27001, which mandate the use of strong encryption methods to protect sensitive information. Proper configuration of SSL policies helps mitigate risks associated with outdated encryption protocols.
  description: This rule checks if SSL policies in Google Cloud's Compute Engine are configured to use secure ciphers only. To verify, ensure that your SSL policies exclude weak ciphers such as RC4 or any ciphers using less than 128-bit encryption. Remediation involves updating the SSL policy to include only strong ciphers, which can be done via the Google Cloud Console or gcloud command-line tool. This ensures compliance with security best practices and regulatory requirements.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.ssllabs.com/projects/best-practices/
- rule_id: gcp.compute.ssl_policy.distributions_using_deprecated_ssl_protocols_configured
  service: compute
  resource: ssl_policy
  requirement: Distributions Using Deprecated SSL Protocols Configured
  scope: compute.ssl_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SSL Policies Avoid Deprecated Protocols
  rationale: Using deprecated SSL protocols can expose your systems to vulnerabilities such as man-in-the-middle attacks, potentially leading to data breaches and loss of customer trust. Compliance with industry standards and regulations, like PCI-DSS and NIST, often requires the use of secure protocols to protect sensitive data in transit.
  description: This rule checks for SSL policies in Google Cloud Platform's Compute service that are configured with deprecated SSL protocols. Deprecated protocols lack modern security features and are more susceptible to exploitation. To mitigate this risk, update your SSL policies to use only supported and secure protocols such as TLS 1.2 or 1.3. Verification can be done by reviewing the SSL policy settings in the GCP Console or using the gcloud command-line tool. Remediation involves modifying the SSL policy to exclude deprecated protocols.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.subnetwork.data_warehouse_subnet_group_private_subnets_only
  service: compute
  resource: subnetwork
  requirement: Data Warehouse Subnet Group Private Subnets Only
  scope: compute.subnetwork.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Data Warehouse Subnets Are Private Only
  rationale: Limiting Data Warehouse subnets to private IP addresses minimizes exposure to the internet, reducing risk of unauthorized access and data breaches. This is crucial for maintaining data confidentiality and meeting compliance requirements like PCI-DSS and GDPR, which mandate strict controls over access to sensitive data.
  description: This rule checks that subnets within the Data Warehouse subnet group are configured to use private IP ranges only. Verify that 'IPV6 access type' is set to 'INTERNAL' and no external IP addresses are assigned. To remediate, ensure that all subnets in the Data Warehouse group are created with 'Private Google Access' enabled and do not attach external IPs to any instances within these subnets.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.subnetwork.logs_enabled
  service: compute
  resource: subnetwork
  requirement: Logs Enabled
  scope: compute.subnetwork.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Subnetwork Flow Logs in GCP Compute Engine
  rationale: Enabling flow logs for subnetworks is crucial for monitoring network traffic, detecting anomalies, and identifying potential security threats. It provides visibility into network flows, aiding in incident response and forensic investigations. Moreover, it supports compliance with regulatory standards that require detailed logging of network activity.
  description: This rule checks if flow logs are enabled for GCP Compute Engine subnetworks. Flow logs capture metadata about network traffic, which can be used for analysis and monitoring. To verify, navigate to the Google Cloud Console, select 'VPC networks', and check the 'Flow logs' status for each subnetwork. To enable flow logs, edit the subnetwork settings and set 'Flow logs' to 'On'.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.compute.subnetwork.network_subnet_private_subnets_no_igw_route
  service: compute
  resource: subnetwork
  requirement: Network Subnet Private Subnets No Igw Route
  scope: compute.subnetwork.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure No Internet Gateway Route for Private Subnets
  rationale: Allowing internet access to private subnets via an internet gateway route can lead to unauthorized data exposure and potential attacks from public networks. This exposure can increase the risk of data breaches, non-compliance with regulations like PCI-DSS, and violation of internal security policies. Maintaining strict control over network routes helps preserve data confidentiality and integrity.
  description: This rule checks that private subnets in GCP do not have routes directing traffic through an internet gateway. Ensure that private subnets only communicate through internal or VPN/Interconnect routes, avoiding public internet exposure. To verify, inspect the routing table of each subnetwork and ensure that no route with a destination of 0.0.0.0/0 is pointing to an internet gateway. As a remediation step, remove any such routes and restrict the route table to internal addresses only.
  references:
  - https://cloud.google.com/vpc/docs/routes
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/vpc/docs/vpc-best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.subnetwork.network_subnet_public_subnets_use_nacl_restrictions
  service: compute
  resource: subnetwork
  requirement: Network Subnet Public Subnets Use Nacl Restrictions
  scope: compute.subnetwork.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Enforce Network ACLs on Public Subnets
  rationale: Public subnets without proper Network ACLs pose significant security risks, as they can allow unauthorized access to resources within the network. This can lead to data breaches, unauthorized data exfiltration, and potential non-compliance with regulatory frameworks such as PCI-DSS and GDPR which mandate strict access controls. Ensuring that public subnets are protected by Network ACLs mitigates these risks by controlling inbound and outbound traffic effectively.
  description: This rule checks that all public subnets in your GCP environment have Network ACLs (Access Control Lists) applied to restrict unauthorized access. Network ACLs should be configured to allow only the necessary traffic to and from your public subnets. To verify, ensure that each public subnet has an ACL applied with rules that define explicit allow and deny permissions. Remediation involves creating or updating ACLs to include rules that limit exposure to critical resources by denying unnecessary traffic.
  references:
  - https://cloud.google.com/vpc/docs/vpc#subnet-ranges
  - https://cloud.google.com/vpc/docs/firewalls#firewall_rules_implementation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.compute.subnetwork.network_subnet_route_table_association_present
  service: compute
  resource: subnetwork
  requirement: Network Subnet Route Table Association Present
  scope: compute.subnetwork.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Subnet Route Table Association in GCP
  rationale: Associating a subnet with a route table is critical for defining how network traffic is directed within your GCP environment. Without proper association, there could be a risk of unauthorized access or data exfiltration as traffic routes may not comply with security policies. This is essential for maintaining compliance with regulatory frameworks that require stringent network access controls.
  description: This rule checks that each Google Cloud subnetwork is associated with a route table, ensuring that traffic is routed according to defined security policies. To verify, review the subnet configurations in the GCP Console under VPC Network settings to ensure route tables are assigned correctly. Remediation involves associating the missing route table with the subnetwork to ensure all ingress and egress traffic is appropriately controlled.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/network-connectivity/docs
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.target_https_proxy.balancing_ssl_listeners_configured
  service: compute
  resource: target_https_proxy
  requirement: Balancing SSL Listeners Configured
  scope: compute.target_https_proxy.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure SSL Listeners Configured for HTTPS Proxies
  rationale: Configuring SSL listeners for target HTTPS proxies is critical to encrypt data in transit, preventing unauthorized interception and data breaches. Without SSL, data is vulnerable to man-in-the-middle attacks, potentially leading to sensitive information exposure. This configuration also aligns with compliance requirements for data protection and privacy, such as PCI-DSS and HIPAA.
  description: This rule checks that all target HTTPS proxies have SSL listeners properly configured to ensure secure communication. To verify, inspect the target HTTPS proxy settings in your GCP environment and ensure that SSL certificates are applied to each listener. Remediation involves updating the proxy configuration to include valid SSL certificates and enforcing HTTPS for all incoming requests.
  references:
  - https://cloud.google.com/load-balancing/docs/https/
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/
  - https://cloud.google.com/security/encryption-in-transit
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.url_map.alb_https_redirect_configured
  service: compute
  resource: url_map
  requirement: Alb HTTPS Redirect Configured
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure HTTPS Redirect is Configured for ALB URL Maps
  rationale: Configuring HTTPS redirects for Application Load Balancer URL maps ensures that all traffic to your web applications is encrypted, protecting sensitive data from eavesdropping and man-in-the-middle attacks. This is essential for maintaining data confidentiality, integrity, and securing your applications against potential threats. It also helps in meeting compliance requirements such as PCI-DSS which mandate secure transmission of data.
  description: This rule checks if URL maps in Google Cloud's Application Load Balancer have HTTPS redirects configured. Without this configuration, traffic can be transmitted over insecure HTTP, exposing it to interception. To verify, inspect each URL map configuration for HTTPS redirect settings. Remediation involves updating the URL map to enforce HTTPS by setting up appropriate redirect response configurations within the load balancing settings.
  references:
  - https://cloud.google.com/load-balancing/docs/https/setting-up-https-redirect
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.url_map.edge_cache_policy_allowlists_minimal_query_headers_cookies
  service: compute
  resource: url_map
  requirement: Edge Cache Policy Allowlists Minimal Query Headers Cookies
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Edge Cache Policy for Minimal Query Headers and Cookies
  rationale: Allowing excessive query headers and cookies in an edge cache policy can expose sensitive information, increase the attack surface, and lead to cache poisoning attacks. Restricting this ensures that only the necessary data is cached, reducing the risk of unauthorized access and data breaches, while complying with security frameworks like PCI-DSS and HIPAA which mandate minimal data exposure.
  description: This rule checks that the Edge Cache Policy in `url_map` resources minimizes the use of query headers and cookies, allowing only essential ones. Verify that the configuration explicitly specifies which headers and cookies are permitted. Remediation involves reviewing the `url_map` configuration and updating the `edge_cache_policy` to restrict query headers and cookies to only those required for functionality. This reduces potential security vulnerabilities and limits data exposure.
  references:
  - https://cloud.google.com/load-balancing/docs/https/edge-cache
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.compute.url_map.edge_cache_policy_no_sensitive_headers_cached
  service: compute
  resource: url_map
  requirement: Edge Cache Policy No Sensitive Headers Cached
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Caching of Sensitive Headers in Edge Cache Policies
  rationale: Caching sensitive headers at the edge can expose confidential information such as authentication tokens or personal data, increasing the risk of unauthorized access and data breaches. This practice can compromise user privacy and violate compliance standards like GDPR, PCI-DSS, and HIPAA, leading to potential legal penalties and reputational damage.
  description: This rule checks if URL maps in GCP Compute Engine have edge cache policies that improperly cache sensitive headers. Configuration should exclude headers like Authorization or Set-Cookie from being cached. To verify, inspect the edge cache configuration within each URL map and ensure sensitive headers are omitted. Remediation involves updating the edge cache settings to exclude sensitive headers and aligning with best practices for secure data handling.
  references:
  - https://cloud.google.com/load-balancing/docs/https/traffic-management#cache-key-policy
  - https://cloud.google.com/security/compliance/cis#gcp-compute-engine-2-0
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices#data_protection
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.url_map.edge_cache_policy_respects_cache_control_no_store_private
  service: compute
  resource: url_map
  requirement: Edge Cache Policy Respects Cache Control No Store Private
  scope: compute.url_map.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Edge Cache Policy Honors Cache Control Directives
  rationale: Respecting Cache-Control directives such as 'no-store' and 'private' is crucial to prevent sensitive data from being cached at the edge, which can lead to unauthorized data access or data leaks. This practice minimizes the risk of exposing private information and aligns with privacy regulations and security best practices, ensuring that end-user data is handled securely.
  description: 'This rule verifies that the Edge Cache Policy for URL Maps in Google Cloud respects ''Cache-Control: no-store'' and ''Cache-Control: private'' directives. To check, ensure that URL Maps are correctly configured to prevent caching of sensitive data when these directives are present. Remediation involves adjusting Edge Cache Settings in the URL Map to comply with these directives, preventing potentially sensitive information from being stored in edge caches.'
  references:
  - https://cloud.google.com/load-balancing/docs/https/edge-cache
  - https://cloud.google.com/architecture/best-practices-for-content-delivery#cache-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org
  - https://www.hipaajournal.com
- rule_id: gcp.compute.url_map.edge_origin_request_policy_forward_cookies_minimal
  service: compute
  resource: url_map
  requirement: Edge Origin Request Policy Forward Cookies Minimal
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Minimal Cookie Forwarding in URL Map Policy
  rationale: Minimizing cookie forwarding in URL Map policies reduces the risk of exposing sensitive data to unintended endpoints, thereby mitigating potential lateral movement or data breaches. This control is crucial for maintaining a secure and compliant environment, especially under regulations like GDPR, which emphasize data protection and privacy.
  description: This rule verifies that URL Map configurations in GCP's Compute Engine enforce minimal cookie forwarding settings. This involves checking that only essential cookies are forwarded to the origin, preventing unnecessary exposure of sensitive information. To remediate, configure the URL Map to restrict cookie forwarding by selecting the 'Minimal' option under 'Edge Origin Request Policy'. Verification can be done through the Google Cloud Console or gcloud CLI by reviewing URL Map settings.
  references:
  - https://cloud.google.com/load-balancing/docs/https/https-load-balancer-url-map
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.compute.url_map.edge_origin_request_policy_forward_headers_minimal
  service: compute
  resource: url_map
  requirement: Edge Origin Request Policy Forward Headers Minimal
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Minimize Forwarded Headers in Edge Origin Requests
  rationale: Minimizing the headers forwarded in edge origin requests reduces the risk of exposing sensitive information and limits the attack surface. This is crucial for maintaining secure communication channels and protecting data integrity. Ensuring minimal header forwarding aligns with best practices for data protection and supports compliance with privacy regulations such as GDPR and CCPA.
  description: This rule checks that Compute Engine URL Maps are configured to forward only necessary headers to origins. To verify, inspect the URL Map settings in the GCP Console or via gcloud CLI to ensure the 'edgeSecurityPolicy' is set to forward minimal headers. Remediation involves adjusting the policy to exclude unnecessary headers, thus enhancing data privacy and security.
  references:
  - https://cloud.google.com/load-balancing/docs/https/policies
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.compute.url_map.edge_origin_request_policy_forward_querystrings_minimal
  service: compute
  resource: url_map
  requirement: Edge Origin Request Policy Forward Querystrings Minimal
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Minimal Forwarding of Querystrings in Edge Origin Requests
  rationale: Forwarding unnecessary querystrings in edge origin requests can expose sensitive data and lead to security vulnerabilities such as information leakage. This configuration minimizes data exposure, reducing the risk of attacks such as query injection and helps maintain compliance with data protection regulations by ensuring only essential data is transmitted.
  description: This rule checks whether your GCP URL Maps are configured to forward only the minimal necessary set of querystrings in edge origin requests. To verify, review the URL Maps in your GCP project and ensure the 'forwardQueryString' setting is minimized. Remediation involves modifying the URL Map configuration to restrict querystring forwarding to only those necessary for application functionality, which can be done through the GCP Console or gcloud command line tool.
  references:
  - https://cloud.google.com/load-balancing/docs/https/setting-up-https-server#url-map
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/docs/security
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.url_map.edge_origin_request_policy_no_secret_headers_forwarded
  service: compute
  resource: url_map
  requirement: Edge Origin Request Policy No Secret Headers Forwarded
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Prevent Forwarding Secret Headers in URL Map Policies
  rationale: Forwarding secret headers in URL map configurations can expose sensitive information to unauthorized parties, leading to data breaches and compliance violations. Proper handling of such headers is crucial to prevent threat actors from exploiting them to gain unauthorized access or perform malicious actions, aligning with compliance requirements like PCI-DSS and HIPAA.
  description: This rule checks whether URL map configurations in GCP's Compute Engine are forwarding secret headers to the origin. The absence of restrictions on forwarding secret headers can lead to security vulnerabilities. To ensure security, verify the URL map's edge origin request policies to ensure no secret headers are forwarded. Remediation involves updating the URL map configuration to explicitly disallow forwarding of sensitive headers.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/load-balancing/docs/https
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.url_map.edge_regex_set_no_overly_broad_patterns
  service: compute
  resource: url_map
  requirement: Edge Regex Set No Overly Broad Patterns
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure URL Maps Do Not Use Overly Broad Regex Patterns
  rationale: Overly broad regex patterns in URL maps can inadvertently expose sensitive endpoints, leading to unauthorized access and potential data breaches. This can increase the attack surface and complicate incident response efforts. Ensuring precise regex patterns aligns with security best practices and helps meet compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks URL maps for the presence of overly broad regex patterns that could match unintended paths. To verify, examine the 'pathMatcher' configuration in your URL map resources to ensure regex patterns are specific and targeted. Remediation involves refining regex patterns to only match the intended paths, reducing potential exposure of sensitive resources.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map-concepts
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.url_map.edge_regex_set_not_empty
  service: compute
  resource: url_map
  requirement: Edge Regex Set Not Empty
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge Regex Set is Defined for URL Maps
  rationale: Defining an edge regex set for URL maps in GCP is crucial to controlling traffic routing accurately. Without it, misrouted traffic could lead to security vulnerabilities, such as exposure to unauthorized access or data leakage. Proper configuration helps in meeting compliance requirements like PCI-DSS by ensuring data flows through secure and verified paths.
  description: This rule checks that the edge regex set for URL maps is not empty, ensuring that traffic is routed according to defined policies. To verify, inspect the URL map configuration in the GCP Console and ensure that a regex set is specified for edge routing. Remediation involves updating the URL map to include a comprehensive regex set that aligns with your routing and security policies, preventing potential misrouting or unauthorized access.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map-concepts
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.url_map.edge_regex_set_used_by_at_least_one_rule
  service: compute
  resource: url_map
  requirement: Edge Regex Set Used By At Least One Rule
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Regex Sets Are Utilized in URL Map Rules
  rationale: Utilizing edge regex sets in URL maps enhances the security and performance of load-balanced applications by enabling more precise routing and traffic management. Failure to employ regex can result in inefficient routing rules, potential exposure to malformed requests, and reduced security posture. This practice aligns with regulatory requirements for protecting data in transit and maintaining a secure configuration baseline.
  description: This rule checks if at least one rule in a URL map utilizes an edge regex set for matching requests. An edge regex set allows for advanced pattern matching, ensuring that traffic is directed based on specific and complex criteria. To verify, review the load balancer configuration for URL maps and ensure regex patterns are used where applicable. Remediation involves updating the URL map to include regex sets in the rules to improve traffic management and security.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.compute.url_map.edge_rule_group_no_permit_all_rule
  service: compute
  resource: url_map
  requirement: Edge Rule Group No Permit All Rule
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Prevent Open Access in Edge Rule Groups for URL Maps
  rationale: Allowing unrestricted access in edge rule groups can expose critical application endpoints to potential attacks such as DDoS and unauthorized data access. Controlling access ensures that only legitimate traffic reaches your applications, thereby mitigating security risks and helping comply with industry standards like PCI-DSS and ISO 27001.
  description: This rule checks for 'permit all' configurations in edge rule groups of URL maps within your Google Cloud Platform environment. A 'permit all' rule can lead to exposing your application to the internet without restrictions, leading to security vulnerabilities. To remediate, configure specific allow or deny rules based on IP ranges, headers, or other attributes to limit access. Verification can be done by reviewing the URL map edge rule groups in the GCP Console or using gcloud commands to ensure no overly permissive rules exist.
  references:
  - https://cloud.google.com/load-balancing/docs/https#url_map
  - CIS GCP Foundation Benchmark v1.3.0 - Section 7.2
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.compute.url_map.edge_rule_group_references_only_approved_managed_sets_w_used
  service: compute
  resource: url_map
  requirement: Edge Rule Group References Only Approved Managed Sets W Used
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure URL Maps Reference Approved Managed Sets
  rationale: Using only approved managed sets in URL Maps helps mitigate the risk of unauthorized access and configuration drift, which can lead to potential exposure of sensitive data and service disruption. This practice supports maintaining consistent security policies across the cloud environment, aligns with compliance requirements, and helps prevent misconfigurations that could be exploited by attackers.
  description: This rule checks that URL Maps in your GCP project reference only approved managed sets for edge rule groups. By enforcing this control, organizations ensure that only vetted and authorized configurations are applied, reducing the risk of security vulnerabilities. To verify, inspect the URL Maps configurations to confirm they are exclusively linked to the pre-approved sets. Remediate by updating URL Maps to reference only those managed sets that have been reviewed and approved by your security team.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/blog/products/identity-security/ensuring-compliance-in-the-cloud
- rule_id: gcp.compute.url_map.edge_rule_group_unique_priorities
  service: compute
  resource: url_map
  requirement: Edge Rule Group Unique Priorities
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Unique Priorities for Edge Rule Groups in URL Maps
  rationale: Unique priorities for edge rule groups within a URL map are crucial to preventing conflicts and ensuring predictable routing behavior. This reduces the risk of misrouting traffic, which can lead to application downtime, loss of service availability, and potential security vulnerabilities. Additionally, maintaining unique priorities aligns with regulatory requirements for system integrity and availability.
  description: This rule checks that each edge rule group within a Google Cloud Platform URL map has a unique priority. Edge rule groups with non-unique priorities can cause conflicts in traffic routing, leading to unexpected behavior. To verify, review the URL map configuration and ensure each rule group has a distinct priority. Remediation involves adjusting the priorities so that no two edge rule groups share the same value, ensuring proper traffic flow and application functionality.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.url_map.edge_rule_group_visibility_config_enabled
  service: compute
  resource: url_map
  requirement: Edge Rule Group Visibility Config Enabled
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Edge Rule Group Visibility for URL Maps
  rationale: Enabling Edge Rule Group Visibility for URL Maps is crucial for maintaining transparency and control over HTTP(S) load balancing configurations. This setting helps detect unauthorized changes and ensures that traffic rules align with organizational policies, reducing the risk of misconfigurations that could lead to data breaches or service disruptions. Furthermore, it supports compliance with regulatory standards by providing a clear audit trail of load balancing configurations.
  description: This check verifies that the Edge Rule Group Visibility Config is enabled for URL Maps within Google Cloud's HTTP(S) Load Balancing. When enabled, this feature allows administrators to see and audit the rules applied at the edge, ensuring proper configuration and facilitating troubleshooting. To enable this setting, navigate to the Google Cloud Console, select the appropriate URL Map, and activate the visibility configuration under the advanced settings. This action enhances visibility and management of traffic routing rules.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/load-balancing/docs/https
- rule_id: gcp.compute.url_map.edge_rule_has_effective_action_not_count_only
  service: compute
  resource: url_map
  requirement: Edge Rule Has Effective Action Not Count Only
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure URL Map Edge Rule Has Action Beyond Count Only
  rationale: Using 'count only' as the sole action in URL map edge rules can lead to inadequate response measures against potential threats. This configuration might result in missed opportunities for automated security enforcement, increasing the risk of malicious traffic going unchecked and potentially impacting application availability and data integrity.
  description: This rule checks URL map edge rules to ensure that they include actions beyond 'count only'. Edge rules should be configured with effective actions such as 'deny', 'redirect', or 'allow' to actively manage and mitigate malicious traffic. To verify, inspect the URL map configurations in the GCP Console or via gcloud CLI for edge rules without effective actions and update them accordingly to enhance security measures.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.url_map.edge_rule_priority_unique_within_web_acl
  service: compute
  resource: url_map
  requirement: Edge Rule Priority Unique Within Web ACL
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Unique Edge Rule Priority in URL Maps
  rationale: Unique priorities for edge rules within a URL map are crucial to ensuring predictable traffic routing and security rule application. Duplicate priorities can lead to conflicts and unexpected behavior, potentially exposing services to unauthorized access or data leakage. Maintaining unique priorities helps in complying with organizational security policies and regulatory requirements by ensuring clarity and control over network traffic management.
  description: This rule checks that each edge rule within a URL map in Google Cloud has a unique priority. URL maps are used to route requests to appropriate backend services, and edge rules define specific behaviors based on request attributes. Conflicting priorities can lead to ambiguous routing decisions, so it is important to assign distinct priorities to each rule. To remediate, review and update the URL map configurations to ensure each edge rule has a unique priority value, and test routing behavior post-update.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map-concepts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.url_map.edge_rule_references_valid_ip_or_regex_sets_only
  service: compute
  resource: url_map
  requirement: Edge Rule References Valid Ip Or Regex Sets Only
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Validate IP/Regex Sets in URL Map Edge Rules
  rationale: Ensuring edge rules in URL maps reference valid IP or regex sets helps prevent unauthorized access and potential misconfigurations that could lead to exposure of sensitive data. This is crucial for maintaining the integrity of network traffic management and aligning with security best practices. Proper validation supports compliance with regulatory standards by ensuring that only intended traffic patterns are matched and processed.
  description: This rule checks that edge rules in GCP URL maps only reference valid IP or regex sets. Invalid references can lead to unintended access or misdirected traffic, compromising the security of applications. To verify compliance, review your URL map configurations in the Google Cloud Console or via the gcloud command-line tool, ensuring all edge rules are correctly referencing existing and authorized IP or regex sets. Remediate by updating any invalid references to point to correct and authorized sets.
  references:
  - https://cloud.google.com/load-balancing/docs/https/https-load-balancer-url-map
  - https://cloud.google.com/load-balancing/docs/https/setting-up-https
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-cloud-security-controls
- rule_id: gcp.compute.url_map.network_lb_access_logs_enabled
  service: compute
  resource: url_map
  requirement: Network Lb Access Logs Enabled
  scope: compute.url_map.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Network Load Balancer Access Logs
  rationale: Enabling access logs for Network Load Balancers provides critical visibility into traffic patterns and potential security threats, allowing for timely detection and response to unauthorized access attempts or anomalies. This is essential for compliance with auditing requirements and helps in forensic investigations and operational oversight.
  description: This rule checks whether access logging is enabled for Network Load Balancers using URL maps in Google Cloud Platform. Logs capture important details about incoming requests, which can be used for monitoring and troubleshooting. To enable logging, ensure that the 'logConfig' field is set within the URL map's configuration in the Google Cloud Console or via gcloud CLI. Regularly review logs to identify any suspicious activity and maintain a secure environment.
  references:
  - https://cloud.google.com/load-balancing/docs/network/network-logging
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/security-foundations
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.compute.url_map.network_lb_listener_tls_min_1_2
  service: compute
  resource: url_map
  requirement: Network Lb Listener TLS Min 1 2
  scope: compute.url_map.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Use TLS 1.2 or Higher for Network LB Listener on URL Maps
  rationale: Ensuring TLS 1.2 or higher for Network Load Balancer listeners mitigates risks from outdated encryption protocols, protecting data in transit from interception and tampering. This is essential for maintaining the confidentiality and integrity of data, meeting compliance standards such as PCI-DSS and HIPAA, and preventing potential data breaches.
  description: This rule checks that Network Load Balancer listeners associated with URL maps enforce a minimum TLS version of 1.2. TLS 1.0 and 1.1 have known vulnerabilities that can be exploited by attackers. To verify, inspect the URL map configurations in the GCP Console or via the gcloud CLI, ensuring 'minTlsVersion' is set to 'TLS_1_2'. If not compliant, update the configuration to enforce TLS 1.2 or higher to ensure secure communications.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
- rule_id: gcp.compute.url_map.network_lb_valid_certificate_attached
  service: compute
  resource: url_map
  requirement: Network Lb Valid Certificate Attached
  scope: compute.url_map.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Valid TLS Certificate on GCP Network Load Balancer
  rationale: Ensuring that a valid TLS certificate is attached to a Google Cloud Platform (GCP) Network Load Balancer is critical for protecting data in transit. Without a valid certificate, sensitive data may be exposed to interception or tampering, leading to potential data breaches and non-compliance with standards such as PCI-DSS and HIPAA, which require secure data transmission.
  description: This rule checks that a valid TLS certificate is attached to the URL map of a Network Load Balancer in GCP. A URL map with an invalid or missing certificate can lead to unencrypted traffic, exposing data to security threats. To verify, ensure that the URL map configuration includes a valid SSL certificate. Remediation involves updating or attaching a valid certificate to the load balancer using the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-certificates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.compute.url_map.network_lb_waf_attached_where_supported
  service: compute
  resource: url_map
  requirement: Network Lb Waf Attached Where Supported
  scope: compute.url_map.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network LB WAF is Configured on Supported URL Maps
  rationale: Attaching a Web Application Firewall (WAF) to Network Load Balancer URL Maps mitigates security threats such as SQL injection and cross-site scripting by filtering and monitoring HTTP traffic. This reduces the risk of application-layer attacks, which can lead to data breaches or service disruptions, ensuring compliance with security frameworks like PCI-DSS and enhancing overall system resilience.
  description: This rule verifies that a Web Application Firewall is attached to URL Maps where supported within Network Load Balancer configurations. A WAF protects applications by monitoring and filtering HTTP traffic between a web application and the Internet. To ensure compliance, inspect the URL Map settings in the Google Cloud Console and confirm that a WAF is enabled for each applicable URL Map. Remediate by configuring a WAF via the Console or gcloud CLI to enhance security posture.
  references:
  - https://cloud.google.com/load-balancing/docs/https/setting-up-https#waf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security
  - https://cloud.google.com/armor/docs/rules-overview
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.vpn_tunnel.network_cgw_ip_addresses_valid_and_owned
  service: compute
  resource: vpn_tunnel
  requirement: Network Cgw Ip Addresses Valid And Owned
  scope: compute.vpn_tunnel.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Validate and Own VPN Tunnel Network CGW IP Addresses
  rationale: Ensuring the VPN tunnel's network Customer Gateway (CGW) IP addresses are valid and owned by your organization mitigates risks of unauthorized access and data breaches. It prevents misconfigurations that could lead to traffic interception or redirection by malicious actors. This is crucial for maintaining secure and compliant network architecture as required by standards such as PCI-DSS and HIPAA.
  description: This rule checks that the IP addresses configured for the VPN tunnel's CGW are valid and owned by your organization. Verify that these IPs are part of your allocated address space and not mistakenly using external or public IPs that could expose your network to unauthorized access. Remediation involves auditing your VPN configurations, updating any incorrect IPs, and ensuring they are registered and controlled by your organization.
  references:
  - https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-vpns
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-77.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.vpn_tunnel.network_cgw_no_public_management_interfaces_exposed
  service: compute
  resource: vpn_tunnel
  requirement: Network Cgw No Public Management Interfaces Exposed
  scope: compute.vpn_tunnel.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Prevent Public Exposure of VPN Tunnel Management Interfaces
  rationale: Exposing VPN tunnel management interfaces to the public internet increases the risk of unauthorized access, data breaches, and potential service disruptions. Such exposure can lead to compliance violations with frameworks like NIST and PCI-DSS, which require secure data transmission channels. Ensuring that management interfaces are only accessible from trusted networks mitigates these risks and protects sensitive data in transit.
  description: This rule checks that VPN tunnel management interfaces are not publicly accessible on Google Cloud Platform. It ensures that these interfaces are restricted to known and trusted networks by configuring appropriate firewall rules. Verification involves checking the inbound rules for the relevant VPN tunnel and confirming that no public IP addresses are allowed. Remediation includes updating firewall configurations to restrict access to internal or specific external IP ranges only.
  references:
  - https://cloud.google.com/vpn/docs/concepts/topologies#best_practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices/identity-access
- rule_id: gcp.compute.vpn_tunnel.network_health_monitoring_enabled
  service: compute
  resource: vpn_tunnel
  requirement: Network Health Monitoring Enabled
  scope: compute.vpn_tunnel.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Network Health Monitoring for VPN Tunnels
  rationale: Enabling network health monitoring for VPN tunnels is crucial for ensuring the reliability and performance of your network connections. Without it, potential issues such as packet loss or latency might go unnoticed, leading to disrupted services and potential downtime. This monitoring is also important for meeting compliance requirements that mandate continuous network oversight to ensure data integrity and availability.
  description: This rule checks whether network health monitoring is enabled for Google Cloud VPN tunnels. To verify, ensure that the VPN tunnel configuration includes health check settings that actively monitor tunnel status and performance metrics. Remediation involves configuring VPN tunnels with appropriate health check parameters via the GCP Console or using the gcloud command-line tool. This proactive approach helps in maintaining optimal performance and quickly identifying any issues that may arise.
  references:
  - https://cloud.google.com/network-connectivity/docs/vpn/how-to/monitor-vpn
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/monitoring
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.compute.vpn_tunnel.network_vpn_ike_phase_encryption_strong
  service: compute
  resource: vpn_tunnel
  requirement: Network Vpn Ike Phase Encryption Strong
  scope: compute.vpn_tunnel.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Strong Encryption for VPN IKE Phase
  rationale: Using strong encryption algorithms for IKE phases ensures data integrity and confidentiality during VPN communication. Weak encryption can expose data to interception and unauthorized access, leading to potential data breaches and non-compliance with regulations such as GDPR and HIPAA. Strong encryption is vital for protecting sensitive business data and maintaining trust with customers and partners.
  description: This rule checks that the VPN tunnel uses strong encryption algorithms, such as AES-256, for IKE phase 1 and phase 2. Proper configuration mitigates risks of data interception and unauthorized access. Verify configurations in the GCP Console under VPN configurations or through the gcloud command-line tool. Remediation involves updating the IKE phase settings to use strong encryption, ensuring both phases employ recommended algorithms.
  references:
  - https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-vpns
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-77r1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.vpn_tunnel.network_vpn_pre_shared_keys_rotation_policy_defined
  service: compute
  resource: vpn_tunnel
  requirement: Network Vpn Pre Shared Keys Rotation Policy Defined
  scope: compute.vpn_tunnel.network_security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure VPN Tunnel Pre-Shared Keys Rotation Policy is Defined
  rationale: Regular rotation of VPN tunnel pre-shared keys mitigates the risk of unauthorized access due to key compromise. This practice is essential for maintaining the confidentiality and integrity of data transmitted over VPN connections and is often a requirement for compliance with security standards such as PCI-DSS and NIST. Failing to rotate keys can lead to prolonged exposure and increased susceptibility to brute force attacks.
  description: This rule checks if a key rotation policy for VPN tunnel pre-shared keys is defined and enforced within GCP. To verify, ensure that the VPN tunnel configurations include a documented schedule for key rotation, ideally automating the process where possible. Implementing a policy involves setting a rotation frequency that aligns with your organization's security posture and regulatory requirements. Remediation includes configuring key rotation intervals in Google Cloud Console and using automation tools like Terraform for consistent enforcement.
  references:
  - https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-vpns
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-77.pdf
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.container.addon.containers_kubernetes_from_trusted_channel
  service: container
  resource: addon
  requirement: Containers Kubernetes From Trusted Channel
  scope: container.addon.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Runs From GCP Trusted Channels
  rationale: Running Kubernetes from trusted channels ensures that clusters receive timely security patches and updates, minimizing the risk of exploiting known vulnerabilities. This practice is crucial for maintaining the integrity and availability of containerized applications and aligns with regulatory requirements for maintaining secure infrastructure.
  description: This rule checks if GKE clusters are configured to use Kubernetes versions from GCP's trusted release channels, such as Rapid, Regular, or Stable. Configuring clusters to follow these channels ensures automatic updates and patches. To verify, inspect the GKE cluster settings in the Google Cloud Console or via gcloud CLI to confirm the channel configuration. Remediate by updating cluster settings to align with a trusted release channel.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/release-channels
  - https://cloud.google.com/kubernetes-engine/docs/how-to/creating-managing-release-channels
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.addon.containers_kubernetes_no_privileged_permissions
  service: container
  resource: addon
  requirement: Containers Kubernetes No Privileged Permissions
  scope: container.addon.least_privilege
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Prohibit Privileged Permissions for Kubernetes Add-ons
  rationale: Using privileged permissions in Kubernetes add-ons can expose the cluster to security risks, including unauthorized access to sensitive resources and potential privilege escalation attacks. Ensuring that add-ons operate with the least privilege necessary helps mitigate vulnerabilities and align with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that Kubernetes add-ons deployed in GCP do not run with privileged permissions. It verifies that the securityContext.privileged field is set to false in the Pod specifications of add-ons. Remediation involves auditing add-on configurations for privileged access and modifying them to adhere to the principle of least privilege by updating security settings to non-privileged modes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
- rule_id: gcp.container.addon.containers_kubernetes_version_pinned_and_supported
  service: container
  resource: addon
  requirement: Containers Kubernetes Version Pinned And Supported
  scope: container.addon.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Pin and Support Kubernetes Versions for Container Addons
  rationale: Pinning and supporting specific Kubernetes versions reduces the risk of vulnerabilities by ensuring that clusters run on versions that are actively maintained and patched. Unpinned or unsupported versions may expose clusters to security flaws, impacting business operations and leading to potential data breaches. This practice also helps in meeting compliance requirements by aligning with standards that mandate regular updates and patches.
  description: This rule verifies that Kubernetes versions for GCP container addons are pinned and supported. It checks if the clusters are running on a specific, supported version to ensure they receive security updates. Administrators should configure their GKE clusters to use supported Kubernetes versions by selecting a specific version and regularly updating to newer supported versions. This can be done via the GCP Console or gcloud CLI. Remediation involves reviewing the current version, comparing it against the supported list, and upgrading as necessary.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/release-notes
  - https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
- rule_id: gcp.container.cluster.abac_status_configured
  service: container
  resource: cluster
  requirement: Abac Status Configured
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure ABAC is Disabled for GKE Clusters
  rationale: Enabling ABAC (Attribute-Based Access Control) on GKE clusters can lead to security vulnerabilities as it allows overly permissive access to Kubernetes resources. Disabling ABAC and using RBAC (Role-Based Access Control) helps in minimizing access permissions, aligning with the principle of least privilege, and meeting compliance requirements like PCI-DSS and SOC2 by reducing the risk of unauthorized access.
  description: This rule checks if ABAC is disabled for Google Kubernetes Engine (GKE) clusters. By default, GKE uses RBAC, which is more secure and manageable. To verify, navigate to the GKE cluster settings in the Google Cloud Console and ensure that ABAC is turned off. Remediation involves disabling ABAC and configuring RBAC policies to control access to Kubernetes resources effectively.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#disable_abac
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.alpha_features_disabled_for_production
  service: container
  resource: cluster
  requirement: Alpha Features Disabled For Production
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Alpha Features in Production GKE Clusters
  rationale: Alpha features in Google Kubernetes Engine (GKE) are experimental and may not be stable or secure enough for production environments. Allowing these features increases the risk of potential vulnerabilities, instability, and non-compliance with certain security standards, which could lead to unauthorized access or data breaches.
  description: This rule checks if any GKE cluster in the environment is using alpha features, which are not recommended for production use due to their experimental nature. To ensure a stable and secure production environment, disable alpha features in all production clusters. Verify by reviewing the cluster configurations and ensure that no alpha APIs are enabled. If alpha features are present, consider migrating to stable or beta features where possible.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/alpha-features
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/standards_overview
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.and_nodepool_workload_identity_enforced
  service: container
  resource: cluster
  requirement: And Nodepool Workload Identity Enforced
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Workload Identity is Enforced for GKE Clusters and Nodepools
  rationale: Enforcing Workload Identity in GKE clusters and nodepools mitigates the risk of credential theft by ensuring that Google Kubernetes Engine (GKE) workloads run with specific and minimal IAM permissions. This approach reduces the attack surface by eliminating the need for long-lived credentials and helps in meeting compliance requirements by adhering to the principle of least privilege.
  description: This rule checks whether Workload Identity is enforced for GKE clusters and their associated nodepools. Workload Identity allows Kubernetes service accounts to act as IAM service accounts, providing secure access to Google Cloud services. To verify, ensure that the 'workloadIdentityConfig' field is configured with a valid 'identityNamespace'. Remediation involves updating the GKE cluster configuration to enable Workload Identity, using the command 'gcloud container clusters update CLUSTER_NAME --workload-pool=PROJECT_ID.svc.id.goog'.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview#workload-identity
  - CIS Google Kubernetes Engine Benchmark v1.0.0 - Section 6.10
  - NIST SP 800-53 Rev. 5 - AC-3 Access Enforcement
  - 'PCI DSS v3.2.1 - Requirement 7: Restrict access to cardholder data by business need to know'
  - ISO/IEC 27001:2013 - A.9.4.1 Information access restriction
- rule_id: gcp.container.cluster.autoscaling_enabled
  service: container
  resource: cluster
  requirement: Autoscaling Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Cluster Autoscaling is Enabled
  rationale: Enabling autoscaling for Kubernetes clusters ensures that resources are efficiently managed, automatically adjusting to load demands. This prevents over-provisioning and reduces costs while maintaining application performance. It also mitigates potential denial-of-service scenarios by ensuring sufficient resources during traffic surges, aligning with best practices for operational resilience and compliance with scalability requirements.
  description: This rule checks if autoscaling is enabled on GCP Kubernetes clusters. Autoscaling adjusts the number of nodes in a cluster based on workload and resource utilization metrics. To verify, ensure the `autoscaling` configuration is set in the cluster's node pool settings. Remediation involves enabling autoscaling by configuring the minimum and maximum number of nodes in the Google Cloud Console or via the `gcloud` command-line tool for existing clusters.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning
  - https://cloud.google.com/architecture/best-practices-for-scaling-your-app-on-gke
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.container.cluster.binary_authorization_enforced
  service: container
  resource: cluster
  requirement: Binary Authorization Enforced
  scope: container.cluster.authorization
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Binary Authorization is Enforced on GKE Clusters
  rationale: Binary Authorization in Google Kubernetes Engine (GKE) enhances security by ensuring only trusted container images are deployed. This mitigates the risk of deploying vulnerable or malicious images, protecting the integrity of the application. Enforcing Binary Authorization supports compliance with industry standards like PCI-DSS and SOC 2, which require strict control over software deployment processes.
  description: This rule checks whether Binary Authorization is enforced on GKE clusters. To verify, ensure that the `binaryAuthorization` setting is enabled in the cluster configuration. Enforcing Binary Authorization requires configuring attestors to validate container images against defined policies. Remediation involves updating the cluster settings to enable Binary Authorization and configuring policies that align with your security requirements.
  references:
  - https://cloud.google.com/binary-authorization/docs/enforce
  - https://cloud.google.com/kubernetes-engine/docs/how-to/binary-authorization
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.container.cluster.client_certificate_authentication_disabled
  service: container
  resource: cluster
  requirement: Client Certificate Authentication Disabled
  scope: container.cluster.authentication
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Disable Client Certificate Authentication on GKE Clusters
  rationale: Disabling client certificate authentication in Google Kubernetes Engine (GKE) is critical to mitigating risks associated with certificate-based attacks, including man-in-the-middle and unauthorized access. By relying on alternative authentication methods like IAM, organizations reduce complexity and adhere to security best practices, aligning with compliance frameworks such as NIST SP 800-53 and PCI-DSS 3.2.1.
  description: This rule checks if client certificate authentication is disabled for GKE clusters, a setting that prevents unauthorized access via compromised client certificates. To verify, inspect the cluster's authentication settings and ensure the 'clientCertificateConfig.issueClientCertificate' is set to 'false'. Remediation involves modifying the cluster configuration through the GCP Console or using 'gcloud' CLI to update the cluster settings, ensuring stronger security posture by leveraging IAM-based authentication.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.container.cluster.containers_kubernetes_admission_host_namespace_usage_denied
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Host Namespace Usage Denied
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Restrict Host Namespace Usage in GKE Clusters
  rationale: Allowing containers to share the host's namespace can lead to privilege escalation and potential exposure of sensitive host-level data. This configuration can be exploited by attackers to gain unauthorized access to the host system, posing a significant security risk. Ensuring that containers do not use the host's namespace helps maintain isolation and aligns with security best practices and compliance requirements.
  description: This rule checks that Kubernetes clusters in GCP do not allow containers to use the host's namespace, which is a potential security risk. Verify that the PodSecurityPolicy or similar admission controls are configured to deny the usage of 'hostPID', 'hostIPC', and 'hostNetwork'. To remediate, update your cluster's security policies to restrict these settings, ensuring that all container deployments adhere to these constraints.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#admission_controllers
  - https://cloud.google.com/kubernetes-engine/docs/concepts/pod-security-policies
  - CIS Google Kubernetes Engine (GKE) Benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
- rule_id: gcp.container.cluster.containers_kubernetes_admission_image_registry_allo_enforced
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Image Registry Allo Enforced
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Image Registry Admission Control in Kubernetes Clusters
  rationale: Enforcing image registry admission policies in Kubernetes clusters is crucial for ensuring that only trusted container images are deployed. This reduces the risk of introducing vulnerabilities from unverified sources, thereby protecting sensitive data and maintaining service integrity. Additionally, it helps in meeting compliance requirements related to software supply chain security.
  description: This rule checks whether Kubernetes clusters enforce admission controls to allow only images from approved registries. It involves setting up admission controllers or policies within the cluster to verify image sources during the deployment process. To implement, configure the Kubernetes admission controller to whitelist trusted registries and continuously monitor for policy compliance. This can be verified by reviewing the cluster's admission controller settings and ensuring they are configured correctly.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/pod-security-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://pci-dss.org/pci_security_standards/
  - https://cloud.google.com/architecture/best-practices-for-building-secure-gke-clusters
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
- rule_id: gcp.container.cluster.containers_kubernetes_admission_image_signature_veri_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Image Signature Veri Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Admission Image Signature Verification
  rationale: Enabling image signature verification in GCP Kubernetes clusters mitigates risks of deploying untrusted or malicious container images, which can lead to data breaches, service disruptions, and compliance violations. It ensures only signed and trusted images are deployed, aligning with security best practices and regulatory frameworks.
  description: This rule checks if Kubernetes clusters in GCP have image signature verification enabled, which is crucial for ensuring that only verified images are allowed to run. To verify, ensure that the policy controller is configured with the appropriate admission webhook to enforce signature checks. Remediation involves configuring the Binary Authorization to enforce image signature policies, thereby preventing unauthorized images from being deployed.
  references:
  - https://cloud.google.com/binary-authorization/docs/overview
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_admission_pod_security_admissi_default
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Pod Security Admissi Default
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Pod Security Admission for Clusters
  rationale: Enabling Kubernetes Pod Security Admission helps mitigate risks associated with unauthorized or unsafe pod configurations, reducing potential attack surfaces such as privilege escalation and resource exhaustion. It aligns with regulatory demands for secure container operations and aids in maintaining a compliant and secure Kubernetes environment, crucial for safeguarding sensitive data and business operations.
  description: This rule checks if Kubernetes Pod Security Admission is enabled in GCP Kubernetes clusters. Proper configuration helps enforce security policies at the pod level, preventing the deployment of pods that do not meet security requirements. To verify, ensure that the Pod Security Admission controller is set up in the GKE cluster configurations. Remediation involves configuring the PodSecurityPolicy in your Kubernetes Engine settings to restrict pod deployments based on specified security standards.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://snyk.io/blog/kubernetes-security-best-practices-cheat-sheet/
- rule_id: gcp.container.cluster.containers_kubernetes_admission_privilege_escalation_denied
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Privilege Escalation Denied
  scope: container.cluster.least_privilege
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Deny Privilege Escalation in Kubernetes Admission
  rationale: Preventing privilege escalation in Kubernetes containers is crucial to maintain the security boundaries of your applications. If a container can escalate its privileges, it could potentially gain access to sensitive data or control over other resources, leading to data breaches or service disruptions. This practice is important for meeting compliance requirements such as PCI-DSS and HIPAA, which mandate strict access controls.
  description: This rule checks if Kubernetes admission controllers deny privilege escalation in GKE clusters. It ensures that containers cannot gain additional privileges beyond their initial configuration, reducing the risk of unauthorized access. To verify, check that the PodSecurityPolicy or equivalent configurations are set to prevent privilege escalation. Remediation involves updating your Kubernetes admission policies to deny any containers attempting to request escalated privileges.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#admission_controllers
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_anonymous_auth_disabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Anonymous Auth Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Anonymous Auth on Kubernetes API Server
  rationale: Anonymous authentication allows unauthenticated access to the Kubernetes API server, posing a security risk by potentially exposing sensitive operations to unauthorized users. This can lead to unauthorized access, data breaches, and compromise of cluster integrity. Disabling anonymous authentication helps in maintaining strict access control, complying with security best practices and regulatory standards like CIS and NIST.
  description: This rule checks if the Kubernetes API server on GCP Kubernetes Engine clusters has anonymous authentication disabled. Anonymous access should be turned off to ensure that all requests to the API server are authenticated. Verification involves examining the 'enableAnonymousAuth' field in the 'kube-apiserver' configuration and ensuring it is set to 'false'. To remediate, modify the cluster configuration to disable anonymous authentication, typically through the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#anonymous-requests
  - https://cloud.google.com/kubernetes-engine/docs/concepts/using-rbac
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_audit_logging_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Audit Logging Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Kubernetes API Server Audit Logging
  rationale: Enabling Kubernetes API server audit logging is crucial for tracking access and activity within your GKE clusters, which can help identify potential security breaches and unauthorized access attempts. This measure supports compliance with industry standards like PCI-DSS and HIPAA by ensuring that detailed logs are available for auditing and incident investigation. It also helps in maintaining the integrity and security of applications running on GKE.
  description: This check verifies that audit logging is enabled for the Kubernetes API server in Google Kubernetes Engine (GKE) clusters. Audit logs provide a record of actions taken within your cluster, including access to the Kubernetes API. To enable audit logging, navigate to the Google Cloud Console, select Kubernetes Engine, and configure the cluster's logging settings to include audit logs. Ensure that logs are stored securely and monitored regularly for suspicious activity.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging
  - https://cloud.google.com/security/compliance/cis-kubernetes-benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/sites/default/files/hipaa-privacy-rule.pdf
  - https://cloud.google.com/security/best-practices
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_authorization_mode_rbac
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Authorization Mode RBAC
  scope: container.cluster.authorization
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable RBAC for Kubernetes API Server Authorization
  rationale: Implementing Role-Based Access Control (RBAC) for the Kubernetes API server is critical for minimizing unauthorized access and controlling who can perform specific actions within the cluster. This enhances the security posture by ensuring that permissions are granted based on the principle of least privilege, reducing the risk of privilege escalation and potential data breaches. It also aligns with compliance requirements for regulated industries.
  description: This rule checks if the Kubernetes API server in your GCP container clusters is configured to use RBAC for authorization. RBAC allows for fine-grained access control by defining roles and binding them to users or service accounts. To verify, ensure that the 'authorization-mode' parameter includes 'RBAC' in the API server configuration. Remediation involves updating your cluster's API server settings to include 'RBAC' in the authorization modes, which can be done via the GCP console or command line interface.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_etcd_connection_encrypted
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Etcd Connection Encrypted
  scope: container.cluster.encryption
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes API Server to Etcd Connections are Encrypted
  rationale: Encrypting connections between the Kubernetes API server and Etcd is crucial to protect sensitive data, such as cluster configurations and secrets, from interception and unauthorized access. Unencrypted connections can expose the cluster to man-in-the-middle attacks, increasing the risk of data breaches and compliance violations with standards like PCI-DSS and HIPAA.
  description: This rule checks whether the communication between the Kubernetes API server and Etcd is encrypted with Transport Layer Security (TLS). To verify, ensure that the '--etcd-certfile' and '--etcd-keyfile' flags are specified in the API server configuration. If not, update the API server configuration to include these flags with the correct certificate and key files. This ensures data integrity and confidentiality during transmission.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/hipaa
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_insecure_port_disabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Insecure Port Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Insecure Port on Kubernetes API Server
  rationale: Disabling the insecure port for the Kubernetes API server is crucial as it prevents unauthenticated access to the cluster, reducing the risk of unauthorized control or data exposure. This practice supports compliance with security frameworks requiring secure communication channels and protects against potential attack vectors targeting open, insecure ports.
  description: This rule checks if the Kubernetes API server on GCP container clusters has the insecure port (default 8080) disabled. It ensures that all API server communications occur over secure channels (HTTPS). To verify, review cluster configurations in GCP Console or use gcloud CLI to ensure the '--insecure-port' is set to '0'. Remediation involves updating the cluster configuration to set '--insecure-port=0' and ensuring all communications use the secure port (default 443).
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_tls_min_1_2_enforced
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver TLS Min 1 2 Enforced
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce TLS 1.2+ for Kubernetes API Server in GKE
  rationale: Enforcing TLS 1.2 or higher for the Kubernetes API server in GKE protects data-in-transit from interception and tampering, reducing the risk of man-in-the-middle attacks. This is critical for maintaining the confidentiality and integrity of the communication between clients and the API server, aligning with industry standards and regulatory requirements, such as PCI-DSS and NIST SP 800-52 for secure communications.
  description: This rule checks if the Kubernetes API server in a GKE cluster is configured to enforce a minimum of TLS 1.2 for secure communications. To verify, ensure the API server configuration in your GKE cluster specifies TLS 1.2 as the minimum version. Remediation involves updating the GKE cluster's API server settings to reject connections below TLS 1.2, which can be done via the `gcloud` command-line tool or the GCP Console by setting the appropriate security policies.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-clusters#using-tls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_audit_logging_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Audit Logging Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Kubernetes Audit Logging for GKE Clusters
  rationale: Enabling Kubernetes audit logging is crucial for tracking important events and changes within your GKE clusters. It helps in identifying unauthorized access attempts, policy violations, and potential security threats, thus supporting compliance with regulatory standards such as PCI-DSS and HIPAA. Without audit logs, detecting and investigating security incidents becomes significantly more challenging, increasing the risk of undetected breaches.
  description: This rule checks whether Kubernetes audit logging is enabled for Google Kubernetes Engine (GKE) clusters. Audit logging should be configured to capture all API requests and responses, which can be achieved by setting the 'enableKubernetesAuditLogging' field to true in the cluster's logging configuration. To verify, use the Google Cloud Console or gcloud command-line tool to ensure that audit logging is activated. Remediation involves updating the cluster's logging settings to include Kubernetes audit logs.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/kubernetes-engine/docs/best-practices
  - https://cloud.google.com/kubernetes-engine/docs/concepts/audit-logging
- rule_id: gcp.container.cluster.containers_kubernetes_controller_manager_root_ca__configured
  service: container
  resource: cluster
  requirement: Containers Kubernetes Controller Manager Root Ca Configured
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Ensure Kubernetes Controller Manager Root CA is Configured
  rationale: Proper configuration of the Kubernetes Controller Manager Root CA is crucial to maintaining the security integrity of GKE clusters. Failing to configure this can lead to unauthorized access or data breaches, as the root CA is a critical component in the authentication and encryption infrastructure. Compliance with standards like PCI-DSS and ISO 27001 often requires robust certificate management practices.
  description: This rule checks that the Kubernetes Controller Manager in GCP's GKE clusters has been configured with a valid Root CA, which is essential for secure communication and authentication within the cluster. Verify this setting by inspecting the Controller Manager's configuration for a properly defined root CA file. Remediation involves updating the Controller Manager settings to include a valid root CA file path, ensuring all certificates are correctly signed and trusted by the cluster.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/certificates/
- rule_id: gcp.container.cluster.containers_kubernetes_controller_manager_secure_port_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Controller Manager Secure Port Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Secure Port for Kubernetes Controller Manager
  rationale: Enabling the secure port for the Kubernetes Controller Manager helps protect sensitive communication within the Kubernetes control plane. It mitigates the risk of unauthorized access and data interception, which are crucial for maintaining the integrity and confidentiality of cluster operations. This configuration supports compliance with security standards that mandate encryption of sensitive data in transit.
  description: This rule checks if the Kubernetes Controller Manager in your GCP cluster is configured to use a secure port for communication. By default, the secure port (443) should be enabled to ensure that all data exchanged between components is encrypted. To verify, check the `--secure-port` flag in the Controller Manager's configuration. If not set to 443, update the configuration to enable this secure port, ensuring that the control plane communications are encrypted.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_secure_communication_channels
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
- rule_id: gcp.container.cluster.containers_kubernetes_controller_manager_use_service_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Controller Manager Use Service Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Controller Manager Service in GKE
  rationale: Enabling the Kubernetes Controller Manager service in Google Kubernetes Engine (GKE) is crucial for maintaining control over the cluster's state and ensuring that the desired configurations are implemented. Without this service, automated management of node and pod lifecycle processes could be disrupted, leading to potential security vulnerabilities and downtime. Compliance with security standards like CIS benchmarks often requires such services to be enabled to ensure proper cluster management and monitoring.
  description: This rule checks whether the Kubernetes Controller Manager service is enabled in your GKE cluster. The Controller Manager is responsible for managing controllers that regulate node and pod lifecycle actions. To verify, access your GKE cluster settings and ensure that the Controller Manager is active. Remediation involves enabling the service through the GCP Console or using the `gcloud` CLI to update the cluster configuration, ensuring continuous management of cluster operations.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#control_plane
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://kubernetes.io/docs/concepts/overview/components/#kube-controller-manager
- rule_id: gcp.container.cluster.containers_kubernetes_default_service_account_autom_disabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Default Service Account Autom Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Kubernetes Default Service Account for GKE Clusters
  rationale: Using the default service account in Kubernetes clusters can lead to privilege escalation and unauthorized access within Google Kubernetes Engine (GKE). Misuse of default service accounts may result in potential exposure of sensitive data or unauthorized actions by workloads. Ensuring that workloads do not automatically use the default service account helps in maintaining least privilege and reducing attack surface.
  description: This rule checks if the default Kubernetes service account is disabled for automatic use within GKE clusters. By default, Kubernetes assigns the default service account to workloads, which can inadvertently grant excessive permissions. To verify, ensure that workloads specify custom service accounts with minimal permissions. Remediation involves explicitly specifying a custom service account with the required permissions in pod specifications, and disabling the automatic mounting of default service accounts by setting the 'automountServiceAccountToken' to false.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa
  - https://cloud.google.com/kubernetes-engine/docs/concepts/service-accounts
  - CIS Google Kubernetes Engine Benchmark
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - NIST SP 800-190 Application Container Security Guide
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.containers_kubernetes_etcd_auth_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Etcd Auth Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Etcd Authentication in GKE Clusters
  rationale: Enabling authentication for etcd in Kubernetes clusters is crucial to protect sensitive data stored in etcd, including configuration information and secrets. Unauthorized access to etcd can result in data breaches, service disruptions, and compliance violations, particularly those related to data protection regulations like GDPR and HIPAA.
  description: This rule checks whether authentication is enabled for etcd in Google Kubernetes Engine (GKE) clusters. Etcd serves as a key-value store for Kubernetes cluster data, and enabling authentication ensures that only authorized users and services can access it. To verify, ensure that the GKE cluster is configured with etcd authentication enabled. Remediation involves setting up secure communication channels and restricting access to etcd endpoints using IAM roles and policies.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
- rule_id: gcp.container.cluster.containers_kubernetes_etcd_client_tls_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Etcd Client TLS Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Etcd Client TLS is Enabled on Clusters
  rationale: Enabling TLS for Kubernetes etcd client communications is crucial to protect sensitive data from interception and unauthorized access. Without TLS, data transmitted between clients and etcd, which often includes configuration details and secrets, could be exposed to attackers. This is vital for maintaining data integrity and confidentiality, thus supporting compliance with security standards such as PCI-DSS and ISO 27001.
  description: This rule checks if TLS is enforced for client communications with the etcd service in Kubernetes clusters. Ensure that the etcd server is configured to use TLS encryption for client communications by specifying the '--client-cert-auth' and providing appropriate certificates. This can be verified by examining the etcd configuration files or through the Kubernetes API server settings. Remediate any configurations not using TLS by updating the etcd settings and redeploying the cluster with secured configurations.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-cluster
  - CIS Google Kubernetes Engine Benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.containers_kubernetes_etcd_encryption_at_rest_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Etcd Encryption At Rest Enabled
  scope: container.cluster.encryption
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Ensure Kubernetes ETCD Encryption at Rest is Enabled
  rationale: Enabling encryption at rest for Kubernetes ETCD data is critical to protect sensitive cluster metadata and secrets from unauthorized access. Unencrypted ETCD data can lead to serious security breaches, exposing configurations, secret keys, and user information. Compliance with standards such as PCI-DSS and HIPAA often requires encryption of sensitive data, ensuring protection against data theft and unauthorized disclosures.
  description: This rule checks if Kubernetes ETCD data is encrypted at rest within the GCP container cluster. Enabling this feature ensures that all ETCD data is stored in an encrypted format, safeguarding against data exposure in the event of a breach. To verify and remediate, navigate to the GCP Console, select Kubernetes Engine, choose the desired cluster, and ensure that 'Enable Kubernetes secrets encryption' is enabled under the Security settings. Follow the GCP documentation for detailed steps on configuring ETCD encryption.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-144.pdf
- rule_id: gcp.container.cluster.containers_kubernetes_etcd_peer_tls_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Etcd Peer TLS Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable TLS for Kubernetes Etcd Peer Communication
  rationale: Enabling TLS for Etcd peer communication in a Kubernetes cluster is critical to protect sensitive data exchanged between nodes. Unsecured peer communication can lead to data breaches, unauthorized access, and man-in-the-middle attacks. Compliance with security standards such as NIST and ISO 27001 often requires encryption of data in transit, making this a key aspect of regulatory adherence.
  description: This rule checks if Transport Layer Security (TLS) is enabled for communication between Etcd peers in a Kubernetes cluster. TLS encrypts the data exchanged, preventing eavesdropping and unauthorized access. To verify, ensure the Etcd configuration in your Kubernetes setup specifies TLS-enabled endpoints. Remediate by configuring Etcd to use TLS certificates for peer communication, which can be done through the GCP Console or by updating the Kubernetes API server settings.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-cluster-components
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_fargate_profile_execution_ro_privilege
  service: container
  resource: cluster
  requirement: Containers Kubernetes Fargate Profile Execution Ro Privilege
  scope: container.cluster.least_privilege
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Restrict Execution of Kubernetes Fargate Profiles with Ro Privilege
  rationale: Allowing read-only (Ro) privilege in Kubernetes Fargate profiles can lead to unauthorized access to container data, increasing the risk of data exfiltration and privilege escalation. This misconfiguration can impact compliance with data protection regulations such as GDPR and HIPAA, and can result in significant business and reputational damage.
  description: This rule checks for Kubernetes Fargate profiles configured with read-only (Ro) execution privileges within Google Kubernetes Engine (GKE). Ensuring that Fargate profiles do not have unnecessary privileges minimizes the attack surface and adheres to the principle of least privilege. To remediate, review the IAM roles associated with Fargate profiles and adjust permissions to the minimum required for functionality. Verify that service accounts used by Fargate profiles are limited to necessary operations only.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/fargate-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library?category=pcidss
- rule_id: gcp.container.cluster.containers_kubernetes_fargate_profile_logging_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Fargate Profile Logging Enabled
  scope: container.cluster.logging
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Enable Logging for GKE Autopilot Clusters
  rationale: Enabling logging for GKE Autopilot clusters ensures that all container activities are recorded, allowing for thorough monitoring and auditing. This visibility helps identify and respond to potential security incidents, compliance violations, and operational issues quickly. Without logging, detecting unauthorized access or misconfigurations becomes significantly more challenging, increasing the risk of data breaches and non-compliance with standards such as PCI-DSS and SOC2.
  description: This rule checks if logging is enabled for GKE Autopilot clusters, ensuring that all container activities are recorded. To verify, confirm that Cloud Operations is configured to capture logs for all workloads running in the cluster. If logging is not enabled, enable it in the cluster settings through the Google Cloud Console or via the gcloud CLI by setting the `enable-stackdriver-kubernetes` flag. This configuration is crucial for maintaining visibility into the cluster's operations and security posture.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/logging
  - https://cloud.google.com/security/compliance/cis#section-5
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/stackdriver/docs/solutions/gke
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.isaca.org/resources/cobit
- rule_id: gcp.container.cluster.containers_kubernetes_fargate_profile_private_subnets_only
  service: container
  resource: cluster
  requirement: Containers Kubernetes Fargate Profile Private Subnets Only
  scope: container.cluster.private_networking
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Fargate Profiles Use Private Subnets
  rationale: Using private subnets for Kubernetes Fargate profiles reduces the exposure of your container workloads to the internet, mitigating the risk of unauthorized access and potential attacks. This configuration supports compliance with data protection regulations by ensuring sensitive workloads remain within a controlled network environment, enhancing both security posture and operational integrity.
  description: This rule verifies that Kubernetes Fargate profiles within your GCP container clusters are configured to use only private subnets. To ensure compliance, review the network settings of your Fargate profiles and configure them to operate exclusively within private subnets. This setup limits the exposure of your workloads to the public internet, thereby reducing attack vectors. Remediation involves editing the cluster's networking settings to ensure Fargate profiles are associated with private subnets, which can be done through the GCP Console or using gcloud commands.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_networkpolicy_default_deny_e_namespace
  service: container
  resource: cluster
  requirement: Containers Kubernetes Networkpolicy Default Deny E Namespace
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Enforce Default Deny Network Policies in Kubernetes Namespaces
  rationale: Implementing a default deny network policy is crucial for minimizing the attack surface in Kubernetes environments. It helps prevent unauthorized access and lateral movement by ensuring that only explicitly allowed traffic is permitted. This approach is essential for maintaining data confidentiality and integrity, particularly in multi-tenant or sensitive environments, and aligns with compliance frameworks that require strict network segmentation.
  description: This check ensures that a Kubernetes NetworkPolicy is configured with a default deny rule for ingress and egress traffic in each namespace. By default, Kubernetes allows all traffic, which could lead to potential security breaches. To verify, review the NetworkPolicy resources in each namespace and ensure a policy exists that denies all traffic unless explicitly allowed. Remediation involves creating or updating NetworkPolicy resources to include a default deny rule, thus enforcing strict traffic controls.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://cloud.google.com/architecture/best-practices-for-using-kubernetes-network-policies
- rule_id: gcp.container.cluster.containers_kubernetes_networkpolicy_default_deny_i_namespace
  service: container
  resource: cluster
  requirement: Containers Kubernetes Networkpolicy Default Deny I Namespace
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Implement Default Deny Policy for Kubernetes Namespaces
  rationale: Applying a default deny network policy in Kubernetes namespaces is crucial for minimizing the attack surface by restricting both ingress and egress traffic to only what is necessary. This control reduces the risk of unauthorized access and potential data exfiltration, thereby safeguarding sensitive applications and data. It also aligns with compliance requirements for data protection and network security, such as PCI-DSS and NIST.
  description: This rule checks that all Kubernetes namespaces within a GCP container cluster have a default deny network policy implemented. A default deny policy ensures that no traffic is allowed by default, and only explicitly allowed traffic can pass through. To verify, review the network policies for each namespace and ensure they start with a deny-all rule. Remediation involves creating or updating network policies to include a 'deny all ingress and egress' rule, then selectively allowing necessary traffic.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.containers_kubernetes_networkpolicy_required_allowl_services
  service: container
  resource: cluster
  requirement: Containers Kubernetes Networkpolicy Required Allowl Services
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Ensure Kubernetes NetworkPolicies Allow Only Essential Services
  rationale: Implementing Kubernetes NetworkPolicies is crucial to control and restrict traffic between pods within a GKE cluster, minimizing the attack surface and reducing the risk of unauthorized access. Without these policies, clusters could be vulnerable to lateral movement by an attacker who gains access to a single pod, potentially violating compliance requirements like PCI-DSS and HIPAA, which mandate strict network segmentation and data protection.
  description: This rule checks if Kubernetes NetworkPolicies are applied to all services in the GKE cluster, ensuring that only allowed traffic can communicate with the pods. NetworkPolicies should be configured to permit only required ingress and egress traffic, effectively isolating workloads and protecting sensitive data. To verify compliance, inspect the NetworkPolicy resources in your cluster using 'kubectl get networkpolicies --all-namespaces'. Remediation involves creating and applying NetworkPolicies for each service, defining allowed ingress and egress traffic according to your security policy.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_private_control_plane_endpoint_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Private Control Plane Endpoint Enabled
  scope: container.cluster.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enable Private Endpoint for Kubernetes Control Plane
  rationale: Enabling a private endpoint for the Kubernetes control plane minimizes exposure to the internet, reducing the risk of unauthorized access and potential attacks on the cluster's management interfaces. This configuration aligns with the principle of least privilege by restricting access to the control plane to only those resources and users within the specified private network, thus enhancing the overall security posture and compliance with data protection regulations.
  description: This rule checks if the Kubernetes control plane endpoint is configured to use a private IP address accessible only within the VPC network. To verify, ensure that the cluster's private endpoint is enabled in the Google Cloud Console under Kubernetes Engine settings, or via the gcloud command-line tool by setting the 'privateEndpoint' attribute. Remediation involves updating the cluster configuration to enable private endpoint access, which can be done during cluster creation or by updating an existing cluster.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/containers-kubernetes/3-ways-to-secure-your-kubernetes-engine-cluster
  - https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
- rule_id: gcp.container.cluster.containers_kubernetes_scheduler_authentication_ku_configured
  service: container
  resource: cluster
  requirement: Containers Kubernetes Scheduler Authentication Ku Configured
  scope: container.cluster.authentication
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Ensure Kubernetes Scheduler Authentication Configured
  rationale: Configuring authentication for the Kubernetes scheduler in GCP ensures that only authorized users and services can interact with the scheduling component of your container orchestration. This minimizes the risk of unauthorized access, which could lead to workload disruptions, resource misallocation, or unauthorized data exposure, impacting business operations and compliance with regulations such as PCI-DSS and HIPAA.
  description: This rule checks if the Kubernetes scheduler in a GCP container cluster has authentication mechanisms properly configured. It involves verifying settings that enforce identity verification for scheduler API interactions. Remediation includes enabling authentication in the Kubernetes API server configuration, ensuring all communications use secure credentials and adhere to least privilege principles. This can be verified by checking the GCP Console or using gcloud CLI commands to review and update cluster settings.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - CIS Google Kubernetes Engine (GKE) Benchmark
  - NIST SP 800-190 Application Container Security Guide
  - 'PCI-DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_scheduler_authorization_kub_configured
  service: container
  resource: cluster
  requirement: Containers Kubernetes Scheduler Authorization Kub Configured
  scope: container.cluster.authorization
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Scheduler RBAC is Properly Configured
  rationale: Proper configuration of Kubernetes Scheduler Authorization is crucial for preventing unauthorized access to cluster resources. Misconfigured RBAC can lead to privilege escalation, allowing attackers to execute malicious workloads, potentially leading to data breaches or service disruptions. Ensuring correct permissions align with compliance frameworks like PCI-DSS and HIPAA, minimizing business and legal risks.
  description: This rule checks whether the Kubernetes Scheduler RBAC policies are properly configured within GCP Kubernetes Engine clusters. Specifically, it verifies that the scheduler has the least privilege necessary to perform its functions. To verify, inspect the RBAC role bindings and ensure they follow the principle of least privilege. Remediation involves reviewing and updating the RBAC settings to restrict permissions to only what is necessary for the scheduler's operation.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
- rule_id: gcp.container.cluster.containers_kubernetes_scheduler_secure_port_tls_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Scheduler Secure Port TLS Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Scheduler Secure Port uses TLS
  rationale: Enabling TLS for the Kubernetes Scheduler Secure Port ensures encrypted communication, mitigating risks of data interception and unauthorized access. This is crucial for maintaining the confidentiality and integrity of sensitive cluster operations, supporting compliance with security standards like NIST and SOC2.
  description: This rule checks if the Kubernetes Scheduler Secure Port has TLS enabled. To verify, ensure the cluster configuration includes `--secure-port` and `--tls-cert-file` parameters set appropriately. Remediate by updating the scheduler configuration to include these parameters, ensuring communications are encrypted.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.container.cluster.containers_kubernetes_secrets_encryption_kms_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Secrets Encryption KMS Enabled
  scope: container.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS for Kubernetes Secrets Encryption in GKE
  rationale: Encrypting Kubernetes secrets using Customer-Managed Encryption Keys (CMEK) enhances security by ensuring sensitive information is protected with a robust encryption mechanism. It mitigates the risk of unauthorized access and data breaches, which can lead to significant financial loss and damage to brand reputation. Additionally, it helps meet compliance requirements for data protection standards such as PCI-DSS and HIPAA, which mandate strong encryption practices.
  description: This rule checks whether Google Kubernetes Engine (GKE) clusters have Kubernetes secrets encrypted with CMEK through Google Cloud Key Management Service (KMS). To verify, ensure clusters are configured to use KMS keys for encrypting secrets by setting the '--kms-key' flag during cluster creation. Remediation involves updating existing clusters to enable CMEK by modifying the encryption settings via the GCP console or gcloud CLI, ensuring that the KMS key is properly managed and rotated regularly.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/encryption-at-rest/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
- rule_id: gcp.container.cluster.containers_kubernetes_service_dns_and_metrics_from_a_sources
  service: container
  resource: cluster
  requirement: Containers Kubernetes Service Dns And Metrics From A Sources
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Monitor DNS and Metrics Access in GKE Clusters
  rationale: Monitoring DNS and metrics access in GKE clusters is essential to detect unusual behavior, prevent data exfiltration, and maintain visibility into cluster operations. Unauthorized access can lead to sensitive data exposure and compromise cluster integrity, impacting business continuity and compliance with standards such as ISO 27001 and SOC2.
  description: This rule checks for configurations that allow monitoring and logging of DNS requests and metrics access in Google Kubernetes Engine (GKE) clusters. Ensure that only authorized sources can access Kubernetes services to maintain a secure and observable environment. Configure Network Policies and enable logging to track access to sensitive endpoints. Remediate by reviewing access controls and ensuring that all access is logged and monitored.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/logs-dashboards
  - https://cloud.google.com/monitoring
- rule_id: gcp.container.cluster.containers_kubernetes_service_kube_system_services_no_public
  service: container
  resource: cluster
  requirement: Containers Kubernetes Service Kube System Services No Public
  scope: container.cluster.public_access
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Restrict Public Access to Kubernetes System Services
  rationale: Exposing Kubernetes system services to the public internet can lead to unauthorized access and potential compromise of the cluster's control plane. This exposure increases the risk of data breaches, denial-of-service attacks, and compliance violations, particularly for standards like PCI-DSS and HIPAA, which require strict access controls.
  description: This check ensures that Kubernetes system services within the kube-system namespace are not publicly accessible. It verifies the network configurations and firewall rules associated with the Kubernetes Engine cluster to restrict access to internal IPs only. To remediate, configure private clusters and update firewall rules to allow access only from specific IP ranges or VPCs, ensuring that no public IP addresses can reach the kube-system services.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://kubernetes.io/docs/concepts/security/overview/
- rule_id: gcp.container.cluster.control_plane_authorized_networks_configured
  service: container
  resource: cluster
  requirement: Control Plane Authorized Networks Configured
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Configure Control Plane Authorized Networks in GKE Clusters
  rationale: Configuring Control Plane Authorized Networks in Google Kubernetes Engine (GKE) clusters is vital for restricting access to the Kubernetes API server, reducing the risk of unauthorized access and potential data breaches. This configuration enhances security by ensuring that only trusted IP addresses can interact with the control plane, aligning with regulatory requirements such as NIST SP 800-53 and ISO 27001.
  description: This rule checks that Control Plane Authorized Networks are configured for GKE clusters, which involves specifying IP ranges that are allowed to access the Kubernetes API server. Verification can be done by reviewing the cluster settings in the Google Cloud Console or using the `gcloud` command-line tool. To remediate, update the cluster's settings to include authorized networks, ensuring that all entries are well-managed and documented. This can be done by navigating to the GKE cluster's 'Networking' settings and adding the required IP ranges.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview
- rule_id: gcp.container.cluster.google_managed_ssl_certificates_compliance
  service: container
  resource: cluster
  requirement: Google Managed SSL Certificates Compliance
  scope: container.cluster.compliance
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Use of Google Managed SSL Certificates in GKE Clusters
  rationale: Using Google Managed SSL Certificates in GKE clusters ensures that your Kubernetes workloads are protected with certificates that are automatically managed and renewed by Google Cloud. This minimizes the risk of expired certificates leading to insecure communication and potential data breaches, which can have significant business and legal repercussions, especially in industries governed by data protection regulations such as PCI-DSS and HIPAA.
  description: This rule checks if Google Kubernetes Engine (GKE) clusters are configured to use Google Managed SSL Certificates for securing communication. By default, GKE offers the option to use managed certificates, which are automatically maintained and renewed, reducing manual overhead and the risk of human error. To verify compliance, ensure that Google Managed SSL Certificates are enabled and properly configured in your GKE clusters. If not compliant, configure your GKE Ingress to use managed certificates by setting up the appropriate `ManagedCertificate` resource and associating it with your Ingress.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/ingress-xlb#managed-certs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.container.cluster.google_managed_ssl_certificates_enabled
  service: container
  resource: cluster
  requirement: Google Managed SSL Certificates Enabled
  scope: container.cluster.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Google Managed SSL Certificates for GKE Clusters
  rationale: Using Google Managed SSL Certificates ensures that SSL/TLS encryption is consistently applied to all traffic to your Google Kubernetes Engine (GKE) clusters, reducing the risk of data interception and ensuring compliance with standards requiring encryption in transit. This approach also simplifies certificate management, reducing the risk of human error and expired certificates, which can lead to service disruption and potential data breaches.
  description: This rule checks if Google Managed SSL Certificates are enabled for GKE clusters, ensuring that all ingress traffic is automatically secured with managed certificates. To verify, navigate to the GKE cluster settings in the Google Cloud Console and ensure that your ingress configurations utilize Google Managed SSL Certificates. Remediation involves configuring your ingress resources to use 'managedCertificate' with a 'spec.tls' field, thereby automating certificate provisioning and renewal.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs
  - https://cloud.google.com/security/compliance/cis-gke-benchmark
  - https://www.nist.gov/publications/nist-special-publication-800-57-part-1-revision-5
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.container.cluster.intranode_visibility_and_vpc_flow_logs_enabled
  service: container
  resource: cluster
  requirement: Intranode Visibility And VPC Flow Logs Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Intranode Visibility and VPC Flow Logs for Clusters
  rationale: Enabling intranode visibility and VPC flow logs enhances network monitoring and security by providing detailed insights into internal cluster communications and network traffic. This aids in the detection of anomalies, prevents potential data breaches, and supports compliance with regulations requiring detailed network activity logging, such as PCI-DSS and HIPAA.
  description: This rule checks if GKE clusters have both intranode visibility and VPC flow logs enabled. Intranode visibility allows for monitoring traffic between nodes within a cluster, while VPC flow logs capture information about network flows sent from and received by VM instances. To verify and enable these settings, navigate to the GKE cluster settings in the Google Cloud Console, enable intranode visibility under networking, and ensure VPC flow logs are activated in the VPC network settings. Remediation involves configuring these settings for each cluster to ensure comprehensive network monitoring.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/intranode-visibility
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.container.cluster.k8s_apiserver_admission_psa_enforce_mode
  service: container
  resource: cluster
  requirement: K8s Apiserver Admission Psa Enforce Mode
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce PSA Mode in K8s Apiserver Admission
  rationale: Enforcing Pod Security Admission (PSA) modes helps mitigate risks associated with unauthorized or insecure workloads running in your Kubernetes clusters. Without enforcement, malicious or poorly configured pods could compromise cluster security, leading to potential data breaches or service disruptions. Compliance frameworks often require strict access controls, making PSA enforcement necessary for regulatory adherence.
  description: This rule checks whether the Kubernetes API server's admission controller is configured to enforce Pod Security Admission (PSA) modes. PSA modes control the security context of pods, ensuring they adhere to defined security policies. Administrators should verify that the 'PodSecurityPolicy' admission controller is enabled and properly configured. To remediate, update the API server configuration to enable PSA with enforce mode by setting the appropriate flags and policy definitions.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/pod-security-admission
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/concepts/security/pod-security-admission/
- rule_id: gcp.container.cluster.k8s_apiserver_anonymous_auth_disabled
  service: container
  resource: cluster
  requirement: K8s Apiserver Anonymous Auth Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Anonymous Auth on Kubernetes API Server
  rationale: Disabling anonymous authentication on the Kubernetes API server is crucial to prevent unauthorized access to the cluster's control plane. Allowing anonymous requests can lead to potential exploitation by malicious actors, which may result in unauthorized data access, cluster disruption, or compromise of sensitive workloads. This configuration aligns with compliance requirements such as PCI-DSS and ISO 27001, ensuring that only authenticated and authorized users can interact with the Kubernetes API.
  description: This rule checks if the Kubernetes API server on GKE clusters has anonymous authentication disabled. Anonymous authentication allows requests to the API server without credentials, posing a security risk. To verify, ensure the 'anonymous-auth' flag is set to 'false' in the API server configuration. Remediation involves updating the GKE cluster configuration to explicitly disable anonymous authentication, which can typically be managed via the Google Cloud Console or using gcloud CLI commands.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_methods
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#configuring-authentication
- rule_id: gcp.container.cluster.k8s_apiserver_audit_logging_enabled
  service: container
  resource: cluster
  requirement: K8s Apiserver Audit Logging Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Kubernetes API Server Audit Logging
  rationale: Enabling Kubernetes API Server audit logging is crucial for tracking and analyzing requests made to the Kubernetes API. This capability helps detect unauthorized access and potential security breaches, while also supporting compliance with regulations that mandate detailed activity logging. It enhances visibility into operations, aiding in the forensic investigation of security incidents.
  description: This rule checks whether the Kubernetes API Server audit logging is enabled for GCP Kubernetes Engine clusters. Audit logging records all requests to the Kubernetes API server, providing a comprehensive trail of activity. To verify and enable this setting, navigate to the Google Cloud Console, select Kubernetes Engine, choose the desired cluster, and ensure that 'Enable Kubernetes API Server audit logging' is checked under the logging section. This ensures that all interactions with the Kubernetes API are logged for security and compliance purposes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/framework/security/design-for-security#logging_and_monitoring
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.k8s_apiserver_tls_min_version_1_2
  service: container
  resource: cluster
  requirement: K8s Apiserver TLS Min Version 1 2
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure K8s API Server Uses TLS v1.2 or Higher
  rationale: Enforcing a minimum TLS version of 1.2 for Kubernetes API server communication helps protect against vulnerabilities associated with older versions of the protocol. This ensures a higher standard of data encryption and integrity, reducing the risk of data breaches and man-in-the-middle attacks. Compliance with regulations such as PCI-DSS and HIPAA often mandates the use of strong encryption protocols, which includes TLS 1.2 or higher.
  description: This rule verifies that the Kubernetes API server within a Google Kubernetes Engine (GKE) cluster is configured to use TLS version 1.2 or higher. It is essential to configure the API server with the `--tls-min-version` flag set to '1.2' or higher to ensure secure communication. To check this setting, inspect the cluster's API server configuration. Remediation involves updating the cluster's API server settings via the GCP console or GCloud CLI to enforce the minimum TLS version requirement.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-your-cluster#api-server-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.k8s_controllermanager_secure_port_enabled
  service: container
  resource: cluster
  requirement: K8s Controllermanager Secure Port Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Secure Port for Kubernetes Controller Manager
  rationale: Enabling the secure port for the Kubernetes Controller Manager is crucial for ensuring that communication with the API server is encrypted and authenticated. This security measure mitigates risks such as man-in-the-middle attacks and unauthorized access, which can compromise cluster operations and sensitive data. Aligning with security standards like CIS GCP Benchmarks also helps meet regulatory compliance requirements.
  description: This rule checks if the Kubernetes Controller Manager is configured to use a secure port for communication. To verify, ensure that the `--secure-port` flag is set to a non-zero value in the Controller Manager's configuration. Remediation involves updating the cluster configuration to specify a secure port, typically 10257, in accordance with best practices. This action helps secure the control plane's communication channels.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/cis-benchmarks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
- rule_id: gcp.container.cluster.k8s_controllermanager_service_account_token_signing_enabled
  service: container
  resource: cluster
  requirement: K8s Controllermanager Service Account Token Signing Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure K8s Service Account Token Signing is Enabled
  rationale: Enabling service account token signing in Kubernetes ensures that tokens are cryptographically secure, reducing the risk of unauthorized access and token forgery. This feature is critical for maintaining the integrity and confidentiality of service account tokens, thus protecting sensitive workloads and data in the cluster. Compliance with security standards such as NIST and CIS often requires such measures to safeguard cloud environments.
  description: This check verifies whether the Kubernetes Controller Manager has service account token signing enabled in GCP clusters. To ensure this, check the cluster's configuration for the --service-account-signing-key-file and --root-ca-file flags. If these flags are not set, it is crucial to configure them to enable token signing. Remediation involves updating the Kubernetes Controller Manager configuration to include these flags, ensuring that all service account tokens are signed and verified.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restricting_service_account_usage
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
- rule_id: gcp.container.cluster.k8s_etcd_client_cert_auth_enabled
  service: container
  resource: cluster
  requirement: K8s Etcd Client Cert Auth Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable K8s Etcd Client Certificate Authentication
  rationale: Enabling client certificate authentication for etcd in Kubernetes clusters enhances security by ensuring that only authenticated clients can access the etcd server. This reduces the risk of unauthorized access and potential data exfiltration, supporting compliance with data protection regulations like GDPR and CCPA, which mandate robust access controls to sensitive data.
  description: This rule checks whether client certificate authentication is enabled for etcd in Kubernetes clusters. Etcd is a critical component in Kubernetes, storing all cluster data, and securing it with client certificates ensures that only verified entities can communicate with it. To verify, check the cluster's etcd configuration for the '--client-cert-auth' flag. Remediation involves updating your etcd configuration to include client certificate authentication, thus enhancing the security posture of the cluster.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-cluster
  - https://www.cisecurity.org/benchmark/kubernetes
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview
- rule_id: gcp.container.cluster.k8s_etcd_encryption_at_rest_enabled
  service: container
  resource: cluster
  requirement: K8s Etcd Encryption At Rest Enabled
  scope: container.cluster.encryption
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Enable Kubernetes Etcd Encryption at Rest in GKE Clusters
  rationale: Encrypting etcd data at rest in Kubernetes clusters is critical to protect sensitive configuration data, such as secrets and configuration maps. Without encryption, this data is vulnerable to unauthorized access in the event of a data breach or insider threat. Enabling encryption helps meet compliance requirements for data protection standards like PCI-DSS and HIPAA, reducing the risk of data exposure.
  description: This rule checks if etcd, the key-value store used by Kubernetes for configuration data, is encrypted at rest in GCP Kubernetes Engine (GKE) clusters. To verify, ensure the encryptionConfig field is enabled in the cluster's configuration. Remediation involves configuring the cluster with a Customer Managed Encryption Key (CMEK) to enable etcd encryption, providing an additional layer of security for sensitive data stored in clusters.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
  - https://cloud.google.com/architecture/best-practices-for-running-production-workloads-on-gke
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - 'NIST SP 800-53: System and Communications Protection'
  - 'PCI DSS Requirement 3: Protect Stored Cardholder Data'
- rule_id: gcp.container.cluster.k8s_scheduler_leader_election_enabled
  service: container
  resource: cluster
  requirement: K8s Scheduler Leader Election Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Scheduler Leader Election
  rationale: Enabling leader election for the Kubernetes scheduler ensures high availability and fault tolerance by allowing multiple schedulers to be deployed with only one acting as the leader at any time. This reduces the risk of a single point of failure in cluster management, which can impact application availability and lead to compliance violations with standards requiring fault-tolerant architecture.
  description: This rule checks if the Kubernetes scheduler in a GCP container cluster is configured with leader election enabled. Leader election allows a backup scheduler to take over seamlessly if the current leader fails, ensuring continuous cluster operations. To verify, ensure that the scheduler manifest includes the '--leader-elect=true' flag. Remediation involves updating the scheduler configuration to enable this flag if it is not set.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-architectures
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.container.cluster.k8s_scheduler_profiling_disabled
  service: container
  resource: cluster
  requirement: K8s Scheduler Profiling Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Kubernetes Scheduler Profiling in GCP Clusters
  rationale: Disabling Kubernetes Scheduler profiling reduces exposure to potential unauthorized access and information disclosure risks. Profiling data can be exploited by attackers to gain insights into cluster performance and behavior, potentially leading to targeted attacks. Ensuring profiling is disabled aligns with security best practices and helps maintain compliance with regulatory frameworks requiring minimal exposure of sensitive data.
  description: This rule checks whether Kubernetes Scheduler profiling is disabled in GCP clusters. Profiling should be turned off to prevent unnecessary exposure of internal data that could aid an attacker. To verify, examine the cluster configuration to ensure the '--profiling' flag is set to 'false' for the kube-scheduler. Remediation involves updating the cluster configuration to disable profiling and redeploying the scheduler component if needed.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/scheduler
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
- rule_id: gcp.container.cluster.k8s_scheduler_secure_port_enabled
  service: container
  resource: cluster
  requirement: K8s Scheduler Secure Port Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Secure Port on Kubernetes Scheduler
  rationale: Enabling the secure port for the Kubernetes scheduler is crucial to ensure encrypted communications between the scheduler and other cluster components. This mitigates the risk of man-in-the-middle attacks and unauthorized data interception, enhancing compliance with security standards like NIST SP 800-53 and ISO 27001 that emphasize secure communication protocols.
  description: This rule checks whether the Kubernetes scheduler in a GCP container cluster is configured to use a secure port for communications. The secure port should be enabled to ensure all communications are encrypted using TLS. Verify this setting by checking the scheduler's configuration file or parameters. Remediation involves configuring the scheduler to use the secure port, typically 10259, by updating the cluster's API server settings.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-cluster-components
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
- rule_id: gcp.container.cluster.kubernetes_dashboard_disabled
  service: container
  resource: cluster
  requirement: Kubernetes Dashboard Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Kubernetes Dashboard in GKE Clusters
  rationale: The Kubernetes Dashboard can expose sensitive cluster information and administrative capabilities if not secured properly, leading to potential unauthorized access and control over cluster resources. Disabling it mitigates risks of exploitation by attackers who may leverage vulnerabilities in the dashboard to compromise the cluster. This precaution aligns with compliance requirements to minimize attack surfaces and protect data integrity.
  description: This rule checks that the Kubernetes Dashboard is disabled in Google Kubernetes Engine (GKE) clusters. The dashboard can be a vector for attacks if exposed to the internet without proper authentication and authorization. To verify, ensure that the Kubernetes Dashboard add-on is not installed on your GKE clusters. If installed, remove it using the GCP Console or gcloud CLI. Disabling the dashboard enhances security by reducing potential entry points for attackers.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/dashboards#disabling_the_kubernetes_dashboard
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://cloud.google.com/security/best-practices
  - https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
- rule_id: gcp.container.cluster.logging_monitoring_service_enabled
  service: container
  resource: cluster
  requirement: Logging Monitoring Service Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure GKE Cluster Logging and Monitoring is Enabled
  rationale: Enabling logging and monitoring for GKE clusters is crucial for detecting unauthorized access attempts, diagnosing operational issues, and maintaining audit trails. Without this, organizations may face increased risks of undetected security breaches and non-compliance with standards like PCI-DSS and SOC2, which require robust logging and monitoring mechanisms.
  description: This rule checks if Google Kubernetes Engine (GKE) clusters have both logging and monitoring services enabled. These services should be configured to collect system and application logs and metrics. To verify, inspect the cluster settings in the GCP Console or use gcloud CLI to ensure that 'loggingService' and 'monitoringService' are not set to 'none'. Remediation involves enabling these services through the GCP Console or using gcloud commands to update the cluster.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/logging-and-monitoring
  - https://cloud.google.com/kubernetes-engine/docs/concepts/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/logging
- rule_id: gcp.container.cluster.master_authorized_networks_enabled
  service: container
  resource: cluster
  requirement: Master Authorized Networks Enabled
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Enable Master Authorized Networks for GKE Clusters
  rationale: Enabling Master Authorized Networks in GKE clusters restricts access to the Kubernetes API server, minimizing the risk of unauthorized access. This control helps prevent potential attacks such as credential stuffing or brute force attempts from unauthorized IP ranges, which is crucial for maintaining the integrity and confidentiality of cluster workloads and data.
  description: This rule checks if Master Authorized Networks are enabled for Google Kubernetes Engine (GKE) clusters. When enabled, it allows access to the Kubernetes API server only from specific IP addresses, enhancing security by limiting potential attack vectors. To verify, navigate to the GKE cluster settings in the GCP Console and ensure that Master Authorized Networks are configured with the intended IP ranges. If not enabled, update the cluster settings to specify the authorized networks.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.cluster.network_policy_support_configured
  service: container
  resource: cluster
  requirement: Network Policy Support Configured
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Ensure Network Policy is Enabled for GKE Clusters
  rationale: Enabling network policy in Google Kubernetes Engine (GKE) is crucial for controlling traffic flow between pods, reducing the risk of lateral movement in the event of a breach. This enhances the security posture by allowing restrictions based on IP addresses, ports, and protocols, which is essential for compliance with security standards and protecting sensitive data from unauthorized access.
  description: This rule checks that network policy support is configured for GKE clusters, ensuring that network policies can be applied to control traffic. To verify, check if the network policy is enabled in your GKE cluster settings. Remediation involves enabling network policy by configuring the cluster with the `--enable-network-policy` flag during creation or updating existing clusters via GCP Console or `gcloud` CLI. This helps in isolating workloads and enforcing security controls efficiently.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.private_endpoint_and_public_access_disabled
  service: container
  resource: cluster
  requirement: Private Endpoint And Public Access Disabled
  scope: container.cluster.public_access
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Ensure Private Endpoint and Disable Public Access for GKE Clusters
  rationale: Disabling public access and configuring private endpoints for Google Kubernetes Engine (GKE) clusters reduces the attack surface by preventing unauthorized access from the internet. This setup mitigates risks such as unauthorized data exposure and potential DDoS attacks, which can lead to service disruptions and data breaches. Ensuring private-only access aligns with compliance requirements like PCI-DSS and ISO 27001, which mandate strict network access controls.
  description: This rule checks if GKE clusters have public access disabled and are configured with a private endpoint. To verify, inspect the cluster's networking configuration to ensure 'privateClusterConfig.enablePrivateEndpoint' is true and 'privateClusterConfig.enablePrivateNodes' is true, while ensuring 'publicClusterConfig' is not set. Remediation involves using the GCP Console or gcloud command to update the cluster settings to enable private endpoints and disable public access.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://cloud.google.com/kubernetes-engine/docs/best-practices-for-private-clusters
  - CIS Google Cloud Platform Foundation Benchmark
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.private_nodes_configured
  service: container
  resource: cluster
  requirement: Private Nodes Configured
  scope: container.cluster.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure GKE Clusters Have Private Nodes Configured
  rationale: Configuring private nodes in GKE clusters is crucial for reducing exposure to the public internet, minimizing the attack surface, and adhering to best practices for data protection and compliance with regulations such as HIPAA and PCI-DSS. This configuration helps protect sensitive workloads from unauthorized access and potential data breaches by ensuring that nodes communicate over private IP addresses.
  description: This rule checks whether Google Kubernetes Engine (GKE) clusters are configured to use private nodes. Private nodes must be enabled to ensure that all node-to-node and node-to-master communications occur over a private network, enhancing security and reducing public exposure. To verify, check the cluster's privateClusterConfig settings via the GCP Console or gcloud CLI. Remediation involves enabling private clusters through the GCP Console or updating the cluster with the `--enable-private-nodes` flag using the gcloud CLI.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/itl/applied-cybersecurity/nistcyberframework
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.container.cluster.private_nodes_enabled
  service: container
  resource: cluster
  requirement: Private Nodes Enabled
  scope: container.cluster.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enable Private Nodes in GKE Clusters
  rationale: Enabling private nodes in Google Kubernetes Engine (GKE) clusters ensures that the nodes have internal IP addresses only, reducing exposure to the public internet and minimizing the attack surface. This configuration helps protect sensitive workloads and complies with regulatory frameworks that mandate network segmentation and access control, such as PCI DSS and HIPAA. It also mitigates risks associated with data breaches and unauthorized access by limiting the network pathways available to potential attackers.
  description: This rule verifies that private nodes are enabled in GKE clusters, ensuring that nodes do not have external IP addresses and communicate over a private network. To verify, check the cluster configuration for 'privateNodesConfig' and ensure 'enablePrivateNodes' is set to true. Remediation involves modifying the cluster to enable private nodes, which can be done via the GCP Console, gcloud CLI, or Terraform. This configuration enhances security by isolating nodes from the public internet while still allowing communication with other Google Cloud services through private IPs.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-private-gke-clusters
  - https://cloud.google.com/security/best-practices/networking
- rule_id: gcp.container.cluster.rbac_privilege_escalation_permissions
  service: container
  resource: cluster
  requirement: RBAC Privilege Escalation Permissions
  scope: container.cluster.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Prevent RBAC Privilege Escalation in Kubernetes Clusters
  rationale: Improperly configured RBAC permissions can lead to privilege escalation, allowing unauthorized users to gain elevated access within Kubernetes clusters. This poses a significant security risk, potentially leading to data breaches, service disruptions, and non-compliance with standards such as NIST and PCI-DSS. Ensuring least privilege access is critical for maintaining the integrity and security of containerized applications.
  description: This rule checks for Kubernetes Role-Based Access Control (RBAC) permissions that could enable privilege escalation within GCP container clusters. It identifies roles with broad 'bind' or 'impersonate' permissions that could be exploited. To remediate, review and restrict these permissions, ensuring they adhere to the principle of least privilege. Use the Google Cloud Console or gcloud CLI to audit and adjust IAM roles accordingly.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://cloud.google.com/blog/products/identity-security/enabling-least-privilege-administration-with-rbac-a-best-practice
- rule_id: gcp.container.cluster.release_channel_regular_or_stable
  service: container
  resource: cluster
  requirement: Release Channel Regular Or Stable
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure GKE Clusters Use Regular or Stable Release Channels
  rationale: Using Regular or Stable release channels for Google Kubernetes Engine (GKE) clusters reduces the risk of deploying untested or unstable software, minimizing the potential for security vulnerabilities and operational disruptions. This approach aligns with best practices for maintaining a secure and reliable Kubernetes environment, ensuring that clusters receive timely security patches and updates. Compliance with security frameworks often mandates using stable software versions to mitigate risks associated with experimental features.
  description: This rule checks if GKE clusters are configured to use either the Regular or Stable release channels, which are known for their balance of feature updates and stability. Regular release channels offer a compromise between receiving new features and maintaining stability, while Stable channels focus on delivering well-tested, reliable updates. To verify, navigate to the Google Cloud Console, go to Kubernetes Engine, and check the Release Channel setting for each cluster. If necessary, change the channel by editing the cluster settings to ensure compliance with security best practices.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/release-channels
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/how-to/upgrade-a-cluster
  - https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.container.cluster.role_cluster_admin_restrictions
  service: container
  resource: cluster
  requirement: Role Cluster Admin Restrictions
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Restrict Container Cluster Admin Role Assignment
  rationale: The Cluster Admin role grants extensive permissions that can lead to unauthorized access or modification of cluster configurations, posing significant security risks. Limiting this role's assignment mitigates the potential for privilege escalation, reduces the attack surface, and aligns with compliance requirements such as PCI-DSS and ISO 27001, enhancing overall security posture.
  description: This rule checks for the assignment of the 'roles/container.clusterAdmin' role to users and service accounts. To secure the environment, ensure this role is assigned only to trusted personnel by reviewing IAM policies and using Google Cloud IAM Conditions for more granular control. Remediation involves auditing current assignments and implementing least privilege principles by minimizing the number of users with this role.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/iam
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.secrets_encryption_kms_enabled
  service: container
  resource: cluster
  requirement: Secrets Encryption KMS Enabled
  scope: container.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS for Secrets Encryption in GKE Clusters
  rationale: Encrypting secrets with a Key Management Service (KMS) reduces the risk of unauthorized access and exposure of sensitive data, which can lead to data breaches and compliance violations. Using KMS enhances security by providing additional control over the encryption keys, which is crucial for meeting regulatory requirements such as PCI DSS, HIPAA, and GDPR.
  description: This rule checks if Google Kubernetes Engine (GKE) clusters have Secrets Encryption enabled using Google Cloud Key Management Service (KMS). To verify, ensure that the cluster encryption configuration is set to use a Customer Managed Encryption Key (CMEK). Remediation involves updating the cluster's encryption settings to integrate with KMS by specifying a valid KMS key in the cluster's configuration, enhancing the security of stored secrets.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/
- rule_id: gcp.container.cluster.secrets_rbac_minimal_access
  service: container
  resource: cluster
  requirement: Secrets RBAC Minimal Access
  scope: container.cluster.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Limit RBAC Access to Secrets in GKE Clusters
  rationale: Minimizing RBAC access to secrets in Google Kubernetes Engine (GKE) clusters reduces the risk of unauthorized access to sensitive information, such as database credentials and API keys. Excessive permissions can lead to data breaches or service disruptions if credentials are misused or leaked. This practice supports compliance with regulations such as GDPR and PCI-DSS, which mandate strict access controls to sensitive data.
  description: This rule checks that RBAC roles in GKE clusters do not grant excessive permissions to access Kubernetes secrets. It ensures that only necessary roles have read or write access to secrets, aligning with the principle of least privilege. To verify, review the RBAC role bindings in your GKE clusters and ensure only intended service accounts and users have access to secrets. Remediation involves updating role bindings to remove excessive permissions and creating custom roles as needed.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#rbac
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security
- rule_id: gcp.container.cluster.security_posture_config_mode
  service: container
  resource: cluster
  requirement: Security Posture Config Mode
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Security Posture Config Mode in GKE Clusters
  rationale: Configuring Security Posture Config Mode in Google Kubernetes Engine (GKE) clusters helps enterprises maintain a robust security posture by automatically enforcing security policies and monitoring for compliance. This reduces the risk of misconfigurations that could lead to unauthorized access or data breaches, and ensures alignment with regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule checks if the GKE cluster has Security Posture Config Mode enabled, which is crucial for proactive security management. To verify, navigate to the Google Cloud Console, select Kubernetes Engine, and check if the Security Posture Config Mode is set. To enable, update your GKE cluster settings to include security policy configurations. This helps maintain compliance and enhances the security monitoring of your container workloads.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/security-posture-config
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.cluster.shielded_nodes_enabled
  service: container
  resource: cluster
  requirement: Shielded Nodes Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Shielded Nodes are Enabled in GKE Clusters
  rationale: Enabling Shielded Nodes in Google Kubernetes Engine (GKE) clusters enhances node security by protecting against rootkit and bootkit attacks, which can compromise the integrity of the node operating system. This feature is crucial for maintaining the confidentiality, integrity, and availability of workloads, particularly in environments subject to compliance requirements like PCI-DSS or HIPAA, where data protection is paramount.
  description: This rule checks if Shielded Nodes are enabled in GKE clusters. Shielded Nodes provide verifiable integrity of node boot processes and protect against unauthorized changes to the guest operating system. To enable Shielded Nodes, update the cluster to set the 'enableShieldedNodes' field to 'true'. Verification can be done via the GCP Console under Kubernetes Engine settings or using the 'gcloud' CLI with 'gcloud container clusters describe'. If not enabled, modify the cluster configuration to activate Shielded Nodes and apply changes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes
  - https://cloud.google.com/kubernetes-engine/docs/concepts/shielded-nodes
  - CIS Google Kubernetes Engine Benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.system_masters_group_absence
  service: container
  resource: cluster
  requirement: System Masters Group Absence
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure System Masters Group is Absent from GKE Clusters
  rationale: The presence of the system masters group in GKE clusters poses a security risk by granting overly broad permissions that may lead to unauthorized access and privilege escalation. Removing this group mitigates the risk of insider threats and aligns with the principle of least privilege, which is critical for maintaining a secure and compliant Kubernetes environment.
  description: This rule checks for the absence of the system masters group in Google Kubernetes Engine (GKE) clusters. To verify, inspect cluster role bindings and ensure that the system:masters group is not bound to any roles. Remediation involves modifying or removing any existing role bindings that include this group, thereby preventing excessive permissions. This configuration enhances security by limiting access to only necessary personnel and services.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restricting_access
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 6.5
  - 'NIST SP 800-53 AC-6: Least Privilege'
  - https://kubernetes.io/docs/concepts/security/overview/#privilege-separation
- rule_id: gcp.container.cluster.vpc_native_alias_ip_enabled
  service: container
  resource: cluster
  requirement: VPC Native Alias Ip Enabled
  scope: container.cluster.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure VPC Native Alias IP is Enabled for GKE Clusters
  rationale: Enabling VPC Native Alias IPs on Google Kubernetes Engine (GKE) clusters allows for better network isolation, scalability, and IP address management. This configuration reduces the risk of IP exhaustion and ensures that pods have access to all VPC network services securely. It is essential for meeting compliance requirements related to network segmentation and access control, thereby minimizing the risk of unauthorized access to resources.
  description: This rule checks if GKE clusters are using VPC Native Alias IPs, which allow each pod to receive a unique IP address from a range assigned to the cluster. To verify, ensure that the cluster configuration includes 'ipAllocationPolicy' with 'useIpAliases' set to true. Remediation involves updating the cluster settings to enable 'VPC Native' alias IPs, which can be done via the Google Cloud Console or gcloud CLI, enhancing the network security posture.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/solutions/security-overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.namespace.administrative_boundary_enforced
  service: container
  resource: namespace
  requirement: Administrative Boundary Enforced
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Enforce Administrative Boundaries in Kubernetes Namespaces
  rationale: Enforcing administrative boundaries in Kubernetes namespaces is critical to prevent unauthorized access and potential lateral movement within a GCP Kubernetes Engine cluster. This helps in maintaining clear separation of duties, ensuring that different teams or applications do not interfere with each other, which reduces the risk of accidental or malicious changes. Proper namespace isolation supports compliance with regulations requiring data segregation and access control, such as PCI-DSS and HIPAA.
  description: This rule checks if Kubernetes namespaces within a GCP Container service have enforced administrative boundaries using Role-Based Access Control (RBAC). It ensures namespaces are configured with specific roles and bindings to restrict user and service account permissions, preventing unauthorized access to resources. To verify, inspect the RBAC policies associated with each namespace and ensure they align with least privilege principles. Remediation involves reviewing and adjusting role bindings in the Kubernetes cluster to ensure proper isolation and control.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
- rule_id: gcp.container.namespace.containers_kubernetes_default_service_account_autom_disabled
  service: container
  resource: namespace
  requirement: Containers Kubernetes Default Service Account Autom Disabled
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Automatic Default Service Account in Kubernetes
  rationale: Disabling the automatic assignment of the default service account for Kubernetes containers reduces the risk of privilege escalation and unauthorized access to resources. This practice helps maintain the principle of least privilege, ensuring that applications run with only the permissions they need, thus minimizing potential attack vectors and aligning with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks if the default service account is disabled for Kubernetes containers in a namespace. The default service account often has elevated permissions that could be misused if exploited. To verify, review the namespace configurations and ensure no pods are using the default service account. Remediation involves setting custom service accounts with tailored permissions and updating pod specifications to reference these accounts.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#default-service-account
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/blog/topics/developers-practitioners/best-practices-using-google-kubernetes-engine-gke
- rule_id: gcp.container.namespace.containers_kubernetes_network_policies_present
  service: container
  resource: namespace
  requirement: Containers Kubernetes Network Policies Present
  scope: container.namespace.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Ensure Kubernetes Network Policies are Configured in Namespaces
  rationale: Kubernetes Network Policies are essential for defining the allowed traffic between pods in a namespace and external endpoints. Without these policies, there is a risk of unauthorized access and data breaches due to unrestricted network communications. Implementing network policies helps organizations comply with regulatory requirements such as PCI-DSS and ISO 27001 by enforcing strict access control and segmentation.
  description: This rule checks whether Kubernetes Network Policies are present for every namespace in your GCP Kubernetes Engine clusters. Network Policies act as a security boundary, allowing administrators to specify which connections are permitted, thereby minimizing attack surfaces. To verify, ensure that each namespace has at least one network policy defined that specifies ingress and egress rules as per your security requirements. Remediation involves creating and applying a NetworkPolicy resource in each namespace using Kubernetes YAML configurations.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/kubernetes
  - https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2_1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.namespace.containers_kubernetes_pod_security_level_restricted
  service: container
  resource: namespace
  requirement: Containers Kubernetes Pod Security Level Restricted
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Pods Use Restricted Security Context
  rationale: Implementing restricted pod security contexts in Kubernetes namespaces minimizes security risks by preventing privilege escalation and reducing attack surfaces. This is crucial for protecting sensitive data and workloads from unauthorized access, which can lead to data breaches, service disruptions, and non-compliance with regulatory standards such as PCI-DSS and HIPAA.
  description: This rule checks if Kubernetes pods within a namespace are configured with a restricted security context, which includes setting runAsNonRoot, disallowing privileged mode, and restricting capabilities. To verify compliance, review the PodSecurityPolicy or Pod Security Standards in place. Remediation involves applying or updating a PodSecurityPolicy to enforce these restrictions and ensuring developers adhere to these policies when deploying applications.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies
  - https://cloud.google.com/security/compliance/cis-benchmark
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.container.namespace.default_service_account_usage_disabled
  service: container
  resource: namespace
  requirement: Default Service Account Usage Disabled
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Default Service Account Usage in Kubernetes Namespaces
  rationale: Using the default service account in Kubernetes namespaces can lead to elevated security risks. It might provide unnecessary permissions that could be exploited by attackers if a container is compromised. Disabling its usage reduces the attack surface and aligns with the principle of least privilege, thus protecting sensitive data and operations.
  description: This rule checks whether the default service account is disabled in all Kubernetes namespaces within GCP. To verify, inspect the service account configurations in each namespace and ensure no workloads are using the default account. Remediation involves creating custom service accounts with minimal permissions tailored to application needs and updating workloads to use these instead of the default account.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - NIST SP 800-53 AC-6 Least Privilege
  - 'PCI-DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - ISO/IEC 27001:2013 A.9.1.2 Access control to networks and network services
- rule_id: gcp.container.namespace.network_policy_defined
  service: container
  resource: namespace
  requirement: Network Policy Defined
  scope: container.namespace.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Network Policies Are Defined for Kubernetes Namespaces
  rationale: Defining network policies within Kubernetes namespaces is crucial to controlling traffic flow between pods and services. Without explicit network policies, namespaces are vulnerable to unrestricted communication, increasing the risk of lateral movement by attackers within the cluster. This can lead to data breaches, unauthorized access, and potential regulatory non-compliance, impacting business operations and reputation.
  description: This rule checks if Kubernetes namespaces in GCP have associated network policies to manage ingress and egress traffic. Network policies allow you to specify how groups of pods can communicate with each other and other network endpoints. To verify, ensure that a policy exists for each namespace, specifying allowed traffic sources and destinations. Remediation involves creating and applying network policies using Kubernetes manifests or via the GCP Console, tailored to your security requirements.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://www.pcisecuritystandards.org/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://cloud.google.com/security/compliance/
- rule_id: gcp.container.namespace.pod_security_baseline_enforcement_configured
  service: container
  resource: namespace
  requirement: Pod Security Baseline Enforcement Configured
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Configure Pod Security Baseline Enforcement in Namespaces
  rationale: Pod Security Baseline Enforcement is crucial for minimizing security risks within Kubernetes clusters by ensuring pods adhere to defined security standards. Without enforcement, pods may run with excessive permissions, exposing the cluster to privilege escalation attacks and non-compliance with security frameworks like NIST and PCI-DSS.
  description: This rule checks whether Pod Security Baseline Enforcement is configured for all namespaces in GCP Kubernetes clusters. To verify, ensure that PodSecurityPolicy objects are defined and associated with namespaces to control the security context of pods. Remediation involves creating or updating PodSecurityPolicy resources to align with baseline security requirements and binding them to the necessary namespaces via RoleBindings.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.container.node.pool_auto_repair_status_configured
  service: container
  resource: node
  requirement: Pool Auto Repair Status Configured
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Node Pool Auto-Repair is Enabled
  rationale: Enabling auto-repair for Kubernetes node pools ensures that any unhealthy nodes are automatically repaired, reducing downtime and maintaining application availability. This feature helps mitigate risks associated with node failures, which can lead to service disruptions and potential data loss, impacting business continuity and customer trust.
  description: This rule checks whether auto-repair is enabled for all node pools in a GCP Kubernetes Engine cluster. Auto-repair automatically fixes unhealthy nodes, which can occur due to hardware or software issues. To verify, navigate to the GCP Console, go to Kubernetes Engine, select your cluster, and check the settings for each node pool to ensure 'Auto-repair' is enabled. If not configured, enable it to ensure nodes are self-healing and minimize manual intervention.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
- rule_id: gcp.container.node.pool_auto_upgrade_status_configured
  service: container
  resource: node
  requirement: Pool Auto Upgrade Status Configured
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure GKE Node Pool Auto-Upgrades Are Enabled
  rationale: Enabling auto-upgrades for GKE node pools ensures that nodes receive the latest security patches and feature updates, reducing the risk of vulnerabilities being exploited. This is critical for maintaining a secure Kubernetes environment and aligning with compliance requirements that mandate up-to-date software. Failure to do so could lead to increased exposure to known vulnerabilities and potential breaches.
  description: This check verifies that the auto-upgrade feature is enabled for all Google Kubernetes Engine (GKE) node pools. Auto-upgrades help automatically keep the nodes updated with the latest Kubernetes versions, which include critical security patches and improvements. To verify, navigate to the Google Cloud Console, select Kubernetes Engine, then select Clusters, and review each node pool's settings to ensure auto-upgrade is enabled. Remediation involves updating the node pool settings to enable auto-upgrades through the console or using the gcloud command line tool.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-upgrades
  - https://cloud.google.com/security/compliance/cis-gke-benchmark
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.node.pool_boot_disk_cmek_enabled
  service: container
  resource: node
  requirement: Pool Boot Disk Cmek Enabled
  scope: container.node.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable CMEK for GKE Node Pool Boot Disks
  rationale: Using Customer-Managed Encryption Keys (CMEK) for GKE node pool boot disks enhances data security by providing greater control over encryption keys. This mitigates risks of unauthorized data access and supports compliance with regulations requiring customer-controlled encryption, such as GDPR and CCPA.
  description: This rule checks if GKE node pool boot disks are encrypted using CMEK. By default, GCP uses Google-managed keys, but enabling CMEK allows you to manage key lifecycle and access. To verify, inspect the node pool configuration for the 'diskEncryptionKmsKey' property. To enable CMEK, update the node pool with a specified KMS key using the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-storage
  - https://cloud.google.com/security/compliance/cis#gke
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.container.node.pool_gke_sandbox_enabled
  service: container
  resource: node
  requirement: Pool Gke Sandbox Enabled
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable GKE Sandbox for Node Pools
  rationale: Enabling GKE Sandbox provides an additional layer of security by isolating container workloads using gVisor, reducing the risk of container escape and potential damage to the host system. This is crucial for maintaining a secure Kubernetes environment, particularly for organizations processing sensitive or regulated data, and helps in mitigating risks associated with multi-tenancy.
  description: This rule checks whether GKE Sandbox is enabled for Kubernetes node pools. GKE Sandbox uses gVisor to provide a secure isolation boundary between container workloads and the host operating system. To verify, ensure that node pools are configured with the 'sandbox.gke.io/runtime' node label. If not enabled, apply this label to enhance security by preventing kernel-level attacks. Consult the GCP console or use the gcloud command-line tool to update your node pool configurations.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods
  - https://cloud.google.com/kubernetes-engine/docs/concepts/sandbox-pods
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.node.pool_image_type_cos_containerd_configured
  service: container
  resource: node
  requirement: Pool Image Type Cos Containerd Configured
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Node Pool Image Type is COS with Containerd
  rationale: Using COS with containerd as the node image type enhances security by providing a minimal, hardened OS that reduces the attack surface and ensures compatibility with GKE features. This configuration aligns with security best practices and can help meet compliance requirements by ensuring that nodes run a consistent, secure OS environment, minimizing risks associated with vulnerabilities in more complex OS setups.
  description: This check verifies that the node pool image type is set to Container-Optimized OS (COS) with containerd for Google Kubernetes Engine (GKE) clusters. To ensure this, inspect the node pool configurations in the GCP Console or using gcloud CLI. Remediation involves updating any node pools using non-COS images to use COS with containerd, which is handled via the GCP Console, gcloud CLI, or Terraform. This setup offers improved security and stability by leveraging the lightweight, secure nature of COS.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/node-images#cos
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/concepts/containers/runtime-class/#containerd
  - https://cloud.google.com/containers/security/container-optimized-os-overview
- rule_id: gcp.container.node.pool_service_account_privilege_configured
  service: container
  resource: node
  requirement: Pool Service Account Privilege Configured
  scope: container.node.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Node Pool Service Account Uses Least Privilege
  rationale: Configuring node pools with the principle of least privilege minimizes the risk of unauthorized access and potential security breaches. Overprivileged service accounts can lead to data exfiltration, unauthorized resource access, and compliance violations with frameworks such as NIST and PCI-DSS. This configuration is crucial for maintaining a secure and compliant cloud environment.
  description: This rule checks if the service account used by GCP Kubernetes Engine node pools is configured with only the necessary permissions. Verify that the service account associated with the node pool has roles that are strictly required for its operations. Remediate by assigning only specific roles essential for the node's functions and avoid using default or overly permissive service accounts. Regularly audit service account permissions to ensure adherence to the principle of least privilege.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.node.pool_shielded_secure_boot_enabled
  service: container
  resource: node
  requirement: Pool Shielded Secure Boot Enabled
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Shielded VM Secure Boot is Enabled for Node Pools
  rationale: Enabling Shielded VM Secure Boot for Kubernetes node pools protects against persistent, low-level malware threats and rootkits. This security feature ensures the integrity of the boot process, providing a trusted path for system startup, which is essential for maintaining the overall security posture of your containerized applications and meeting compliance frameworks like PCI-DSS and NIST.
  description: This rule checks if Shielded VM Secure Boot is enabled for all node pools in Google Kubernetes Engine (GKE). Secure Boot is a critical feature that helps prevent unauthorized software and rootkits from executing during the boot process. To verify, ensure that the Shielded VM options, specifically Secure Boot, are enabled in the node pool configuration. Remediation involves updating existing node pools or creating new node pools with Shielded VM Secure Boot enabled.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/shielded-gke-nodes
  - https://cloud.google.com/security/compliance/cis-coredns
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/specialpublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.node_pool.authorization_policy_compliance
  service: container
  resource: node_pool
  requirement: Authorization Policy Compliance
  scope: container.node_pool.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Authorization Policies on GKE Node Pools
  rationale: Implementing authorization policies on GKE node pools reduces unauthorized access risks, protects sensitive workloads, and ensures compliance with security standards. Unauthorized access to node pools can lead to data breaches and service disruptions, negatively impacting business operations and reputation.
  description: This rule verifies that Google Kubernetes Engine (GKE) node pools have proper authorization policies in place to control access and permissions. It checks for configurations that ensure only authorized identities can access and manage workloads. To verify compliance, review and configure IAM policies on node pools to limit access to essential users. Remediation involves setting up role-based access controls (RBAC) and using IAM policies to enforce the principle of least privilege.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
- rule_id: gcp.container.node_pool.containers_kubernetes_container_instance_env_no_plai_secrets
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Container Instance Env No Plai Secrets
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Avoid Plaintext Secrets in Kubernetes Container Environment Variables
  rationale: Exposing secrets as plaintext in environment variables can lead to unauthorized access and data breaches if a container is compromised. This practice poses significant security risks, as attackers can easily extract sensitive information such as API keys and credentials, leading to potential data loss, service disruptions, and non-compliance with regulations like PCI-DSS and HIPAA.
  description: This rule checks if Kubernetes container instances within a node pool have environment variables set with plaintext secrets. The recommended practice is to use Kubernetes Secrets to manage sensitive data securely. Verify configurations using tools like kubectl to inspect environment variables and ensure they are sourced from Secrets. To remediate, refactor workloads to use Kubernetes Secrets and update deployment configurations accordingly.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/secrets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/best-practices
- rule_id: gcp.container.node_pool.containers_kubernetes_container_instance_no_privi_containers
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Container Instance No Privi Containers
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Containers Run Without Privileged Mode
  rationale: Running containers in privileged mode grants them elevated access to the host system, which can lead to potential security breaches if compromised. This poses risks such as unauthorized data access, privilege escalation, and compliance violations with standards like PCI-DSS and HIPAA, which mandate strict control over system privileges.
  description: This rule checks that Kubernetes containers in GCP's node pools are not running in privileged mode. Privileged containers have extensive permissions, potentially allowing them to escape the container environment and access the host system. To verify, inspect your Kubernetes pod specifications for the 'privileged' flag and ensure it is set to 'false'. Remediation involves updating your Kubernetes configurations to remove any privileged settings from your container specifications.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/
- rule_id: gcp.container.node_pool.containers_kubernetes_container_instance_read_onl_filesystem
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Container Instance Read Onl Filesystem
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Containers Use Read-Only Filesystems
  rationale: Enforcing read-only filesystems in Kubernetes containers reduces the risk of unauthorized modifications and mitigates the impact of potential security vulnerabilities. This approach minimizes the attack surface, making it harder for attackers to exploit writable filesystems to escalate privileges or execute malicious code. Additionally, it supports compliance with security frameworks that emphasize data integrity and protection.
  description: This rule checks if Kubernetes containers within a node pool have their filesystems set to read-only. A read-only filesystem ensures that application code and libraries cannot be tampered with at runtime, enhancing container security. To verify this, inspect the container spec and ensure the 'securityContext.readOnlyRootFilesystem' field is set to true. Remediation involves updating the container specifications in your deployment configurations to enforce this setting.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_read-only_filesystems_for_containers
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.node_pool.containers_kubernetes_disk_encryption_enabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Disk Encryption Enabled
  scope: container.node_pool.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Kubernetes Disk Encryption for Node Pools
  rationale: Enabling disk encryption for Kubernetes node pools ensures that sensitive data is protected at rest, mitigating unauthorized access risks in the event of a data breach. It addresses compliance requirements by safeguarding data according to regulatory standards, such as GDPR and HIPAA, which mandate encryption for data protection.
  description: This rule checks whether Kubernetes node pools in Google Kubernetes Engine (GKE) have disk encryption enabled. Disk encryption ensures that the data stored on the persistent disks of nodes is encrypted using encryption keys, providing an additional layer of security. To enable encryption, configure the node pool to use customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK) through the Google Cloud Console or gcloud command line tool. Regularly review and update encryption settings to adhere to security policies.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-disks
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/document-2498
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.node_pool.containers_kubernetes_kubelet_anonymous_auth_disabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Kubelet Anonymous Auth Disabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Kubelet Anonymous Auth is Disabled
  rationale: Disabling anonymous authentication for Kubernetes Kubelet is critical to prevent unauthorized access to the kubelet API, which could lead to potential security breaches such as unauthorized data exposure or resource manipulation. This measure helps in maintaining the integrity and confidentiality of cluster operations, aligning with compliance requirements like PCI-DSS and SOC2 that emphasize access control and data protection.
  description: This rule checks that anonymous authentication is disabled on Kubernetes Kubelet nodes within a GCP container node pool. Anonymous auth allows requests to be made to the kubelet API without authentication, posing a security risk. To verify, ensure that the kubelet configuration does not include the `--anonymous-auth` flag set to true. Remediate by setting this flag to false in the node pool configuration to restrict kubelet API access to authenticated users only.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_methods
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.node_pool.containers_kubernetes_kubelet_authz_webhook_or_rbac_enabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Kubelet Authz Webhook Or RBAC Enabled
  scope: container.node_pool.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Kubelet Authorization Webhook or RBAC is Enabled
  rationale: Enabling Kubelet Authorization Webhook or RBAC is crucial for enforcing fine-grained access control on Kubernetes nodes. Without these mechanisms, unauthorized users or processes may gain access to sensitive workloads, leading to potential data breaches or system compromise. This configuration helps meet compliance requirements such as PCI-DSS and ISO 27001 by ensuring proper access controls are in place.
  description: This rule checks that the Kubelet on GKE node pools has either the authorization webhook or RBAC enabled, which are critical for controlling access to the Kubernetes API. To verify, inspect the Kubelet configuration for 'authorization-mode' set to 'Webhook' or 'RBAC'. If neither is enabled, configure your GKE cluster to use RBAC by default or set up a webhook authorization. This ensures only authenticated and authorized requests are processed by the Kubernetes nodes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/projects/risk-management
  - https://www.pcisecuritystandards.org/document_library
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.node_pool.containers_kubernetes_kubelet_read_only_port_disabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Kubelet Read Only Port Disabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Kubernetes Kubelet Read-Only Port on GCP Node Pools
  rationale: Enabling the read-only port on Kubernetes Kubelet exposes the node's metrics and status information to unauthenticated users, which can lead to unauthorized access and potential exploitation of the node. This misconfiguration can be leveraged by attackers to conduct reconnaissance, leading to more serious attacks such as privilege escalation or denial of service. Disabling this port is crucial for maintaining the confidentiality and integrity of the cluster's operational data and is often required to meet compliance with security standards.
  description: This rule checks if the read-only port for Kubernetes Kubelet is disabled on all node pools within a Google Kubernetes Engine (GKE) cluster. By default, the read-only port is set to 10255, which can be accessed without authentication. To enhance security, it should be set to '0' to disable it. This can be verified and configured by checking the node pool's Kubelet settings in the GCP Console or using the gcloud command-line tool. Remediation involves updating the node configuration to ensure the read-only port is disabled, thus reducing potential attack vectors.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_methods
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster
  - https://cloud.google.com/security/best-practices
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.node_pool.containers_kubernetes_kubelet_tls_min_1_2_enforced
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Kubelet TLS Min 1 2 Enforced
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Kubelet TLS Min Version 1.2 in GKE Node Pools
  rationale: Ensuring that the Kubelet uses a minimum TLS version of 1.2 helps protect data in transit from being intercepted or altered by unauthorized parties. This setting mitigates risks associated with weak encryption protocols and is critical for maintaining compliance with industry standards such as PCI-DSS and SOC 2, which mandate strong encryption practices.
  description: This rule checks if Kubelet in Google Kubernetes Engine (GKE) node pools enforces a minimum TLS version of 1.2. The TLS settings for Kubelet should be configured to reject connections that do not support at least version 1.2. To verify, inspect the kubelet configuration on each node and ensure the `--tls-min-version` flag is set to `1.2` or higher. Remediate by updating the cluster configuration to enforce the required TLS version, which may involve adjusting the node pool settings or upgrading the GKE version if necessary.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_authz
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-communications
- rule_id: gcp.container.node_pool.containers_kubernetes_nodes_no_public_ip
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Nodes No Public Ip
  scope: container.node_pool.public_access
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Prevent Public IPs on GKE Node Pools
  rationale: Exposing Kubernetes nodes with public IP addresses increases the attack surface, allowing potential unauthorized access to your cluster. This can lead to data breaches, service disruptions, and non-compliance with industry standards like PCI-DSS and HIPAA, which mandate strict control over network access and data protection.
  description: This rule checks that Google Kubernetes Engine (GKE) node pools do not have public IP addresses assigned to their nodes. To ensure that nodes are not publicly accessible, configure your GKE cluster to use private IP addresses only. Verification involves reviewing the networking settings of your node pools and adjusting them via Google Cloud Console or gcloud CLI to disable public IP allocation. Remediation can be done by creating node pools with private IP addresses only, enhancing security and compliance.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.container.node_pool.containers_kubernetes_shielded_secure_boot_enabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Shielded Secure Boot Enabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Shielded Secure Boot for Node Pools
  rationale: Enabling Shielded Secure Boot on Kubernetes node pools reduces the risk of firmware attacks by ensuring that the system boots with only trusted software. This enhances the security posture by protecting against unauthorized changes to the boot environment, a critical defense in maintaining data integrity and compliance with standards like PCI-DSS and SOC2.
  description: 'This rule checks if Shielded Secure Boot is enabled on node pools within GCP Kubernetes Engine. Shielded Secure Boot ensures that each boot component is cryptographically verified, safeguarding against tampered boot firmware. To verify, ensure that the node pool configuration includes ''shieldedInstanceConfig: { enableSecureBoot: true }''. Remediation involves updating your node pool configurations through GCP Console or CLI to enable Shielded Secure Boot.'
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#shielded_nodes
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/shielded-cloud
- rule_id: gcp.container.node_pool.containers_kubernetes_workload_identity_enabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Workload Identity Enabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Workload Identity on GKE Node Pools
  rationale: Enabling Kubernetes Workload Identity on GKE node pools enhances security by ensuring that workloads have minimal permissions necessary to access Google Cloud services, reducing the risk of privilege escalation and data breaches. This configuration aligns with the principle of least privilege, mitigating risks associated with credential management and unauthorized access to resources.
  description: This rule verifies that Kubernetes Workload Identity is enabled on Google Kubernetes Engine (GKE) node pools. Workload Identity allows Kubernetes service accounts to act as Google service accounts, providing secure and fine-grained access control. To enable it, configure the node pool with Workload Identity during creation or update by setting the `--workload-pool` flag. Ensure that Kubernetes service accounts are properly annotated to utilize this identity feature. Remediation involves updating existing node pools to enable this setting through the GCP Console or `gcloud` CLI.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity
  - https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/nodePools
  - 'CIS GCP Benchmark: Section 5.3.1 Enable Workload Identity for GKE Clusters'
  - 'NIST SP 800-53: AC-6 Least Privilege'
- rule_id: gcp.container.node_pool.disk_cmek_enforcement
  service: container
  resource: node_pool
  requirement: Disk Cmek Enforcement
  scope: container.node_pool.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Node Pool Disks Use Customer-Managed Encryption Keys
  rationale: Enforcing the use of Customer-Managed Encryption Keys (CMEK) for node pool disks enhances data security by allowing organizations to maintain control over encryption keys. This minimizes the risk of unauthorized access to sensitive data, aligns with data protection regulations, and reduces the impact of potential data breaches. Implementing CMEK also supports compliance with standards like PCI-DSS and HIPAA that require strong encryption practices.
  description: This rule checks if node pools in Google Kubernetes Engine (GKE) clusters are configured to use Customer-Managed Encryption Keys for their persistent disks. To verify, inspect the node pool's configuration for CMEK settings in the GCP Console or use the gcloud CLI. Remediation involves updating the node pool to specify a CMEK during creation or through a configuration update. This ensures that data at rest is encrypted using keys managed and controlled by the organization, rather than Google-managed keys.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-disks
  - https://cloud.google.com/security/compliance/cis#cis-gke
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.container.node_pool.k8s_kubelet_anonymous_auth_disabled
  service: container
  resource: node_pool
  requirement: K8s Kubelet Anonymous Auth Disabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubelet Anonymous Authentication is Disabled
  rationale: Disabling anonymous authentication for kubelet nodes reduces the risk of unauthorized access to Kubernetes nodes, which can prevent data breaches and unauthorized data manipulation. This is crucial for maintaining the confidentiality and integrity of workloads and aligns with compliance requirements such as PCI-DSS and ISO 27001 that mandate secure access controls.
  description: This rule checks whether anonymous authentication is disabled for kubelets in your GCP Kubernetes Engine node pools. Anonymous authentication allows access to the kubelet API without authentication, which can be exploited by attackers. To verify, review node pool configurations and ensure the `--anonymous-auth` flag is set to `false`. Remediate by updating the node pool configurations to disable anonymous authentication.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_kubelet_authorization
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/architecture/nodes/#kubelet
- rule_id: gcp.container.node_pool.k8s_kubelet_client_cert_rotation_enabled
  service: container
  resource: node_pool
  requirement: K8s Kubelet Client Cert Rotation Enabled
  scope: container.node_pool.key_rotation
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable K8s Kubelet Client Cert Rotation in Node Pools
  rationale: Enabling Kubelet client certificate rotation helps ensure that the certificates used by each node's Kubelet component are automatically and regularly renewed, reducing the risk of certificate expiration which can lead to service disruptions. It mitigates potential attack vectors where expired certificates could be exploited, thus maintaining continuous, secure communication in your Kubernetes environment.
  description: This rule checks if the automatic rotation of client certificates for Kubelets in GCP Kubernetes Engine node pools is enabled. Without this setting, node-level security is weakened as certificates can expire without notice, leading to potential denial of service or unauthorized access. Verify this setting via the GCP Console under Kubernetes Engine settings or use gcloud CLI commands to ensure that the --rotate-certificates flag is enabled. Remediate by configuring your node pool with certificate rotation enabled, ensuring Kubelets are set to automatically renew their client certificates.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
  - https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools
  - 'CIS GCP Benchmark: 5.2.3 - Ensure Kubelet client certificate rotation is enabled'
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://security.googleblog.com/2017/10/introducing-gke-110-security-improvements.html
- rule_id: gcp.container.node_pool.k8s_kubelet_protect_kernel_defaults_enabled
  service: container
  resource: node_pool
  requirement: K8s Kubelet Protect Kernel Defaults Enabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubelet Protect Kernel Defaults is Enabled
  rationale: Enabling Kubelet Protect Kernel Defaults is crucial for maintaining the integrity and security of the node's kernel parameters. It prevents unauthorized changes that could expose vulnerabilities or lead to misconfigurations. This setting helps in meeting compliance requirements by ensuring the node's kernel settings are consistent with security policies and frameworks, thereby reducing the risk of exploitation.
  description: This rule checks if the 'Protect Kernel Defaults' feature is enabled for Kubelet on GKE node pools. This setting ensures that the Kubernetes nodes adhere to secure kernel configurations, preventing potential security vulnerabilities arising from incorrect kernel parameters. Verify this setting through the GCP Console or the gcloud command-line tool. To remediate, configure the node pool with 'Protect Kernel Defaults' enabled, ensuring compliance with security best practices.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#protect-kernel-defaults
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.node_pool.k8s_kubelet_read_only_port_disabled
  service: container
  resource: node_pool
  requirement: K8s Kubelet Read Only Port Disabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable K8s Kubelet Read Only Port in Node Pools
  rationale: Disabling the Kubelet read-only port prevents unauthorized access to sensitive metrics and data, reducing the risk of information disclosure and potential exploitation by attackers. This is crucial for maintaining the confidentiality and integrity of cluster operations and is aligned with best practices for securing Kubernetes environments.
  description: This rule checks whether the read-only port for the Kubelet is disabled on all nodes within a GCP Kubernetes Engine node pool. The Kubelet's read-only port, when enabled, can expose sensitive endpoint information without authentication, making it a target for attackers. Ensure the '--read-only-port=0' flag is set in the Kubelet configuration on each node. Remediation involves updating the node pool configuration to disable the read-only port and applying the changes to the cluster nodes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.cisecurity.org/benchmark/kubernetes
- rule_id: gcp.container.node_pool.node_pool_integrity_monitoring_enabled
  service: container
  resource: node_pool
  requirement: Node Pool Integrity Monitoring Enabled
  scope: container.node_pool.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Integrity Monitoring on GKE Node Pools
  rationale: Enabling integrity monitoring on GKE node pools helps detect unauthorized changes to the operating system and kernel. This is crucial for maintaining the security of containerized applications, as it can prevent potential breaches that may lead to data loss or service disruption. Furthermore, it supports compliance with industry standards demanding regular system integrity checks.
  description: This rule checks if integrity monitoring is enabled for Google Kubernetes Engine (GKE) node pools. Integrity monitoring ensures that any unexpected changes to the node's kernel or system packages are detected. To verify, ensure that the 'enable-integrity-monitoring' flag is set when creating or updating a node pool. Remediation involves using the GCP Console or gcloud CLI to enable integrity monitoring on existing node pools, enhancing the overall security posture by protecting against unauthorized modifications.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#node_pool_security
  - https://cloud.google.com/security/compliance/cis-gke-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/security-foundations/compliance
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.nodepool.metadata_server_enabled
  service: container
  resource: nodepool
  requirement: Metadata Server Enabled
  scope: container.nodepool.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Metadata Server on GKE Node Pools
  rationale: Enabling the metadata server on GKE node pools is crucial for managing service accounts, accessing instance metadata securely, and ensuring proper authorization checks are in place. This configuration mitigates the risk of unauthorized access to sensitive metadata that could lead to data breaches or privilege escalation attacks, thereby supporting compliance with data protection regulations like GDPR and HIPAA.
  description: This rule checks if the metadata server is enabled on Google Kubernetes Engine (GKE) node pools. The metadata server provides a secure channel for workloads to access metadata and service account credentials necessary for operations. Verifying this setting involves ensuring that the 'workload-metadata' configuration is set to 'GKE_METADATA' or 'SECURE', which can be adjusted through the GCP Console or gcloud CLI. Remediation involves updating the node pool configuration to enable the metadata server if it is not already active.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/protecting-cluster-metadata
  - CIS Google Kubernetes Engine Benchmark v1.2.0, Section 6.3.1
  - NIST SP 800-53 Rev. 5, AC-3 Access Enforcement
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.container.pod.seccomp_profile_runtime_default_enabled
  service: container
  resource: pod
  requirement: Seccomp Profile Runtime Default Enabled
  scope: container.pod.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Seccomp Profile Runtime Default for GKE Pods
  rationale: Seccomp (Secure Computing Mode) provides a way to filter system calls that a container can make, enhancing security by reducing the attack surface. Enforcing the Runtime Default profile helps prevent privilege escalation and protects against kernel vulnerabilities. This mitigation is critical for meeting compliance with standards like NIST SP 800-190 and PCI-DSS that emphasize container isolation and protection.
  description: 'This rule checks that GKE pods are configured to use the Seccomp Runtime Default profile, which restricts system calls to a defined set. To verify, ensure the pod''s securityContext includes ''seccompProfile'' set to ''type: RuntimeDefault''. If not configured, update your Kubernetes pod specifications to include this setting. This can be done by editing the YAML file of the pod or deployment and redeploying it. Enabling this profile provides a baseline level of security against container escape vulnerabilities.'
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_syscalls_with_seccomp
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tutorials/clusters/seccomp/
- rule_id: gcp.container.pod.service_account_token_mount_configured
  service: container
  resource: pod
  requirement: Service Account Token Mount Configured
  scope: container.pod.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Service Account Token Mount Configuration in Pods
  rationale: Proper configuration of service account token mounts in Kubernetes pods is crucial to prevent unauthorized access to the Kubernetes API, reducing the risk of privilege escalation and data breaches. Misconfigured tokens can expose sensitive information, leading to potential compliance violations with standards like NIST SP 800-53 and PCI-DSS.
  description: This rule checks if Kubernetes pods have their service account token volumes properly configured, ensuring that tokens are not automatically mounted unless explicitly needed. To verify, inspect the pod specifications for the 'automountServiceAccountToken' field and ensure it is set to 'false' for pods that do not require API access. Remediation involves updating the pod configuration to disable automatic token mounting unless required for specific workloads.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#token-volume-projection
  - https://www.cisecurity.org/benchmark/kubernetes
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.rbac.containers_kubernetes_authorization_mode_rbac_enabled
  service: container
  resource: rbac
  requirement: Containers Kubernetes Authorization Mode RBAC Enabled
  scope: container.rbac.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Kubernetes RBAC Authorization Mode is Enabled
  rationale: Enabling RBAC (Role-Based Access Control) in Kubernetes is crucial for managing permissions and access to resources within your cluster. Without RBAC, there is a risk of unauthorized access, which can lead to data breaches, unauthorized resource modifications, and potential downtime. This configuration supports compliance with frameworks like NIST and ISO 27001, which emphasize the importance of access control.
  description: This rule checks if the Kubernetes cluster is configured with RBAC enabled, which is essential for granular access management. To verify, inspect the Kubernetes API server startup parameters to ensure '--authorization-mode' includes 'RBAC'. Remediation involves updating the cluster configuration to include 'RBAC' in the authorization mode settings, ensuring that roles and role bindings are properly defined to control access.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
- rule_id: gcp.container.rbac.containers_kubernetes_no_cluster_admin_bindings_to_users
  service: container
  resource: rbac
  requirement: Containers Kubernetes No Cluster Admin Bindings To Users
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Prevent Cluster Admin Role Bindings to Individual Users
  rationale: Assigning the cluster-admin role to individual users increases the risk of privilege abuse, leading to potential unauthorized access and control over Kubernetes clusters. This practice can result in non-compliance with frameworks like NIST and ISO 27001, as it violates the principle of least privilege and can expose sensitive data to unauthorized users.
  description: This rule checks for any direct bindings of the cluster-admin role to individual users in Kubernetes clusters on GCP. Such bindings should be avoided to prevent excessive privilege allocation. Instead, users should be assigned specific roles that match their required permissions. Remediation involves reviewing the existing role bindings, identifying any users with the cluster-admin role, and modifying their permissions to adhere to least privilege principles.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
- rule_id: gcp.container.rbac.containers_kubernetes_no_wildcard_rules_in_clusterroles
  service: container
  resource: rbac
  requirement: Containers Kubernetes No Wildcard Rules In Clusterroles
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Avoid Wildcard Permissions in Kubernetes ClusterRoles
  rationale: Wildcard permissions in Kubernetes ClusterRoles can lead to unauthorized access to cluster resources, increasing the risk of privilege escalation and data breaches. By restricting permissions to only those necessary for specific tasks, organizations mitigate potential attack vectors and adhere to the principle of least privilege, aligning with compliance mandates such as PCI-DSS and ISO 27001.
  description: This rule checks for the presence of wildcard ('*') permissions in Kubernetes ClusterRoles within GCP. Wildcard permissions can grant excessive access, allowing actions beyond intended use. To verify, inspect ClusterRole configurations in your GCP Kubernetes Engine and ensure permissions are explicitly defined. Remediation involves updating ClusterRoles to specify exact resources and actions required for each role, avoiding the use of '*'.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
- rule_id: gcp.container.rbac.containers_kubernetes_subjects_scoped_to_namespaces
  service: container
  resource: rbac
  requirement: Containers Kubernetes Subjects Scoped To Namespaces
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes RBAC Subjects Are Namespace Scoped
  rationale: Limiting RBAC subjects to specific namespaces minimizes the risk of privilege escalation and unauthorized access to resources across the cluster. This practice helps organizations comply with the principle of least privilege, reducing the attack surface and potential impact of malicious activities. It also aligns with compliance requirements to enforce strict access controls.
  description: This rule checks if Kubernetes RBAC subjects are restricted to specific namespaces rather than having cluster-wide permissions. To verify, review the RBAC RoleBindings and ClusterRoleBindings to ensure roles are assigned within namespaces. Remediation involves auditing existing bindings and adjusting them to use namespace-specific roles and bindings. This ensures that users and services have access only to the necessary resources within their designated namespaces.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/role-based-access-control
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.container.rbac.k8s_default_sa_not_cluster_admin
  service: container
  resource: rbac
  requirement: K8s Default Sa Not Cluster Admin
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Prevent Default K8s Service Account from Cluster Admin Role
  rationale: Allowing the default Kubernetes service account to have cluster-admin role grants it excessive permissions, increasing the risk of privilege escalation and unauthorized access. This can lead to data breaches, unauthorized resource manipulations, and compliance violations with frameworks like NIST and PCI-DSS, thus compromising organizational security posture.
  description: This rule checks if the default Kubernetes service account is assigned the cluster-admin role within GCP Kubernetes Engine clusters. To mitigate security risks, ensure that the default service account does not have cluster-admin privileges by adjusting role bindings through GCP IAM. Review current role assignments and modify them as necessary to adhere to the principle of least privilege.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/iam
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.container.rbac.k8s_no_cluster_admin_to_authenticated
  service: container
  resource: rbac
  requirement: K8s No Cluster Admin To Authenticated
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Restrict Cluster Admin Role for Authenticated Users
  rationale: Assigning the cluster admin role to all authenticated users can lead to unauthorized access and potential compromise of the entire Kubernetes cluster. This poses a significant security risk, allowing users to deploy malicious workloads or exfiltrate sensitive data. Limiting cluster admin privileges is crucial for maintaining a secure and compliant environment, ensuring adherence to best practices and regulatory standards such as NIST and PCI-DSS.
  description: This rule checks that the Kubernetes cluster does not assign the cluster admin role to all authenticated users by reviewing RBAC configurations. Administrators should ensure that only specific, trusted identities are granted cluster admin privileges. To remediate, audit the existing role bindings and adjust permissions by creating specific roles with the least privilege principle in mind and assigning them to designated users or service accounts. Regularly monitor and update permissions to maintain a secure setup.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.container.rbac.k8s_wildcard_verbs_disallowed
  service: container
  resource: rbac
  requirement: K8s Wildcard Verbs Disallowed
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disallow Wildcard Verbs in Kubernetes RBAC Policies
  rationale: Allowing wildcard verbs such as '*' in Kubernetes RBAC policies can lead to over-permissioning, increasing the risk of unauthorized access and potential exploitation of the Kubernetes environment. This can result in data breaches, service disruptions, and non-compliance with security standards like NIST SP 800-53 and ISO 27001, which require least privilege access controls.
  description: This rule checks for Kubernetes RBAC policies that use wildcard verbs, which allow unrestricted actions on resources. To secure your environment, replace wildcard verbs with specific actions such as 'get', 'list', or 'watch', based on the intended use case. Verify current policies through the GCP Console or CLI, and update any policies using wildcard verbs to ensure compliance with the principle of least privilege.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/73906.html
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.rbac.no_system_anonymous_bindings
  service: container
  resource: rbac
  requirement: No System Anonymous Bindings
  scope: container.rbac.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent System Anonymous Role Bindings in GKE Clusters
  rationale: Allowing system anonymous bindings in Kubernetes Engine can expose your cluster to unauthorized access, leading to potential data breaches or service disruptions. This practice can violate compliance requirements such as PCI-DSS and SOC2, where strict access controls are mandated. Unauthorized users could exploit this access to perform malicious actions, compromising the integrity and availability of your workloads.
  description: This rule checks for the presence of role bindings in Google Kubernetes Engine (GKE) that grant permissions to the 'system:anonymous' user. To verify, review the RoleBindings and ClusterRoleBindings in your GKE clusters for any roles assigned to 'system:anonymous'. To remediate, remove or replace these bindings with specific user or group bindings that adhere to the principle of least privilege. This ensures only authenticated and authorized users have access to sensitive operations within your clusters.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
- rule_id: gcp.container.rbac.nondefault_unauthenticated_bindings_absent
  service: container
  resource: rbac
  requirement: Nondefault Unauthenticated Bindings Absent
  scope: container.rbac.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure No Unauthenticated RBAC Bindings in Kubernetes Clusters
  rationale: Allowing unauthenticated access to Kubernetes clusters can lead to unauthorized users exploiting services, accessing sensitive data, or causing service disruptions. This poses significant security risks and could result in financial loss, reputational damage, and compliance violations with standards like PCI-DSS and HIPAA.
  description: This check ensures that Kubernetes Role-Based Access Control (RBAC) configurations in GCP do not have unauthenticated bindings outside of default settings. Verify that no roles are bound to unauthenticated users by reviewing the RBAC policies in your GKE clusters. Remediation involves auditing and removing any nondefault bindings to the unauthenticated group, ensuring only authenticated and authorized users have access.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.service.containers_kubernetes_external_ips_not_used
  service: container
  resource: service
  requirement: Containers Kubernetes External Ips Not Used
  scope: container.service.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable External IPs for Kubernetes Services
  rationale: Exposing Kubernetes services with external IPs can lead to unauthorized access and potential data breaches. By restricting external IPs, you minimize the attack surface of your cluster, thus protecting sensitive workloads and meeting compliance with industry standards such as NIST and ISO 27001.
  description: This rule checks if any Kubernetes service within your GCP environment is configured with an external IP. To enhance security, it's recommended to use internal IPs and leverage ingress controllers for controlled external access. Review your Kubernetes service configurations in the GCP Console or via kubectl, and ensure external IPs are not specified. Transition to internal-only IPs to mitigate exposure risks.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps
  - https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/73906.html
  - https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
- rule_id: gcp.container.service.containers_kubernetes_nodeport_usage_restricted
  service: container
  resource: service
  requirement: Containers Kubernetes Nodeport Usage Restricted
  scope: container.service.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Restrict Kubernetes NodePort Service Usage
  rationale: Open NodePorts can expose Kubernetes services to the entire internet, increasing the risk of unauthorized access and potential attacks such as Denial of Service (DoS). Restricting NodePort usage helps mitigate these risks and aligns with best practices for minimizing exposure of internal services, which is crucial for maintaining a secure and compliant cloud environment.
  description: This rule checks if Kubernetes services are configured to use NodePort, which exposes an application on a static port on each node's IP. It is recommended to minimize the use of NodePorts due to their exposure to the internet. Verification involves inspecting service configurations for NodePort usage. Remediation includes using internal or ClusterIP services, or implementing ingress controllers for controlled external access.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps
  - https://cloud.google.com/security-command-center/docs/how-to-remediate-container-vulnerabilities
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.service.containers_kubernetes_type_loadbalancer_internal_on_required
  service: container
  resource: service
  requirement: Containers Kubernetes Type Loadbalancer Internal On Required
  scope: container.service.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure LoadBalancer Services are Internal Only
  rationale: Requiring Kubernetes LoadBalancer services to be internal minimizes exposure to external networks, reducing the attack surface and protecting against unauthorized access. This is crucial for safeguarding sensitive data in compliance with regulations like GDPR and PCI-DSS, and for mitigating risks such as data breaches and DDoS attacks.
  description: This rule checks if Kubernetes LoadBalancer services in GCP are configured as internal-only, which restricts traffic to within the Google Cloud Virtual Private Cloud (VPC). Verify this setting by ensuring the spec.loadBalancerIP field is not specified or the spec.loadBalancerSourceRanges field is set to internal IP ranges. To remediate, update the service manifest to remove any external IP configurations and specify internal-only settings in the GCP Console or via kubectl.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing
  - https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview#external_load_balancing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer
- rule_id: gcp.container.workload.containers_kubernetes_drop_net_raw_and_reduce_caps
  service: container
  resource: workload
  requirement: Containers Kubernetes Drop Net Raw And Reduce Caps
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Least Privilege on Kubernetes Container Capabilities
  rationale: Restricting container capabilities, particularly dropping NET_RAW, minimizes the attack surface by preventing containers from performing potentially harmful network operations. This is crucial for maintaining a secure Kubernetes environment and protecting against privilege escalation attacks, which can lead to unauthorized access and data breaches. Complying with this helps meet security standards and reduces risks associated with misconfigured containers.
  description: This rule ensures that Kubernetes containers have the NET_RAW capability dropped and other unnecessary capabilities minimized. Containers should only be granted the capabilities they need to function, adhering to the principle of least privilege. Verify the security context of your Kubernetes Pod specifications and adjust them to drop NET_RAW and other superfluous capabilities. Remediation involves updating your Pod or Deployment configurations to include a securityContext section that explicitly drops these capabilities.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/pod-security-policies
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/security_and_compliance/using-scc
- rule_id: gcp.container.workload.containers_kubernetes_env_no_plaintext_secrets
  service: container
  resource: workload
  requirement: Containers Kubernetes Env No Plaintext Secrets
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Prevent Plaintext Secrets in Kubernetes Container Environments
  rationale: Storing secrets in plaintext within Kubernetes environments poses significant security risks, such as unauthorized access and data breaches. This practice violates compliance with standards like PCI-DSS and HIPAA, which require robust data protection measures. Ensuring secrets are encrypted protects against insider threats and external attacks, maintaining trust and compliance.
  description: This rule checks for the presence of plaintext secrets in environment variables in Kubernetes containers. Organizations should use Kubernetes Secrets to manage sensitive information securely. Remediation involves auditing container configurations, replacing plaintext secrets with references to encrypted Kubernetes Secrets, and ensuring all deployments adhere to this standard. Regular audits and automated checks can help maintain compliance.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/secret
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://cloud.google.com/architecture/best-practices-for-secret-management
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf
- rule_id: gcp.container.workload.containers_kubernetes_host_network_pid_ipc_disabled
  service: container
  resource: workload
  requirement: Containers Kubernetes Host Network Pid Ipc Disabled
  scope: container.workload.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Disable Host Network, PID, and IPC in Kubernetes Workloads
  rationale: Allowing containers to use the host's network, PID, or IPC namespaces can expose sensitive host information and resources to potentially malicious workloads. This increases the attack surface and the risk of container escape, compromising the entire node. Adhering to this practice supports compliance with security frameworks such as NIST and ensures isolation between containerized applications.
  description: This rule checks whether Kubernetes workloads are configured to use their own network, PID, and IPC namespaces, rather than sharing the host's. To verify, inspect the Pod specifications to ensure 'hostNetwork', 'hostPID', and 'hostIPC' are set to 'false'. Remediation involves updating the Pod definitions to disable host namespace sharing, enhancing workload isolation and security.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_pod_and_container_capabilities
  - https://cloud.google.com/architecture/best-practices-for-running-containers
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.container.workload.containers_kubernetes_image_tag_not_latest
  service: container
  resource: workload
  requirement: Containers Kubernetes Image Tag Not Latest
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Avoid Using 'latest' Tag for Kubernetes Container Images
  rationale: Using the 'latest' tag for container images can lead to unpredictable deployments, as it may pull different versions over time. This increases the risk of introducing vulnerabilities and reduces the ability to audit or roll back changes effectively. It can also lead to non-compliance with standards requiring controlled software deployment processes.
  description: This rule checks whether Kubernetes workloads are configured with container image tags set to 'latest'. Instead, specify a specific version tag to ensure consistent and predictable deployments. Verify by reviewing Kubernetes manifests or deployment configurations for image tags. To remediate, update the container image tag in your Kubernetes configuration to a specific version number and redeploy the workload.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/container-image-security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/containers/images/#updating-images
- rule_id: gcp.container.workload.containers_kubernetes_no_hostpath_mounts
  service: container
  resource: workload
  requirement: Containers Kubernetes No Hostpath Mounts
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Avoid HostPath Mounts in Kubernetes Workloads
  rationale: HostPath mounts can expose sensitive parts of the host filesystem to containers, increasing the risk of privilege escalation and data leakage. This may lead to unauthorized access, potential data breaches, and non-compliance with security standards such as NIST and PCI-DSS which mandate the protection of sensitive data and system integrity.
  description: This rule checks Kubernetes workloads for the use of HostPath mounts, which should be avoided to prevent exposing the host's filesystem to containers. Verify workloads by scanning their configurations and removing any HostPath volumes. Instead, use persistent storage solutions like PersistentVolumeClaims (PVCs) to safely manage storage. Remediation involves updating workload configurations to eliminate HostPath usage and implementing secure storage practices.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-123.pdf
  - https://cloud.google.com/architecture/security-foundations/compliance
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.workload.containers_kubernetes_no_privileged_containers
  service: container
  resource: workload
  requirement: Containers Kubernetes No Privileged Containers
  scope: container.workload.least_privilege
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Ensure No Privileged Containers in Kubernetes Workloads
  rationale: Running containers in privileged mode grants them elevated permissions, potentially allowing attackers to gain root access to the host node, leading to data breaches or infrastructure compromise. This can result in severe business impacts, including financial losses, reputational damage, and non-compliance with regulations like PCI-DSS and SOC2.
  description: This rule checks Kubernetes workloads to ensure that no containers are running with privileged access. Privileged containers can bypass security controls and have unrestricted access to the host system. To verify, inspect the security context configurations in your Kubernetes deployment YAML files and ensure the 'privileged' flag is set to false. Remediate by updating configurations to disable privileged mode and redeploy the workloads.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.workload.containers_kubernetes_read_only_root_filesystem
  service: container
  resource: workload
  requirement: Containers Kubernetes Read Only Root Filesystem
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Ensure Kubernetes Containers Have Read-Only Root Filesystem
  rationale: Enforcing a read-only root filesystem for Kubernetes containers minimizes the risk of unauthorized changes to the container's filesystem, which can prevent persistent malware and limit the impact of attacks. This control is crucial for maintaining the integrity of applications running in containers and aligns with compliance frameworks that emphasize system integrity and data protection.
  description: This rule checks that Kubernetes containers in your GCP environment are configured with a read-only root filesystem. A read-only root filesystem helps mitigate the risk of filesystem tampering by attackers, thus reducing the attack surface. To verify compliance, ensure that the 'readOnlyRootFilesystem' attribute is set to 'true' in the container's security context. Remediation involves updating your Kubernetes deployment configuration to enforce this setting across all applicable containers.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#read-only-root-filesystem
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
- rule_id: gcp.container.workload.containers_kubernetes_run_as_non_root
  service: container
  resource: workload
  requirement: Containers Kubernetes Run As Non Root
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Ensure Kubernetes Containers Run as Non-Root User
  rationale: Running containers as a non-root user reduces the risk of privilege escalation within the container, thereby mitigating potential impacts from vulnerabilities or malicious attacks. This practice is crucial for maintaining a secure environment, aligning with compliance requirements such as PCI-DSS and ISO 27001, which demand strict control over user permissions.
  description: This rule verifies that Kubernetes workloads are configured to run containers as non-root users. The check ensures the 'runAsNonRoot' field in the security context of a PodSpec is set to true. To remediate, update your Kubernetes deployment configurations to include a security context that specifies a non-root user ID, ensuring that no container within the workload can execute processes as the root user.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#prevent_root
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups
- rule_id: gcp.container.workload.containers_kubernetes_seccomp_profile_runtime_default
  service: container
  resource: workload
  requirement: Containers Kubernetes Seccomp Profile Runtime Default
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Seccomp Profile Runtime Default in Kubernetes Containers
  rationale: Using the default seccomp profile in Kubernetes containers enhances security by minimizing the attack surface and mitigating potential exploitation of kernel vulnerabilities. It protects against unauthorized system calls that could lead to privilege escalation or container escape, thereby safeguarding sensitive workloads and maintaining compliance with security standards.
  description: 'This rule checks if Kubernetes containers in GCP workloads are using the seccomp profile set to ''RuntimeDefault'', which restricts system call capabilities to a safe subset. To verify, ensure that your pod specifications include ''securityContext.seccompProfile.type: RuntimeDefault''. If not configured, update your Kubernetes deployment manifests to include this setting. This practice helps in adhering to least privilege principles and enhances container security posture.'
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/seccomp
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/tutorials/clusters/seccomp/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.workload.secrets_as_env_var_detection
  service: container
  resource: workload
  requirement: Secrets As Env Var Detection
  scope: container.workload.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Detect and Mitigate Secrets in Env Vars for GKE Workloads
  rationale: Storing secrets such as API keys or passwords in environment variables can expose sensitive information if the pod is compromised, leading to unauthorized access and potential data breaches. This practice violates compliance requirements such as PCI-DSS and HIPAA, which mandate strict controls over sensitive data handling.
  description: This rule checks if Kubernetes workloads in Google Kubernetes Engine (GKE) clusters are configured with secrets exposed as environment variables, which is a risky practice. To verify, inspect the workload configurations for any environment variable that contains sensitive information. Remediation involves using Kubernetes Secrets to store and manage sensitive data securely, and referencing them in workloads without directly exposing them as environment variables.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/secrets
  - 'CIS GCP Benchmark: 5.4.1 Ensure that Secrets are not stored in the pod specification'
  - https://www.pcisecuritystandards.org/pci_security/maintaining_payment_security
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://kubernetes.io/docs/concepts/configuration/secret/
- rule_id: gcp.datacatalog.entry.data_catalog_crawler_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Crawler Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Crawler Logs for Enhanced Monitoring
  rationale: Enabling Data Catalog Crawler Logs is crucial for monitoring and auditing data access patterns, which helps in identifying unauthorized access and potential data breaches. It aids in compliance with regulations such as GDPR, HIPAA, and SOC 2 by maintaining a detailed audit trail. This logging capability supports proactive security measures by providing insights into data management activities.
  description: This rule checks if logging is enabled for Data Catalog crawlers to monitor their activity. Specifically, it ensures that crawler logs are captured and stored in Google Cloud Logging. To verify, navigate to the Data Catalog service in the Google Cloud Console and ensure logging is configured for crawler activities. Remediation involves setting up logging by configuring the appropriate IAM roles and enabling logging in the Data Catalog settings to capture all crawler-related activities.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/logging
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_crawler_network_private_only
  service: datacatalog
  resource: entry
  requirement: Data Catalog Crawler Network Private Only
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Data Catalog Crawler Operates in Private Networks
  rationale: Restricting Data Catalog crawlers to private networks minimizes exposure to unauthorized internet access, reducing the risk of data breaches and ensuring compliance with regulatory frameworks like GDPR and HIPAA. This control helps prevent unauthorized data scraping and potential exfiltration, safeguarding sensitive data assets.
  description: This rule verifies that Data Catalog crawlers operate solely within private networks, ensuring that data discovery processes do not interact with the public internet. To enforce this, configure Data Catalog entries to use private IP addresses and limit access through VPC Service Controls. Remediation involves adjusting network configurations to ensure private connectivity and reviewing IAM policies to enforce network restrictions.
  references:
  - https://cloud.google.com/data-catalog/docs/networking
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/nist-special-publication-800-53-revision-5-security-and-privacy-controls-information
  - https://cloud.google.com/vpc-service-controls/docs
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.datacatalog.entry.data_catalog_crawler_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Crawler Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Data Catalog Crawler Role Enforces Least Privilege
  rationale: Enforcing least privilege for Data Catalog crawler roles minimizes the risk of unauthorized access and data exposure. Misconfigured roles can lead to privilege escalation, data breaches, and non-compliance with regulatory standards such as GDPR and HIPAA. By limiting permissions, organizations can reduce their attack surface and improve their security posture.
  description: This rule checks that the Data Catalog crawler role is configured to follow the principle of least privilege by granting only necessary permissions. Verify that roles assigned to crawlers do not exceed the minimum required permissions for their tasks. To remediate, audit the IAM policies associated with the Data Catalog and adjust permissions accordingly, ensuring compliance with least privilege principles.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/manage-iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_crawler_scope_restricted_to_allowlist
  service: datacatalog
  resource: entry
  requirement: Data Catalog Crawler Scope Restricted To Allowlist
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Data Catalog Crawler Scope to Allowlist
  rationale: Limiting the Data Catalog Crawler's scope to a predefined allowlist helps prevent unauthorized access and data leaks by ensuring that only specified data sources are indexed and managed. This controls access, minimizes exposure to sensitive information, and supports compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule verifies that the Data Catalog Crawler's scope is restricted to an allowlist of approved data sources. To ensure compliance, regularly review and update the allowlist to include only necessary and authorized data sets. Remediation involves configuring the Data Catalog settings to specify an allowlist, which can be managed through the Google Cloud Console or via the gcloud command-line tool.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/catalog-entry
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/data-catalog/docs/security
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.datacatalog.entry.data_catalog_credentials_in_secrets_manager
  service: datacatalog
  resource: entry
  requirement: Data Catalog Credentials In Secrets Manager
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: high
  title: Ensure Data Catalog Credentials are Stored in Secrets Manager
  rationale: Storing Data Catalog credentials in Secrets Manager prevents unauthorized access and reduces the risk of credential leakage. This practice aligns with security best practices and compliance requirements, protecting sensitive data from being exposed through insecure storage methods, thus mitigating potential data breaches and ensuring regulatory compliance.
  description: This rule checks if Data Catalog credentials are securely stored in Google Cloud Secrets Manager. Ensure that all credentials related to Data Catalog entries are not hardcoded or stored in plaintext but are instead managed by Secrets Manager. To verify, audit the storage locations of your Data Catalog credentials and migrate any found outside of Secrets Manager. Remediation involves using the Secrets Manager API or Console to create secret entries for these credentials and updating your applications to fetch them securely.
  references:
  - https://cloud.google.com/secret-manager/docs
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 4.1
  - 'NIST SP 800-53 Rev 5: AC-2, AC-3, AC-6'
  - https://cloud.google.com/datacatalog/docs/how-to
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_database_cross_account_sharing_restricted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Database Cross Account Sharing Restricted
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Data Catalog Database Cross Account Sharing
  rationale: Restricting cross-account sharing of Data Catalog entries minimizes unauthorized access risks, protects sensitive data from exposure, and helps maintain data sovereignty. It is crucial to control who can access and share data across accounts to prevent data breaches and comply with regulatory requirements such as GDPR and CCPA.
  description: This rule checks if there are policies allowing cross-account sharing of Data Catalog database entries. To verify, review IAM policies and ensure that permissions for sharing entries are limited to trusted accounts only. Remediation involves adjusting IAM policies to restrict sharing permissions and applying least privilege principles to Data Catalog entries.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/policies
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_database_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Database Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of Data Catalog Database Entries
  rationale: Encrypting Data Catalog entries protects sensitive metadata from unauthorized access, reducing the risk of data breaches. This is crucial for maintaining customer trust and adhering to regulatory requirements such as GDPR and CCPA, which mandate the protection of personal data. Encryption at rest ensures that even if physical drives are compromised, the data remains protected.
  description: This rule checks whether entries in the Data Catalog have encryption enabled. It requires configuring the Data Catalog to use Google-managed or customer-managed encryption keys (CMEK) to encrypt metadata at rest. To verify, inspect the encryption settings of Data Catalog entries in the GCP Console or via the gcloud command line. Remediation involves enabling encryption on existing entries or ensuring new entries are configured with encryption from the start.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_database_policy_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Database Policy Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Database Policies
  rationale: Implementing least privilege in Data Catalog helps minimize potential unauthorized access to sensitive data, thereby reducing the risk of data breaches and ensuring compliance with regulatory requirements like GDPR and CCPA. Misconfigured permissions can lead to data exposure, which could result in significant financial and reputational damage.
  description: This rule checks that policies applied to Data Catalog entries grant only the necessary permissions required for users to perform their job functions. It involves verifying IAM roles assigned to users and service accounts against the principle of least privilege. To remediate, audit existing IAM policies and adjust permissions to ensure they are strictly necessary, using predefined roles where possible. Regularly review and update policies to adapt to changes in user roles or organizational requirements.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.datacatalog.entry.data_catalog_dev_endpoint_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Dev Endpoint Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dev Endpoint Encryption in Data Catalog
  rationale: Encrypting data at rest in the Data Catalog is crucial to prevent unauthorized access and maintain data confidentiality, integrity, and compliance with standards such as NIST and ISO 27001. Unencrypted endpoints can lead to data breaches, resulting in financial loss, reputational damage, and legal consequences.
  description: This rule checks if the development endpoints in GCP Data Catalog entries are encrypted at rest. Encryption is achieved using Cloud KMS keys to protect sensitive metadata. To verify, ensure that all entries in the Data Catalog have encryption configured using customer-managed keys. Remediation involves setting up Cloud KMS and configuring the Data Catalog to use these keys for encryption.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypt-data
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datacatalog.entry.data_catalog_dev_endpoint_no_public_access
  service: datacatalog
  resource: entry
  requirement: Data Catalog Dev Endpoint No Public Access
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Restrict Public Access to Data Catalog Dev Endpoints
  rationale: Allowing public access to Data Catalog development endpoints can expose sensitive metadata and configuration details to unauthorized users, leading to potential data breaches and compliance violations. By restricting public access, organizations can reduce attack surfaces and meet regulatory requirements such as GDPR, HIPAA, and PCI-DSS that mandate controlled access to sensitive data.
  description: This rule checks for public access configurations on Data Catalog development endpoints to ensure they are not exposed to the internet. Verify that access policies for Data Catalog entries are set to deny public access by default. Remediation involves reviewing IAM roles and permissions to ensure that only authorized users within the organization can access these endpoints. Implement firewall rules and network policies to further enforce restricted access.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc/docs/firewalls
- rule_id: gcp.datacatalog.entry.data_catalog_dev_endpoint_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Dev Endpoint Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Dev Endpoint Role
  rationale: Enforcing least privilege for Data Catalog roles minimizes the risk of unauthorized access and potential data breaches. Over-privileged roles can lead to accidental or malicious data exposure, violating compliance standards such as GDPR and CCPA, and could result in significant financial and reputational damage.
  description: This rule checks that the Data Catalog Dev Endpoint role has permissions limited to only those necessary for development activities. Verify role assignments to ensure they do not include unnecessary permissions such as 'roles/datacatalog.admin'. Remediation involves auditing current roles and removing any excessive permissions, ensuring they align with the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/manage-iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry.data_catalog_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Entries are Encrypted at Rest
  rationale: Encrypting Data Catalog entries at rest is crucial for protecting sensitive metadata against unauthorized access and potential data breaches. This measure reduces the risk of data exposure, aligns with industry best practices, and helps meet compliance standards such as GDPR, HIPAA, and ISO 27001 that mandate encryption of sensitive data.
  description: This rule checks whether your Data Catalog entries are configured to be encrypted at rest using Google-managed encryption keys (GMEK) or customer-managed encryption keys (CMEK). To verify, ensure that encryption settings are enabled for all entries within the Data Catalog. Remediation involves configuring the Data Catalog service to use GMEK by default or setting up CMEK through Cloud KMS for enhanced control over encryption keys.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypting-entries
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.datacatalog.entry.data_catalog_job_logs_and_metrics_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Job Logs And Metrics Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Data Catalog Job Logs and Metrics are Enabled
  rationale: Enabling logs and metrics for Data Catalog jobs is crucial for monitoring data access and modifications, which helps identify and mitigate potential data breaches. It supports incident response and forensic investigations by providing a trail of activities. Additionally, it aids in meeting compliance requirements like GDPR and CCPA by ensuring accountability and transparency in data operations.
  description: This rule verifies that logging and metrics are enabled for Google Cloud Data Catalog entries, ensuring that all access and changes to data are tracked. To check this, confirm that audit logs are configured for Data Catalog within the Google Cloud Console under Logging > Logs Router. Remediation involves setting up appropriate sinks and enabling audit logging for 'DATA_READ', 'DATA_WRITE', and 'ADMIN_READ' activities for better visibility and control.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_job_network_private_only
  service: datacatalog
  resource: entry
  requirement: Data Catalog Job Network Private Only
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Network for Data Catalog Jobs
  rationale: Restricting Data Catalog jobs to private networks minimizes exposure to unauthorized access, reducing the risk of data breaches. This control is crucial for maintaining data integrity and confidentiality, especially when dealing with sensitive information. It aligns with regulatory requirements for protecting data in transit and supports compliance with frameworks like ISO 27001 and NIST.
  description: This check ensures that Data Catalog jobs are configured to run exclusively within private networks, preventing exposure to public internet threats. Verify that the service account executing Data Catalog jobs has network restrictions applied to ensure the jobs communicate only over private IPs. Remediation involves setting network tags and firewall rules to permit job execution solely on private networks, ensuring secure data handling.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/private-networks
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/best-practices-vpc-design
- rule_id: gcp.datacatalog.entry.data_catalog_job_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Job Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Job Roles
  rationale: Ensuring least privilege in Data Catalog roles minimizes the attack surface by restricting access to only necessary resources. This reduces potential damage from compromised accounts and aligns with compliance requirements such as GDPR and SOC 2, which mandate access controls and data protection. Implementing least privilege helps prevent unauthorized data exposure and promotes accountability by ensuring users operate within their defined boundaries.
  description: This rule checks for adherence to the principle of least privilege in Data Catalog job roles. It ensures that job roles assigned to users or service accounts only provide the minimum permissions necessary to perform their tasks. To verify, review IAM policies for Data Catalog entries and adjust them to remove unnecessary permissions. Remediation involves auditing current role assignments, identifying excessive permissions, and updating roles to align with least privilege principles.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/document-171
  - https://cloud.google.com/security/compliance/soc-2/
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_job_script_location_private_and_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Job Script Location Private And Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Job Script Locations Are Secure and Encrypted
  rationale: Securing and encrypting job script locations in Google Cloud's Data Catalog ensures that sensitive data scripts are protected from unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, especially when handling regulated data, and helps organizations comply with data protection laws such as GDPR and CCPA.
  description: This rule checks that all job script locations in GCP's Data Catalog are set to be private and encrypted. Scripts should be stored in Cloud Storage buckets configured with Bucket Policy Only, ensuring that only authorized users can access them. Encryption at rest should be enabled using Google-managed or customer-managed encryption keys (CMEK). Verification involves reviewing bucket IAM policies and encryption settings. Remediation includes updating bucket policies to restrict access and enabling CMEK as needed.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypt-data
  - https://cloud.google.com/security/encryption-at-rest/default-encryption/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/storage/docs/access-control/iam
- rule_id: gcp.datacatalog.entry.data_catalog_ml_transform_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Ml Transform Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable ML Transform Logs in Data Catalog
  rationale: Enabling logging for ML transforms in Data Catalog is vital for tracking access and changes to machine learning-related metadata, which can help prevent unauthorized access and modifications. It assists in forensic analysis and compliance with regulations such as GDPR and HIPAA, which mandate detailed logging of data access and transformations.
  description: This rule checks if logging is enabled for ML transforms within Google Cloud's Data Catalog service. To verify, ensure that audit logs for ML transformations are active by configuring the appropriate log sinks and filters in the Cloud Logging service. Remediation involves setting up or correcting log configuration to capture ML transformation activities, thereby ensuring that all relevant access and changes are recorded.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/logging/docs/audit
  - CIS GCP Foundation Benchmark v1.3.0, Section 7.3
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_ml_transform_network_private_only
  service: datacatalog
  resource: entry
  requirement: Data Catalog Ml Transform Network Private Only
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Data Catalog ML Transform Uses Private Network
  rationale: Enforcing private network usage for Data Catalog ML Transform reduces the risk of unauthorized access and data exfiltration by restricting access to internal networks only. This is crucial for maintaining data integrity and confidentiality, especially when handling sensitive data subject to regulatory compliance such as GDPR or HIPAA.
  description: This rule checks that all Data Catalog ML Transform operations are conducted over private networks. To verify, ensure that the Data Catalog service is configured to use VPC Service Controls and restricts public network access. Remediation involves updating network configurations to ensure that the service endpoints are only accessible within a private network, leveraging VPC peering or private service access.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/vpc-service-controls/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security
- rule_id: gcp.datacatalog.entry.data_catalog_ml_transform_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Ml Transform Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog ML Transform Role
  rationale: Implementing least privilege for Data Catalog roles mitigates the risk of unauthorized access and potential data breaches, which can lead to data loss, compliance violations, and reputational damage. Ensuring that ML Transform roles have only necessary permissions protects sensitive data and aligns with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks that the ML Transform role in Data Catalog is configured with the minimum permissions necessary for its operation. Verify that the role does not have excess permissions by reviewing IAM policies attached to it. Remediation involves adjusting IAM policies to ensure only required permissions are granted, removing any superfluous access that does not align with explicit job functions.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_partition_access_policies_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Partition Access Policies Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Partition Access
  rationale: Minimizing access to Data Catalog entries reduces the risk of unauthorized data exposure and potential misuse. Implementing least privilege principles helps protect sensitive data, maintain compliance with regulatory standards, and mitigate the impact of insider threats or compromised credentials.
  description: This rule checks for Data Catalog entries to ensure that access policies are configured with the principle of least privilege. It verifies that only necessary permissions are granted to users, groups, or service accounts for data partitions. To remediate, review and adjust IAM policies to align with the minimum access required for each role, removing unnecessary permissions and ensuring proper logging for audit purposes.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-manage-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_partition_catalog_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Partition Catalog Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Partition Catalog is Encrypted at Rest
  rationale: Encrypting data at rest in Data Catalog is crucial for protecting sensitive information from unauthorized access and data breaches. Unencrypted data can be vulnerable to theft or exposure, posing compliance risks with standards like GDPR, HIPAA, and PCI-DSS. Encryption at rest helps mitigate these risks by ensuring that data remains secure even if the storage media is compromised.
  description: This rule checks whether Data Catalog entries, specifically partition catalogs, are encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that encryption settings are configured in the Data Catalog API or via the Google Cloud Console. Remediation involves enabling encryption on existing entries and ensuring new entries are configured with encryption by default.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/encryption-at-rest/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_private_networking_enforced
  service: datacatalog
  resource: entry
  requirement: Data Catalog Private Networking Enforced
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for Data Catalog Entries
  rationale: Enforcing private networking for Data Catalog entries mitigates the risk of unauthorized access and data exposure by restricting access to a secure, internal network. This is crucial for protecting sensitive metadata managed within GCP's Data Catalog, ensuring that only authenticated internal traffic can access it. By aligning with best practices for network segmentation, organizations can reduce the attack surface and comply with security standards such as PCI-DSS and ISO 27001.
  description: This rule checks if private networking is enforced for Data Catalog entries, ensuring they are only accessible from internal IPs. To verify, inspect the network settings of your Data Catalog to confirm that private IPs are used and that public access is disabled. Remediation involves configuring VPC Service Controls and setting up private Google access, ensuring all access requests come from within the defined network perimeter.
  references:
  - https://cloud.google.com/data-catalog/docs/networking
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/vpc-service-controls/docs/overview
- rule_id: gcp.datacatalog.entry.data_catalog_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog RBAC Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Entry Access
  rationale: Implementing least privilege access in Data Catalog is critical to minimize the risk of unauthorized data exposure and potential data breaches. By restricting permissions to only what is necessary, organizations can reduce the attack surface and comply with regulatory requirements like GDPR and CCPA, which mandate stringent data access controls.
  description: This rule checks if the Data Catalog entry permissions adhere to the principle of least privilege. It verifies that roles assigned to users or service accounts are limited to essential access only. To remediate, review current IAM policies on Data Catalog entries and adjust permissions to ensure users have only the minimum necessary access. Audit logs should be enabled to monitor any unauthorized access attempts.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
- rule_id: gcp.datacatalog.entry.data_catalog_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Roles
  rationale: Implementing least privilege in Data Catalog roles minimizes the risk of unauthorized data access and potential data breaches. By restricting permissions to only what is necessary, organizations can reduce their attack surface and comply with regulatory standards such as GDPR and CCPA, which emphasize data protection and privacy.
  description: This rule checks that roles assigned to Data Catalog entries adhere to the principle of least privilege by ensuring that users and service accounts have only the permissions necessary for their tasks. To verify compliance, review IAM policies for Data Catalog entries and adjust role assignments to align with least privilege principles. Remediation involves auditing current role configurations, identifying excessive permissions, and reassigning roles with reduced privileges while ensuring necessary operations are not hindered.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/datacatalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.datacatalog.entry.data_catalog_table_access_policies_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Table Access Policies Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege on Data Catalog Access Policies
  rationale: Implementing least privilege on Data Catalog table access ensures that users and services have only the permissions necessary to perform their tasks, reducing the risk of data breaches and unauthorized access. Failure to apply this principle can lead to excessive permissions, increasing the attack surface and potentially violating compliance frameworks such as GDPR, HIPAA, and NIST SP 800-53.
  description: This rule verifies that access policies on Data Catalog entries are configured following the principle of least privilege. It checks for overly permissive roles or any misconfigurations that grant unnecessary access. To remediate, audit permissions regularly and use predefined roles that align with job functions. Remove any roles that exceed the necessary permissions and implement strict access reviews.
  references:
  - https://cloud.google.com/datacatalog/docs/how-to/granting-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices-for-enterprise-organizations
- rule_id: gcp.datacatalog.entry.data_catalog_table_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Table Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Table Entries are Encrypted at Rest
  rationale: Encrypting Data Catalog table entries at rest is crucial to protect sensitive information from unauthorized access and potential breaches. This measure reduces the risk of data exposure in case of a security incident and helps organizations comply with regulatory requirements such as GDPR and CCPA, which mandate the protection of personal data. Encryption at rest also safeguards intellectual property and maintains customer trust.
  description: This rule checks whether Data Catalog table entries are encrypted at rest using Google-managed encryption keys or customer-managed encryption keys (CMEK). To verify, examine the encryption settings of your entries in the Google Cloud Console under the Data Catalog service. If entries are not encrypted, enable encryption by configuring CMEK or relying on default Google-managed encryption. Remediation involves updating the Data Catalog configuration to ensure all entries are encrypted at rest.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.datacatalog.entry.data_catalog_table_public_sharing_disabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Table Public Sharing Disabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Prevent Public Sharing of Data Catalog Tables
  rationale: Disabling public sharing of Data Catalog tables is crucial to prevent unauthorized access to sensitive information, which could lead to data breaches and non-compliance with regulatory standards. Publicly accessible tables can expose critical business and customer data, increasing the risk of exploitation by malicious actors. Compliance with frameworks such as NIST, PCI-DSS, and ISO 27001 often requires strict access controls to protect data integrity and confidentiality.
  description: This rule checks for any Data Catalog tables that are publicly accessible and ensures public sharing is disabled. Verify that no tables have IAM policies granting 'allUsers' or 'allAuthenticatedUsers' roles. Remediation involves reviewing the IAM policies of each Data Catalog table and removing any permissions that allow public access. Use the Google Cloud Console or gcloud CLI to audit and adjust permissions as necessary.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://cloud.google.com/iam/docs/overview
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_tls_required
  service: datacatalog
  resource: entry
  requirement: Data Catalog TLS Required
  scope: datacatalog.entry.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS is Required for Data Catalog Entries
  rationale: Requiring TLS for Data Catalog entries ensures that data in transit is encrypted, protecting sensitive metadata from interception and unauthorized access. This is crucial for maintaining data integrity and confidentiality, reducing the risk of data breaches, and complying with regulations such as GDPR and HIPAA that mandate encryption of data in transit.
  description: This rule checks if TLS is required for all interactions with Data Catalog entries to ensure secure communication. To verify, inspect the network configuration settings of your Data Catalog service to confirm that TLS is enforced. Remediation involves configuring your service to require TLS by enabling endpoints that support HTTPS and disabling any non-encrypted endpoints. This ensures that all data transmissions are encrypted.
  references:
  - https://cloud.google.com/data-catalog/docs/security
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.datacatalog.entry.data_catalog_trigger_event_sources_restricted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Trigger Event Sources Restricted
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Event Sources for Data Catalog Triggers
  rationale: Restricting event sources for Data Catalog triggers is crucial to prevent unauthorized data access and manipulation, which could lead to data breaches and non-compliance with regulations like GDPR and HIPAA. Unrestricted event sources increase the risk of accidental or malicious data exposure, impacting business operations and damaging trust.
  description: This rule checks that only approved event sources are allowed to trigger actions within the Google Cloud Data Catalog. Ensure that event source restrictions are configured by setting appropriate IAM roles and permissions, limiting who can create and modify these triggers. Remediation involves auditing existing triggers, removing non-essential sources, and implementing least privilege access for those that remain.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/triggers
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS GCP Benchmark v1.3.0 - 6.2
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_trigger_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Trigger Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Entry Trigger Logs
  rationale: Enabling trigger logs for Data Catalog entries is crucial for tracking changes and access patterns, which helps in identifying unauthorized access or modifications. This enhances data governance and compliance with regulatory requirements such as GDPR and CCPA, reducing the risk of data breaches and ensuring accountability.
  description: This rule checks whether audit logging is enabled for Data Catalog entries, ensuring all interactions are logged. To verify, ensure the 'dataAccess' audit logs are enabled in the Cloud Audit Logs for the Data Catalog service. Remediation involves navigating to the IAM & Admin section of the GCP Console, accessing the Audit Logs settings, and enabling 'dataAccess' logs for Data Catalog.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logs
  - https://cloud.google.com/logging/docs/audit/configure-data-access#config-console
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_trigger_targets_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Trigger Targets Least Privilege
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: high
  title: Ensure Data Catalog Trigger Targets Use Least Privilege
  rationale: Applying the principle of least privilege to Data Catalog triggers minimizes the risk of unauthorized access and potential data breaches. This approach is crucial for protecting sensitive information and avoiding compliance violations with standards such as PCI-DSS and HIPAA, which mandate strict access controls to maintain data integrity and confidentiality.
  description: This rule checks if Data Catalog triggers have permissions exceeding their functional requirements. Specifically, it examines IAM roles assigned to triggers and ensures they only have the minimum necessary permissions. To verify, review IAM policies associated with Data Catalog entries and adjust roles to align with least privilege principles. Remediation involves auditing roles and permissions, and restricting access to essential operations only.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.datacatalog.entry.data_catalog_version_immutability_enforced
  service: datacatalog
  resource: entry
  requirement: Data Catalog Version Immutability Enforced
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Immutability of Data Catalog Entry Versions
  rationale: Enforcing immutability on Data Catalog entries ensures that once data is cataloged, it cannot be altered, mitigating risks of data tampering and unauthorized changes. This enhances the integrity and reliability of metadata, which is crucial for compliance with regulations like GDPR and CCPA, and supports audit trails for forensic analysis.
  description: This rule checks if Data Catalog entries have version immutability enforced, ensuring that once a version is created, it cannot be modified or deleted. To verify, review the Data Catalog settings within the GCP Console to confirm that immutability policies are configured. Remediation involves setting up a policy that enforces version immutability, which can be done through the IAM policy bindings or using gcloud CLI commands to lock entry versions.
  references:
  - https://cloud.google.com/data-catalog/docs/
  - https://cloud.google.com/security/compliance/gdpr/
  - https://cloud.google.com/security/compliance/ccpa/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.datacatalog.entry.data_catalog_workflow_cross_account_trusts_restricted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Workflow Cross Account Trusts Restricted
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Cross Account Trusts in Data Catalog Workflows
  rationale: Cross-account trusts in Data Catalog workflows can lead to unauthorized access and data leakage if not properly managed. Restricting these trusts reduces the risk of data exposure and ensures that data processing workflows are only accessed by authorized accounts, aligning with compliance obligations and minimizing potential attack vectors.
  description: This rule checks for configurations in Data Catalog entries where cross-account trusts are allowed in workflows. Ensure that only necessary accounts have trust relationships and that all accounts involved are vetted and monitored. To verify, review the IAM policies associated with Data Catalog entries and ensure that cross-account permissions are minimized. Remediation involves editing IAM policies to limit trust relationships and implementing audit logging to monitor access patterns.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.datacatalog.entry.data_catalog_workflow_execution_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Workflow Execution Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Workflow Roles
  rationale: Configuring least privilege for workflow execution roles in Data Catalog reduces the risk of unauthorized access and potential data breaches. Overly permissive roles can lead to accidental or malicious data exposure, impacting compliance with regulations like GDPR and CCPA, and causing financial and reputational damage.
  description: This rule checks if Data Catalog workflow execution roles are configured with excessive privileges. It ensures roles only have permissions necessary for their intended tasks, adhering to the principle of least privilege. To verify, review IAM policies associated with Data Catalog entries and adjust them to minimize permissions. Remediation involves auditing current roles, identifying excessive permissions, and refining roles to align with operational requirements while maintaining security.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/workflows
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.datacatalog.entry.data_catalog_workflow_kms_encryption_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Workflow KMS Encryption Enabled
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Data Catalog Workflows
  rationale: Enabling KMS encryption for Data Catalog workflows is crucial to protect sensitive data and metadata from unauthorized access and potential data breaches. This measure helps mitigate the risk of data exposure and supports compliance with data protection regulations such as GDPR and HIPAA, which require robust encryption for stored data.
  description: This rule verifies that Google Cloud's Data Catalog workflows use Customer-Managed Encryption Keys (CMEK) to encrypt data at rest. To enable this, configure your Data Catalog entries to utilize a KMS key from Google Cloud Key Management Service (KMS). Verify the encryption settings in the Google Cloud console under the Data Catalog section and ensure that the 'Encryption' field indicates a KMS key. If not, update the configuration to attach a KMS key to the Data Catalog entry.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/overview
- rule_id: gcp.datacatalog.entry.data_governance_access_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Governance Access RBAC Least Privilege
  scope: datacatalog.entry.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Entry Access
  rationale: Implementing least privilege access for Data Catalog entries minimizes the risk of unauthorized data exposure and helps prevent potential data breaches. This is crucial for protecting sensitive information and maintaining compliance with data protection regulations such as GDPR and CCPA. A least privilege model limits access to only what is necessary for users to perform their job functions, reducing the attack surface and potential for insider threats.
  description: This rule checks that IAM roles and permissions granted to users for Data Catalog entries adhere to the principle of least privilege. It ensures that users have access only to the necessary resources and operations required for their role. To verify compliance, review IAM policies on Data Catalog entries and remove any excessive permissions. Remediation may involve adjusting IAM roles, customizing policies, or using predefined roles that align with job responsibilities.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.datacatalog.entry.data_governance_capture_enabled
  service: datacatalog
  resource: entry
  requirement: Data Governance Capture Enabled
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Governance Capture is Enabled for Data Catalog Entries
  rationale: Enabling Data Governance Capture in GCP Data Catalog entries ensures that sensitive data is adequately monitored and protected through lifecycle management. This is crucial for mitigating risks such as unauthorized access and data breaches, while also fulfilling compliance requirements for data privacy regulations like GDPR and CCPA.
  description: This check verifies that Data Governance Capture is activated for entries within the GCP Data Catalog. This setting helps in maintaining audit trails and managing data access policies effectively. To verify, ensure the Data Catalog service is configured to log access and changes to entries. Remediation involves enabling logging and governance features in the Data Catalog settings to track data usage and modifications comprehensively.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/privacy/gdpr
- rule_id: gcp.datacatalog.entry.data_governance_export_destination_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Governance Export Destination Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Governance Export Destinations are Encrypted
  rationale: Encrypting export destinations in Google Data Catalog mitigates risks of unauthorized data access and potential data breaches, ensuring compliance with data protection regulations such as GDPR and CCPA. Encrypting data at rest is crucial for maintaining data confidentiality and integrity in the event of unauthorized access or system compromise.
  description: This rule checks whether export destinations for data governance within Google Data Catalog are encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). To verify, ensure that the export destination storage buckets or databases are configured with encryption at rest. Remediation involves enabling or enforcing encryption settings on these resources to secure sensitive data.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.datacatalog.entry.datalake_access_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Datalake Access RBAC Least Privilege
  scope: datacatalog.entry.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Datalake Access Uses RBAC with Least Privilege
  rationale: Implementing RBAC with least privilege for datalake access in GCP's Data Catalog reduces the risk of unauthorized data manipulation and exposure, ensuring sensitive information is accessed only by necessary personnel. This aligns with compliance mandates like GDPR and HIPAA, which require strict access controls to protect personal and sensitive data, thereby mitigating the risk of data breaches and financial penalties.
  description: This rule checks that access to datalake entries in GCP Data Catalog is configured using Role-Based Access Control (RBAC) with the principle of least privilege. Verify that only necessary roles are assigned to users or service accounts, and regularly audit permissions to ensure relevance. Remediation involves reviewing IAM policies within the Data Catalog, removing excessive permissions, and assigning roles that align with user responsibilities.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.datacatalog.entry.datalake_capture_enabled
  service: datacatalog
  resource: entry
  requirement: Datalake Capture Enabled
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Datalake Capture is Enabled
  rationale: Enabling Datalake Capture in Google Cloud Data Catalog ensures that all metadata and data lineage information is securely captured and stored. This feature helps in data governance, compliance with data protection regulations, and reduces the risk of data breaches by maintaining an auditable trail of data access and modifications.
  description: This rule verifies that Datalake Capture is enabled for entries in Google Cloud Data Catalog. Enabling this feature ensures that metadata changes, data lineage, and access patterns are logged and preserved. To verify, check the Data Catalog settings under the Datalake Capture configuration and ensure it is activated. If not enabled, configure the Data Catalog entry settings to activate Datalake Capture, enhancing traceability and compliance with data privacy standards.
  references:
  - https://cloud.google.com/data-catalog/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.datalake_catalog_audit_logging_enabled
  service: datacatalog
  resource: entry
  requirement: Datalake Catalog Audit Logging Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Datalake Catalog Audit Logging is Enabled
  rationale: Enabling audit logging for Datalake Catalog entries is crucial for maintaining visibility over data access and modifications. This helps identify unauthorized access, mitigate data exfiltration risks, and supports compliance with regulatory requirements such as GDPR and CCPA. By monitoring access and changes, organizations can quickly respond to potential security incidents and ensure data integrity.
  description: This rule verifies that audit logging is enabled for all entries in the Datalake Catalog within Google Cloud's Data Catalog service. Audit logs should be configured to capture 'ADMIN_READ', 'DATA_READ', and 'DATA_WRITE' activities to provide comprehensive visibility. To enable logging, configure the appropriate IAM roles and permissions, and ensure that Cloud Audit Logs are set up in the Google Cloud Console under 'IAM & Admin' -> 'Audit Logs'. Regularly review the logs to ensure compliance and address any anomalies.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logs
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - NIST SP 800-53 Rev. 5
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.datacatalog.entry.datalake_catalog_cross_account_sharing_restricted
  service: datacatalog
  resource: entry
  requirement: Datalake Catalog Cross Account Sharing Restricted
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Cross-Account Sharing of Data Catalog Entries
  rationale: Cross-account sharing of Data Catalog entries can lead to unauthorized access and potential data breaches, impacting business integrity and compliance with data protection regulations such as GDPR and CCPA. Limiting this access ensures that sensitive metadata and data lake information are only available to approved accounts, reducing the risk of data exfiltration.
  description: This rule checks for configurations that allow cross-account sharing of Data Catalog entries in GCP, which should be restricted to prevent unauthorized access. Verify that IAM policies do not grant cross-account access to Data Catalog entries. Remediation involves reviewing and updating IAM roles and policies to ensure they align with least privilege principles, and auditing logs to detect unauthorized access attempts.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.datacatalog.entry.datalake_catalog_metadata_encryption_enabled
  service: datacatalog
  resource: entry
  requirement: Datalake Catalog Metadata Encryption Enabled
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Metadata Encryption for Data Catalog Entries
  rationale: Enabling encryption for Data Catalog metadata protects sensitive data from unauthorized access and potential breaches. This is critical in maintaining data confidentiality and integrity, especially in industries with strict regulatory requirements such as finance and healthcare. Encryption at rest safeguards against threats like data theft or unauthorized disclosure, aligning with compliance mandates like GDPR and HIPAA.
  description: This rule verifies that encryption is enabled for metadata stored in Google Cloud Data Catalog entries, ensuring data is encrypted at rest. To verify, check the encryption configuration of your Data Catalog entries to confirm they use Google-managed or customer-managed keys. Remediation involves configuring the Data Catalog to use encryption keys, preferably customer-managed keys, to enhance control over the encryption processes.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.datacatalog.entry.datalake_catalog_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Datalake Catalog RBAC Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Datalake Catalog Entry RBAC Follows Least Privilege
  rationale: Implementing the principle of least privilege for Datalake Catalog entries mitigates the risk of unauthorized data access, reducing potential data breaches and maintaining compliance with regulations such as GDPR and HIPAA. Overly permissive roles can lead to inadvertent exposure of sensitive data, impacting business reputation and incurring regulatory fines.
  description: This rule checks that access permissions for Datalake Catalog entries are granted based on the principle of least privilege. It ensures that roles assigned to users or service accounts are scoped appropriately, avoiding overly broad permissions. Verification requires auditing IAM policies associated with data catalog entries and adjusting them to limit access to what is strictly necessary. Remediation involves reviewing and modifying IAM policies to remove excessive permissions and ensure only necessary roles are granted.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS GCP Benchmark 1.0.0 - Section 4.3
  - NIST SP 800-53 Rev. 5 - AC-6 Least Privilege
  - ISO 27001:2013 - A.9.1.2 Access Control
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.datalake_crawler_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Datalake Crawler Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Datalake Crawler Logs in Data Catalog Entries
  rationale: Enabling Datalake Crawler Logs is crucial for maintaining visibility into data access and modification activities within GCP's Data Catalog. Without logging, organizations cannot effectively monitor or audit data interactions, leading to potential compliance violations and an increased risk of undetected data breaches.
  description: This rule checks whether logging is enabled for datalake crawler entries in the GCP Data Catalog. To verify, ensure that audit logs for Data Catalog are activated, capturing all reads, writes, and administrative actions. Remediation involves configuring the appropriate IAM permissions and enabling audit logging in the Cloud Console under the Logging section of the Data Catalog settings.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logs
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.datalake_crawler_network_private_only
  service: datacatalog
  resource: entry
  requirement: Datalake Crawler Network Private Only
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Datalake Crawler Uses Private Network Only
  rationale: Using private networks for Datalake Crawlers minimizes exposure to potential threats by restricting access to authorized internal networks only. This reduces the risk of data breaches and unauthorized access, which is critical for maintaining the confidentiality and integrity of sensitive data. Compliance with regulations like GDPR and HIPAA often requires ensuring data processing activities are secure and access is limited to trusted environments.
  description: This rule checks that Datalake Crawlers within Google Cloud's Data Catalog are configured to operate solely on private networks. It involves verifying that network interfaces are not exposed to the public internet and ensuring that private IPs are used. Remediation involves updating network configurations to restrict Datalake Crawler access to internal IP addresses only, potentially using VPCs or other network segmentation methods.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/document-800-53
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.datalake_crawler_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Datalake Crawler Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Datalake Crawler Role Uses Least Privilege
  rationale: Limiting permissions to the minimum necessary reduces the attack surface and potential for data breaches. Over-privileged roles can lead to unauthorized data access, which may result in regulatory non-compliance and financial penalties. Implementing least privilege is crucial for protecting sensitive data and maintaining trust with users and stakeholders.
  description: This rule verifies that the roles assigned to Datalake crawlers in Google Cloud's Data Catalog service are configured with the principle of least privilege. Specifically, it checks whether the crawler role has only the necessary permissions to perform its function without excess capabilities. To remediate, review and adjust the IAM policy to ensure the crawler role only has essential permissions. Use the 'iam.roles.get' and 'iam.roles.update' commands for this purpose, and regularly audit these roles to maintain compliance.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-special-publication-800-53
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.datalake_crawler_scope_restricted_to_allowlist
  service: datacatalog
  resource: entry
  requirement: Datalake Crawler Scope Restricted To Allowlist
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Datalake Crawler Scope to Allowlist in Data Catalog
  rationale: Restricting the scope of datalake crawlers to a defined allowlist minimizes the risk of unauthorized data exposure and ensures that only approved data sets are processed. This is critical for maintaining data privacy, meeting compliance obligations such as GDPR, and preventing data breaches by limiting access to sensitive information.
  description: This check ensures that the Datalake Crawler in Google Cloud Data Catalog is configured to only access resources specified in an allowlist. Verify that the crawler's configuration includes an allowlist of approved data sources and no wildcards or unrestricted patterns. Remediation involves reviewing the crawler's current access scope, identifying necessary resources, and updating the configuration to restrict access accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.datalake_export_destination_encrypted
  service: datacatalog
  resource: entry
  requirement: Datalake Export Destination Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Datalake Export Destinations are Encrypted
  rationale: Encrypting datalake export destinations mitigates the risk of unauthorized access and data breaches. Unencrypted data can be exploited by attackers, leading to potential financial loss, reputational damage, and non-compliance with regulations such as GDPR, CCPA, and HIPAA which mandate encryption of sensitive data at rest.
  description: This rule checks whether the export destination of datalake entries in Google Cloud Data Catalog is encrypted. Verify that the destination uses Google-managed or customer-managed encryption keys (CMEK) to protect data at rest. Remediate by configuring the export destination bucket with CMEK through the Google Cloud Console or gcloud CLI, ensuring compliance with security policies and regulations.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Control 7.1
  - https://cloud.google.com/kms/docs/cmek
  - NIST SP 800-53 Rev. 5, SC-28 Protection of Information at Rest
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_crawler_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Crawler Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Lineage Crawler Logs in Data Catalog
  rationale: Enabling Lineage Crawler Logs in Data Catalog is crucial for maintaining an audit trail of data access and modifications. This logging feature helps in detecting unauthorized access, meeting compliance requirements such as GDPR and CCPA, and enhancing data governance by providing visibility into data lineage and usage patterns.
  description: This rule checks whether Lineage Crawler Logs are enabled for Data Catalog entries. Ensuring these logs are active involves verifying that audit logging is configured to capture lineage crawler activities. To enable logging, navigate to the GCP Console, access the Data Catalog settings, and ensure the audit logs for lineage crawlers are turned on. This setup aids in monitoring data flow and identifying potential security incidents.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_crawler_network_private_only
  service: datacatalog
  resource: entry
  requirement: Lineage Crawler Network Private Only
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Lineage Crawlers Use Private Networks Only
  rationale: Restricting Data Catalog lineage crawlers to private networks minimizes exposure to unauthorized access and potential data exfiltration. This practice is crucial for protecting sensitive metadata from being accessed by malicious actors over the internet. Compliance with network security standards like PCI-DSS and SOC2 requires strict access controls to maintain data integrity and confidentiality.
  description: This rule checks if Data Catalog lineage crawlers operate solely within private networks, ensuring they do not have public internet access. Verify that the network configuration for lineage crawlers is set to use only private IP addresses. To remediate, configure the network settings in the GCP console to restrict the crawler's communication to VPCs and disable any public IP allocation.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/network-connectivity/docs/vpc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry.lineage_crawler_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Crawler Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Lineage Crawler Role
  rationale: Limiting the privileges of the Lineage Crawler Role minimizes the risk of unauthorized access and potential data breaches. Excessive permissions can lead to exposure of sensitive information and non-compliance with data protection regulations such as GDPR and HIPAA, thereby increasing the organization's liability and risk profile.
  description: This rule verifies that the Lineage Crawler Role is assigned only the necessary permissions to perform its function within the Data Catalog service. The role should not have permissions beyond what is required for data lineage tracking. To assess compliance, review the IAM policy bindings for the role and ensure it does not include permissions unrelated to lineage crawling. Remediation involves adjusting the IAM policy to remove unnecessary permissions, aligning with the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_crawler_scope_restricted_to_allowlist
  service: datacatalog
  resource: entry
  requirement: Lineage Crawler Scope Restricted To Allowlist
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Lineage Crawler Scope to Allowlist
  rationale: Restricting the Lineage Crawler to an allowlist minimizes the risk of unauthorized data exposure and ensures adherence to data protection regulations. This limitation is crucial for maintaining data integrity and confidentiality, reducing the risk of data breaches, and aligning with compliance requirements such as GDPR and HIPAA.
  description: This rule verifies that the Data Catalog Lineage Crawler's scope is restricted to a defined allowlist, preventing the crawler from accessing unauthorized data entries. To ensure compliance, configure the Lineage Crawler to only scan entries listed in a pre-approved allowlist. This can be verified by reviewing the crawler's configuration settings in the GCP Console or via API. Remediation includes updating the crawler configuration to specify an allowlist of trusted entries.
  references:
  - https://cloud.google.com/data-catalog/docs/reference/rest
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_credentials_in_secrets_manager
  service: datacatalog
  resource: entry
  requirement: Lineage Credentials In Secrets Manager
  scope: datacatalog.entry.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Store Data Catalog Lineage Credentials in Secrets Manager
  rationale: Storing lineage credentials in Secrets Manager helps prevent unauthorized access to sensitive data and limits the risk of credential exposure. Failure to securely manage credentials can lead to data breaches, compliance violations with frameworks like PCI-DSS and HIPAA, and potential financial and reputational damage.
  description: This rule checks if lineage credentials used in Data Catalog entries are securely stored in Google Cloud Secret Manager. Ensure that all lineage credentials are not hardcoded in the application or stored in plaintext, but instead managed through Secret Manager with appropriate IAM policies limiting access. Remediation involves migrating credentials to Secret Manager and updating applications to retrieve them securely.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/datacatalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_database_cross_account_sharing_restricted
  service: datacatalog
  resource: entry
  requirement: Lineage Database Cross Account Sharing Restricted
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross Account Data Catalog Lineage Database Sharing
  rationale: Restricting cross-account sharing of Data Catalog lineage databases is crucial to prevent unauthorized access and data breaches. Exposing sensitive metadata across accounts can lead to data leakage and compliance violations, impacting business trust and risking penalties under regulations such as GDPR and CCPA.
  description: This rule checks for any Data Catalog lineage databases shared across accounts and ensures that sharing is restricted to only necessary entities. Verify configurations by reviewing IAM policies associated with Data Catalog entries and adjust permissions to minimize exposure. Remediate by restricting access to specific roles or identities within the same account or trusted domains, following the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/entries
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_database_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Database Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of Lineage Database in Data Catalog
  rationale: Encrypting the lineage database in GCP Data Catalog is crucial to protect sensitive metadata from unauthorized access and to comply with data protection regulations. Unencrypted metadata can expose critical lineage information, risking data leaks and non-compliance with standards like GDPR and HIPAA. Encrypting data at rest mitigates these risks and ensures that sensitive information is protected even if the storage is compromised.
  description: This rule checks if the lineage database associated with GCP Data Catalog entries is encrypted at rest. Ensure that encryption is enabled to protect metadata integrity and confidentiality. Verification involves checking the encryption configuration settings in your Data Catalog setup. To remediate, configure the Data Catalog to use Customer-Managed Encryption Keys (CMEK) or Google-managed encryption for all stored metadata.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.datacatalog.entry.lineage_database_policy_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Database Policy Least Privilege
  scope: datacatalog.entry.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Database Policies
  rationale: Implementing least privilege access in Data Catalog lineage databases reduces the risk of unauthorized data exposure or modification, which can lead to data breaches, non-compliance with data protection regulations, and potential financial and reputational damage. This principle helps ensure that users and services have only the permissions necessary to perform their intended functions, mitigating the impact of compromised accounts or insider threats.
  description: This rule checks that Data Catalog entry policies for lineage databases are configured with the least privilege principle. Specifically, it verifies that permissions granted to users and services are restricted to only those necessary for their roles. To remediate, review and adjust IAM policies to remove excessive permissions and ensure alignment with organizational access control policies. Regularly audit and update these policies to reflect changes in user roles or data access requirements.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/policy
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/privacy/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry.lineage_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Lineage is Encrypted at Rest
  rationale: Encrypting lineage data at rest protects sensitive metadata from unauthorized access and potential breaches. This is crucial for maintaining data integrity and confidentiality, especially in regulated industries such as healthcare and finance where compliance with standards like HIPAA and GDPR is required. Failure to encrypt data can lead to data exposure, violating compliance requirements and resulting in significant legal and financial repercussions.
  description: This rule checks that all Data Catalog entry lineage information is encrypted at rest using Customer-Managed Encryption Keys (CMEK). To verify, ensure that the Data Catalog service is configured to use CMEK by setting the appropriate encryption key on the resources handling sensitive lineage data. Remediation involves updating the Data Catalog configuration to select a suitable CMEK for encrypting the data at rest, thus enhancing the security posture and meeting compliance mandates.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/entry-overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.datacatalog.entry.lineage_job_logs_and_metrics_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Job Logs And Metrics Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Lineage Job Logs and Metrics in Data Catalog
  rationale: Enabling lineage job logs and metrics provides visibility into data processing activities within Google Cloud's Data Catalog. This is crucial for identifying unauthorized data access or anomalies, ensuring data integrity, and maintaining audit trails for compliance with standards like SOC 2 and ISO 27001. Failure to log these activities could result in undetected data breaches or compliance violations.
  description: This rule checks if lineage job logs and metrics are enabled in the Data Catalog entries. To verify, ensure that logging and metrics collection are configured in the Google Cloud Console under Data Catalog settings. Remediation involves enabling the necessary logging options to capture detailed information about data lineage processes, which can be done via the GCP Console or by using Terraform with the appropriate logging configuration for Data Catalog.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/monitoring
- rule_id: gcp.datacatalog.entry.lineage_job_network_private_only
  service: datacatalog
  resource: entry
  requirement: Lineage Job Network Private Only
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Lineage Jobs Use Private Networks Only
  rationale: Using private network connectivity for lineage jobs in Google Cloud Data Catalog minimizes exposure to public internet threats, reducing the risk of data breaches and unauthorized access. This practice supports compliance with data protection regulations like GDPR and enhances the security posture by limiting network access to authorized entities only.
  description: This rule checks that lineage jobs within Data Catalog entries are configured to use private networks exclusively. To verify, ensure that the network settings for lineage jobs are set to private IPs, avoiding public IP exposure. Remediation involves updating the network configuration of these jobs to use VPC Service Controls or Private Google Access, thereby restricting access to internal networks only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.datacatalog.entry.lineage_job_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Job Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Jobs
  rationale: Applying the principle of least privilege to Data Catalog lineage jobs minimizes the risk of unauthorized data access and potential data breaches. This is critical for maintaining data integrity and confidentiality, especially in environments subject to regulatory compliance such as GDPR or HIPAA. Misconfigured permissions can lead to over-privileged roles that expose sensitive data or disrupt operations.
  description: This rule checks that IAM roles associated with Data Catalog lineage jobs are configured with the minimal set of permissions required for their function. Verify that roles do not have excessive permissions by auditing IAM policies linked to Data Catalog entries for lineage jobs. Remediate by adjusting IAM policies in Google Cloud Console or via gcloud CLI to limit access to necessary resources only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.datacatalog.entry.lineage_job_script_location_private_and_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Job Script Location Private And Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Job Script Locations Are Private & Encrypted
  rationale: Lineage job scripts in Data Catalog contain sensitive metadata that can expose data lineage and dependencies. If not properly secured, unauthorized users could access or tamper with these scripts, leading to potential data breaches and compliance violations. Ensuring scripts are stored in private and encrypted locations mitigates these risks and aligns with data protection standards.
  description: This rule checks if the locations where lineage job scripts are stored are both private and encrypted. Specifically, it verifies that these scripts are in Google Cloud Storage buckets with appropriate IAM policies that restrict access and have default encryption enabled. To verify, review bucket permissions and ensure default encryption keys are set. Remediation involves adjusting IAM policies for least privilege and enabling default bucket encryption with customer-managed keys (CMEK) if necessary.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.datacatalog.entry.lineage_ml_transform_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Ml Transform Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Lineage ML Transform Logs Are Enabled for Data Catalog Entries
  rationale: Enabling lineage ML transform logs in Data Catalog is crucial for maintaining visibility into data transformations, which helps in tracking data provenance and ensuring data integrity. This is important for identifying unauthorized data manipulations and detecting potential data breaches. Moreover, it supports compliance with regulatory standards that require detailed data handling records.
  description: This rule verifies that logging is enabled for machine learning data transformations within Data Catalog entries. To ensure compliance, ensure that the Data Catalog service has audit logs activated to capture all ML transformation events. Remediation involves configuring the appropriate logging settings in the Google Cloud Console or via gcloud commands to ensure that lineage logs capture all necessary transformation details. This setup assists in monitoring and auditing data workflows effectively.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_ml_transform_network_private_only
  service: datacatalog
  resource: entry
  requirement: Lineage Ml Transform Network Private Only
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Lineage ML Transform Uses Private Network Only
  rationale: Restricting Lineage ML Transform operations to private networks mitigates risks associated with unauthorized access and data exfiltration. This control is crucial for protecting sensitive data and maintaining compliance with privacy regulations such as GDPR and HIPAA, which require stringent data security measures. It also aligns with best practices for minimizing exposure to the public internet, reducing potential attack vectors.
  description: This rule verifies that all Lineage ML Transform operations within Google Cloud Data Catalog are configured to use private network endpoints exclusively. To achieve this, ensure that the network configuration for the Data Catalog entries associated with ML transforms does not include public IP addresses. Verify by checking the network settings in the GCP Console for each relevant entry and update configurations to restrict access to private IP ranges. Remediation involves modifying network settings to disable public access and enable private Google Access for VPCs.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 4.5
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
- rule_id: gcp.datacatalog.entry.lineage_ml_transform_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Ml Transform Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Lineage Ml Transform Role in Data Catalog
  rationale: Implementing the principle of least privilege for the Lineage ML Transform Role in GCP Data Catalog minimizes the risk of unauthorized access and potential data breaches. This is crucial for maintaining data integrity and confidentiality, especially when handling sensitive information that could be exposed or altered by overly permissive permissions. Adhering to this principle also supports compliance with regulatory standards such as GDPR and CCPA, reducing legal and financial risks.
  description: This rule verifies that the Lineage ML Transform Role in GCP Data Catalog is configured with the minimum permissions necessary to perform its intended functions, ensuring it does not have excessive access rights. To verify, review the IAM policies associated with Data Catalog entries and ensure roles are appropriately scoped. If a role has more permissions than needed, modify the policy to restrict access. Use the GCP Console or gcloud CLI to audit and update IAM policies, ensuring they align with the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_partition_access_policies_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Partition Access Policies Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege on Data Catalog Lineage Partition Access
  rationale: Implementing least privilege access policies for Data Catalog lineage partitions minimizes the risk of unauthorized data exposure and potential data breaches. It helps safeguard sensitive information from insider threats and reduces the attack surface by limiting user permissions to only what is necessary. Compliance with regulatory standards like GDPR and HIPAA often mandates strict access controls to protect sensitive data.
  description: This rule checks that Data Catalog lineage partitions have access policies configured to adhere to the principle of least privilege. It verifies that only users and service accounts with a legitimate need can access specific partitions. To remediate, review the IAM policies associated with Data Catalog entries and adjust permissions to ensure they are restricted to the minimum required roles. Use GCP IAM policy management tools to audit and modify access levels accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/fine-grained-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_partition_catalog_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Partition Catalog Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Partition Catalog is Encrypted at Rest
  rationale: Encrypting the Lineage Partition Catalog data at rest mitigates risks associated with unauthorized data access and breaches. This practice supports compliance with data protection regulations like GDPR and HIPAA by ensuring sensitive metadata remains confidential. Failure to encrypt can expose organizations to potential data theft and legal penalties.
  description: This rule verifies that Lineage Partition Catalog within Google Cloud Data Catalog is encrypted using Customer-Managed Encryption Keys (CMEK). To check compliance, ensure that all Data Catalog entries have encryption configured in their metadata settings. Remediation involves configuring the Data Catalog service to use CMEK for encrypting data at rest, which can be done via the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.datacatalog.entry.lineage_private_networking_enforced
  service: datacatalog
  resource: entry
  requirement: Lineage Private Networking Enforced
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Lineage Private Networking is Enforced for Data Catalog
  rationale: Enforcing private networking for data catalog lineage ensures that metadata is not exposed to the public internet, reducing the risk of unauthorized access and potential data breaches. This control aligns with best practices for network security by minimizing attack surfaces and is crucial for meeting compliance requirements related to data protection and privacy, such as GDPR and HIPAA.
  description: This rule verifies that Data Catalog lineage entries are configured to use private networking, which restricts access to resources via Google Cloud's internal network. To achieve this, ensure that the entry's network settings are configured to use VPC Service Controls or Private Google Access. Verification involves checking the network configurations in the Google Cloud Console or via the gcloud CLI. Remediation includes updating the network settings to enforce private networking for all lineage data.
  references:
  - https://cloud.google.com/data-catalog/docs/networking
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://cloud.google.com/vpc-service-controls/docs/concepts
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry.lineage_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage RBAC Least Privilege
  scope: datacatalog.entry.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Lineage RBAC is Configured for Least Privilege
  rationale: Implementing least privilege access in Data Catalog lineage RBAC minimizes the risk of unauthorized data access and potential data breaches, thereby protecting sensitive information and ensuring compliance with data protection regulations. Over-permissioned access can lead to privilege escalation and data exfiltration, posing significant security and compliance risks.
  description: This rule checks that Data Catalog lineage RBAC settings are configured to grant only the necessary permissions required for users to perform their job functions. Verify that IAM policies associated with Data Catalog entries restrict access to the minimum privileges needed. Remediation involves reviewing and updating IAM roles and permissions to align with the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/docs/security/iam#least-privilege
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Data Catalog Lineage Role Follows Least Privilege
  rationale: Applying the principle of least privilege to Data Catalog lineage roles minimizes the risk of unauthorized access and potential data breaches. Over-permissioned roles can lead to inadvertent data exposure and non-compliance with regulations such as GDPR and HIPAA, which require stringent access controls to protect sensitive data.
  description: This rule checks that the roles assigned to users or service accounts for Data Catalog entries are limited to the minimum necessary permissions. Verify that roles such as 'roles/datacatalog.viewer' or custom roles with specific permissions are used instead of broad roles like 'owner' or 'editor'. To remediate, audit current role assignments and adjust them to ensure they grant only required permissions, using IAM policies and Google Cloud Console.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.datacatalog.entry.lineage_table_access_policies_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Table Access Policies Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Lineage Table Access Policies Follow Least Privilege Principle
  rationale: Implementing least privilege access control for lineage tables in Data Catalog helps minimize the risk of unauthorized data exposure and potential data breaches. This is crucial for protecting sensitive data assets and maintaining compliance with regulatory frameworks such as GDPR and CCPA, which mandate strict data access controls. By limiting access to only those who absolutely need it, organizations can reduce their attack surface and prevent insider threats.
  description: This rule checks whether access policies for lineage tables in Google Cloud Data Catalog are configured to adhere to the principle of least privilege. It ensures that only authorized users with a legitimate need have access. To verify, review IAM roles and permissions assigned to Data Catalog resources and adjust them to limit access to the minimum necessary. Remediation involves auditing current access permissions, identifying unnecessary privileges, and revoking or restricting access accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_table_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Table Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Table Encryption in GCP Data Catalog
  rationale: Encrypting lineage tables in GCP Data Catalog is crucial to protect sensitive metadata from unauthorized access and potential data breaches. This practice mitigates risks associated with data exposure and helps organizations comply with data privacy regulations such as GDPR and CCPA, ensuring that sensitive information remains confidential and secure.
  description: This rule checks if lineage tables in GCP Data Catalog entries are encrypted using Google-managed or customer-managed encryption keys. Verify the encryption status by reviewing the Data Catalog entry settings in the GCP Console or using the gcloud CLI. To remediate non-compliance, enable encryption at rest by configuring the appropriate encryption key settings for the Data Catalog entries.
  references:
  - https://cloud.google.com/data-catalog/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/docs/security-overview
- rule_id: gcp.datacatalog.entry.lineage_table_public_sharing_disabled
  service: datacatalog
  resource: entry
  requirement: Lineage Table Public Sharing Disabled
  scope: datacatalog.entry.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Sharing for Data Catalog Lineage Tables
  rationale: Publicly shared lineage tables in GCP's Data Catalog can expose sensitive metadata, leading to data breaches and unauthorized access. This poses a significant risk to data integrity and confidentiality, potentially violating compliance requirements such as GDPR, HIPAA, and PCI-DSS. Ensuring lineage tables are not publicly accessible helps maintain strict access controls and reduces the attack surface.
  description: This check ensures that lineage tables within GCP's Data Catalog are not shared with the public. It verifies that the 'allAuthenticatedUsers' and 'allUsers' members are not granted roles on Data Catalog entries. To remediate, review the IAM policy bindings for Data Catalog entries and remove any public access, ensuring only authorized users have the necessary permissions. This can be done via the GCP Console or using the gcloud CLI by inspecting and modifying the IAM policies.
  references:
  - https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.entryGroups.entries
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_tls_required
  service: datacatalog
  resource: entry
  requirement: Lineage TLS Required
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS for Data Catalog Entry Lineage
  rationale: TLS encryption is critical for protecting data in transit, ensuring that lineage information in Data Catalog entries is not exposed to unauthorized access. Without TLS, data could be intercepted, leading to potential data breaches and non-compliance with standards such as GDPR and HIPAA.
  description: This rule checks whether TLS is enforced for network connections when accessing lineage data within Data Catalog entries. To verify, inspect the network settings of your Data Catalog service to ensure TLS/SSL is configured and enforced. Remediation involves configuring the Data Catalog to only accept TLS connections, which can typically be done through the Google Cloud Console by setting the appropriate security policies.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.datacatalog.entry.lineage_trigger_event_sources_restricted
  service: datacatalog
  resource: entry
  requirement: Lineage Trigger Event Sources Restricted
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Lineage Trigger Event Sources in Data Catalog
  rationale: Restricting lineage trigger event sources is crucial to prevent unauthorized data modifications and lineage tracking from untrusted sources. This enhances data integrity and confidentiality, reducing the risk of exposure to data breaches and ensuring compliance with regulatory requirements such as GDPR and CCPA. It helps organizations maintain trust with stakeholders by safeguarding sensitive data.
  description: This rule checks if Data Catalog entries have restricted lineage trigger event sources. Ensure that only trusted and necessary services are allowed to trigger lineage events to prevent unauthorized data access. To verify, review the Data Catalog entry settings and configure access policies to limit event sources. Remediation involves setting IAM policies that explicitly grant permissions to trusted services and users only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.datacatalog.entry.lineage_trigger_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Trigger Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Lineage Trigger Logs
  rationale: Enabling lineage trigger logs in GCP Data Catalog provides detailed insights into data movement and transformations, allowing for enhanced visibility and accountability. This is crucial for identifying unauthorized access or anomalies, thus mitigating risks of data breaches. Compliance with regulations such as GDPR and CCPA often requires detailed audit trails, making this a key component in meeting legal obligations.
  description: This rule checks whether lineage trigger logs are enabled for GCP Data Catalog entries. To verify, ensure that the logging configuration for each Data Catalog entry includes lineage triggers. Remediation involves configuring the Data Catalog service to log all lineage events through the Google Cloud Console or via the CLI. This ensures all data lineage activities are captured and can be audited for security and compliance purposes.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance/resources
- rule_id: gcp.datacatalog.entry.lineage_trigger_targets_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Trigger Targets Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Implement Least Privilege for Data Catalog Lineage Triggers
  rationale: Ensuring that lineage triggers in GCP Data Catalog are configured with the least privilege principle minimizes the risk of unauthorized access and potential data breaches. Misconfigured permissions can lead to exposure of sensitive metadata, impacting regulatory compliance under frameworks such as GDPR, HIPAA, and SOC2. By adhering to least privilege, organizations can reduce the attack surface and fulfill compliance obligations.
  description: This rule checks whether lineage trigger targets in GCP Data Catalog entries are granted only the necessary permissions to perform their functions. Users must review and configure IAM policies to ensure that roles are limited to the essential permissions required for specific tasks. Remediation involves auditing current permissions, removing unnecessary roles, and applying more restrictive IAM roles to limit access. Verification can be done through the GCP Console or gcloud CLI by inspecting IAM settings on Data Catalog entries.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_version_immutability_enforced
  service: datacatalog
  resource: entry
  requirement: Lineage Version Immutability Enforced
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Lineage Version Immutability in Data Catalog
  rationale: Enforcing version immutability for data lineage in GCP's Data Catalog is crucial as it ensures the integrity and authenticity of data history. This mitigates risks associated with unauthorized changes that could lead to data breaches or compliance violations, such as those outlined in GDPR and CCPA. Immutable versions also help in maintaining accurate audit trails and support forensic investigations, thereby enhancing trust in data governance practices.
  description: This rule checks whether lineage version immutability is enforced for entries in the GCP Data Catalog. Configuring immutable versions prevents any alterations to recorded data lineage, preserving a reliable history of data transformations and movements. Verification involves reviewing Data Catalog settings to ensure that versioning policies are properly configured and enforced. To remediate, enable versioning policies that specify immutability for lineage records, and audit configurations regularly to ensure ongoing compliance.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_workflow_cross_account_trusts_restricted
  service: datacatalog
  resource: entry
  requirement: Lineage Workflow Cross Account Trusts Restricted
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Lineage Workflow Trusts in Data Catalog
  rationale: Restricting cross-account trusts in lineage workflows is crucial to prevent unauthorized access and data leaks across different GCP accounts. Such restrictions help mitigate risks associated with data sovereignty and ensure compliance with data protection regulations such as GDPR and CCPA. Organizations can avoid potential breaches and maintain control over sensitive data by enforcing strict access controls.
  description: This rule ensures that cross-account trusts in Data Catalog lineage workflows are restricted to prevent unauthorized access to metadata entries. Verify that IAM policies for Data Catalog entries do not allow broad access to external accounts. Remediation involves reviewing IAM policies and limiting permissions to only necessary GCP accounts, ensuring that trust relationships are explicitly defined and scoped appropriately.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/workflows
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/data-catalog/docs/concepts
- rule_id: gcp.datacatalog.entry.lineage_workflow_execution_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Workflow Execution Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Role
  rationale: Granting excessive permissions to the Lineage Workflow Execution Role in Data Catalog can lead to unauthorized data access, modification, or deletion. This increases the risk of data breaches and non-compliance with regulations such as GDPR and CCPA, which mandate stringent data access controls. Minimizing permissions aligns with the principle of least privilege, reducing potential attack vectors and safeguarding sensitive data.
  description: This rule checks that the Lineage Workflow Execution Role in Data Catalog is configured with the minimum necessary permissions. Review the IAM policies associated with this role to ensure they are restrictive yet sufficient for operational needs. Remediation involves auditing current permissions, removing any superfluous roles, and implementing more granular IAM policies to align with least privilege principles.
  references:
  - https://cloud.google.com/datacatalog/docs/how-to-manage-iam
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.datacatalog.entry.lineage_workflow_kms_encryption_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Workflow KMS Encryption Enabled
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Data Catalog Lineage Workflows
  rationale: Enabling KMS encryption for Data Catalog lineage workflows helps protect sensitive metadata and lineage information from unauthorized access, reducing the risk of data breaches. This is critical for compliance with data protection regulations such as GDPR and CCPA, which mandate strong encryption practices to safeguard personal and sensitive data. Failing to encrypt this information could expose organizations to significant legal and reputational risks.
  description: This rule checks if Cloud KMS encryption is enabled for Data Catalog lineage workflows, ensuring that metadata and lineage data are encrypted at rest using customer-managed keys. To verify, confirm that the 'encryptionSpec' field is configured with a valid KMS key in the Data Catalog entry settings. Remediation involves assigning a Cloud KMS key to the respective Data Catalog entries by updating the 'encryptionSpec' to include the 'kmsKeyName' parameter pointing to a Cloud KMS key resource.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/lineage
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.datacatalog.entry_group.data_catalog_audit_logging_enabled
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Audit Logging Enabled
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Data Catalog Audit Logging is Enabled
  rationale: Enabling audit logging for Data Catalog entry groups is crucial for maintaining visibility into data access and modifications. It helps identify unauthorized access attempts, supports forensic analysis in the event of a data breach, and ensures compliance with regulations such as GDPR and CCPA that require detailed logging of data access activities.
  description: This rule checks if audit logging is enabled for Data Catalog entry groups on GCP. Audit logs capture all read and write operations, providing an audit trail of interactions with data assets. To verify, ensure that audit logs for the Data Catalog service are configured in the GCP Console under 'Audit Logs' settings. Remediation involves enabling 'Admin Read', 'Data Read', and 'Data Write' logs for Data Catalog in the Cloud Console.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry_group.data_catalog_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Cross Account Sharing Restricted
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Cross-Account Sharing in Data Catalog Entry Groups
  rationale: Restricting cross-account sharing in Data Catalog minimizes the risk of unauthorized data access and potential data breaches. It ensures that sensitive metadata is only accessible to trusted accounts, aligning with data governance policies and compliance requirements such as GDPR and CCPA.
  description: This rule checks for configurations in Google Cloud Data Catalog to ensure entry groups are not shared across accounts unless explicitly required. To verify, inspect IAM policies on entry groups to confirm that cross-account principals are not granted access. Remediation involves removing unnecessary cross-account permissions and configuring IAM roles to limit access to necessary internal accounts only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.datacatalog.entry_group.data_catalog_metadata_encryption_enabled
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Metadata Encryption Enabled
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Data Catalog Metadata Encryption is Enabled
  rationale: Encrypting metadata in Data Catalog protects sensitive information from unauthorized access, reducing the risk of data breaches and ensuring compliance with regulations such as GDPR and HIPAA. Metadata often contains critical business insights and intellectual property, making its protection essential to maintain organizational privacy and data integrity.
  description: This rule checks that metadata within GCP Data Catalog entry groups is encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK) to safeguard data at rest. To verify, ensure that the entry group settings specify an encryption key. Remediation involves configuring the Data Catalog to use CMEK or GMEK by accessing the 'entry group' configuration in the GCP Console or using the gcloud CLI to set the necessary encryption options.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - CIS GCP Foundation Benchmark v1.3.0, Control 5.1
  - NIST SP 800-53 Rev. 5, Control SC-28
  - https://cloud.google.com/kms/docs
- rule_id: gcp.datacatalog.entry_group.data_catalog_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog RBAC Least Privilege
  scope: datacatalog.entry_group.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege on Data Catalog Entry Groups
  rationale: Implementing least privilege principles for Data Catalog Entry Groups reduces the risk of unauthorized data access and potential data breaches. This control helps prevent privilege escalation and limits the impact of compromised accounts, aligning with compliance mandates such as GDPR and CCPA which emphasize data protection and privacy.
  description: This rule verifies that RBAC (Role-Based Access Control) policies for Data Catalog Entry Groups are configured to ensure users and service accounts have only the permissions necessary to perform their roles. It checks for overly permissive roles and provides guidance on assigning predefined roles with the minimum required permissions. To remediate, review and adjust IAM policies for Entry Groups, removing excessive permissions and applying least privilege principles.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.datacatalog.entry_group.data_catalog_registry_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Registry Cross Account Sharing Restricted
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Cross-Account Sharing in Data Catalog
  rationale: Restricting cross-account sharing in Data Catalog is crucial to prevent unauthorized access to sensitive metadata. Cross-account sharing can lead to accidental data exposure and increase the risk of data breaches. It helps in maintaining compliance with data protection regulations by ensuring that only authorized accounts have access to specific metadata entries.
  description: This rule checks that Data Catalog Entry Groups are not shared across different GCP accounts unless explicitly required. To verify, review the IAM policies attached to the Entry Groups and ensure they do not grant permissions to external accounts. Remediation involves auditing current sharing settings and updating IAM policies to restrict access to trusted accounts only. Implement least privilege access and regularly monitor access logs to detect any unauthorized sharing activities.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/data-catalog/docs/how-to
- rule_id: gcp.datacatalog.entry_group.data_catalog_registry_metadata_encryption_enabled
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Registry Metadata Encryption Enabled
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Data Catalog Registry Metadata is Encrypted at Rest
  rationale: Encrypting metadata in Google Cloud Data Catalog safeguards sensitive information against unauthorized access and data breaches, which can lead to compliance violations with regulations such as GDPR, HIPAA, and PCI-DSS. Failure to encrypt metadata at rest increases the risk of exposure of critical data assets and can result in financial penalties and reputational damage.
  description: This rule verifies that Google Cloud Data Catalog entry groups have encryption enabled for registry metadata at rest. Specifically, it checks if the Data Catalog entry group utilizes customer-managed encryption keys (CMEK) or Google-managed keys to protect metadata. To ensure compliance, configure your Data Catalog entry groups to use CMEK by setting the appropriate encryption settings in the GCP Console or via the gcloud command-line tool. Regularly audit your configuration to maintain security posture.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/privacy/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.datacatalog.entry_group.data_catalog_registry_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Registry RBAC Least Privilege
  scope: datacatalog.entry_group.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Data Catalog Entry Group Follows Least Privilege Model
  rationale: Implementing least privilege access in GCP Data Catalog Entry Groups mitigates the risk of unauthorized data exposure and manipulation. It reduces the attack surface by ensuring users and services have access only to the resources they need, aligning with compliance requirements such as GDPR and ISO 27001, thereby protecting sensitive business data from breaches.
  description: This rule checks that IAM roles granted to Data Catalog Entry Groups adhere to the principle of least privilege. Verify that only necessary permissions are assigned by reviewing the IAM policy bindings associated with each entry group. Remediation involves auditing the roles and permissions and modifying IAM policies to remove excessive permissions, ensuring users and services have the minimum necessary access.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/overview/whitepapers
- rule_id: gcp.datacatalog.entry_group.data_governance_change_audit_logging_enabled
  service: datacatalog
  resource: entry_group
  requirement: Data Governance Change Audit Logging Enabled
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Data Governance Change Audit Logging for Entry Groups
  rationale: Enabling audit logging for data governance changes in Google Cloud's Data Catalog helps track the modification history of entry groups. This is crucial for identifying unauthorized changes, ensuring accountability, and meeting compliance requirements such as SOC2 and ISO 27001. Without audit logs, detecting potential security breaches or misconfigurations becomes challenging, increasing organizational risk.
  description: This rule checks whether audit logging is enabled for data governance changes in Data Catalog entry groups. Audit logs should capture 'admin read' and 'admin write' operations to ensure all modifications are tracked. Verify this setting by reviewing the audit log configuration in the Cloud Console under the Logging section or using the gcloud command-line tool. To enable, configure the appropriate logging settings in the IAM & Admin section, specifying the 'dataAccess' log type for entry group resources.
  references:
  - https://cloud.google.com/data-catalog/docs/audit-logs
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.datacatalog.entry_group.data_governance_cross_account_sharing_review_required
  service: datacatalog
  resource: entry_group
  requirement: Data Governance Cross Account Sharing Review Required
  scope: datacatalog.entry_group.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Review Cross-Account Sharing in Data Catalog Entry Groups
  rationale: Cross-account sharing of data catalog entry groups without proper governance can lead to unauthorized data access, resulting in potential data breaches and compliance violations. This is particularly critical when considering sensitive data governed by regulations such as GDPR or HIPAA, where data sharing must be scrutinized and controlled to prevent unauthorized disclosures.
  description: This rule checks for entry groups in GCP Data Catalog that are shared across accounts, ensuring a governance review is in place for such configurations. It examines IAM policies associated with entry groups to identify cross-account access and recommends a review process to validate necessity and compliance with data governance policies. Remediation involves auditing the IAM policies, consulting stakeholders for data-sharing requirements, and removing unnecessary cross-account permissions.
  references:
  - https://cloud.google.com/datacatalog/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.datacatalog.entry_group.data_governance_update_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Data Governance Update RBAC Least Privilege
  scope: datacatalog.entry_group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Entry Group Updates
  rationale: Ensuring least privilege access to Data Catalog entry groups is critical to prevent unauthorized modifications which can lead to data exposure or integrity issues. Misconfigured permissions can increase the risk of data breaches, violate compliance regulations, and ultimately result in financial and reputational damage. Implementing precise RBAC policies helps in aligning with compliance frameworks and securing sensitive data effectively.
  description: This rule verifies that only necessary roles are granted permissions to update Data Catalog entry groups, aligning with the principle of least privilege. Check IAM policies to ensure that roles like Viewer and Metadata Viewer do not have unnecessary update permissions. Remediate by auditing current role assignments and adjusting permissions using `gcloud` commands or the GCP Console to restrict update capabilities to roles like Editor or Owner only when justified.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-roles#data-catalog-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices-for-roles-and-permissions
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.datacatalog.entry_group.datalake_registry_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Datalake Registry Cross Account Sharing Restricted
  scope: datacatalog.entry_group.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Sharing of Data Catalog Entry Groups
  rationale: Restricting cross-account sharing of Data Catalog entry groups is crucial to prevent unauthorized access to sensitive data within your datalake registry. Unauthorized cross-account sharing can lead to data breaches, non-compliance with data protection regulations (such as GDPR or CCPA), and potential financial and reputational damage to your organization.
  description: This rule checks for configurations in Google Cloud's Data Catalog to ensure that entry groups are not shared across different accounts without explicit authorization. Verify that IAM policies attached to Data Catalog entry groups do not include principals from external accounts. Remediation involves auditing IAM policies and removing any unauthorized external account permissions, ensuring only intended and trusted accounts have access.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/entry-group-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry_group.datalake_registry_metadata_encryption_enabled
  service: datacatalog
  resource: entry_group
  requirement: Datalake Registry Metadata Encryption Enabled
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Encryption for Datalake Registry Metadata
  rationale: Ensuring encryption for datalake registry metadata protects sensitive information from unauthorized access and potential breaches. This is crucial for mitigating risks such as data leaks and unauthorized data manipulation, while also fulfilling compliance requirements like GDPR, which mandate data protection and privacy. Encryption at rest is a key control for defending against insider threats and external attacks.
  description: This rule checks whether encryption is enabled for datalake registry metadata within Google Cloud's Data Catalog. To verify, ensure that the entry group's metadata is encrypted using Cloud KMS keys. This can be configured by setting the 'kmsKeyName' field in the entry group settings. Remediation involves using Google Cloud Console or gcloud command-line tool to update the entry group configuration to include a valid KMS key for encryption.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypting
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.datacatalog.entry_group.datalake_registry_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Datalake Registry RBAC Least Privilege
  scope: datacatalog.entry_group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Datalake Registry Entry Group
  rationale: Implementing least privilege in RBAC for Data Catalog entry groups helps minimize potential data breaches by restricting access to necessary personnel only. This reduces the attack surface and aids in compliance with regulations like GDPR and HIPAA, which mandate controlled access to sensitive data. It also mitigates insider threats and limits the damage from compromised accounts.
  description: This rule verifies that RBAC policies for Data Catalog entry groups are configured to adhere to the principle of least privilege. It checks that only roles necessary for specific tasks are granted to users, groups, or service accounts. To remediate, review IAM policies for entry groups and remove excessive permissions, ensuring that each role aligns with its required data access. Use GCP IAM policy analysis tools to audit permissions effectively.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/manage-iam
  - https://cloud.google.com/security/compliance/cis#cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry_group.lineage_audit_logging_enabled
  service: datacatalog
  resource: entry_group
  requirement: Lineage Audit Logging Enabled
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Lineage Audit Logging for Data Catalog Entry Groups
  rationale: Enabling Lineage Audit Logging for Data Catalog Entry Groups helps track data access and modifications, which is crucial for identifying unauthorized activities and maintaining data integrity. It supports compliance with regulations such as GDPR and HIPAA by providing audit trails necessary for data governance and incident response. Without this logging, organizations may face increased risks of data breaches and non-compliance fines.
  description: This rule checks whether Lineage Audit Logging is enabled for entry groups within Google Cloud's Data Catalog service. To ensure compliance, configure audit logging by navigating to the Google Cloud Console, selecting Data Catalog, and verifying that the 'Audit Logs' settings include both 'Admin Read' and 'Data Access'. If not enabled, update the IAM policies to support logging. Proper configuration ensures that all access and changes to data entries are logged for monitoring and forensic purposes.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry_group.lineage_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Lineage Cross Account Sharing Restricted
  scope: datacatalog.entry_group.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Lineage Cross Account Sharing in Data Catalog
  rationale: Restricting cross-account sharing of lineage data in GCP Data Catalog is crucial to prevent unauthorized access to sensitive metadata, which can lead to data leaks and compliance violations. By ensuring that lineage data is not shared across accounts without proper controls, organizations can mitigate risks associated with data privacy breaches and align with regulatory requirements such as GDPR and CCPA.
  description: This rule checks for configurations in GCP Data Catalog where entry group lineage data is shared across accounts. To verify, review IAM policies associated with entry groups and ensure that only authorized accounts have access. Remediation involves refining IAM roles and permissions to limit access to lineage data, ensuring it is only available to trusted accounts within the organization.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/entry-groups
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/privacy/gdpr
- rule_id: gcp.datacatalog.entry_group.lineage_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Lineage RBAC Least Privilege
  scope: datacatalog.entry_group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Data Catalog Entry Group RBAC Follows Least Privilege
  rationale: Implementing least privilege for Data Catalog entry group lineage ensures that users and services have only the permissions essential for their roles, reducing the risk of unauthorized access and data breaches. This approach mitigates potential insider threats and limits the blast radius in case of credential compromise, aligning with compliance mandates such as NIST and ISO 27001.
  description: This rule checks that IAM roles assigned to Data Catalog entry group lineage follow the principle of least privilege. Verify that only necessary roles with limited permissions are granted to users or service accounts. Remediation involves reviewing IAM policies and adjusting roles to ensure that overly permissive roles are replaced with roles that strictly adhere to the minimum permissions needed for functionality.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry_group.lineage_registry_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Lineage Registry Cross Account Sharing Restricted
  scope: datacatalog.entry_group.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Lineage Registry Sharing
  rationale: Restricting cross-account sharing of lineage registries in GCP Data Catalog minimizes the risk of unauthorized access to sensitive metadata, which could lead to data leaks or breaches. It is crucial for maintaining data sovereignty and complying with data privacy regulations such as GDPR and CCPA. Unrestricted sharing could expose critical lineage information to external accounts, posing a significant security risk.
  description: This rule checks if lineage registries within GCP Data Catalog entry groups are shared across accounts, which should be restricted to prevent unauthorized access. Verify that IAM policies for entry groups do not grant access to external accounts. Remediation involves reviewing and refining IAM policies to limit access to trusted internal accounts only, ensuring compliance with organizational data protection policies. Use the GCP Console or CLI to audit and manage access control settings.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-guide
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry_group.lineage_registry_metadata_encryption_enabled
  service: datacatalog
  resource: entry_group
  requirement: Lineage Registry Metadata Encryption Enabled
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Lineage Registry Metadata Encryption is Enabled
  rationale: Enabling encryption for Lineage Registry Metadata in GCP Data Catalog is crucial to protect sensitive data from unauthorized access and potential breaches. This measure mitigates risks associated with data exposure and helps meet compliance with regulations such as GDPR and HIPAA, which mandate data protection standards. Encrypting metadata at rest reduces the attack surface, safeguarding data integrity and confidentiality.
  description: This rule checks whether encryption is enabled for Lineage Registry Metadata in GCP Data Catalog's entry groups. It verifies that the metadata is encrypted using Google-managed keys or customer-managed keys, ensuring that data is protected at rest. To enable encryption, configure the entry group's settings to use the appropriate key management service. Verification involves reviewing the Data Catalog settings to confirm the encryption status and the key type in use. Remediation requires updating the settings to enforce encryption if not already enabled.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypting-data
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datacatalog.entry_group.lineage_registry_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Lineage Registry RBAC Least Privilege
  scope: datacatalog.entry_group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Registry
  rationale: Implementing least privilege access for the Data Catalog Lineage Registry is crucial to minimize potential data breaches and unauthorized access. By restricting permissions to only what is necessary, organizations can reduce the risk of data leaks, maintain the integrity of lineage data, and comply with regulatory requirements such as GDPR and CCPA.
  description: This rule checks that roles assigned to users or service accounts on Data Catalog entry groups related to the Lineage Registry adhere to the principle of least privilege. Specifically, it verifies that entities are not granted more permissions than necessary to perform their job functions. To remediate, review IAM policies and remove unnecessary roles or privileges from users and service accounts. Utilize IAM Recommender for identifying and revoking excessive permissions.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/iam/docs/recommender-overview
  - https://www.nist.gov/privacy-framework
- rule_id: gcp.datacatalog.entry_group.lineage_store_encrypted
  service: datacatalog
  resource: entry_group
  requirement: Lineage Store Encrypted
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Store Encryption in Data Catalog Entry Groups
  rationale: Encrypting lineage data in GCP Data Catalog is crucial for protecting sensitive information from unauthorized access and potential data breaches. Encryption at rest helps mitigate risks associated with data exfiltration and ensures compliance with data protection regulations such as GDPR and HIPAA, which mandate safeguarding personal and sensitive data.
  description: This rule checks that all lineage store data within GCP Data Catalog entry groups are encrypted at rest. By default, GCP encrypts data using Google-managed keys, but organizations can enhance security by using customer-managed encryption keys (CMEK). To verify and configure this setting, navigate to the Data Catalog settings and ensure that the 'lineage store encryption' option is enabled. Remediation involves setting up CMEK through Cloud Key Management Service (KMS) and associating it with your Data Catalog entry group.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/privacy/gdpr
- rule_id: gcp.datacatalog.policy_tag.data_governance_policy_admin_change_audit_logging_enabled
  service: datacatalog
  resource: policy_tag
  requirement: Data Governance Policy Admin Change Audit Logging Enabled
  scope: datacatalog.policy_tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Enable Audit Logging for Policy Tag Admin Changes
  rationale: Enabling audit logging for policy tag admin changes in Google Cloud Data Catalog is crucial for maintaining a secure data governance framework. Without logging, unauthorized changes to policy tags could go undetected, leading to potential data exposure, loss of data integrity, and non-compliance with industry regulations such as GDPR or HIPAA. Audit logs help in tracking changes, facilitating forensic investigations, and ensuring accountability.
  description: This rule checks that audit logging is enabled for all administrative changes to policy tags within Google Cloud Data Catalog. To verify, ensure that the Data Access audit logs for Data Catalog are configured to log admin write actions on policy tags. Remediation involves enabling audit logging in the Google Cloud Console under the IAM & Admin section, specifically within the audit logs settings for each project where Data Catalog is used.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nice/resources/nist-cybersecurity-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.policy_tag.data_governance_policy_admin_mfa_required
  service: datacatalog
  resource: policy_tag
  requirement: Data Governance Policy Admin MFA Required
  scope: datacatalog.policy_tag.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce MFA for Data Governance Policy Admins
  rationale: Requiring Multi-Factor Authentication (MFA) for Data Governance Policy Admins in Google Cloud's Data Catalog reduces the risk of unauthorized access to sensitive data classification and tagging policies. This measure protects against credential theft and insider threats, which can lead to data breaches, regulatory non-compliance, and significant financial and reputational damage.
  description: This rule checks whether MFA is enforced for users with administrative privileges on Data Catalog policy tags. Administrators must enable MFA in their Identity and Access Management (IAM) settings, ensuring an additional layer of security. To verify compliance, review IAM policies for admin roles and confirm that MFA is enabled. Remediation involves configuring IAM settings to enforce MFA for all users with 'roles/datacatalog.policyTagAdmin' permissions.
  references:
  - https://cloud.google.com/iam/docs/using-mfa
  - https://cloud.google.com/security-command-center/docs/security-health-analytics-best-practices
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 1.12
  - NIST SP 800-63B Digital Identity Guidelines
  - ISO/IEC 27001:2013 - A.9.2.1 User Access Management
  - https://cloud.google.com/datacatalog/docs/how-to/policy-tags
- rule_id: gcp.datacatalog.policy_tag.data_governance_policy_admin_rbac_least_privilege
  service: datacatalog
  resource: policy_tag
  requirement: Data Governance Policy Admin RBAC Least Privilege
  scope: datacatalog.policy_tag.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Least Privilege for Data Governance Policy Admin in Data Catalog
  rationale: Implementing least privilege for Data Governance Policy Admin roles minimizes the risk of unauthorized access and potential data breaches, safeguarding sensitive data. This is crucial to comply with data protection regulations like GDPR and CCPA, which mandate strict access controls to protect personal data and mitigate security threats.
  description: This rule checks whether users assigned the 'Data Governance Policy Admin' role in Data Catalog have permissions strictly necessary for their duties. It verifies that no excessive permissions are granted that could lead to privilege escalation or data exposure. Remediation involves reviewing existing role assignments, removing unnecessary permissions, and ensuring roles align with the principle of least privilege. This can be done via the Google Cloud Console or gcloud CLI by inspecting IAM policies and revising them accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/policy-tag-manager
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag.data_catalog_data_quality_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Data Catalog Data Quality Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Quality Logs in GCP Data Catalog
  rationale: Enabling data quality logs in Google Cloud's Data Catalog is crucial for maintaining visibility into data integrity and compliance. Without these logs, organizations may face challenges in identifying data anomalies, auditing data-related activities, and meeting regulatory requirements such as GDPR and HIPAA. This logging capability helps mitigate risks associated with data breaches and ensures accountability in data management processes.
  description: This rule verifies that data quality logs are enabled for tags in Google Cloud's Data Catalog. To ensure this setting, navigate to the Data Catalog settings in the GCP Console and enable logging for data quality. This allows for comprehensive monitoring and auditing of data quality issues. Remediation involves adjusting the configuration settings to activate logging, ensuring that all data interactions are recorded for future analysis and compliance audits.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/set-up-logging
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.tag.data_catalog_data_quality_output_location_private__encrypted
  service: datacatalog
  resource: tag
  requirement: Data Catalog Data Quality Output Location Private Encrypted
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Data Quality Output is Private and Encrypted
  rationale: Encrypting Data Catalog output locations ensures sensitive data is protected from unauthorized access and potential breaches, which could lead to financial loss, reputational damage, and non-compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks that Data Catalog data quality output locations are both private and encrypted using Google-managed or customer-managed encryption keys. Verify by ensuring the output location is in a Google Cloud Storage bucket that is not publicly accessible and has encryption enabled. To remediate, configure the bucket to disable public access and enable encryption settings.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datacatalog.tag.data_catalog_data_quality_role_least_privilege
  service: datacatalog
  resource: tag
  requirement: Data Catalog Data Quality Role Least Privilege
  scope: datacatalog.tag.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Data Quality Role
  rationale: Applying the principle of least privilege in GCP's Data Catalog service mitigates the risk of unauthorized access and potential data breaches. It reduces the attack surface by limiting permissions to only those necessary for users to perform their roles. This practice aligns with compliance standards like ISO 27001 and NIST SP 800-53, which emphasize minimizing access to sensitive data to prevent insider threats and accidental data exposures.
  description: This rule checks if roles assigned for managing Data Catalog tags adhere to the principle of least privilege. It verifies that only users or service accounts with a justified business need have access to create, update, or delete tags. To remediate, audit all current role assignments to ensure they are scoped to the minimum necessary permissions. Use predefined roles where possible, and regularly review and update access controls to reflect changes in job responsibilities.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/tag-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices/identity-access-management
- rule_id: gcp.datacatalog.tag.data_governance_quality_jobs_role_least_privilege
  service: datacatalog
  resource: tag
  requirement: Data Governance Quality Jobs Role Least Privilege
  scope: datacatalog.tag.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Governance Quality Jobs Role
  rationale: Implementing least privilege for roles managing Data Catalog tags minimizes the risk of unauthorized data access or modification, protecting sensitive metadata from internal and external threats. This approach supports compliance with regulations such as GDPR and HIPAA, which mandate strict access controls to safeguard data privacy and integrity.
  description: This rule checks that roles assigned to manage Data Catalog tags for data governance quality jobs adhere to the principle of least privilege. Ensure that only essential permissions are granted, such as 'datacatalog.tags.create' and 'datacatalog.tags.update', and avoid broader permissions that are unnecessary for the task. Regularly audit permissions using IAM policy analysis and adjust roles to remove excessive privileges. Remediation involves using the Google Cloud Console, IAM Policy Analyzer, or gcloud CLI to refine role permissions.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/governance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.datacatalog.tag.data_governance_quality_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Data Governance Quality Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Tag Governance Logging
  rationale: Enabling data governance quality logs in Google Cloud Data Catalog helps organizations maintain visibility over tag management and usage. This is crucial for ensuring data integrity, preventing unauthorized access, and meeting compliance requirements such as GDPR and CCPA. Without proper logging, misconfigurations or unauthorized changes could go unnoticed, leading to potential data breaches or loss of compliance.
  description: This rule checks whether logging is enabled for Data Catalog tags to monitor the governance quality of data. To verify, ensure that audit logging is configured for Data Catalog in the Google Cloud Console under the 'IAM & Admin' section, then 'Audit Logs'. Remediation involves enabling Data Catalog audit logs by selecting the appropriate log type (Admin Activity, Data Access, etc.) and specifying the necessary log sinks for long-term storage and analysis.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.tag.data_governance_quality_outputs_encrypted_and_private
  service: datacatalog
  resource: tag
  requirement: Data Governance Quality Outputs Encrypted And Private
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Tags Are Encrypted and Private
  rationale: Encrypting data governance quality outputs ensures that sensitive data is protected against unauthorized access, mitigating the risk of data breaches. It supports compliance with regulatory requirements such as GDPR and HIPAA, which mandate the protection of personal and sensitive information. By maintaining data privacy and confidentiality, organizations reduce legal liabilities and preserve their reputation.
  description: This rule checks that all data governance quality outputs in Google Cloud Data Catalog are encrypted at rest and are configured to be private. Ensure that the tagging system in Data Catalog is set up to use customer-managed encryption keys (CMEK) for robust security control. Verification involves reviewing the Data Catalog settings in the GCP Console under the 'Security' section or using the gcloud CLI to ensure encryption settings use CMEK. Remediation requires configuring the Data Catalog to use CMEK and setting access controls to restrict visibility to authorized users only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/encryption-at-rest/
  - https://www.cisecurity.org/benchmark/google_cloud_computing/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_access_rbac_least_privilege
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Access RBAC Least Privilege
  scope: datacatalog.tag.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for DataCatalog Tag Access
  rationale: Implementing least privilege access for Data Catalog tags minimizes the risk of unauthorized access and potential data breaches. This practice is crucial in preventing exposure of sensitive metadata and ensuring compliance with regulatory frameworks such as GDPR and CCPA. Unauthorized access to data quality recommendations can lead to data integrity issues and undermine trust in data-driven decisions.
  description: This rule checks if access to Data Catalog tags related to data quality recommendations is granted based on the principle of least privilege. Specifically, it verifies that only users or service accounts with a legitimate need have the necessary roles assigned. To remediate, review IAM policies and adjust roles to ensure minimal access is granted. Use the Google Cloud Console or gcloud CLI to audit and modify roles associated with Data Catalog, focusing on removing excessive permissions.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Computing Foundations Benchmark v1.1.0, section 4.1
  - NIST SP 800-53 Rev. 5, AC-6 Least Privilege
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_outputs_encrypted_and_private
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Outputs Encrypted And Private
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Tags Are Encrypted and Private
  rationale: Data quality recommendation outputs in Google Cloud's Data Catalog can contain sensitive metadata that, if exposed, may lead to data breaches or unauthorized access. Encrypting these outputs and ensuring their privacy helps in mitigating risks such as data tampering, unauthorized exposure, and compliance violations with standards like GDPR and HIPAA.
  description: This rule verifies that data quality recommendation outputs in Data Catalog are encrypted using a customer-managed encryption key (CMEK) and are not publicly accessible. To ensure compliance, configure Data Catalog to use CMEK for all tags and verify IAM policies to restrict access to authorized users only. Remediation involves updating encryption settings and auditing permissions through the Google Cloud Console or CLI.
  references:
  - https://cloud.google.com/datacatalog/docs/how-to/encrypt-data
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/kms/docs
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_run_execution_role_lea_privilege
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Run Execution Role Lea Privilege
  scope: datacatalog.tag.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Quality Execution Role
  rationale: Assigning excessive privileges to roles can lead to unauthorized access and potential data breaches. Ensuring that the Data Quality Recommendation Run Execution Role adheres to the principle of least privilege is crucial to minimize risk exposure and meet compliance requirements such as PCI-DSS and ISO 27001. This practice not only protects sensitive data but also helps in maintaining audit trails and preventing privilege escalation attacks.
  description: This check verifies that the Data Quality Recommendation Run Execution Role in Google Cloud Data Catalog is configured with the least privilege necessary to perform its tasks. Specifically, it examines permissions assigned to the role to ensure they are limited to only what is required for executing data quality recommendations. To remediate, review and adjust the role's permissions in the IAM policy to remove any unnecessary access rights. Regular audits and updates to permissions are recommended to maintain security posture.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/datacatalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_run_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Run Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Quality Recommendation Logs in Data Catalog
  rationale: Enabling logs for Data Quality Recommendations in Data Catalog is crucial for ensuring auditability and traceability of data quality assessments. It aids in identifying potential misconfigurations or errors in data tagging processes, which can lead to inaccurate data insights. This practice supports compliance with data governance frameworks by providing evidence of data accuracy and integrity assessments.
  description: This rule verifies if logging is enabled for data quality recommendation runs in Google Cloud's Data Catalog. To ensure this, users should configure the Data Catalog to log all recommendation activities by enabling the appropriate logging settings in Stackdriver (or Cloud Logging). This can be done by setting up an audit log that captures 'Admin Read' and 'Data Write' actions. Failure to do so may result in the inability to trace data quality recommendations and could hinder troubleshooting and compliance efforts.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/logging
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/architecture/data-governance/data-quality
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_run_metadata_logging_enabled
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Run Metadata Logging Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Metadata Logging for Data Quality Recommendations
  rationale: Enabling metadata logging for data quality recommendations in Google Cloud Data Catalog ensures that all actions related to data quality are tracked and auditable. This is crucial for identifying data quality issues promptly, preventing data misuse, and ensuring compliance with regulatory requirements such as GDPR and CCPA. Failure to log metadata can lead to undetected data anomalies, impacting data integrity and potentially resulting in financial penalties.
  description: This rule checks whether metadata logging is enabled for data quality recommendation runs in GCP Data Catalog. To verify, ensure that audit logs are configured to capture all data quality recommendation events. This involves setting up a logging sink to export audit logs to a centralized logging solution. Remediation involves navigating to the Data Catalog in GCP Console, enabling audit logging, and configuring log sinks to capture necessary metadata.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/solutions/security-overview
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_run_output_encrypted_private
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Run Output Encrypted Private
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Quality Outputs are Encrypted and Private
  rationale: Encrypting Data Quality Recommendation outputs in Google Cloud's Data Catalog ensures that sensitive information is protected against unauthorized access and potential data breaches. This mitigates the risk of data exposure and aligns with compliance requirements such as GDPR, PCI-DSS, and HIPAA. Proper encryption also helps in maintaining customer trust and safeguarding intellectual property.
  description: This rule checks that outputs generated from Data Quality Recommendations in Google Cloud's Data Catalog are encrypted and kept private. It ensures that the data is secured using Google-managed or customer-managed encryption keys. To verify compliance, review the Data Catalog settings to confirm that encryption is enabled and configured correctly. Remediation involves setting up and applying the appropriate encryption keys to the Data Catalog outputs to ensure they are encrypted at rest.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryptions
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.datacatalog.tag.data_quality_rule_definition_version_pinned
  service: datacatalog
  resource: tag
  requirement: Data Quality Rule Definition Version Pinned
  scope: datacatalog.tag.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Quality Rule Definition Version is Pinned
  rationale: Pinning data quality rule definitions is crucial to maintain consistency and reliability in data quality assessments. Without version control, changes could inadvertently affect data integrity, leading to potential data mishandling, and impact compliance with standards like GDPR or HIPAA. This control helps mitigate the risk of unauthorized or accidental changes that could compromise data protection and privacy.
  description: This rule checks whether the data quality rule definitions in Data Catalog tags are pinned to a specific version. To verify, inspect the Data Catalog configurations for tag templates and ensure each rule definition is associated with a fixed version. Remediate by updating rule definitions to specify a version, preventing unintended impacts from future updates.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-guides
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/data-catalog/docs/concepts
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.tag.data_quality_rule_parameters_no_plaintext_secrets
  service: datacatalog
  resource: tag
  requirement: Data Quality Rule Parameters No Plaintext Secrets
  scope: datacatalog.tag.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Prevent Plaintext Secrets in Data Quality Rule Parameters
  rationale: Storing secrets in plaintext within Data Catalog tags can expose sensitive information to unauthorized users, leading to data breaches and compliance violations. Protecting secrets is crucial for maintaining data confidentiality and integrity, especially in highly regulated environments where data privacy laws like GDPR and HIPAA apply.
  description: This rule checks that Data Quality Rule Parameters in Data Catalog tags do not contain plaintext secrets, such as API keys or passwords. It ensures that any sensitive information is encrypted or stored securely using Google Cloud's Secret Manager. To remediate, verify the tags do not include plaintext secrets and migrate any such data to a secure storage solution, updating references in your applications accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-guides
  - https://cloud.google.com/secret-manager/docs
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://gdpr-info.eu/
  - https://www.hipaajournal.com/hipaa-security-rule/
- rule_id: gcp.datacatalog.tag.data_quality_rule_source_trusted
  service: datacatalog
  resource: tag
  requirement: Data Quality Rule Source Trusted
  scope: datacatalog.tag.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Trusted Sources for Data Quality Rules in Data Catalog
  rationale: Ensuring that data quality rules in Data Catalog are sourced from trusted entities is crucial to maintain data integrity and prevent unauthorized data manipulation. Untrusted sources can introduce erroneous data quality rules, leading to data misinterpretation and potential compliance breaches with standards like GDPR or CCPA. By validating the source of these rules, organizations can mitigate risks associated with data breaches and maintain trust with their stakeholders.
  description: This rule checks that all data quality rules in Google Cloud's Data Catalog are sourced from verified, trusted sources. It involves verifying the source identity and ensuring their permissions align with the least privilege principle. To remediate, review and update IAM policies to restrict rule creation to trusted service accounts and regularly audit rule sources. Use Cloud Audit Logs to track changes and identify unauthorized modifications.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.datacatalog.tag.datalake_data_quality_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Datalake Data Quality Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Quality Logs for Datalake in Data Catalog
  rationale: Enabling data quality logging in a datalake environment helps identify inconsistencies, errors, and non-compliance with data standards, reducing the risk of poor data quality impacting business intelligence and decision-making processes. Additionally, it aids in compliance with data governance policies by providing an audit trail of data quality checks.
  description: This rule checks whether data quality logging is enabled for datalake tags in Google Cloud Data Catalog. To verify, ensure that the Data Catalog service is configured to log data quality metrics for all tags associated with datalake resources. Remediation involves accessing the Data Catalog settings in the GCP Console and enabling logging for data quality metrics under the specific tags used by the datalake. This enhances visibility into data integrity and supports data governance and compliance efforts.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag.datalake_data_quality_output_location_private_and_encrypted
  service: datacatalog
  resource: tag
  requirement: Datalake Data Quality Output Location Private And Encrypted
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Datalake Outputs are Private & Encrypted at Rest
  rationale: Securing datalake outputs with encryption at rest and private access mitigates risks of unauthorized data exposure and data breaches. This practice ensures that sensitive data is protected against internal and external threats, aligns with compliance mandates like GDPR and HIPAA, and maintains customer trust by safeguarding personal information.
  description: This rule checks that all data quality outputs from datalakes in Data Catalog are stored in locations that enforce encryption at rest and are not publicly accessible. Verify that the storage buckets used have uniform access configured to private and utilize Cloud KMS for encryption. If a bucket is public or lacks encryption, update its IAM policy to restrict access and apply a valid KMS key for data encryption.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/architecture/encryption-and-key-management
- rule_id: gcp.datacatalog.tag.datalake_data_quality_role_least_privilege
  service: datacatalog
  resource: tag
  requirement: Datalake Data Quality Role Least Privilege
  scope: datacatalog.tag.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Datalake Data Quality Roles in Data Catalog
  rationale: Implementing least privilege for datalake roles mitigates the risk of unauthorized access and data breaches, protecting sensitive data and maintaining data integrity. Over-privileged roles can lead to inadvertent data exposure, non-compliance with regulations such as GDPR and CCPA, and potential financial and reputational damage.
  description: This rule checks if roles assigned to manage datalake data quality in Data Catalog adhere to the principle of least privilege. Specifically, it verifies that roles are configured to only allow necessary permissions for data quality tasks without granting excessive access. To remediate, review and update IAM policies to ensure roles are strictly aligned with job functions, using predefined roles where possible and custom roles when necessary. Regular audits and adjustments should be conducted to maintain compliance with this principle.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.datacatalog.tag.lineage_data_quality_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Lineage Data Quality Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Lineage Data Quality Logs
  rationale: Enabling lineage data quality logs in GCP's Data Catalog helps organizations maintain visibility into data asset changes and ensure data integrity. This is crucial for identifying potential data quality issues and unauthorized access attempts, thereby reducing the risk of data breaches and non-compliance with regulatory standards such as GDPR and HIPAA.
  description: This rule checks if lineage data quality logging is enabled for Data Catalog tags in GCP. When enabled, it captures detailed logs about data lineage and quality, which can be used for auditing and debugging purposes. Administrators can verify this setting through the GCP Console by navigating to the Data Catalog settings and ensuring 'Lineage Data Quality Logs' is turned on. To enable, go to the specific Data Catalog entry, access the settings, and activate logging for lineage data quality.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.datacatalog.tag.lineage_data_quality_output_location_private_and_encrypted
  service: datacatalog
  resource: tag
  requirement: Lineage Data Quality Output Location Private And Encrypted
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Data Quality Output is Private and Encrypted
  rationale: Storing lineage data quality outputs insecurely can expose sensitive information, leading to data breaches and non-compliance with regulations like GDPR and CCPA. Encrypting this data at rest and ensuring its storage location is private mitigates risks of unauthorized access and data leakage, protecting organizational and customer data integrity.
  description: This rule checks that the storage location for lineage data quality outputs in Google Cloud's Data Catalog is both private and encrypted. It ensures that Cloud Storage buckets used for this purpose have appropriate IAM policies restricting public access and that bucket-level encryption is enabled. Verification involves reviewing bucket settings in the Cloud Console or using command-line tools. Remediation steps include configuring IAM policies to restrict access and enabling encryption either by using Google-managed keys or customer-managed keys.
  references:
  - https://cloud.google.com/datacatalog/docs
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.datacatalog.tag.lineage_data_quality_role_least_privilege
  service: datacatalog
  resource: tag
  requirement: Lineage Data Quality Role Least Privilege
  scope: datacatalog.tag.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Data Quality Role
  rationale: Implementing least privilege for roles in GCP Data Catalog is crucial to minimize the risk of unauthorized access and potential data breaches. Over-permissioned roles can lead to data leaks, non-compliance with regulations like GDPR and HIPAA, and increased attack surface for malicious insiders. Ensuring roles are tightly scoped helps protect sensitive data and maintains trust with customers and partners.
  description: This rule checks that the roles assigned to users or service accounts for managing lineage data quality in Data Catalog adhere to the principle of least privilege. It ensures that permissions are limited to only those necessary for their job functions. Administrators should regularly audit IAM policies associated with Data Catalog tags and adjust them to prevent excessive permissions, thus reducing the risk of unauthorized actions. Remediation involves modifying IAM policies to remove unnecessary permissions, using predefined roles where possible, and employing custom roles only when absolutely necessary.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 5
  - https://cloud.google.com/iam/docs/best-practices
- rule_id: gcp.datacatalog.tag_template.data_catalog_classifier_definition_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Classifier Definition Version Pinned
  scope: datacatalog.tag_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Data Catalog Classifier Version is Pinned
  rationale: Pinning the Data Catalog classifier definition version is crucial to maintain consistent and predictable metadata tagging across data assets. It helps mitigate risks associated with unexpected changes that could result in misclassification of data, potentially leading to unauthorized data exposure or compliance violations. This control supports maintaining data integrity and compliance with regulations that require strict data classification protocols.
  description: This rule checks if the Data Catalog classifier definition version is pinned in tag templates. A pinned version ensures that the metadata tagging behavior remains consistent, preventing potential breakdowns in data governance strategies. To verify, review the tag template configurations in the Data Catalog and ensure the classifier version is explicitly specified. If not pinned, update the tag template to a specific version, documented and tested, to prevent discrepancies during metadata tagging operations.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/tag-templates
  - https://cloud.google.com/security/compliance/cis#section-5
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/data-catalog/docs/concepts
  - https://cloud.google.com/architecture/framework/security
- rule_id: gcp.datacatalog.tag_template.data_catalog_classifier_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Classifier RBAC Least Privilege
  scope: datacatalog.tag_template.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Tag Templates
  rationale: Implementing least privilege access for Data Catalog Tag Templates minimizes the risk of unauthorized access and data breaches. It helps protect sensitive metadata by ensuring only necessary permissions are granted, reducing the attack surface and aiding compliance with standards like ISO 27001 and PCI-DSS, which mandate strict access controls.
  description: This rule checks that permissions on Data Catalog Tag Templates are configured to follow the principle of least privilege. Verify that roles assigned to users or service accounts have only the minimum necessary permissions. Regularly review IAM policies to ensure they do not grant excessive access. To remediate, audit current role bindings and adjust IAM policies to align with least privilege principles, removing unnecessary permissions.
  references:
  - https://cloud.google.com/datacatalog/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.datacatalog.tag_template.data_catalog_classifier_source_trusted
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Classifier Source Trusted
  scope: datacatalog.tag_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Data Catalog Classifier Sources Are Trusted
  rationale: Trusting only verified data sources in Data Catalog prevents unauthorized or malicious data from being classified and used by your organization, reducing the risk of data breaches and ensuring data integrity. This is vital for meeting compliance requirements such as GDPR and CCPA, which mandate secure handling of sensitive data.
  description: This rule checks whether the sources of data classified within Google Cloud Data Catalog are from trusted origins. Ensure that tag templates are configured to only include data from authenticated and verified sources. Verification can be done by setting up appropriate IAM permissions and using Google Cloud's identity and access management to restrict access to trusted users and services. Regularly audit and update your IAM policies to reflect changes in your data landscape.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.tag_template.data_catalog_ruleset_encrypted_at_rest
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Ruleset Encrypted At Rest
  scope: datacatalog.tag_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Ruleset Encryption at Rest
  rationale: Encrypting Data Catalog rulesets at rest protects sensitive metadata from unauthorized access and helps mitigate the risk of data breaches. This security measure is critical for maintaining data confidentiality and integrity, especially in sectors subject to regulatory compliance such as finance, healthcare, and government. Failure to apply encryption could lead to significant financial penalties and reputational damage.
  description: This rule verifies that Data Catalog tag templates are encrypted at rest using Google-managed or customer-managed encryption keys. To confirm compliance, check that the Data Catalog API is configured to use encryption for tag templates. Remediation involves ensuring that all tag templates are stored in a project where encryption at rest is enforced, utilizing Google Cloud Key Management Service (KMS) if necessary.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.tag_template.data_catalog_ruleset_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Ruleset RBAC Least Privilege
  scope: datacatalog.tag_template.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Tag Template Roles
  rationale: Implementing least privilege in Data Catalog tag templates minimizes the risk of unauthorized access, data breaches, and non-compliance with regulations such as GDPR and HIPAA. By restricting permissions to only those necessary for specific roles, organizations reduce their attack surface and maintain better control over sensitive metadata.
  description: This rule checks that roles assigned to Data Catalog tag templates adhere to the principle of least privilege. Review permissions granted to users and service accounts to ensure they are limited to necessary actions such as viewing or editing tags. Remediate by adjusting IAM policies to restrict roles to essential operations only, using predefined roles where possible. Verification can be done through the GCP Console or gcloud CLI by examining IAM policy bindings on tag templates.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag_template.data_catalog_ruleset_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Ruleset Version Pinned
  scope: datacatalog.tag_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Data Catalog Tag Template Ruleset Version is Pinned
  rationale: Pinning the version of a Data Catalog ruleset ensures consistency and reliability in data classification and governance processes. Unpinned versions may lead to unexpected behavior and potential security risks as changes in newer versions could introduce vulnerabilities or non-compliance with data governance policies.
  description: This rule checks whether the Data Catalog tag templates have their ruleset versions explicitly pinned. To verify, review the tag templates in the Data Catalog to ensure each has a specific ruleset version defined. Remediation involves updating the tag templates to specify a version number, thereby preventing automatic upgrades that could disrupt data governance operations.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/tag-templates
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/data-catalog/docs/how-to/policy-tag-manager#manage_policies
- rule_id: gcp.datacatalog.tag_template.data_quality_ruleset_encrypted_at_rest
  service: datacatalog
  resource: tag_template
  requirement: Data Quality Ruleset Encrypted At Rest
  scope: datacatalog.tag_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Quality Ruleset is Encrypted at Rest
  rationale: Encrypting data at rest is crucial for protecting sensitive information from unauthorized access and potential data breaches. This aligns with compliance requirements such as GDPR and HIPAA, which mandate the protection of personal and sensitive data. Failure to encrypt data at rest can expose the organization to legal liabilities and reputational damage.
  description: This rule checks whether all Data Quality Rulesets within Google Cloud's Data Catalog are encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that encryption settings are configured in the tag template properties of the Data Catalog. Remediation involves applying appropriate encryption policies, such as enabling Google Cloud KMS to manage encryption keys, to secure data at rest.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/data-catalog/docs/how-to
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 5.3
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.hipaajournal.com/hipaa-encryption-requirements
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag_template.data_quality_ruleset_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Data Quality Ruleset RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Tag Templates
  rationale: Implementing least privilege for Data Catalog tag templates minimizes potential unauthorized access and data exposure risks. This is crucial for maintaining data integrity and confidentiality, especially in regulated environments where compliance with standards like HIPAA or PCI-DSS is required. Misconfigured permissions could lead to data breaches or compliance violations, impacting both financial and reputational aspects of the business.
  description: This rule checks that IAM roles granted on Data Catalog tag templates adhere to the principle of least privilege. It ensures that only necessary permissions are delegated, reducing the risk of unauthorized access. Verify that roles assigned have the minimal set of permissions needed for their function by auditing IAM policies associated with tag templates. To remediate, adjust the roles to limit permissions to only those that are essential for the task, and regularly review IAM policy assignments.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/tag-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.datacatalog.tag_template.data_quality_ruleset_version_immutable
  service: datacatalog
  resource: tag_template
  requirement: Data Quality Ruleset Version Immutable
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Quality Ruleset Version is Immutable
  rationale: Immutability of data quality ruleset versions prevents unauthorized changes that could compromise data integrity and privacy. It ensures that once a ruleset is applied, it remains consistent, reducing the risk of accidental or malicious alterations. This integrity is crucial for maintaining trust with stakeholders and complying with regulations such as GDPR and HIPAA.
  description: This rule checks that the data quality ruleset version within Google Cloud Data Catalog tag templates is immutable. To verify, ensure that once a ruleset version is created, it cannot be edited or deleted. Remediation involves configuring the tag template settings to lock the ruleset version upon creation. This can be done through the GCP Console or using the gcloud CLI, ensuring only authorized personnel can manage these settings.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/tag-templates
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.datacatalog.tag_template.datalake_classifier_definition_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Datalake Classifier Definition Version Pinned
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Datalake Classifier Definition Version is Pinned
  rationale: Pinning the version of Datalake classifier definitions is crucial to maintain consistent data classification and prevent unauthorized or unintended changes that could lead to data misclassification. This helps in mitigating risks of data breaches and ensures compliance with data protection regulations such as GDPR and CCPA, which require stringent data management controls.
  description: This rule checks whether the Datalake classifier definitions within Google Cloud's Data Catalog have their versions pinned to avoid unexpected updates that might compromise data classification integrity. To verify this, check the tag template configurations in the Data Catalog and ensure that the classifier version is set to a specific version rather than 'latest'. Remediation involves explicitly setting the classifier definition version in the tag template configuration.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-guides
  - https://cloud.google.com/security/compliance/cis-google-cloud-computing-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/privacy
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datacatalog.tag_template.datalake_classifier_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Datalake Classifier RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Datalake Classifier Tag Templates
  rationale: Implementing least privilege access for the Datalake Classifier in Data Catalog helps minimize the risk of unauthorized data access and potential data breaches. Excessive permissions can lead to accidental or intentional misuse of sensitive data, impacting business operations and violating compliance mandates such as GDPR and CCPA.
  description: This rule checks that only necessary permissions are granted to users or groups for accessing tag templates associated with the Datalake Classifier in the Data Catalog. It verifies that permissions align with least privilege principles, specifically focusing on reducing over-privileged accounts. Remediation involves reviewing and adjusting IAM policies to ensure that only essential roles are assigned, utilizing predefined roles where possible instead of overly permissive custom roles.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/policy-tags
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag_template.datalake_classifier_source_trusted
  service: datacatalog
  resource: tag_template
  requirement: Datalake Classifier Source Trusted
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Trusted Source for Datalake Classifier Tags
  rationale: Ensuring that only trusted sources can apply tags in Data Catalog helps prevent unauthorized data classification, which can lead to data leakage, misclassification, and potential breaches. Mismanagement of data classification tags can result in non-compliance with regulations like GDPR and CCPA, exposing the organization to legal and financial penalties.
  description: This rule verifies that only trusted entities are allowed to create or modify tag templates in Google Cloud Data Catalog, specifically for Datalake classifiers. Check that IAM policies for tag templates strictly limit permissions to verified sources. Remediation involves auditing current IAM roles and permissions, then revoking access from untrusted entities and ensuring that only secured service accounts or authenticated users have the necessary roles to apply tags.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/tag-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0 - 6.7
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.tag_template.lineage_classifier_definition_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Lineage Classifier Definition Version Pinned
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Classifier Definition Version is Pinned
  rationale: Pinning the lineage classifier definition version in Data Catalog ensures consistency and integrity of data classification processes. Unpinned versions may lead to unintentional updates, potentially exposing sensitive data or causing compliance violations. This practice supports data governance and mitigates risks associated with unauthorized data access.
  description: This rule verifies that the lineage classifier definition version in Data Catalog's tag templates is explicitly pinned. To check, review the tag template configurations to ensure a fixed version number is specified. If not pinned, update the tag template settings in the Google Cloud Console or via gcloud command-line to include a version identifier. This prevents automatic updates to newer, untested versions which could introduce vulnerabilities.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.tagTemplates
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/data-catalog/docs/how-to
- rule_id: gcp.datacatalog.tag_template.lineage_classifier_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Lineage Classifier RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Tag Template Access
  rationale: Implementing least privilege access for Data Catalog tag templates is crucial to minimize the risk of unauthorized data modification or exposure. Improper access configurations can lead to accidental data leaks or insider threats, impacting compliance with regulations like GDPR and CCPA and potentially causing significant business reputational and financial damage.
  description: This rule checks if IAM roles assigned to Data Catalog tag templates adhere to the principle of least privilege. Ensure that only necessary permissions are granted to users and service accounts, limiting access to those required for specific tasks. To verify compliance, review IAM policies associated with each tag template and adjust roles to restrict permissions. Remediation involves using the Cloud Console or gcloud CLI to update IAM policies, ensuring only essential roles like 'roles/datacatalog.viewer' or 'roles/datacatalog.editor' are assigned as per operational needs.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/overview/whitepapers
- rule_id: gcp.datacatalog.tag_template.lineage_classifier_source_trusted
  service: datacatalog
  resource: tag_template
  requirement: Lineage Classifier Source Trusted
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Trusted Sources for Lineage Classifier in Data Catalog
  rationale: Using untrusted sources for lineage classification can lead to inaccurate data governance, impacting decision-making and increasing the risk of data breaches. Trustworthy sources are essential to maintain data integrity and comply with regulations such as GDPR and CCPA, which mandate stringent data protection and privacy controls.
  description: This rule verifies that all sources used for lineage classification in Google Cloud's Data Catalog are from trusted origins. It checks the configuration of tag templates to ensure that only approved and verified sources are used. To remediate, review the list of sources configured in the tag template and update them to include only those that are verified and compliant with your organization's data governance policies.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-entries
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-compliance-requirements/
- rule_id: gcp.datacatalog.tag_template.lineage_rule_definition_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Lineage Rule Definition Version Pinned
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Rule Version in Data Catalog is Pinned
  rationale: Pinning the version of a lineage rule definition is crucial as it ensures that the rule's behavior remains consistent over time, preventing unexpected changes that could lead to data misclassification or unauthorized data access. This practice mitigates the risk of compliance violations by maintaining data integrity and alignment with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks if the version of lineage rule definitions in Google Cloud Data Catalog tag templates is pinned. A pinned version prevents unintended updates which could compromise data classification accuracy. To verify, review your tag template settings within Data Catalog and ensure each lineage rule has a set version. Remediation involves specifying a fixed version number for each lineage rule definition in tag templates to maintain consistent data processing logic.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/tag-templates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.datacatalog.tag_template.lineage_rule_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Lineage Rule RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege on Data Catalog Tag Template Lineage
  rationale: Applying the principle of least privilege to Data Catalog tag templates minimizes the risk of unauthorized data access or modification, protecting sensitive metadata associated with Google Cloud resources. This is critical for maintaining data integrity and confidentiality, especially in compliance with regulations such as GDPR and HIPAA, which mandate strict access controls.
  description: This rule checks whether IAM roles assigned to Data Catalog tag templates adhere to the principle of least privilege, ensuring only necessary permissions are granted. Verify that users and service accounts have the minimum required roles, such as 'roles/datacatalog.tagTemplateUser' instead of broader roles like 'roles/datacatalog.admin'. Remediation involves auditing current IAM policies, removing excessive permissions, and implementing least privilege access through role adjustments or custom roles as needed.
  references:
  - https://cloud.google.com/datacatalog/docs/how-to/tag-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.datacatalog.tag_template.lineage_rule_source_trusted
  service: datacatalog
  resource: tag_template
  requirement: Lineage Rule Source Trusted
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Rule Source in Data Catalog is Trusted
  rationale: Ensuring the source of lineage rules within Data Catalog is trusted helps protect against unauthorized data access and manipulation. This is crucial for maintaining data integrity and privacy, as well as for compliance with regulations such as GDPR and CCPA that require stringent data protection measures. Untrusted sources may introduce security vulnerabilities that could lead to data breaches and financial penalties.
  description: This rule checks whether the sources of lineage rules in Data Catalog's tag templates are from trusted origins. To verify, review the lineage rules associated with your tag templates and confirm they originate from known, secure sources. Remediation involves configuring access controls to restrict the creation of lineage rules to trusted entities only, and auditing existing rules for compliance with security policies.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.datacatalog.tag_template.lineage_ruleset_encrypted_at_rest
  service: datacatalog
  resource: tag_template
  requirement: Lineage Ruleset Encrypted At Rest
  scope: datacatalog.tag_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Lineage Ruleset is Encrypted at Rest
  rationale: Encrypting data at rest is critical to protect sensitive information from unauthorized access and ensure compliance with data protection regulations. Without encryption, data stored in the cloud is vulnerable to breaches, potentially leading to financial loss, reputational damage, and regulatory penalties.
  description: This rule checks that all Lineage Rulesets associated with Data Catalog Tag Templates are encrypted at rest using Google-managed or customer-managed keys. To verify, ensure that the Encryption Configuration in the Data Catalog settings specifies a valid key. Remediation involves configuring the Data Catalog to use Cloud KMS keys for encrypting data at rest, providing an additional layer of security.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/security/encryption-at-rest/
- rule_id: gcp.datacatalog.tag_template.lineage_ruleset_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Lineage Ruleset RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Tag Template Lineage
  rationale: Implementing least privilege for lineage rulesets in GCP Data Catalog minimizes the risk of unauthorized access and data breaches. Misconfigured permissions can lead to data leaks, violating compliance requirements such as GDPR and CCPA, and may result in financial and reputational damage. Proper access control is critical for maintaining data integrity and confidentiality.
  description: This rule checks that access to Data Catalog tag templates with lineage rulesets is restricted to only those identities that require it. Verify that IAM roles assigned to users or service accounts have the least privileges necessary. Remediate by auditing current permissions, removing unnecessary roles, and using predefined roles over primitive roles whenever possible. Regularly review and adjust access rights to adapt to organizational changes.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag_template.lineage_ruleset_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Lineage Ruleset Version Pinned
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Ruleset Version is Pinned in Data Catalog
  rationale: Pinning the lineage ruleset version in Google Cloud Data Catalog ensures that the data classification and lineage tracking use a consistent version of rulesets. This prevents unexpected changes that could impact data security and compliance, reducing the risk of unauthorized data alterations and ensuring adherence to data governance policies.
  description: This rule checks if the lineage ruleset version in the Data Catalog tag template is pinned. A pinned version ensures that lineage tracking and data classification remain consistent and predictable. To verify, review the Data Catalog tag templates for lineage ruleset configurations and ensure a specific version is set. If not pinned, update the tag template configuration to specify a version number for the lineage ruleset, ensuring stability and compliance with data governance practices.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dataflow.job.data_pipeline_allowed_values_defined_where_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Allowed Values Defined Where Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Uses Allowed Parameter Values
  rationale: Defining allowed parameter values for Dataflow pipelines is crucial for maintaining data integrity and preventing unauthorized data processing activities. Without clear restrictions, there's an increased risk of data breaches or non-compliance with regulatory standards such as GDPR or HIPAA, especially when dealing with sensitive data. This practice helps organizations maintain control over data processing operations and safeguards against inadvertent data exposure.
  description: This rule checks if Dataflow jobs have clearly defined allowed values for parameters, ensuring that only approved configurations are used. To verify compliance, review the Dataflow job configurations and confirm that parameter values are restricted to pre-approved ranges or sets. If discrepancies are found, update the job configurations to enforce allowed values, using GCP's parameter validation options. This ensures consistent and secure data processing operations.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#data-management
  - https://cloud.google.com/dataflow/docs/guides/setting-pipeline-options
- rule_id: gcp.dataflow.job.data_pipeline_attributes_no_plaintext_secrets
  service: dataflow
  resource: job
  requirement: Data Pipeline Attributes No Plaintext Secrets
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Prevent Plaintext Secrets in Dataflow Job Attributes
  rationale: Storing secrets in plaintext within Dataflow job attributes poses significant security risks, including unauthorized access and data breaches. This is critical for protecting sensitive data and maintaining compliance with regulations like GDPR and HIPAA, which mandate strict data protection measures. Ensuring secrets are encrypted mitigates the risk of exposure and potential financial and reputational damage.
  description: This rule checks that no secrets or sensitive information are stored in plaintext within the attributes of a Google Cloud Dataflow job. Verify that secrets are not included directly in job configurations or command-line arguments. Instead, use Google Cloud Secret Manager to securely reference secrets. Remediation involves updating job configurations to remove any plaintext secrets and replacing them with secure references.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs
  - CIS GCP Benchmark v1.3.0 - 5.1 Ensure that Cloud Dataflow jobs do not contain sensitive information
  - 'NIST SP 800-53: AC-3 Access Enforcement'
  - 'PCI-DSS Requirement 3: Protect stored cardholder data'
  - ISO/IEC 27001:2013 A.8.2 Information Classification
- rule_id: gcp.dataflow.job.data_pipeline_binding_only_expected_params_bound
  service: dataflow
  resource: job
  requirement: Data Pipeline Binding Only Expected Params Bound
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Parameters Are Properly Bound
  rationale: Ensuring that only expected parameters are bound in Dataflow jobs is crucial to prevent unauthorized data access and manipulation. Improper parameter binding can lead to data leakage, unauthorized data manipulation, and compliance breaches, affecting the organization's data integrity and confidentiality. This is particularly important for meeting regulatory requirements such as GDPR and HIPAA that mandate strict data protection controls.
  description: This rule checks that Dataflow jobs only bind expected parameters, which helps prevent injection attacks and unauthorized data access. Verify that Dataflow job configurations do not include extraneous parameters and that all parameters are explicitly defined in job templates. Remediation involves auditing the job configurations and removing any unexpected or unnecessary parameters, ensuring that the job adheres to the security policies defined by your organization.
  references:
  - https://cloud.google.com/dataflow/docs/guides/dataflow-security-best-practices
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.dataflow.job.data_pipeline_binding_secret_refs_resolved_at_runtime
  service: dataflow
  resource: job
  requirement: Data Pipeline Binding Secret Refs Resolved At Runtime
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Dataflow Secret Refs Resolved at Runtime for Security
  rationale: Resolving secret references at runtime minimizes the risk of exposing sensitive information, thereby protecting data integrity and confidentiality. This practice is crucial in mitigating unauthorized access and ensuring compliance with data protection regulations such as GDPR and HIPAA, which mandate rigorous control over sensitive data handling.
  description: This rule checks that Dataflow jobs are configured to resolve secret references at runtime, ensuring that secrets are not hardcoded or stored insecurely. Verify that your Dataflow pipeline uses secret managers or other secure methods to retrieve secrets dynamically during execution. To remediate, configure your Dataflow job to use the GCP Secret Manager API or equivalent to fetch secrets securely at runtime, ensuring these configurations are not exposed in code.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.dataflow.job.data_pipeline_binding_sensitive_params_not_logged
  service: dataflow
  resource: job
  requirement: Data Pipeline Binding Sensitive Params Not Logged
  scope: dataflow.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Prevent Logging of Sensitive Params in Dataflow Jobs
  rationale: Logging sensitive parameters in Dataflow jobs can expose confidential information, leading to potential data breaches and non-compliance with regulations such as GDPR and HIPAA. This rule helps mitigate the risk of unauthorized access to sensitive data, protecting organizational assets and maintaining customer trust.
  description: This rule checks that Dataflow jobs do not log sensitive parameters, which can occur if logging levels are misconfigured. To verify, ensure that logging configurations exclude sensitive data by checking job settings in the GCP Console or using the gcloud command-line tool. Remediation involves adjusting logging settings to mask or exclude sensitive information from being logged.
  references:
  - https://cloud.google.com/dataflow/docs/guides/logging
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/logging/docs/audit
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_constraint_allowlist_defined_when_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Constraint Allowlist Defined When Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipeline Constraints Are Allowlisted
  rationale: Defining an allowlist for data pipeline constraints in GCP Dataflow jobs helps mitigate risks of unauthorized data processing and potential data breaches. It ensures that only approved pipelines can be executed, aligning with security best practices and compliance requirements such as GDPR and HIPAA, which mandate strict data processing controls.
  description: This rule checks whether a Dataflow job has an allowlist defined for pipeline constraints. Without this, there is a risk of executing unauthorized or potentially harmful data processing tasks. Verify the allowlist configuration in the job settings and update it to include only trusted data pipelines. This can be done through the GCP Console or by using gcloud commands to update the job's parameter settings.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://www.iso.org/standard/73906.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/dataflow/docs/guides/using-gcloud
- rule_id: gcp.dataflow.job.data_pipeline_constraint_max_length_defined_when_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Constraint Max Length Defined When Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipeline Max Length is Defined
  rationale: Defining a maximum length for data pipelines in Dataflow jobs helps prevent resource exhaustion and potential service disruptions. This is crucial for maintaining the availability and reliability of services, as well as safeguarding sensitive data from unintentional exposure due to overly complex or long-running processes. Compliance with data protection regulations often requires such constraints to manage and mitigate operational risks.
  description: This rule checks if the maximum length for data pipelines in Dataflow jobs is specified where applicable. Configuring this setting involves defining constraints in the pipeline's configuration files or using the Dataflow API. To verify, review the pipeline configuration in the Dataflow console or via the GCP CLI, ensuring a max length is set. Remediation involves updating the pipeline configuration to include a max length constraint, thereby optimizing resource usage and enhancing security posture.
  references:
  - https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/deployment-best-practices
- rule_id: gcp.dataflow.job.data_pipeline_constraint_min_length_defined_when_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Constraint Min Length Defined When Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Min Length Constraint Is Defined
  rationale: Defining a minimum length constraint for data pipelines in Dataflow is crucial to prevent data breaches by ensuring that data fields meet necessary security and integrity requirements. This minimizes the risk of processing incomplete or malformed data, which can lead to data leakage or processing errors that compromise data privacy. Compliance with industry standards such as GDPR and PCI-DSS often demands stringent data integrity checks.
  description: This rule verifies whether a minimum length constraint is defined for applicable data pipelines within Dataflow jobs. To check compliance, inspect the pipeline configurations to ensure that all relevant data fields have an explicitly set minimum length constraint. If not configured, modify the Dataflow job settings to include these constraints, ensuring that sensitive data is processed with appropriate length validations, enhancing data integrity and security.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/dataflow/docs/guides/dataflow-security
- rule_id: gcp.dataflow.job.data_pipeline_constraint_pattern_defined_when_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Constraint Pattern Defined When Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Constraint Patterns for Data Pipelines
  rationale: Defining constraint patterns for Dataflow jobs is crucial for maintaining data integrity and security. Without these constraints, data pipelines may inadvertently process or expose sensitive data, increasing the risk of data breaches and non-compliance with regulations such as GDPR and HIPAA. By enforcing constraint patterns, organizations can ensure that their data processing activities are within the defined security boundaries, reducing the risk of unauthorized data access and potential legal penalties.
  description: This check verifies that Dataflow jobs have defined constraint patterns when applicable, ensuring that data pipelines adhere to organizational security policies. To define these constraints, configure job parameters to include security specifications, such as encryption at rest using Cloud KMS. Remediation involves modifying the Dataflow job configuration to include these constraints and ensuring they are enforced during job execution. Verify compliance by reviewing job settings in the GCP Console or using gcloud commands to inspect job configurations.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.dataflow.job.data_pipeline_default_not_sensitive
  service: dataflow
  resource: job
  requirement: Data Pipeline Default Not Sensitive
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipelines Are Not Using Default Encryption
  rationale: Using default encryption for Dataflow jobs can expose sensitive data to unauthorized access and potential breaches if the default keys are compromised. Encrypting data with customer-managed keys enhances control over data privacy and compliance with regulations such as GDPR and CCPA, which require stringent data protection measures.
  description: This rule checks if Dataflow jobs use customer-managed encryption keys instead of the default Google-managed keys for encrypting data at rest. To verify, review the Dataflow job configurations for the `kmsKeyName` property. Remediation involves configuring Dataflow jobs to use a specific customer-managed key by specifying its `kmsKeyName`, ensuring enhanced data security and compliance with regulatory requirements.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/security/compliance/gdpr/
  - CIS GCP Foundation Benchmark v1.3.0 - Section 7.10
  - NIST SP 800-57 Part 1
  - ISO/IEC 27001:2013
- rule_id: gcp.dataflow.job.data_pipeline_definition_signed_and_verified
  service: dataflow
  resource: job
  requirement: Data Pipeline Definition Signed And Verified
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Definitions are Signed and Verified
  rationale: Signing and verifying Dataflow pipeline definitions ensure that the jobs are not tampered with and originate from a trusted source. This practice mitigates risks such as unauthorized data manipulation, phishing attacks, and ensures compliance with data protection regulations by maintaining data integrity and authenticity.
  description: This rule checks that all Dataflow job definitions are digitally signed and verified before execution. Organizations should implement a process where pipeline definitions are signed using a GCP-managed key and verified to prevent the execution of malicious code. Remediation involves enabling signing and verification in the Dataflow job submission process, and using Cloud KMS to manage and store cryptographic keys securely.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/kms/docs/best-practices
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_definition_storage_encrypted
  service: dataflow
  resource: job
  requirement: Data Pipeline Definition Storage Encrypted
  scope: dataflow.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipeline Definitions Are Encrypted at Rest
  rationale: Encrypting Dataflow job pipeline definitions at rest safeguards sensitive configuration data from unauthorized access and potential data breaches. This is critical to maintain data confidentiality and integrity, especially when handling sensitive information that may be subject to regulatory compliance such as PCI-DSS or HIPAA. Inadequate encryption can lead to exposure of operational details that could be exploited by malicious actors.
  description: This check verifies that all Dataflow job pipeline definitions are stored with encryption at rest enabled. By default, Google Cloud encrypts data at rest, but it is essential to confirm that customer-managed encryption keys (CMEK) are used for enhanced security. To verify, inspect the Dataflow job settings to ensure that the 'kmsKeyName' parameter is set appropriately. Remediation involves configuring Dataflow jobs to use CMEK by specifying a valid Cloud KMS key, ensuring the key is managed securely and access is restricted to authorized personnel.
  references:
  - https://cloud.google.com/dataflow/docs/guides/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dataflow.job.data_pipeline_definition_storage_private
  service: dataflow
  resource: job
  requirement: Data Pipeline Definition Storage Private
  scope: dataflow.job.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Dataflow Pipeline Definitions are Stored Privately
  rationale: Storing Dataflow pipeline definitions in publicly accessible locations can expose sensitive configuration details and business logic, increasing the risk of data breaches and unauthorized access. Keeping these definitions private helps mitigate potential attacks and aligns with compliance requirements for data protection such as GDPR and HIPAA.
  description: This rule verifies that Dataflow job pipeline definitions are stored in private locations, preventing unauthorized access. To ensure privacy, configure Google Cloud Storage buckets containing pipeline definitions with IAM policies that restrict access to only necessary service accounts and users. Regularly audit bucket permissions to maintain security posture. Remediation involves adjusting bucket permissions via the Google Cloud Console or using gcloud commands to enforce private access.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dataflow.job.data_pipeline_definition_version_immutability_enforced
  service: dataflow
  resource: job
  requirement: Data Pipeline Definition Version Immutability Enforced
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Definition Version Immutability
  rationale: Enforcing immutability of data pipeline definitions in Dataflow reduces the risk of unauthorized changes, ensuring consistent and repeatable data processing. This is crucial for maintaining data integrity and meeting compliance requirements such as SOC2 and PCI-DSS, which mandate strict controls over data processing environments.
  description: This rule checks if the Dataflow pipeline definitions are immutable, preventing modifications to deployed versions. Verify that the Dataflow jobs use settings that lock the pipeline definition upon deployment. To enforce this, configure your CI/CD pipeline to deploy new versions instead of updating existing ones. This ensures auditability and traceability of data processing logic changes.
  references:
  - https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
  - https://cloud.google.com/dataflow/docs/concepts/security-and-compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.dataflow.job.data_pipeline_max_length_defined_where_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Max Length Defined Where Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipeline Max Length is Defined
  rationale: Defining a maximum length for Dataflow job pipelines helps prevent excessive data processing durations which can lead to increased costs, potential data exposure, and compliance violations. By setting clear limits, organizations can mitigate risks associated with long-running data processes that may inadvertently handle sensitive data longer than necessary, aligning with data protection regulations.
  description: This rule checks whether a maximum length is configured for Dataflow job pipelines to ensure they do not run indefinitely. Without these limits, pipelines might process data longer than intended, increasing the likelihood of data retention beyond policy stipulations. To verify, review Dataflow job configurations to ensure a max length is defined and adjust the settings via the GCP Console or gcloud command-line tool. This helps maintain control over data processing durations and aligns with best practices for data lifecycle management.
  references:
  - https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dataflow.job.data_pipeline_metadata_disallowed_attribute_keys_blocked
  service: dataflow
  resource: job
  requirement: Data Pipeline Metadata Disallowed Attribute Keys Blocked
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Block Disallowed Metadata Keys in Dataflow Jobs
  rationale: Blocking disallowed metadata keys in Dataflow jobs is crucial to prevent unauthorized access to sensitive information. Metadata can inadvertently expose information that could be exploited by attackers, leading to data breaches or non-compliance with data protection regulations. Ensuring that only approved metadata keys are used mitigates the risk of data leakage and supports organizational compliance with frameworks like GDPR and HIPAA.
  description: This rule checks that Dataflow jobs do not use metadata keys that are disallowed by policy. Administrators must review and configure the Dataflow job settings to ensure only authorized metadata keys are utilized. Verification involves auditing the job configurations and employing IAM policies to restrict the use of sensitive metadata keys. Remediation requires updating job configurations and applying IAM roles that enforce these restrictions.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_metadata_sensitive_keys_require_secret_type
  service: dataflow
  resource: job
  requirement: Data Pipeline Metadata Sensitive Keys Require Secret Type
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Secret Type for Sensitive Keys in Dataflow Metadata
  rationale: It is crucial to protect sensitive information in data pipeline metadata to prevent unauthorized access and potential data breaches. Ensuring that sensitive keys are stored as secrets reduces the risk of exposure, aligns with best practices for data protection, and helps maintain compliance with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks that sensitive keys within Google Cloud Dataflow job metadata are configured as secret types rather than plain text. Verify that any configuration parameter or metadata key that contains sensitive information, such as API keys or passwords, is stored using the Secret Manager service. Remediation involves identifying these keys and migrating them to Secret Manager, updating your Dataflow jobs to reference the secrets securely.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_metadata_values_not_plaintext_secret
  service: dataflow
  resource: job
  requirement: Data Pipeline Metadata Values Not Plaintext Secret
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Dataflow Metadata Values Are Not Plaintext Secrets
  rationale: Storing plaintext secrets in Dataflow metadata increases the risk of unauthorized access to sensitive information, potentially leading to data breaches. This practice can also violate compliance requirements such as GDPR, HIPAA, and PCI-DSS, where protecting sensitive data is mandated. Ensuring metadata does not contain plaintext secrets helps maintain data confidentiality and integrity, reducing the organization's risk exposure.
  description: This rule checks that no plaintext secrets, such as passwords or API keys, are stored within the metadata of Dataflow jobs. To verify, inspect Dataflow job configurations for any sensitive information stored in plaintext. Remediation involves using secret management services like Google Cloud Secret Manager to store sensitive data securely and referring to them in the configurations via environment variables or other secure methods.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/standard/54534.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.dataflow.job.data_pipeline_min_length_defined_where_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Min Length Defined Where Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Pipeline Min Length is Defined for Dataflow Jobs
  rationale: Defining a minimum data pipeline length in Dataflow jobs is crucial for maintaining data integrity and consistency, especially in complex data processing environments. This requirement helps mitigate risks of data loss or incomplete data processing, which can lead to inaccurate analytics and business decisions. Additionally, compliance with data protection standards such as GDPR and HIPAA may mandate specific data handling processes, which this rule supports by ensuring pipelines are properly configured.
  description: This rule checks that Dataflow jobs have a minimum pipeline length defined where applicable, ensuring that data processing tasks are not prematurely terminated. Verification involves reviewing job configurations to ensure that pipeline length parameters are set appropriately. Remediation requires configuring the Dataflow job settings through the GCP Console or using IaC tools like Terraform to specify a minimum pipeline length that aligns with business and compliance requirements.
  references:
  - https://cloud.google.com/dataflow/docs/guides/specifying-exec-params
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/dataflow/docs/resources/faq
  - https://cloud.google.com/dataflow/docs/guides/setting-pipeline-options
- rule_id: gcp.dataflow.job.data_pipeline_object_environment_no_plaintext_secrets
  service: dataflow
  resource: job
  requirement: Data Pipeline Object Environment No Plaintext Secrets
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Avoid Plaintext Secrets in Dataflow Pipeline Environments
  rationale: Storing plaintext secrets in Dataflow pipeline environments poses significant security risks, including unauthorized access to sensitive data and potential data breaches. Compliance frameworks such as PCI-DSS and HIPAA require strict management of sensitive information, and failing to encrypt secrets can lead to non-compliance and financial penalties.
  description: This rule checks for the presence of plaintext secrets within Dataflow pipeline environment variables. It is essential to encrypt secrets using Google Cloud's Secret Manager or similar services, ensuring they are not exposed in plaintext. Verification involves inspecting Dataflow job configurations for unsecured environment variables and updating them to reference encrypted secrets. Remediation includes using Secret Manager API calls in your pipeline code to securely retrieve sensitive information.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/secret-manager/docs/overview
  - https://cloud.google.com/dataflow/docs/guides/using-public-datasets
- rule_id: gcp.dataflow.job.data_pipeline_object_execution_role_least_privilege
  service: dataflow
  resource: job
  requirement: Data Pipeline Object Execution Role Least Privilege
  scope: dataflow.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataflow Job Roles Follow Least Privilege Principle
  rationale: Applying the principle of least privilege to Dataflow job roles reduces the risk of unauthorized access and potential data breaches. This practice minimizes the impact of compromised credentials and aligns with compliance mandates such as PCI-DSS and ISO 27001, which require strict access controls to protect sensitive data and maintain trust with stakeholders.
  description: This rule checks that the roles assigned to Dataflow jobs only have the permissions necessary to perform their intended tasks. Verify that service accounts and users with access to Dataflow jobs are granted the minimal set of permissions required for their function. Remediation involves auditing current role assignments and tightening permissions using custom roles or predefined roles with the least privilege necessary to execute data pipeline tasks.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/iam/docs/understanding-roles#dataflow-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_object_logging_enabled
  service: dataflow
  resource: job
  requirement: Data Pipeline Object Logging Enabled
  scope: dataflow.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Dataflow Job Data Pipeline Object Logging
  rationale: Enabling logging for Dataflow jobs provides visibility into data processing activities, crucial for detecting anomalies and unauthorized access. This enhances the ability to respond to security incidents and supports compliance with data protection regulations like GDPR and SOC 2, which require audit trails for data processing activities.
  description: This rule checks if Dataflow jobs have object logging enabled to record interactions with data pipeline components. Verify logging settings in the Dataflow console under the job's configuration or via the Google Cloud CLI. To enable logging, adjust job configurations to include appropriate logging flags. Ensure that logs are stored securely and monitored for suspicious activities.
  references:
  - https://cloud.google.com/dataflow/docs/guides/logging
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmarks
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.dataflow.job.data_pipeline_object_network_private_only
  service: dataflow
  resource: job
  requirement: Data Pipeline Object Network Private Only
  scope: dataflow.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Dataflow Jobs Use Private Network for Data Pipeline
  rationale: Utilizing private networks for Dataflow jobs minimizes exposure to the public internet, thereby reducing the risk of unauthorized access and data breaches. This practice is crucial for maintaining data confidentiality and integrity, particularly for organizations handling sensitive information or adhering to strict compliance standards like PCI-DSS or HIPAA.
  description: This rule checks that Dataflow jobs are configured to use private IPs for data pipeline operations, ensuring network traffic remains within Google Cloud's secure network. To verify, inspect Dataflow job configurations for the use of 'enablePrivateIps' within the 'network' settings. Remediation involves setting 'enablePrivateIps' to true in the Dataflow job configuration to route traffic through private IPs only, thereby enhancing network security.
  references:
  - https://cloud.google.com/dataflow/docs/guides/specifying-networks#configuring_private_ip
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.dataflow.job.data_pipeline_pattern_defined_where_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Pattern Defined Where Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Jobs Define Data Pipeline Patterns
  rationale: Defining data pipeline patterns in Dataflow jobs is critical for maintaining data integrity and security. Without defined patterns, there is an increased risk of data mismanagement, unauthorized access, and non-compliance with data protection regulations such as GDPR and HIPAA. This practice also helps in standardizing processes, which reduces errors and enhances auditability.
  description: This rule checks whether Dataflow jobs in GCP have defined data pipeline patterns applicable to their operations. To verify, review the job configurations to ensure patterns are explicitly stated and align with organizational security policies. If patterns are undefined, update the Dataflow job configurations to include appropriate patterns that enforce encryption at rest and control data access. This can be done by using predefined templates or custom JSON files that specify the necessary parameters.
  references:
  - https://cloud.google.com/dataflow/docs/guides/templates/provided-templates
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/privacy/
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_artifacts_private_and_encrypted
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline Artifacts Private And Encrypted
  scope: dataflow.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Artifacts Are Private and Encrypted
  rationale: Encrypting Dataflow pipeline artifacts at rest ensures that sensitive data remains protected from unauthorized access and potential breaches. This is crucial for maintaining data confidentiality, meeting regulatory compliance such as GDPR and HIPAA, and safeguarding intellectual property and customer data from malicious actors.
  description: This rule verifies that Dataflow pipeline artifacts are stored in a private and encrypted manner. Ensure that data storage buckets used for Dataflow jobs are configured with 'Uniform bucket-level access' and use customer-managed encryption keys (CMEK) for encryption at rest. To verify, check the bucket settings in the Google Cloud Console and configure the necessary IAM permissions and CMEK settings to restrict access and enable encryption.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - CIS GCP Benchmark v1.1.0 - 4.1 Ensure that Cloud Storage bucket is not anonymously or publicly accessible
  - NIST SP 800-53 Rev. 5 SC-13 Cryptographic Protection
  - 'PCI-DSS v3.2.1 Requirement 3: Protect Stored Cardholder Data'
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_execution_roles_least_privilege
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline Execution Roles Least Privilege
  scope: dataflow.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataflow Pipeline Roles Use Least Privilege
  rationale: Ensuring least privilege for Dataflow pipeline roles reduces the risk of unauthorized access and potential data leaks. Misconfigured roles can lead to excessive permissions that may be exploited by malicious actors, impacting data integrity and confidentiality. Compliance with least privilege principles is crucial for meeting regulatory standards such as GDPR and HIPAA.
  description: This rule checks that Dataflow pipeline roles are configured with the least privilege necessary for their function. It verifies that roles assigned to Dataflow jobs do not include permissions beyond what is needed for pipeline execution. To remediate, review and update IAM policies to ensure roles have only the necessary permissions, removing any excessive privileges.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_kms_encryption_enabled
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline KMS Encryption Enabled
  scope: dataflow.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Dataflow Job Uses KMS for Pipeline Encryption
  rationale: Enabling KMS encryption for Dataflow pipelines helps protect sensitive data from unauthorized access and meets regulatory requirements such as PCI-DSS and HIPAA. Without encryption, data at rest is vulnerable to breaches, potentially leading to data loss, reputational damage, and financial penalties.
  description: This rule checks whether Google Cloud Dataflow jobs have Key Management Service (KMS) encryption enabled for pipelines. To verify, ensure that a KMS key is specified in the pipeline's configuration settings. Remediation involves setting up a KMS key in Google Cloud and configuring the Dataflow job to use this key for encrypting pipeline data during job initialization.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_logs_and_metrics_enabled
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline Logs And Metrics Enabled
  scope: dataflow.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Dataflow Pipeline Logs and Metrics Collection
  rationale: Enabling logs and metrics for Dataflow jobs is crucial for monitoring and troubleshooting data pipelines. Without these logs, identifying performance bottlenecks, errors, and unusual activity becomes challenging, which can lead to data integrity issues and potential breaches. Additionally, many compliance frameworks require logging and monitoring to ensure data processing activities are auditable.
  description: This check ensures that logging and monitoring are enabled for Dataflow jobs to capture detailed pipeline execution data. Verify that Stackdriver Logging and Monitoring are configured to collect logs and metrics for Dataflow pipelines. Remediation involves enabling these settings in the Dataflow job configuration to facilitate real-time insights and historical analysis of pipeline behavior.
  references:
  - https://cloud.google.com/dataflow/docs/guides/logging-and-monitoring
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/logging/docs/audit
  - CIS Google Cloud Platform Foundation Benchmark
  - NIST SP 800-53
  - ISO/IEC 27001
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_private_networking_enforced
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline Private Networking Enforced
  scope: dataflow.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for Dataflow Pipelines
  rationale: Enforcing private networking in Dataflow pipelines reduces the risk of data exposure by limiting network access to internal IPs only. This mitigates threats such as unauthorized access and data breaches, crucial for maintaining data confidentiality and integrity. Compliance with network security policies and standards such as ISO 27001 and SOC 2 reinforces trust and ensures regulatory adherence.
  description: This rule ensures that Dataflow jobs are configured to use private IP addresses. Verify that the 'network' setting in the Dataflow job configuration specifies a VPC with private Google access enabled. To remediate, update the Dataflow job settings to utilize a private VPC network, ensuring no public IPs are assigned. This enhances security by confining network traffic within the Google Cloud internal network.
  references:
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/dataflow/docs/guides/specifying-networks
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Section 7.8
  - ISO/IEC 27001:2013
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.dataflow.job.data_pipeline_type_set
  service: dataflow
  resource: job
  requirement: Data Pipeline Type Set
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Type is Correctly Configured
  rationale: Misconfiguration of Dataflow pipeline types can lead to ineffective data processing and potential data exposure. By ensuring the correct pipeline type, organizations reduce the risk of processing sensitive data in an unsecured manner, which is crucial for maintaining data privacy and meeting compliance requirements such as GDPR and CCPA.
  description: This rule checks that Dataflow jobs have the appropriate pipeline type set, ensuring data is processed securely and efficiently. Verify that the pipeline type is set to 'streaming' or 'batch' based on the intended use case and data sensitivity. Remediation involves reviewing the Dataflow job configurations and updating the pipeline type to match the data processing requirements, ensuring encryption at rest is enabled for all data processed by the pipeline.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/dataflow/docs/resources/faq
- rule_id: gcp.dataflow.job.data_pipeline_value_from_secrets_manager_when_sensitive
  service: dataflow
  resource: job
  requirement: Data Pipeline Value From Secrets Manager When Sensitive
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Sensitive Data in Dataflow Uses Secrets Manager
  rationale: Using Secrets Manager for sensitive data in Dataflow jobs mitigates the risk of data leaks and unauthorized access. This practice aligns with data protection regulations such as GDPR and CCPA, reducing legal and financial risks. It also enhances security by ensuring that sensitive information is encrypted and managed securely, reducing exposure to threats like insider attacks and data breaches.
  description: This rule checks that any sensitive parameters used in Dataflow jobs are retrieved securely from Google Cloud Secrets Manager. Verify that configurations in Dataflow pipelines reference secrets stored in Secrets Manager rather than hardcoding sensitive values. To remediate, update Dataflow job configurations to use the Secret Manager API for sensitive data retrieval, ensuring data is encrypted at rest and access is controlled via IAM roles.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.dataproc.cluster.data_analytics_admin_access_least_privilege
  service: dataproc
  resource: cluster
  requirement: Data Analytics Admin Access Least Privilege
  scope: dataproc.cluster.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce Least Privilege for Dataproc Cluster Admin Access
  rationale: Ensuring least privilege in Dataproc clusters minimizes the risk of unauthorized data access and potential data breaches. Improperly assigned admin rights can lead to privilege escalation and unauthorized access to sensitive data, posing significant business and regulatory compliance risks, including violation of GDPR or HIPAA standards.
  description: This rule checks that the Data Analytics Admin role in Dataproc clusters is assigned only to users who absolutely need it. Verify that IAM policies on the cluster restrict admin access to essential personnel by reviewing IAM roles and permissions. If admin access is overly permissive, adjust the IAM policy to remove unnecessary permissions and apply the principle of least privilege by assigning only essential roles. Regular audits should be conducted to ensure compliance.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices
  - NIST SP 800-53, Rev. 4 - Access Control
- rule_id: gcp.dataproc.cluster.data_analytics_audit_logging_enabled
  service: dataproc
  resource: cluster
  requirement: Data Analytics Audit Logging Enabled
  scope: dataproc.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Data Analytics Audit Logging in Dataproc Clusters
  rationale: Enabling audit logging for Dataproc clusters is crucial for tracking data access and administrative actions, helping to detect unauthorized activities and support forensic investigations. This practice supports compliance with regulations such as GDPR and CCPA, which mandate detailed logging of data processing activities. Without adequate logging, organizations face increased risks of undetected data breaches and non-compliance penalties.
  description: This rule ensures that audit logging is enabled for Dataproc clusters, capturing details of data access and administrative operations. To verify, check the cluster's IAM policy to ensure the 'roles/logging.logWriter' role is attached to the service account running the cluster. Remediation involves updating the IAM policy to include this role, thereby enabling audit logs to be sent to Cloud Logging for review and alerting.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/logging
  - https://cloud.google.com/logging/docs/audit
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://cloud.google.com/security/compliance
  - 'NIST SP 800-53: AU-2, AU-3'
  - ISO/IEC 27001:2013 - A.12.4
- rule_id: gcp.dataproc.cluster.data_analytics_encryption_at_rest_cmek
  service: dataproc
  resource: cluster
  requirement: Data Analytics Encryption At Rest Cmek
  scope: dataproc.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Dataproc Clusters Use CMEK for Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encrypting data at rest in Dataproc clusters provides control over encryption keys, enhancing data protection and compliance with regulatory standards. Without CMEK, you risk unauthorized data access if Google-managed keys are compromised, which can lead to data breaches and non-compliance with frameworks like PCI-DSS and HIPAA.
  description: This rule checks if Dataproc clusters are configured to use Customer-Managed Encryption Keys (CMEK) for encrypting data at rest. To verify, inspect the Dataproc cluster settings in the Google Cloud Console or use the gcloud CLI to ensure CMEK is enabled. Remediation involves configuring CMEK by creating a Cloud KMS key and specifying it in the Dataproc cluster's encryption settings during cluster creation or updating existing clusters.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.dataproc.cluster.data_analytics_private_networking_enforced
  service: dataproc
  resource: cluster
  requirement: Data Analytics Private Networking Enforced
  scope: dataproc.cluster.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for Dataproc Clusters
  rationale: Enforcing private networking for Dataproc clusters mitigates the risk of exposing sensitive data and processing operations to public networks. This configuration helps prevent unauthorized access, reduces attack surface, and ensures compliance with regulations such as GDPR and HIPAA, which mandate secure data processing environments.
  description: This rule checks if Dataproc clusters are configured to use private IP addresses instead of public IPs, ensuring that communication occurs within a secure VPC. To verify, ensure the 'enablePrivateEndpoint' and 'enablePrivateNodes' fields are set to true in the cluster configuration. Remediation involves updating the cluster settings to disable public IPs and enabling private networking options.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-vpc-design
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr/
- rule_id: gcp.dataproc.cluster.data_analytics_subnet_group_private_subnets_only
  service: dataproc
  resource: cluster
  requirement: Data Analytics Subnet Group Private Subnets Only
  scope: dataproc.cluster.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce Private Subnets for Dataproc Cluster Networking
  rationale: Ensuring that Dataproc clusters operate within private subnets minimizes exposure to public internet threats, thereby reducing the risk of unauthorized access and data breaches. This control helps meet privacy and security requirements crucial for compliance with regulations such as PCI-DSS and GDPR, protecting sensitive data processed and analyzed by Dataproc clusters.
  description: This rule checks if Dataproc clusters are configured to use subnets that are restricted to private IP addresses only. To verify, ensure that the 'enablePrivateEndpoint' setting is true within the cluster's network configuration. This setting prevents the cluster's nodes from obtaining public IP addresses, thereby restricting inbound and outbound traffic to internal network paths only. Remediation involves updating the cluster configuration to enable private subnet usage, which can be accomplished through the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.dataproc.cluster.data_analytics_tls_min_1_2_enforced
  service: dataproc
  resource: cluster
  requirement: Data Analytics TLS Min 1 2 Enforced
  scope: dataproc.cluster.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce TLS 1.2 for Dataproc Cluster Data Analytics
  rationale: Enforcing a minimum of TLS 1.2 for Dataproc clusters ensures that data in transit is protected against interception and tampering. This is crucial for maintaining data integrity and confidentiality, especially when handling sensitive data. It reduces the risk of data breaches through exploitation of older, less secure protocols and aligns with compliance requirements like PCI-DSS and HIPAA, which mandate strong encryption standards.
  description: This rule verifies that all Dataproc clusters are configured to enforce a minimum of TLS 1.2 for data analytics, ensuring secure communication channels. To check compliance, verify the cluster's configuration in the Google Cloud Console or via the gcloud CLI, ensuring that the cluster's properties enforce TLS 1.2. Remediation involves updating cluster configurations to disable older protocols and enforce TLS 1.2 or higher, which can be done by modifying the cluster's security settings.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2_SA_Quick_Reference_Guide.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.dataproc.cluster.datalake_dev_endpoint_encrypted
  service: dataproc
  resource: cluster
  requirement: Datalake Dev Endpoint Encrypted
  scope: dataproc.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataproc Cluster Datalake Endpoint is Encrypted
  rationale: Encryption of data at rest is crucial for protecting sensitive information stored in Dataproc clusters. Without encryption, data is vulnerable to unauthorized access, which could lead to data breaches, financial loss, and non-compliance with regulations like GDPR and HIPAA. Encrypted endpoints help mitigate risks related to data exposure and enhance the overall security posture of your cloud environment.
  description: This rule checks whether Dataproc clusters have encryption enabled for Datalake endpoints. To ensure data protection, configure your clusters to use customer-managed encryption keys (CMEK) or Google-managed keys for all data at rest. Verify this setting via the Google Cloud Console under the 'Clusters' section by ensuring the 'Encryption' field is properly set. If not configured, update the cluster settings to enable encryption, thereby securing the data against unauthorized access.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.dataproc.cluster.datalake_dev_endpoint_no_public_access
  service: dataproc
  resource: cluster
  requirement: Datalake Dev Endpoint No Public Access
  scope: dataproc.cluster.public_access
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Prevent Public Access to Dataproc Dev Endpoints
  rationale: Publicly accessible Dataproc dev endpoints can lead to unauthorized data access and potential data breaches. This configuration exposes sensitive information to external threats, increasing the risk of exploitation and non-compliance with regulations such as GDPR and HIPAA. Ensuring private access helps protect business assets and maintains customer trust.
  description: This rule checks if Dataproc clusters in development environments are configured without public IP addresses. Public IPs should be replaced with private IPs to restrict access to trusted networks only. Verification involves reviewing the cluster's network configuration settings in the GCP Console or via the gcloud command-line tool. To remediate, update the cluster's network settings to use private IPs and configure firewall rules to limit access to specific IP ranges.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.dataproc.cluster.datalake_dev_endpoint_role_least_privilege
  service: dataproc
  resource: cluster
  requirement: Datalake Dev Endpoint Role Least Privilege
  scope: dataproc.cluster.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataproc Cluster Dev Endpoints Use Least Privilege Roles
  rationale: Assigning excessive permissions to Dataproc clusters can lead to unauthorized access and data exfiltration, compromising sensitive data and violating compliance frameworks such as PCI-DSS and HIPAA. By adhering to the principle of least privilege, organizations minimize potential attack vectors and reduce the risk of privilege escalation.
  description: This rule checks that Dataproc clusters used for development purposes in a data lake environment have the least privilege roles assigned. Verify that IAM roles associated with the clusters do not grant permissions beyond what is necessary for their specific tasks. Remediation involves auditing current IAM policies, removing superfluous permissions, and assigning roles that follow the principle of least privilege. Regularly review and update IAM policies to ensure compliance with security best practices.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security
- rule_id: gcp.dataproc.cluster.encrypted_with_cmks_disabled
  service: dataproc
  resource: cluster
  requirement: Encrypted With Cmks Disabled
  scope: dataproc.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataproc Clusters Use CMKs for Encryption
  rationale: Using Customer-Managed Encryption Keys (CMKs) for Dataproc clusters enhances data protection by giving organizations full control over the encryption keys, which can mitigate risks related to unauthorized data access and satisfy regulatory compliance requirements. Failure to use CMKs can expose sensitive data to unauthorized access and may lead to non-compliance with data protection laws such as GDPR and industry standards like PCI-DSS.
  description: This rule checks if Google Cloud Dataproc clusters are using Customer-Managed Encryption Keys (CMKs) for encryption at rest. To comply with this rule, configure the Dataproc clusters to use a CMK from Cloud Key Management Service (Cloud KMS) during cluster creation. Verifying compliance involves checking the cluster's encryption configuration in the Google Cloud Console or using the gcloud command-line tool. Remediation involves updating the cluster configuration to specify a CMK from Cloud KMS.
  references:
  - https://cloud.google.com/dataproc/docs/guides/encryption
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/kms/docs/key-rotation
- rule_id: gcp.dataproc.cluster.master_nodes_no_public_ip
  service: dataproc
  resource: cluster
  requirement: Master Nodes No Public Ip
  scope: dataproc.cluster.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Dataproc Master Nodes Don't Have Public IPs
  rationale: Public IPs on Dataproc master nodes expose critical infrastructure to the internet, increasing the risk of unauthorized access and potential data breaches. This can lead to significant business disruptions and non-compliance with regulatory mandates such as PCI-DSS and HIPAA, which require strict controls on network access.
  description: This rule checks that Google Cloud Dataproc clusters are configured such that master nodes do not have publicly accessible IP addresses. Ensuring only private IPs are used limits exposure to potential threats. Verify by checking the network configuration of Dataproc clusters to confirm that master nodes are only accessible within a VPC. Remediate by modifying the cluster's network settings to use private IPs, or by setting up firewall rules to restrict access.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/dataproc/docs/guides/dataproc-security
- rule_id: gcp.dataproc.cluster.pd_encryption_cmek_enabled
  service: dataproc
  resource: cluster
  requirement: Pd Encryption Cmek Enabled
  scope: dataproc.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure CMEK is Enabled for Dataproc Clusters
  rationale: Enabling Customer-Managed Encryption Keys (CMEK) for Dataproc clusters ensures that the encryption keys are managed by the organization, reducing the risk of unauthorized access and enhancing control over data encryption. This is crucial for complying with regulatory requirements such as GDPR and HIPAA, which mandate strict control over encryption keys to protect sensitive data.
  description: This rule checks if Dataproc clusters are configured with Customer-Managed Encryption Keys (CMEK) for persistent disk (PD) encryption. To verify, ensure that the 'gceClusterConfig.diskConfig.bootDiskKmsKeyName' field is specified in the cluster's configuration. If not enabled, configure CMEK by specifying a valid KMS key during the cluster creation process. This provides enhanced security by allowing organizations to manage encryption keys, ensuring data at rest is protected by keys under their control.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.dataproc.job.datalake_logs_and_metrics_enabled
  service: dataproc
  resource: job
  requirement: Datalake Logs And Metrics Enabled
  scope: dataproc.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging and Metrics for Dataproc Jobs
  rationale: Enabling logs and metrics for Dataproc jobs is crucial for monitoring and auditing data processing activities. It helps detect anomalies, troubleshoot issues, and maintain compliance with regulations like PCI-DSS and HIPAA, which require detailed logging of data access and processing activities. Without these logs, organizations risk undetected data breaches and non-compliance penalties.
  description: This rule checks if logging and metrics are enabled for Dataproc jobs. To verify compliance, ensure that Cloud Logging and Cloud Monitoring are configured for each Dataproc job. This involves enabling the 'Google Cloud's operations suite' in your project settings and ensuring that job submissions include logging configurations. If not enabled, update your Dataproc job configurations to include the necessary logging and monitoring settings for comprehensive audit trails and performance insights.
  references:
  - https://cloud.google.com/dataproc/docs/guides/logging
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud
  - https://cloud.google.com/dataproc/docs/concepts/configurations
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.dataproc.job.datalake_ml_transform_logs_enabled
  service: dataproc
  resource: job
  requirement: Datalake Ml Transform Logs Enabled
  scope: dataproc.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Dataproc Job Datalake ML Transform Logs
  rationale: Enabling logs for Datalake ML Transform jobs in Dataproc allows for auditability and traceability of data transformations, which is crucial for identifying unauthorized access or modifications. This enhances the security posture by providing visibility into operations, aiding in compliance with data protection regulations such as GDPR and HIPAA, and ensuring data integrity.
  description: This rule checks if logging is enabled for Datalake ML Transform jobs in Google Cloud Dataproc. To verify, ensure that the jobs have logging configurations set to capture logs in a designated storage bucket. Remediation involves configuring the Dataproc jobs to enable logging by setting the `driver-log-levels` parameter to an appropriate level and specifying a logging destination, such as Google Cloud Storage or Stackdriver Logging.
  references:
  - https://cloud.google.com/dataproc/docs/guides/logging
  - https://cloud.google.com/security/compliance/gdpr/
  - https://cloud.google.com/hipaa/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/architecture/best-practices-for-policy-compliance
- rule_id: gcp.dataproc.job.datalake_ml_transform_network_private_only
  service: dataproc
  resource: job
  requirement: Datalake Ml Transform Network Private Only
  scope: dataproc.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Dataproc Jobs Use Private Networks Only
  rationale: Using private networks for Dataproc jobs minimizes exposure to unauthorized access, reduces the risk of data breaches, and complies with regulatory requirements for data protection. By restricting Dataproc jobs to private networks, organizations can mitigate risks associated with public internet exposure, such as man-in-the-middle attacks and unauthorized data interception.
  description: This rule checks that Dataproc jobs are configured to run on private IP networks only. To verify, ensure that the Dataproc cluster's network configuration specifies a subnet that does not have 'Private Google Access' enabled. Remediation involves updating the cluster's network settings to restrict traffic to internal IP addresses, thus preventing public internet exposure.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc/docs/private-access-options
- rule_id: gcp.dataproc.job.datalake_ml_transform_role_least_privilege
  service: dataproc
  resource: job
  requirement: Datalake Ml Transform Role Least Privilege
  scope: dataproc.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Dataproc Datalake ML Transform Role
  rationale: Implementing least privilege access for Dataproc jobs minimizes the risk of unauthorized data access and potential data breaches. By restricting permissions to the minimum necessary, organizations reduce their exposure to threats, meet compliance mandates such as GDPR, and protect sensitive data from internal and external threats.
  description: This rule checks that Dataproc jobs using the Datalake ML Transform Role do not have excessive permissions beyond what is necessary for their function. Verify that IAM policies grant only essential permissions. Remediation involves auditing current role assignments, using predefined roles where possible, and customizing roles to remove unnecessary permissions.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS GCP Benchmark v1.3.0, Section 4.6
  - NIST SP 800-53 Rev. 5 AC-6 Least Privilege
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.dataproc.job.datalake_network_private_only
  service: dataproc
  resource: job
  requirement: Datalake Network Private Only
  scope: dataproc.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Dataproc Jobs Use Private Networks Only
  rationale: Restricting Dataproc jobs to private networks mitigates the risk of unauthorized access and data exfiltration by ensuring that jobs cannot be accessed from the public internet. This is crucial for maintaining the confidentiality and integrity of sensitive data processed in Dataproc, and helps organizations comply with regulatory standards such as GDPR and HIPAA by enforcing network access control.
  description: This rule verifies that Dataproc jobs are configured to use only private networks by checking the network settings of each job. To ensure compliance, configure the Dataproc cluster's network to be private and specify the same network for all jobs. Remediate by reviewing Dataproc network configurations and updating jobs to run on private IP-only subnets, thus preventing exposure to public networks.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/network-connectivity/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.dataproc.job.datalake_role_least_privilege
  service: dataproc
  resource: job
  requirement: Datalake Role Least Privilege
  scope: dataproc.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataproc Job Uses Least Privilege Role
  rationale: Applying the principle of least privilege minimizes the potential attack surface by ensuring that Dataproc jobs only have the permissions necessary to perform their tasks. This reduces the risk of accidental or malicious activity that can lead to data breaches or unauthorized access, aligning with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks if Dataproc jobs are assigned roles with the minimum necessary permissions in Google Cloud IAM. To verify, audit the roles assigned to Dataproc jobs for excessive permissions. Remediation involves reviewing and updating IAM roles to match the specific needs of the job while removing unnecessary permissions. This can be achieved by customizing predefined roles or creating custom roles tailored to job requirements.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.dataproc.job.datalake_script_location_private_and_encrypted
  service: dataproc
  resource: job
  requirement: Datalake Script Location Private And Encrypted
  scope: dataproc.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataproc Script Locations are Private and Encrypted
  rationale: Storing Dataproc job scripts in non-private or unencrypted locations exposes sensitive data and intellectual property to unauthorized access. This can lead to data breaches, financial loss, and non-compliance with regulatory standards such as PCI-DSS and HIPAA. Ensuring scripts are private and encrypted mitigates these risks by protecting data at rest and maintaining confidentiality.
  description: This rule checks that Dataproc job scripts are stored in Google Cloud Storage buckets configured with access controls that restrict public access and enforce encryption. Verify that the bucket's 'Public access prevention' setting is enabled and that 'Default encryption' uses a customer-managed encryption key (CMEK). Remediate by updating bucket permissions to remove public access and configure encryption using CMEK to enhance data protection.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security#encryption
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/controls/cis-google-cloud-computing-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption/
- rule_id: gcp.dataproc.job.datalake_trigger_event_sources_restricted
  service: dataproc
  resource: job
  requirement: Datalake Trigger Event Sources Restricted
  scope: dataproc.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Datalake Trigger Event Sources in Dataproc Jobs
  rationale: Restricting event sources for Dataproc jobs helps mitigate unauthorized data processing and access, reducing the risk of data leaks and ensuring compliance with data protection regulations such as GDPR and CCPA. By controlling event sources, organizations can prevent accidental or malicious triggers that could exploit sensitive data within their data lakes.
  description: This rule checks whether Dataproc jobs have restricted trigger event sources to prevent unauthorized or unintended job executions. Ensure that Dataproc jobs are configured to only accept events from trusted and verified sources, such as specific Cloud Pub/Sub topics or GCS buckets. Review and configure event sources in the Dataproc job settings to align with organizational security policies. Remediation involves updating access permissions and configurations to limit event triggers to authorized sources only.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/dataproc/docs/concepts/jobs/overview
- rule_id: gcp.dataproc.job.datalake_trigger_logs_enabled
  service: dataproc
  resource: job
  requirement: Datalake Trigger Logs Enabled
  scope: dataproc.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Dataproc Job Trigger Logs for Audit Monitoring
  rationale: Enabling logs for Dataproc job triggers is crucial for auditing and monitoring data access patterns, which helps identify unauthorized access attempts and misconfigurations. This enhances the security posture by ensuring traceability of data operations, essential for meeting compliance requirements such as PCI-DSS and SOC2.
  description: This rule checks that logging is enabled for all Dataproc jobs to capture trigger events. To verify, ensure that logging is activated in the Dataproc cluster configuration settings under 'Job History'. Remediation involves updating the cluster configuration to enable logging, thus providing a detailed audit trail for data processing activities.
  references:
  - https://cloud.google.com/dataproc/docs/guides/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.dataproc.job.datalake_trigger_targets_least_privilege
  service: dataproc
  resource: job
  requirement: Datalake Trigger Targets Least Privilege
  scope: dataproc.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataproc Jobs Use Least Privilege Principle for Datalake Access
  rationale: Applying the least privilege principle to Dataproc jobs minimizes the risk of unauthorized access to sensitive data within a datalake, reducing potential data breaches and ensuring compliance with data protection regulations such as GDPR and CCPA. Ensuring that jobs have only the permissions necessary to perform their tasks mitigates the risk of privilege escalation and unauthorized data manipulation or exfiltration.
  description: This rule checks that Dataproc jobs accessing a datalake are configured with the minimum required permissions. It involves verifying IAM roles associated with service accounts used by these jobs to ensure they do not exceed necessary access levels. Remediation includes auditing the roles assigned to these service accounts and adjusting permissions to align with the least privilege model. This can be done by reviewing and modifying IAM policy bindings for Dataproc job service accounts to restrict over-privileged permissions.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r3.pdf
  - https://cloud.google.com/security/overview/whitepaper
  - https://pages.nist.gov/800-63-3/
- rule_id: gcp.dataproc.workflow_template.datalake_cross_account_trusts_restricted
  service: dataproc
  resource: workflow_template
  requirement: Datalake Cross Account Trusts Restricted
  scope: dataproc.workflow_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Trusts in Dataproc Workflow Templates
  rationale: Restricting cross-account trusts in Dataproc workflow templates is critical to prevent unauthorized access and potential data breaches. Cross-account access can expose sensitive data and resources to entities outside your organization, increasing the risk of data exfiltration and non-compliance with regulations such as GDPR and CCPA. Properly managing these trusts ensures that only authorized accounts can access and operate on your data lakes, maintaining data integrity and privacy.
  description: This rule verifies that Dataproc workflow templates do not permit cross-account trusts unless explicitly required and securely managed. It checks the IAM policies associated with the workflow templates to ensure that no external accounts have been granted unnecessary permissions. To remediate, review and update IAM policies to limit access to internal accounts only, and establish cross-account roles with the least privilege principle if external access is necessary. Regular audits and monitoring should be conducted to maintain compliance.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.dataproc.workflow_template.datalake_execution_role_least_privilege
  service: dataproc
  resource: workflow_template
  requirement: Datalake Execution Role Least Privilege
  scope: dataproc.workflow_template.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Dataproc Execution Role
  rationale: Limiting privileges for Dataproc execution roles reduces the risk of unauthorized data access and potential data breaches. Over-privileged roles can lead to accidental or malicious operations affecting data integrity and confidentiality, posing significant business risks and non-compliance with standards like PCI-DSS and SOC2.
  description: This rule checks whether Dataproc workflow templates are utilizing roles with the least privilege necessary for execution. It ensures that roles are not over-privileged by auditing IAM policies associated with the workflow templates. Remediation involves reviewing the roles and permissions assigned, ensuring they align strictly with the operational requirements of the data lake tasks, and removing excess permissions.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
  - https://www.nist.gov/cyberframework
- rule_id: gcp.dataproc.workflow_template.datalake_kms_encryption_enabled
  service: dataproc
  resource: workflow_template
  requirement: Datalake KMS Encryption Enabled
  scope: dataproc.workflow_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Dataproc Workflow Templates
  rationale: Enabling KMS encryption for Dataproc workflow templates is crucial for protecting sensitive data processed in data lakes. Without encryption, data at rest is vulnerable to unauthorized access, potentially leading to data breaches and non-compliance with regulations such as GDPR and HIPAA. Encrypting data at rest helps mitigate risks associated with insider threats and external attacks.
  description: This rule checks if Google Cloud Key Management Service (KMS) encryption is enabled for Dataproc workflow templates, ensuring data at rest within the data lake is encrypted. To verify, check the 'encryptionConfig' field in the workflow template configuration. If not set, configure the workflow templates to use a KMS key by specifying it in the 'encryptionConfig' section. This configuration ensures that all data processed by the workflow templates is encrypted at rest using a customer-managed key.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/kms
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datastudio.report.data_analytics_export_controls_enabled
  service: datastudio
  resource: report
  requirement: Data Analytics Export Controls Enabled
  scope: datastudio.report.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Data Analytics Export Controls in Data Studio Reports
  rationale: Ensuring that Data Analytics Export Controls are enabled in Data Studio reports helps prevent unauthorized access and exfiltration of sensitive data. This is crucial for maintaining data privacy and complying with industry regulations such as GDPR and CCPA, which require strict controls over data handling and export. Failure to do so could lead to data breaches, reputational damage, and significant financial penalties.
  description: This rule checks whether the Data Analytics Export Controls are enabled in Google Data Studio reports. These controls restrict the ability to export data, helping to ensure that only authorized users have access to sensitive information. To verify, review the settings in each Data Studio report and enable export controls under the report settings. Remediation involves configuring these settings to limit data export capabilities to authorized personnel only.
  references:
  - https://cloud.google.com/datastudio/docs/security-and-permissions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
- rule_id: gcp.datastudio.report.data_analytics_public_embeds_disabled
  service: datastudio
  resource: report
  requirement: Data Analytics Public Embeds Disabled
  scope: datastudio.report.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Embeds for Data Studio Reports
  rationale: Public embeds of Data Studio reports can expose sensitive analytics data to unauthorized users, leading to potential data breaches and non-compliance with data protection regulations such as GDPR and CCPA. Disabling public embeds mitigates the risk of data exfiltration and helps ensure that only authorized personnel can access critical business insights.
  description: This rule verifies that public embeds are disabled for Data Studio reports to prevent unauthorized access. Administrators should ensure that the 'Public Embed' option is turned off in the sharing settings of each report. Remediation involves reviewing Data Studio report permissions and settings, and configuring the reports to restrict access to authenticated users only.
  references:
  - https://support.google.com/datastudio/answer/9053467?hl=en
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/standard/54534.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.datastudio.report.data_analytics_sharing_restricted_to_org
  service: datastudio
  resource: report
  requirement: Data Analytics Sharing Restricted To Org
  scope: datastudio.report.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Data Studio Report Sharing to Organization
  rationale: Limiting Data Studio report sharing to within the organization minimizes the risk of data leaks and unauthorized access. Exposing analytics data to external entities without proper controls could lead to business intelligence compromises and regulatory non-compliance, particularly concerning data privacy laws like GDPR and CCPA.
  description: This rule checks that Data Studio reports are only shared with users within the organization, preventing external sharing. Verify the sharing settings by accessing the Data Studio report permissions and ensure no external email addresses are granted access. To remediate, adjust the sharing settings to 'Anyone within your organization' and remove any external sharing permissions.
  references:
  - https://support.google.com/datastudio/answer/9053467?hl=en
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/privacy/
- rule_id: gcp.datastudio.report.data_analytics_sso_required
  service: datastudio
  resource: report
  requirement: Data Analytics Sso Required
  scope: datastudio.report.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Single Sign-On for Data Studio Reports
  rationale: Requiring Single Sign-On (SSO) for accessing Data Studio reports enhances security by centralizing authentication and reducing the risk of unauthorized access. It helps prevent data breaches by leveraging strong authentication mechanisms, and it simplifies user management, contributing to compliance with data protection regulations like GDPR and HIPAA.
  description: This check ensures that Single Sign-On (SSO) is enforced for accessing Google Data Studio reports. To verify, administrators should ensure that the Data Studio is integrated with an identity provider (IdP) supporting SSO, and all users are required to authenticate via this system. Remediation involves configuring the IdP settings in the GCP Console to enforce SSO across Data Studio. Ensure that the identity provider is securely set up with multi-factor authentication (MFA) for added security.
  references:
  - https://cloud.google.com/solutions/centralized-identity-access-management
  - https://cloud.google.com/datastudio/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.dlp.inspect_template.data_governance_classification_auto_classification_supported
  service: dlp
  resource: inspect_template
  requirement: Data Governance Classification Auto Classification Supported
  scope: dlp.inspect_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Auto Classification in DLP Inspect Templates
  rationale: Automatic data classification enhances data governance by ensuring sensitive information is consistently identified and protected, reducing the risk of data breaches and aiding in regulatory compliance such as GDPR and CCPA. Failure to utilize automated classification can result in unintentional exposure of sensitive data, financial penalties, and reputational damage.
  description: This rule checks if Google Cloud DLP inspect templates are configured to support automatic data classification, which classifies data using predefined or custom info types. To verify, ensure the inspect template has auto classification enabled. Remediation involves updating the template configuration to include auto classification settings, aligning with your organizational data protection policies.
  references:
  - https://cloud.google.com/dlp/docs/creating-templates
  - CIS GCP Benchmark v1.3.0, Section 10.2
  - https://www.nist.gov/privacy-framework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.dlp.inspect_template.data_governance_classification_policy_blocks_publi_sensitive
  service: dlp
  resource: inspect_template
  requirement: Data Governance Classification Policy Blocks Publi Sensitive
  scope: dlp.inspect_template.policy_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DLP Templates Enforce Publi Sensitive Data Classification
  rationale: Blocking the exposure of sensitive data classified under data governance policies is crucial to avoid data breaches, potential financial losses, and compliance violations. This helps organizations maintain trust with customers by ensuring sensitive data is protected and adheres to privacy regulations such as GDPR and CCPA. Mismanagement of such data can lead to severe penalties and reputational damage.
  description: This rule checks whether Data Loss Prevention (DLP) inspect templates in GCP are configured to enforce data governance classification policies for public sensitive data. It verifies that inspect templates block or redact sensitive data, mitigating unauthorized access risks. To remediate, update the inspect templates to include classification rules aligned with your data governance policies, ensuring sensitive information is not inadvertently exposed.
  references:
  - https://cloud.google.com/dlp/docs/creating-inspect-templates
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.dlp.inspect_template.data_governance_classification_required_sensitivity__present
  service: dlp
  resource: inspect_template
  requirement: Data Governance Classification Required Sensitivity Present
  scope: dlp.inspect_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DLP Inspector Templates Classify Sensitive Data
  rationale: Proper classification of sensitive data is crucial for maintaining data privacy and compliance with regulations such as GDPR and HIPAA. Without appropriate sensitivity levels in DLP inspector templates, sensitive data might be inadequately protected, leading to potential data breaches and financial penalties. Ensuring data governance classifications are present helps mitigate the risk of unauthorized data access and misuse.
  description: This rule checks that all DLP inspector templates in use have defined data governance classifications with the required sensitivity levels. Verify that each template includes sensitivity annotations to classify data types such as PII or financial information. To remediate, review and update DLP inspector templates to include accurate sensitivity labels, ensuring they align with your organization's data protection policies and regulatory requirements.
  references:
  - https://cloud.google.com/dlp/docs/creating-templates
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.dlp.job.data_governance_compliance_access_rbac_least_privilege
  service: dlp
  resource: job
  requirement: Data Governance Compliance Access RBAC Least Privilege
  scope: dlp.job.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure DLP Job RBAC Follows Least Privilege Principle
  rationale: Implementing least privilege access for DLP jobs is critical to minimize unauthorized data exposure and potential data breaches. Inadequate access controls can lead to misuse of sensitive information, regulatory non-compliance, and potential financial and reputational damage. Least privilege access ensures that users and services have only the permissions necessary for their functions, reducing the attack surface.
  description: This rule checks if DLP jobs in GCP adhere to the principle of least privilege by ensuring that Role-Based Access Control (RBAC) configurations are properly set. Specifically, it verifies that only necessary permissions are assigned to users and service accounts interacting with DLP jobs. To remediate, review IAM policies and restrict permissions to only those required for job execution. Use the Google Cloud Console or gcloud CLI to audit and adjust IAM roles, ensuring compliance with data governance policies.
  references:
  - https://cloud.google.com/dlp/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 1.1
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dlp.job.data_governance_compliance_export_destinations_private
  service: dlp
  resource: job
  requirement: Data Governance Compliance Export Destinations Private
  scope: dlp.job.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure DLP Job Export Destinations are Private
  rationale: Ensuring that Data Loss Prevention (DLP) job export destinations are private mitigates the risk of unauthorized data access and leakage, protecting sensitive information from exposure. This practice supports compliance with data protection regulations such as GDPR and CCPA, preventing potential breaches and financial penalties.
  description: This rule verifies that DLP job export destinations are configured to use private networking, ensuring data is not exposed to public internet access. Specifically, it checks if the export destinations, such as Cloud Storage buckets, are set with private IP addresses or reside within a VPC. To remediate, configure the export destination to use private IPs or set appropriate network policies to restrict public access. Validate through the GCP Console by checking network settings associated with the DLP job.
  references:
  - https://cloud.google.com/dlp/docs/concepts-job-triggers
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.dlp.job.data_governance_compliance_reports_storage_encrypted
  service: dlp
  resource: job
  requirement: Data Governance Compliance Reports Storage Encrypted
  scope: dlp.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DLP Job Reports are Encrypted at Rest
  rationale: Encrypting data at rest for DLP job reports is crucial to protect sensitive information from unauthorized access and potential data breaches. This practice supports compliance with data protection regulations such as GDPR and CCPA, mitigating risks of financial penalties and reputational damage. Encryption at rest also helps safeguard against insider threats and physical theft of storage media.
  description: This rule checks whether Data Loss Prevention (DLP) job reports are encrypted when stored in Google Cloud Storage. To verify compliance, ensure that Cloud Storage buckets used for storing DLP job outputs have default encryption enabled, either using Google-managed keys or customer-managed keys (CMEK). If encryption is not enabled, configure the bucket settings to apply encryption for all stored objects. This can be done through the GCP Console, gcloud CLI, or Cloud Storage JSON API.
  references:
  - https://cloud.google.com/dlp/docs
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-111.pdf
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.dlp.job.privacy_anonymization_jobs_private_networking
  service: dlp
  resource: job
  requirement: Privacy Anonymization Jobs Private Networking
  scope: dlp.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure DLP Anonymization Jobs Use Private Networking
  rationale: Configuring DLP anonymization jobs to use private networking minimizes exposure to public internet threats, reducing the risk of data breaches and unauthorized access. It supports compliance with data protection regulations like GDPR and HIPAA by ensuring sensitive data is processed in a secure environment. This setup enhances the security posture by restricting network access to authorized entities only.
  description: This rule checks that Google Cloud Data Loss Prevention (DLP) anonymization jobs are configured to use private networking, such as VPC Service Controls or Private Google Access. Verify that the DLP jobs have network policies enforcing private IP usage to ensure that data does not transit over the public internet. To remediate, configure VPC Service Controls and ensure that all DLP jobs are set to use private IPs, limiting network exposure and enhancing security.
  references:
  - https://cloud.google.com/dlp/docs/concepts-job-triggers
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dlp.job.privacy_anonymization_outputs_encrypted
  service: dlp
  resource: job
  requirement: Privacy Anonymization Outputs Encrypted
  scope: dlp.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DLP Job Outputs are Encrypted for Privacy
  rationale: Encrypting outputs of Data Loss Prevention (DLP) jobs is crucial to protect sensitive information from unauthorized access, ensuring compliance with data protection regulations such as GDPR and CCPA. Anonymization outputs, if left unencrypted, can be vulnerable to breaches, potentially leading to data leaks and significant financial and reputational damage.
  description: This rule verifies that all privacy anonymization outputs from DLP jobs within Google Cloud are encrypted. To ensure compliance, configure DLP jobs to use Cloud KMS for encryption. This can be checked through the DLP settings in the GCP Console or by using gcloud commands. If outputs are not encrypted, adjust the DLP job configuration to specify an appropriate encryption key.
  references:
  - https://cloud.google.com/dlp/docs/concepts-crypto
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/kms/docs
- rule_id: gcp.dlp.job.privacy_masking_execution_roles_least_privilege
  service: dlp
  resource: job
  requirement: Privacy Masking Execution Roles Least Privilege
  scope: dlp.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for DLP Privacy Masking Roles
  rationale: Implementing least privilege for DLP privacy masking roles is crucial to minimize the risk of unauthorized data access and potential data breaches. Over-permissioned roles can lead to exposure of sensitive information, which can result in regulatory non-compliance and damage to the organization's reputation. Ensuring that roles have only the necessary permissions helps mitigate insider threats and meets compliance requirements such as GDPR and HIPAA.
  description: This rule checks for adherence to the principle of least privilege in assigning roles for DLP privacy masking jobs. Ensure that only necessary permissions are granted to roles involved in executing DLP jobs by reviewing IAM policies and removing any excessive permissions. To verify compliance, audit current IAM roles and permissions associated with DLP jobs, and adjust them to meet the minimum required for task execution. Remediate by using predefined roles with the least privilege or custom roles tailored to specific job needs.
  references:
  - https://cloud.google.com/dlp/docs/quickstart
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.dlp.job.privacy_masking_policies_present_for_sensitive_fields
  service: dlp
  resource: job
  requirement: Privacy Masking Policies Present For Sensitive Fields
  scope: dlp.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Privacy Masking for Sensitive Data in DLP Jobs
  rationale: Implementing privacy masking policies for sensitive fields in DLP jobs is crucial to protect confidential information from unauthorized access and data breaches. Without proper masking, sensitive data may be exposed during processing, leading to potential legal liabilities and non-compliance with frameworks like GDPR, HIPAA, and PCI-DSS. Effective masking reduces the risk of data exposure, thereby safeguarding organizational reputation and customer trust.
  description: This rule checks whether privacy masking policies are implemented for sensitive fields in DLP jobs. It ensures that any sensitive data identified during a DLP job is appropriately masked according to predefined policies. To verify, review the DLP job configurations within the GCP Console for presence of masking configurations on sensitive fields. Remediation involves defining and applying masking policies for all identified sensitive fields using GCP's Data Loss Prevention API or DLP templates.
  references:
  - https://cloud.google.com/dlp/docs/creating-job-triggers
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark
  - https://cloud.google.com/dlp/docs/masking-sensitive-data
  - https://cloud.google.com/security/overview
  - NIST SP 800-53, SC-28 Protection of Information at Rest
- rule_id: gcp.dns.managed_zone.dnssec_disabled
  service: dns
  resource: managed_zone
  requirement: Dnssec Disabled
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNSSEC is Enabled for Managed DNS Zones
  rationale: DNS Security Extensions (DNSSEC) protect the integrity and authenticity of DNS responses, safeguarding against attacks such as cache poisoning and spoofing. Disabling DNSSEC in managed zones can expose an organization to potential unauthorized redirections and data breaches, impacting business operations and violating compliance standards such as ISO 27001 and SOC2.
  description: This rule checks if DNSSEC is disabled for Google Cloud DNS managed zones. DNSSEC should be enabled to provide an additional layer of security by digitally signing DNS responses. To verify, check the Cloud DNS settings for each managed zone and ensure DNSSEC is enabled. Remediation involves navigating to the Cloud DNS service in the GCP Console, selecting the managed zone, and enabling DNSSEC under the 'DNSSEC' section.
  references:
  - https://cloud.google.com/dns/docs/dnssec-config
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.cloudflare.com/dns/dnssec/how-to/
- rule_id: gcp.dns.managed_zone.rsasha1_in_use_to_zone_sign_in_dnssec_configured
  service: dns
  resource: managed_zone
  requirement: Rsasha1 In Use To Zone Sign In Dnssec Configured
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Use Stronger DNSSEC Signing Algorithm in Managed Zones
  rationale: Using RSASHA1 for DNSSEC signing poses security risks due to its known vulnerabilities and susceptibility to cryptographic attacks. This can lead to DNS spoofing, data integrity issues, and potential compliance violations with security standards that require the use of stronger cryptographic methods. Transitioning to more secure algorithms like RSASHA256 or RSASHA512 helps mitigate these threats and aligns with industry best practices.
  description: This rule checks if RSASHA1 is being used for DNSSEC signing in GCP managed zones. RSASHA1 is considered weak and should be replaced with stronger algorithms such as RSASHA256 or RSASHA512. To verify, inspect the DNSSEC configuration in the Cloud Console or via gcloud commands to ensure that a more secure algorithm is used. Remediation involves updating the DNSSEC settings in the Google Cloud Console or using the gcloud dns managed-zones update command to configure a stronger signing algorithm.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dns.managed_zone.sign_in_dnssec
  service: dns
  resource: managed_zone
  requirement: Sign In Dnssec
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable DNSSEC on DNS Managed Zones
  rationale: Enabling DNSSEC for DNS managed zones ensures the authenticity and integrity of DNS data, protecting against attacks such as DNS spoofing and cache poisoning. This enhances trust in DNS responses, which is critical for preventing unauthorized redirections that could lead to data breaches or service disruptions. Compliance with regulatory frameworks like NIST and PCI-DSS often mandates the use of DNSSEC for secure DNS operations.
  description: This rule checks if DNS Security Extensions (DNSSEC) are enabled on Google Cloud DNS managed zones. DNSSEC provides cryptographic assurance for DNS data, preventing tampering and ensuring that responses are from the authentic source. To verify, navigate to the Cloud DNS section in the GCP Console, select the managed zone, and ensure DNSSEC status is set to 'On'. To enable, click 'Edit' on the managed zone and toggle DNSSEC to 'On'.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-81r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.ietf.org/rfc/rfc4033.txt
- rule_id: gcp.dns.managed_zone.zone_cross_account_sharing_restricted
  service: dns
  resource: managed_zone
  requirement: Zone Cross Account Sharing Restricted
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Cross-Account Sharing in DNS Managed Zones
  rationale: Restricting cross-account sharing of DNS managed zones is crucial to prevent unauthorized access to DNS configurations, which could lead to domain hijacking or malicious redirection of traffic. Unauthorized access can expose sensitive data, disrupt services, and compromise compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks for DNS managed zones configured with cross-account sharing, which should be restricted to ensure only authorized accounts have access. Verify the IAM policy bindings of managed zones, ensuring that only trusted accounts and users have roles granting DNS management permissions. Remediation involves reviewing and modifying IAM policies to limit access to necessary accounts only, using least privilege principles.
  references:
  - https://cloud.google.com/dns/docs/security
  - https://cloud.google.com/docs/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dns.managed_zone.zone_dnssec_enabled
  service: dns
  resource: managed_zone
  requirement: Zone Dnssec Enabled
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNSSEC is Enabled for DNS Managed Zones
  rationale: Enabling DNSSEC on managed zones helps protect against DNS spoofing and cache poisoning attacks by providing origin authentication of DNS data. This enhances the integrity and authenticity of DNS responses, reducing the risk of malicious redirection and data exfiltration. Compliance with security standards like NIST and PCI-DSS often requires DNSSEC for secure DNS operations.
  description: This rule checks whether DNSSEC is enabled on Google Cloud DNS managed zones. DNSSEC provides a layer of security by digitally signing DNS records, ensuring their integrity and authenticity. To enable DNSSEC, access the Cloud DNS service in the GCP Console and configure the managed zone settings to enable DNSSEC. Verifying DNSSEC status can be done via the GCP Console or the gcloud command-line tool. Remediation involves enabling DNSSEC and ensuring the proper setup of DS (Delegation Signer) records in your domain registrar.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/csrc/media/publications/sp/800-81-2/archive/2010-09-01/documents/sp800-81-2.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://support.google.com/domains/answer/6387176?hl=en
- rule_id: gcp.dns.managed_zone.zone_dnssec_enabled_where_supported
  service: dns
  resource: managed_zone
  requirement: Zone Dnssec Enabled Where Supported
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable DNSSEC for Supported Managed Zones
  rationale: Enabling DNS Security Extensions (DNSSEC) in supported managed zones helps protect against certain types of attacks like DNS spoofing and cache poisoning, which can redirect users to malicious sites. This is crucial for maintaining data integrity and trust in DNS responses, reducing the risk of data breaches and ensuring regulatory compliance with frameworks that mandate secure DNS configurations.
  description: This rule checks whether DNSSEC is enabled for all managed zones where it is supported. DNSSEC provides an additional layer of security by allowing DNS responses to be verified for authenticity and integrity. To verify, ensure that the DNSSEC configuration is set to 'on' in the Google Cloud Console for each applicable managed zone. If not enabled, navigate to 'Cloud DNS' in the Google Cloud Console, select the managed zone, and activate DNSSEC to enhance domain security.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dns.managed_zone.zone_dnssec_no_rsa_sha1_key
  service: dns
  resource: managed_zone
  requirement: Zone Dnssec No Rsa Sha1 Key
  scope: dns.managed_zone.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Avoid RSA SHA-1 Keys in DNSSEC for Managed Zones
  rationale: Using RSA SHA-1 keys in DNSSEC poses a significant security risk due to their weak cryptographic strength, making DNS zones vulnerable to spoofing and data integrity attacks. This can lead to unauthorized access, data corruption, and non-compliance with security standards such as NIST and PCI-DSS, potentially resulting in reputational damage and legal consequences.
  description: This rule checks that DNSSEC configurations for GCP managed zones do not use RSA SHA-1 keys, which are considered obsolete and insecure. Verify that the key-signing and zone-signing algorithms in your DNSSEC settings utilize stronger cryptographic algorithms like RSA SHA-256 or ECDSA. Remediation involves updating the DNSSEC settings in the Cloud DNS console or via gcloud CLI to replace RSA SHA-1 with secure alternatives.
  references:
  - https://cloud.google.com/dns/docs/dnssec-config
  - https://cloud.google.com/security/compliance/cis#gcp_cis_v1.2_1.4
  - https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf
  - https://www.pcisecuritystandards.org/document_library?document=pci_dss_v3-2-1
  - https://tools.ietf.org/html/rfc6781
- rule_id: gcp.dns.managed_zone.zone_dnssec_zone_signing_algorithm_not_rsa_sha1
  service: dns
  resource: managed_zone
  requirement: Zone Dnssec Zone Signing Algorithm Not Rsa Sha1
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Avoid RSA-SHA1 for DNSSEC Zone Signing Algorithm
  rationale: Using RSA-SHA1 for DNSSEC zone signing is risky due to its vulnerability to collision attacks, which can compromise the integrity and authenticity of DNS records. This can lead to DNS spoofing, where users are redirected to malicious sites, posing significant business risks and potential compliance violations, especially under frameworks like PCI-DSS and NIST.
  description: This rule checks that the DNSSEC zone signing algorithm for GCP DNS managed zones is not set to RSA-SHA1, which is considered weak. Instead, use stronger algorithms like RSA-SHA256 or ECDSA to ensure robust cryptographic integrity. To verify, inspect the DNS managed zones in the GCP Console or via gcloud CLI, and update the DNSSEC settings to use a more secure algorithm if necessary.
  references:
  - https://cloud.google.com/dns/docs/dnssec-config
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://owasp.org/www-community/attacks/DNS_Spoofing
- rule_id: gcp.dns.managed_zone.zone_query_logging_enabled
  service: dns
  resource: managed_zone
  requirement: Zone Query Logging Enabled
  scope: dns.managed_zone.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DNS Managed Zone Query Logging is Enabled
  rationale: Enabling query logging for DNS managed zones is crucial for maintaining visibility into DNS queries, which can help detect malicious activities such as data exfiltration, unauthorized access, or misconfigurations. Query logs provide audit trails that are essential for incident response and forensic investigations, and they support compliance with regulatory standards that mandate monitoring of network activities.
  description: This rule checks if DNS managed zones have query logging enabled. Query logging provides detailed records of all DNS queries, which are instrumental in analyzing traffic patterns and identifying security threats. To enable query logging, ensure that the 'logging' configuration is set to 'true' for the DNS managed zone. This can be verified and configured through the Google Cloud Console under 'Cloud DNS' or by using the 'gcloud dns managed-zones update' command.
  references:
  - https://cloud.google.com/dns/docs/query-logging
  - https://cloud.google.com/docs/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library
- rule_id: gcp.dns.managed_zone.zone_zone_transfer_restricted_or_tsig_required
  service: dns
  resource: managed_zone
  requirement: Zone Zone Transfer Restricted Or Tsig Required
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNS Zone Transfers are Securely Configured
  rationale: Restricting DNS zone transfers is crucial as it prevents unauthorized replication of DNS data, which could lead to data exfiltration, DNS hijacking, or other malicious activities. Ensuring zone transfers are restricted or require TSIG (Transaction Signature) helps maintain data integrity and confidentiality, aligning with compliance mandates such as PCI-DSS and ISO 27001.
  description: This rule checks that DNS managed zones have zone transfers either restricted to specific authorized IP addresses or secured using TSIG. To verify, review the DNS managed zone settings in GCP to ensure 'allowTransfer' is set to authorized IPs or TSIG keys. Remediation involves configuring these settings via the GCP Console or Terraform, ensuring that only trusted DNS servers can perform zone transfers.
  references:
  - https://cloud.google.com/dns/docs/zones
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/dns/docs/tsig
- rule_id: gcp.dns.policy.logging_enabled
  service: dns
  resource: policy
  requirement: Logging Enabled
  scope: dns.policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DNS Policy Logging is Enabled
  rationale: Enabling logging for DNS policies is crucial for monitoring and auditing DNS query activities, helping to detect and respond to potential security incidents. Without logging, identifying malicious queries, unauthorized changes, or misconfigurations becomes challenging, increasing the risk of data breaches and non-compliance with regulatory standards.
  description: This rule checks if logging is enabled for DNS policies within your GCP environment. Logging should be configured to capture detailed DNS query logs, which can be verified through the Cloud Console or via the gcloud command-line tool. To enable logging, navigate to the Cloud DNS Policies section, edit the desired policy, and ensure the 'logConfig' is set to capture logs. This aids in maintaining a comprehensive audit trail and supports incident response activities.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://cloud.google.com/dns/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dns.policy.traffic_health_checks_required_for_failover
  service: dns
  resource: policy
  requirement: Traffic Health Checks Required For Failover
  scope: dns.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure DNS Policies Require Traffic Health Checks for Failover
  rationale: Health checks are crucial for ensuring that DNS failover mechanisms work effectively, preventing potential downtime and service disruption. Without health checks, DNS failover might not detect a service outage promptly, leading to business continuity risks and potential financial losses. Compliance with standards such as ISO 27001 requires robust failover mechanisms to protect against service interruptions.
  description: This rule checks if Google Cloud DNS policies are configured to require traffic health checks for failover scenarios. Health checks should be correctly set up to monitor the availability and performance of services, ensuring that DNS policies can automatically redirect traffic in the event of a failure. To verify, review the DNS policy settings in the Google Cloud Console under Cloud DNS -> Policies and ensure health checks are configured. Remediation involves setting up and associating appropriate health checks with DNS policies to enable automatic failover.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://cloud.google.com/architecture/best-practices-for-using-cloud-dns
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dns.policy.traffic_policy_documents_encrypted_and_private
  service: dns
  resource: policy
  requirement: Traffic Policy Documents Encrypted And Private
  scope: dns.policy.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DNS Traffic Policy Documents Are Encrypted and Private
  rationale: Encrypting DNS traffic policy documents is critical to protect sensitive configuration data from unauthorized access and breaches. Unencrypted data can be intercepted or tampered with, leading to potential compliance violations with regulations such as GDPR, HIPAA, or PCI-DSS. Ensuring privacy and encryption of these documents mitigates risks of data leaks and supports business reputation by safeguarding customer trust.
  description: This rule checks that all DNS traffic policy documents are encrypted at rest using Google Cloud KMS and that access to these documents is restricted through IAM policies. To verify, ensure that the DNS policies have encryption enabled by default and review IAM roles to confirm that only authorized personnel have access. Remediation involves applying customer-managed encryption keys (CMEK) to your DNS policies and auditing IAM permissions regularly to maintain least privilege access.
  references:
  - https://cloud.google.com/dns/docs/zones
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/kms/docs/overview
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/privacy/
- rule_id: gcp.dns.policy.traffic_targets_approved_domains_only
  service: dns
  resource: policy
  requirement: Traffic Targets Approved Domains Only
  scope: dns.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict DNS Traffic to Approved Domains Only
  rationale: Restricting DNS traffic to approved domains minimizes exposure to malicious sites, reducing the risk of data breaches and phishing attacks. This control is critical for maintaining data integrity and confidentiality, and is often required by compliance standards like PCI-DSS and ISO 27001.
  description: This rule checks if DNS policies are configured to allow traffic only to domains explicitly approved by the organization. To verify, review the DNS policy settings in the Google Cloud Console to ensure that only specified domains are allowed. Remediate by updating the DNS policy to include only trusted domain names and regularly review and update this list to adapt to changing business needs.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dns.resource_record_set.record_set_alias_targets_in_approved_services
  service: dns
  resource: resource_record_set
  requirement: Record Set Alias Targets In Approved Services
  scope: dns.resource_record_set.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNS Alias Targets Are Approved Services
  rationale: Using unapproved services for DNS alias targets can expose your network to security risks, such as data breaches and unauthorized access, potentially violating regulatory requirements like PCI-DSS and HIPAA. Ensuring alias targets are restricted to approved services helps maintain control over data flow and mitigates the risk of routing traffic to compromised or malicious endpoints.
  description: This rule checks that all DNS resource record sets within your Google Cloud account use alias targets that are part of an approved list of services. To verify, inspect the DNS configurations in Cloud DNS and ensure alias targets match approved services. Remediation involves updating any non-compliant alias targets to align with the organization's approved services list, preventing unauthorized or potentially malicious traffic redirection.
  references:
  - https://cloud.google.com/dns/docs/
  - https://cloud.google.com/security/compliance/cis#section_6.4.1
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/dns/docs/security
- rule_id: gcp.dns.resource_record_set.record_set_caa_records_present_for_root_and_wildcard
  service: dns
  resource: resource_record_set
  requirement: Record Set Caa Records Present For Root And Wildcard
  scope: dns.resource_record_set.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: critical
  title: Ensure CAA Records Exist for Root and Wildcard DNS Entries
  rationale: CAA (Certification Authority Authorization) records control which certificate authorities (CAs) can issue SSL/TLS certificates for a domain, preventing unauthorized certificates that could lead to man-in-the-middle attacks. Ensuring these records exist for both root and wildcard entries helps maintain domain integrity and supports compliance with security standards like PCI-DSS and ISO 27001.
  description: This rule checks for the presence of CAA records in DNS configurations for both root and wildcard entries. A CAA record specifies which CAs are permitted to issue certificates for a domain, reducing the risk of certificate misuse. To remediate, add CAA records in your DNS settings, specifying authorized CAs for each domain and subdomain. Verify using the 'gcloud dns record-sets list' command to ensure correct configuration.
  references:
  - https://cloud.google.com/dns/docs/records/caa-records
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://tools.ietf.org/html/rfc6844
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dns.resource_record_set.record_set_dmarc_record_present_when_mx_present
  service: dns
  resource: resource_record_set
  requirement: Record Set Dmarc Record Present When Mx Present
  scope: dns.resource_record_set.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DMARC Records Exist for Domains with MX Records
  rationale: Ensuring DMARC records are present for domains with MX records helps protect against email spoofing and phishing attacks by specifying policies for handling unauthenticated emails. This is crucial for maintaining brand reputation and adhering to email security best practices, reducing the risk of business email compromise and potential data breaches.
  description: This rule checks that a DMARC (Domain-based Message Authentication, Reporting & Conformance) record is configured for any domain that has an MX (Mail Exchange) record set in Google Cloud DNS. Without a DMARC record, domains are vulnerable to email spoofing. To verify, check for the presence of a TXT record starting with 'v=DMARC1' in the DNS settings. To remediate, add a DMARC record specifying the desired policy (e.g., 'p=none', 'p=quarantine', or 'p=reject') to guide email receivers on handling messages that fail authentication checks.
  references:
  - https://cloud.google.com/dns/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://tools.ietf.org/html/rfc7489
  - https://www.nist.gov/publications/email-security-standards-guidelines
  - https://dmarc.org/resources/specification/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dns.resource_record_set.record_set_no_overly_broad_wildcard_records_in_sensiti_zones
  service: dns
  resource: resource_record_set
  requirement: Record Set No Overly Broad Wildcard Records In Sensiti Zones
  scope: dns.resource_record_set.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Avoid Broad Wildcard DNS Records in Sensitive Zones
  rationale: Overly broad wildcard DNS records can lead to security risks such as domain hijacking and phishing attacks by resolving subdomains that should not exist. This can expose sensitive data and services unintentionally, impacting business integrity and compliance with frameworks like ISO 27001 and SOC2.
  description: This rule checks for the presence of wildcard DNS records in sensitive zones within GCP Cloud DNS. Wildcard records that are too broad can unintentionally expose subdomains, leading to potential security vulnerabilities. To verify, examine DNS records and ensure wildcards are appropriately scoped. Remediation involves restricting wildcard usage to only necessary subdomains, or removing it entirely in sensitive environments.
  references:
  - https://cloud.google.com/dns/docs/zones
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dns.resource_record_set.record_set_no_rfc1918_in_public_zones
  service: dns
  resource: resource_record_set
  requirement: Record Set No Rfc1918 In Public Zones
  scope: dns.resource_record_set.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent RFC1918 Addresses in Public DNS Zones
  rationale: Exposing private IP addresses such as RFC1918 addresses in public DNS zones can lead to unauthorized network exposure, allowing external entities to infer internal network structures. This can increase the risk of targeted attacks and may violate compliance with standards like PCI-DSS and ISO 27001, which emphasize minimizing exposure of internal network details.
  description: This rule checks for the presence of RFC1918 private IP addresses in public DNS zones within GCP. RFC1918 addresses should be restricted to internal use and not published in publicly accessible DNS records. To verify, audit DNS records in public zones for compliance with this rule. Remediation involves removing or modifying any DNS records that expose RFC1918 addresses to ensure they are not publicly resolvable.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://tools.ietf.org/html/rfc1918
- rule_id: gcp.elasticsearch.cluster.domains_access_control_enabled
  service: elasticsearch
  resource: cluster
  requirement: Domains Access Control Enabled
  scope: elasticsearch.cluster.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Elasticsearch Cluster Domains Access Control Enabled
  rationale: Enabling domains access control on Elasticsearch clusters reduces the risk of unauthorized access by enforcing authentication and authorization policies. This is crucial in preventing data breaches and ensuring compliance with regulatory standards such as GDPR and HIPAA, which require robust access controls to protect sensitive information.
  description: This rule checks if Elasticsearch clusters have domains access control enabled, which ensures that only authorized users and applications can access the cluster. To verify, review the cluster's access control settings in the GCP console under Elasticsearch settings. Remediation involves configuring appropriate IAM roles and policies to enforce domain-specific access restrictions, thereby enhancing the security posture of your Elasticsearch deployment.
  references:
  - https://cloud.google.com/elasticsearch/docs/security-controls
  - CIS GCP Benchmark 1.1.0 - Section 4.3
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/overview
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.elasticsearch.cluster.domains_encryption_at_rest_enabled
  service: elasticsearch
  resource: cluster
  requirement: Domains Encryption At Rest Enabled
  scope: elasticsearch.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Elasticsearch Clusters Have Encryption at Rest Enabled
  rationale: Enabling encryption at rest for Elasticsearch clusters on GCP is critical to protect sensitive data from unauthorized access. Without encryption, data is vulnerable to physical threats and insider threats, potentially leading to data breaches. Compliance with regulations such as PCI-DSS and GDPR often mandates encryption at rest to safeguard personal and financial information.
  description: This rule checks if encryption at rest is enabled for Elasticsearch clusters in Google Cloud. It ensures that all data stored within the Elasticsearch domain is encrypted using Google-managed encryption keys, which helps protect data integrity and confidentiality. To verify this setting, navigate to the Elasticsearch cluster settings in the GCP Console and confirm that the 'Encryption at Rest' option is enabled. Remediation involves enabling this option for all clusters without it currently active.
  references:
  - https://cloud.google.com/elasticsearch/docs/security/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/encryption-at-rest/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.elasticsearch.cluster.domains_logging_enabled
  service: elasticsearch
  resource: cluster
  requirement: Domains Logging Enabled
  scope: elasticsearch.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for Elasticsearch Clusters on GCP
  rationale: Enabling logging for Elasticsearch clusters is crucial for tracking access patterns, detecting unauthorized access attempts, and ensuring data integrity. It supports compliance with regulations like PCI-DSS and helps in forensic analysis during security incidents. Without proper logging, businesses risk operational disruptions and data breaches.
  description: This rule checks whether Elasticsearch clusters in GCP have domain-level logging enabled. To verify, ensure that audit logs are configured for all Elasticsearch clusters by navigating to the Logs Viewer in the GCP Console and checking for logs related to Elasticsearch domains. Remediation involves enabling audit logging for your Elasticsearch resources, ensuring all access and configuration changes are logged for monitoring and analysis.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/security-logging-monitoring
  - https://cloud.google.com/solutions/centralized-logging-and-monitoring
- rule_id: gcp.elasticsearch.cluster.domains_node_to_node_encryption_enabled
  service: elasticsearch
  resource: cluster
  requirement: Domains Node To Node Encryption Enabled
  scope: elasticsearch.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Elasticsearch Node-to-Node Encryption is Enabled
  rationale: Enabling node-to-node encryption for Elasticsearch clusters is crucial for protecting sensitive data traversing between nodes. Without this encryption, data in transit can be intercepted by unauthorized parties, leading to potential data breaches. This practice is often required to meet compliance with standards such as PCI-DSS and GDPR, which mandate robust data protection mechanisms.
  description: This rule checks if the Elasticsearch cluster has node-to-node encryption enabled, ensuring that all data transferred between cluster nodes is encrypted. To verify, access the Elasticsearch cluster settings in the GCP Console and confirm that the 'xpack.security.transport.ssl.enabled' configuration is set to 'true'. Remediation involves updating the Elasticsearch settings and ensuring encryption certificates are properly configured and managed.
  references:
  - https://cloud.google.com/elasticsearch/docs/security#encryption_in_transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_access_log_fields_identity_present
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Access Log Fields Identity Present
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Access Log Fields Include Identity Information
  rationale: Including identity information in API access logs is crucial for auditing and tracing activities to specific users or service accounts. This enhances the ability to detect unauthorized access or anomalous behavior, thereby reducing the risk of data breaches and ensuring compliance with regulatory requirements like GDPR and HIPAA, which mandate detailed logging of access to sensitive data.
  description: This rule checks that API monitoring logs for GCP endpoints include the identity fields, enabling detailed tracking of who accessed the APIs. To verify, ensure that the logging configuration for your endpoints includes identity fields in the access logs. If missing, update the logging settings in the Google Cloud Console or via the gcloud command-line tool to include identity information in API access logs.
  references:
  - https://cloud.google.com/endpoints/docs/grpc/logging
  - https://cloud.google.com/logging/docs/audit#configuring_audit_logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_access_log_sink_configured
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Access Log Sink Configured
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Monitoring Access Logs are Configured for Endpoints
  rationale: Configuring API monitoring access logs is crucial for tracking API usage, detecting anomalous behavior, and meeting compliance requirements such as GDPR and SOC 2. Without proper logging, organizations face increased risks of undetected security breaches, data exfiltration, and non-compliance penalties.
  description: This rule checks if API Monitoring API access log sinks are configured for Google Cloud Endpoints. Proper configuration involves setting up a log sink that captures and exports API access logs to a centralized logging service like Cloud Logging. To verify, ensure the existence of a log sink with a filter for API access logs. Remediation involves creating or updating the log sink configuration in the GCP Console or via gcloud CLI.
  references:
  - https://cloud.google.com/endpoints/docs/openapi/logging
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_access_logging_enabled
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Access Logging Enabled
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable API Monitoring API Access Logging on Endpoints
  rationale: Enabling API access logging provides visibility into API usage, helping to detect unauthorized access and potential data breaches. It supports compliance with regulations requiring detailed audit trails, such as GDPR and PCI-DSS, and assists in forensic investigations by maintaining a history of API interactions.
  description: This rule checks if API Monitoring API Access Logging is enabled for Google Cloud Endpoints. To verify, ensure that logging is configured in the Endpoints configuration file by including the 'logging' section with 'access_log' parameters specified. Remediation involves updating the service configuration YAML file to enable access logging, followed by redeploying the service.
  references:
  - https://cloud.google.com/endpoints/docs/openapi/logging
  - https://cloud.google.com/security/compliance/cis/gcp-1.1
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_execution_logging_level_minimum_error
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Execution Logging Level Minimum Error
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Execution Logs at Minimum Error Level for Endpoints
  rationale: Setting the API execution logging level to at least 'Error' ensures that critical issues and failures are captured for troubleshooting and audit purposes. This practice helps in identifying potential security breaches, unauthorized access attempts, and system failures, thus mitigating risks and aiding in compliance with standards like PCI-DSS and SOC2 that require effective logging mechanisms.
  description: This rule checks if the API execution logging level for Google Cloud Endpoints is configured to at least 'Error' level. By doing so, it ensures that all error events are logged, which is crucial for security audits and incident investigations. To verify, inspect the 'logging' section of the endpoint configuration in the Google Cloud Console or via gcloud CLI. If not set to 'Error' or higher, update the configuration by setting the 'logging' parameter to 'ERROR' in the endpoint's YAML configuration file and redeploy the service.
  references:
  - https://cloud.google.com/endpoints/docs/openapi/logging
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/architecture/security-logs-monitoring
  - https://cloud.google.com/endpoints/docs/openapi/restricting-api-access
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_logs_retention_days_minimum
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Logs Retention Days Minimum
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Minimum Retention of API Monitoring Logs
  rationale: Retaining API monitoring logs for a minimum period is crucial for effective incident response, forensic analysis, and compliance with regulatory requirements such as PCI-DSS and HIPAA. Insufficient log retention can lead to undetected malicious activity and non-compliance with data retention policies, increasing the risk of security breaches and legal penalties.
  description: This rule checks whether API monitoring logs for Google Cloud Endpoints are retained for at least the prescribed minimum number of days. It involves verifying the logging configuration of each endpoint to ensure logs are stored for a minimum duration that meets organizational and regulatory compliance needs. Remediation includes configuring the log retention settings in the Google Cloud Console or via the gcloud command-line tool to meet the minimum requirements.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/logging/docs/best-practices
- rule_id: gcp.essentialcontacts.contact.organization_notification_categories_coverage
  service: essentialcontacts
  resource: contact
  requirement: Organization Notification Categories Coverage
  scope: essentialcontacts.contact.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Essential Contacts Coverage for Org Notifications
  rationale: Proper coverage of essential contacts for organization notifications in GCP is crucial to ensure timely communication of security alerts and compliance updates. Lacking coverage can result in missed notifications about critical events or policy changes, potentially leading to security breaches or compliance violations. This is especially important for maintaining organizational resilience and meeting regulatory requirements.
  description: This rule checks whether all relevant notification categories are covered by essential contacts within your GCP organization. Specifically, it verifies that contacts are configured to receive notifications related to security and compliance. To remediate, ensure that each notification category within the essential contacts settings is assigned to an appropriate contact with the correct email address. This can be configured in the GCP Console under the Essential Contacts section of your organization's IAM settings.
  references:
  - https://cloud.google.com/resource-manager/docs/managing-notification-contacts
  - https://cloud.google.com/security-command-center/docs/security-health-analytics-policy-best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.filestore.instance.backup_enabled
  service: filestore
  resource: instance
  requirement: Backup Enabled
  scope: filestore.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure GCP Filestore Instances Have Backups Enabled
  rationale: Enabling backups for GCP Filestore instances is crucial to safeguard data against accidental deletion, corruption, or malicious attacks. Without backups, data loss can result in significant operational disruptions, financial loss, and non-compliance with data protection regulations such as GDPR or HIPAA.
  description: This rule verifies that all GCP Filestore instances have backups enabled, ensuring that data can be recovered in the event of data loss or corruption. Administrators should configure automated backups through the GCP Console or CLI. To verify, check the backup configuration settings in the Filestore instance details. If backups are not enabled, use the console or 'gcloud filestore instances update' command to configure and enable backups.
  references:
  - https://cloud.google.com/filestore/docs/creating-instances#backups
  - https://cloud.google.com/filestore/docs/backups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.filestore.instance.encryption_at_rest_enabled
  service: filestore
  resource: instance
  requirement: Encryption At Rest Enabled
  scope: filestore.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Filestore Instances Must Have Encryption At Rest Enabled
  rationale: Enabling encryption at rest for Filestore instances helps protect sensitive data from unauthorized access and ensures data integrity. In the event of a data breach or physical theft, encryption significantly reduces the risk of data exposure. Compliance with regulations such as GDPR, HIPAA, and PCI-DSS often mandates encryption of data at rest to safeguard personal and financial information.
  description: This rule verifies that all Filestore instances have encryption at rest enabled, using Google-managed encryption keys by default. It ensures that data stored in Filestore is encrypted automatically, providing a layer of security against unauthorized data access. Administrators should check the Filestore instance settings and enable encryption at rest if it is not configured. Remediation involves navigating to the Filestore instance settings in the GCP Console and ensuring that encryption is enabled using either Google-managed or customer-managed keys.
  references:
  - https://cloud.google.com/filestore/docs/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.filestore.instance.have_backup_enabled
  service: filestore
  resource: instance
  requirement: Have Backup Enabled
  scope: filestore.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Filestore Instances Have Backup Enabled
  rationale: Enabling backups for Filestore instances is critical for data resilience and disaster recovery. Without backups, data loss due to accidental deletion, corruption, or cyber attacks can have severe implications on business continuity and compliance with data protection regulations such as GDPR or HIPAA.
  description: This rule checks if Filestore instances have backups enabled to ensure data can be restored in case of loss or corruption. Verify by checking the Filestore instance settings in the GCP Console or using the gcloud command-line tool. To enable backups, navigate to the Filestore instance settings, and activate the backup option or configure a scheduled backup policy. This ensures data availability and minimizes downtime during recovery operations.
  references:
  - https://cloud.google.com/filestore/docs/backups
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.firestore.collection.encryption_enabled_gcp_logging_kms_encryption_enable_enabled
  service: firestore
  resource: collection
  requirement: Encryption Enabled Gcp Logging KMS Encryption Enable Enabled
  scope: firestore.collection.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Collections Use KMS for Encryption at Rest
  rationale: Encrypting Firestore collections using Cloud KMS protects sensitive data from unauthorized access and potential breaches, which can lead to regulatory non-compliance and reputational damage. This approach leverages Google Cloud's robust key management to enhance data security and meet compliance mandates such as PCI-DSS and HIPAA.
  description: This rule checks if Firestore collections are encrypted with a customer-managed encryption key (CMEK) via Cloud KMS. To verify, ensure the Firestore collection's encryption configuration specifies a valid KMS key. Remediation involves enabling CMEK by configuring Firestore to use a Cloud KMS key for data encryption, which can be done through the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/firestore/docs/encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.firestore.collection.encryption_enabled_gcp_logging_kms_encryption_enable_logging
  service: firestore
  resource: collection
  requirement: Encryption Enabled Gcp Logging KMS Encryption Enable Logging
  scope: firestore.collection.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Collections Use KMS for Encryption and Logging
  rationale: Enabling encryption for Firestore collections using Cloud KMS and logging ensures data is protected against unauthorized access and modification. This helps in mitigating risks of data breaches and supports compliance with regulations such as GDPR and HIPAA, which mandate robust data protection measures.
  description: This rule checks whether Google Cloud Firestore collections are encrypted at rest using Customer-Managed Encryption Keys (CMEK) provided by Google Cloud Key Management Service (KMS), and whether logging is enabled for these operations. To verify, ensure that Firestore collections are configured with CMEK and that Cloud Audit Logs are set up to track access and administrative actions. Remediation involves configuring Firestore to use a specific KMS key and enabling logging through the GCP Console or CLI.
  references:
  - https://cloud.google.com/firestore/docs/encryption
  - https://cloud.google.com/kms/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.firestore.database.backup_enabled
  service: firestore
  resource: database
  requirement: Backup Enabled
  scope: firestore.database.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: medium
  title: Ensure Firestore Database Backups are Enabled
  rationale: Enabling backups for Firestore databases is crucial for data resilience and recovery in case of accidental deletions, corruption, or security incidents. Without backups, organizations risk losing critical data, facing operational disruptions, and failing to meet compliance requirements such as PCI-DSS and HIPAA, which mandate data protection and recovery capabilities.
  description: This rule checks whether automated backups are enabled for your Firestore databases. Backups allow you to restore data to a previous state, protecting against data loss. To enable backups, configure the Firestore database to use Cloud Firestore in Datastore mode, which supports daily backups. Verify backup settings in the GCP Console under Firestore settings. Remediation involves enabling automated backups and periodically testing restore processes.
  references:
  - https://cloud.google.com/firestore/docs/manage-data/backup
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.firestore.database.encryption_enabled
  service: firestore
  resource: database
  requirement: Encryption Enabled
  scope: firestore.database.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Databases Have Encryption At Rest Enabled
  rationale: Enabling encryption at rest for Firestore databases protects sensitive data from unauthorized access, ensuring that even if physical storage is compromised, the data remains secure. This is crucial for maintaining customer trust, meeting regulatory compliance requirements like GDPR and HIPAA, and mitigating risks associated with data breaches.
  description: This rule checks if Firestore databases have encryption at rest enabled. Firestore automatically encrypts all data before it is written to disk and decrypts it when read, using Google-managed encryption keys by default. To verify, ensure that Firestore has encryption enabled in the GCP Console under Firestore settings. Remediation involves configuring custom encryption keys via Google Cloud Key Management Service (KMS) if enhanced control is required.
  references:
  - https://cloud.google.com/firestore/docs/server-side-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
- rule_id: gcp.firestore.database.global_tables_enabled
  service: firestore
  resource: database
  requirement: Global Tables Enabled
  scope: firestore.database.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Firestore Global Tables are Enabled
  rationale: Enabling global tables in Firestore ensures that data is automatically replicated across multiple regions, improving availability and durability. This mitigates risks associated with regional outages and data loss, which could severely impact business continuity and data integrity. Additionally, it supports compliance with regulations requiring data resilience.
  description: This check verifies whether Firestore databases have global tables enabled, which ensures that data is replicated across geographically distributed locations. To verify, review the Firestore settings in the GCP Console under the Firestore database section and ensure the 'multi-region' configuration is selected. If not enabled, reconfigure the database settings to use global tables, ensuring data is stored in multiple regions.
  references:
  - https://cloud.google.com/firestore/docs/best-practices
  - https://cloud.google.com/security/compliance/cis#section_5.2
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/firestore/docs/locations
- rule_id: gcp.firestore.database.pitr_enabled
  service: firestore
  resource: database
  requirement: Pitr Enabled
  scope: firestore.database.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Point-In-Time Recovery for Firestore Databases
  rationale: Point-In-Time Recovery (PITR) for Firestore databases is crucial for protecting against accidental data loss, corruption, or malicious attacks that could compromise data integrity. Enabling PITR helps ensure business continuity by allowing restoration of databases to a specific state at an earlier point in time, which aligns with compliance requirements such as GDPR and HIPAA that demand robust data protection measures.
  description: This rule checks if Point-In-Time Recovery (PITR) is enabled for Firestore databases. To verify, ensure that PITR is configured in the Firestore settings within the Google Cloud Console, allowing databases to be restored to any state within the retention period. Remediation involves enabling PITR through the console, which can be done by navigating to the Firestore database settings and activating the PITR feature, ensuring the retention period aligns with organizational policies.
  references:
  - https://cloud.google.com/firestore/docs/manage-data/backups
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.firestore.database.tables_kms_cmk_encryption_enabled
  service: firestore
  resource: database
  requirement: Tables KMS CMK Encryption Enabled
  scope: firestore.database.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Tables Use Customer-Managed Encryption Keys
  rationale: Using Customer-Managed Encryption Keys (CMK) for Firestore tables enhances data security by allowing organizations to control the encryption keys used for data at rest. This reduces the risk of unauthorized access and potential data breaches, and ensures compliance with regulations like GDPR, which require strong encryption and data protection measures.
  description: This rule checks whether Firestore tables are encrypted using Google Cloud KMS Customer-Managed Keys (CMK). To ensure your Firestore data is encrypted with CMK, navigate to the Firestore console, select your database, and verify that Customer-Managed Keys are enabled for encryption. If not, configure your Firestore instance to use CMK by setting the appropriate encryption key in the Google Cloud Console or via the CLI, ensuring that the keys are securely managed and rotated regularly.
  references:
  - https://cloud.google.com/firestore/docs/encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/how-tos
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.firestore.database.tables_pitr_enabled
  service: firestore
  resource: database
  requirement: Tables Pitr Enabled
  scope: firestore.database.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Point-in-Time Recovery for Firestore Databases
  rationale: Point-in-Time Recovery (PITR) is crucial for mitigating data loss due to accidental deletions, corruption, or malicious activities. It ensures business continuity and aligns with compliance requirements for data protection, such as GDPR and SOC 2, by allowing restoration of databases to a specific state in time. Failure to enable PITR can lead to significant operational disruptions and legal liabilities.
  description: This rule checks if Point-in-Time Recovery is enabled for Google Cloud Firestore databases. PITR allows the recovery of a database to a state within a specific time window, providing a robust mechanism to protect against data loss. To verify, ensure that Firestore databases have PITR configured in the Google Cloud Console, under the Firestore settings. Remediation involves enabling PITR in the Firestore settings, which can be done by navigating to the Firestore database in the Google Cloud Console and configuring the PITR settings under the backup and restore options.
  references:
  - https://cloud.google.com/firestore/docs/backups
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/firestore/docs/manage-data/export-import
- rule_id: gcp.firestore.document.data_protection_storage_table_encryption_at_rest_enabled
  service: firestore
  resource: document
  requirement: Data Protection Storage Table Encryption At Rest Enabled
  scope: firestore.document.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Documents are Encrypted at Rest
  rationale: Encrypting Firestore documents at rest is crucial to protect sensitive data from unauthorized access and data breaches. It mitigates risks associated with data theft and ensures compliance with regulatory standards such as GDPR, HIPAA, and PCI-DSS, which mandate encryption to safeguard personal and financial information.
  description: This rule checks if Firestore documents are encrypted at rest using Google-managed encryption keys. To verify, ensure that the Firestore database is configured with encryption settings enabled by default, utilizing Google Cloud's built-in encryption mechanisms. Remediation involves reviewing and enabling encryption settings via the GCP Console or gcloud CLI to protect stored data against unauthorized access.
  references:
  - https://cloud.google.com/firestore/docs/server-side-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.firestore.document.data_protection_storage_table_private_network_only_supported
  service: firestore
  resource: document
  requirement: Data Protection Storage Table Private Network Only Supported
  scope: firestore.document.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Firestore Document Access to Private Networks
  rationale: Restricting access to Firestore documents to private networks reduces the risk of unauthorized access and data breaches, which are critical concerns for business continuity and regulatory compliance. Exposing data to public networks increases the attack surface, enabling potential threat actors to exploit vulnerabilities, leading to loss of sensitive information and detrimental financial and reputational impact.
  description: This rule checks that Firestore documents are only accessible via private networks, preventing public internet exposure. Verify that your Firestore instance is configured with a private IP and restrict access using VPC Service Controls. Remediate by setting up private network configurations and firewall rules to limit access to trusted internal networks only.
  references:
  - https://cloud.google.com/firestore/docs/security/rules-structure
  - https://cloud.google.com/vpc-service-controls/docs/firestore
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.firestore.document.data_protection_storage_table_rbac_least_privilege
  service: firestore
  resource: document
  requirement: Data Protection Storage Table RBAC Least Privilege
  scope: firestore.document.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Firestore Document RBAC Uses Least Privilege Principle
  rationale: Enforcing least privilege in Firestore document access is crucial to minimize unauthorized data exposure, reduce the attack surface, and comply with regulatory requirements such as GDPR and HIPAA. Improper permissions can lead to data breaches, impacting business reputation and incurring financial penalties.
  description: This rule checks Firestore document permissions to ensure that access is granted strictly based on the principle of least privilege. Review IAM roles associated with Firestore documents and verify that only necessary permissions are assigned. Use 'roles/firestore.viewer' or custom roles with minimal permissions for users requiring read access. Audit and remove any excessive permissions regularly.
  references:
  - https://cloud.google.com/firestore/docs/security/rules-structure
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/73906.html
- rule_id: gcp.healthcare.consent_store.privacy_consent_access_rbac_least_privilege
  service: healthcare
  resource: consent_store
  requirement: Privacy Consent Access RBAC Least Privilege
  scope: healthcare.consent_store.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Consent Store Access in GCP Healthcare
  rationale: Implementing least privilege access for consent stores in GCP's Healthcare API is crucial to minimize the risk of unauthorized access to sensitive patient data, which could lead to data breaches and non-compliance with regulations like HIPAA. Ensuring that only authorized users have the necessary permissions reduces the attack surface and potential for insider threats, thereby protecting patient privacy and maintaining trust.
  description: This rule checks whether the RBAC settings for GCP Healthcare consent stores adhere to the principle of least privilege. Specifically, it verifies that access permissions are limited to only those users and roles that require it for their job functions. To verify compliance, review the IAM policy bindings for the consent store, ensuring that roles are assigned based on necessity and aligned with compliance requirements. Remediate by adjusting IAM policies to restrict access and periodically review permissions to maintain minimal access levels.
  references:
  - https://cloud.google.com/healthcare/docs/how-tos/permissions-healthcare-api
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/healthcare/docs/concepts/privacy
- rule_id: gcp.healthcare.consent_store.privacy_encrypted
  service: healthcare
  resource: consent_store
  requirement: Privacy Encrypted
  scope: healthcare.consent_store.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Healthcare Consent Stores Use Privacy Encryption
  rationale: Encryption of consent stores in Google Cloud Healthcare API is critical to prevent unauthorized access to sensitive health data. Failure to encrypt can lead to data breaches, resulting in financial loss, reputational damage, and non-compliance with regulations such as HIPAA and GDPR. Properly encrypted data at rest ensures that even if physical security is breached, the data remains unintelligible and protected.
  description: This rule checks whether consent stores in GCP Healthcare API have encryption enabled. Specifically, it verifies that all data at rest in consent stores is encrypted using Google-managed encryption keys or customer-managed keys. To remediate, ensure that the consent store is configured to use encryption by setting up the appropriate encryption keys during store creation or updating existing stores to include encryption settings.
  references:
  - https://cloud.google.com/healthcare/docs/how-tos/consent-management
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.healthcare.consent_store.privacy_rights_execution_roles_least_privilege
  service: healthcare
  resource: consent_store
  requirement: Privacy Rights Execution Roles Least Privilege
  scope: healthcare.consent_store.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Privacy Rights Roles in Consent Store
  rationale: Implementing least privilege for roles involved in executing privacy rights in the GCP Healthcare Consent Store minimizes unauthorized data access, reducing the risk of data breaches and ensuring compliance with privacy regulations such as HIPAA and GDPR. This approach mitigates the potential impact of insider threats and limits the scope of access if credentials are compromised.
  description: This rule verifies that roles with permissions to execute privacy rights in the Consent Store are configured with the principle of least privilege. Check IAM policies to ensure that only necessary permissions are granted to users and service accounts. Remediation involves auditing current roles, removing excessive permissions, and using predefined roles or creating custom roles tailored to specific operational needs.
  references:
  - https://cloud.google.com/healthcare/docs/how-tos/consent-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.healthcare.consent_store.privacy_rights_workflow_storage_encrypted
  service: healthcare
  resource: consent_store
  requirement: Privacy Rights Workflow Storage Encrypted
  scope: healthcare.consent_store.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Consent Store Data is Encrypted at Rest
  rationale: Encrypting consent store data at rest is crucial for protecting sensitive healthcare information from unauthorized access, mitigating the risk of data breaches, and ensuring compliance with regulations such as HIPAA and GDPR. Failure to encrypt could lead to significant financial penalties, loss of reputation, and legal consequences.
  description: This rule checks whether the data stored in the healthcare consent store is encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that all consent stores have encryption configured in the Google Cloud Console under the Healthcare API settings. Remediation involves setting up encryption during the creation of consent stores or updating existing stores to enable encryption settings.
  references:
  - https://cloud.google.com/healthcare/docs/how-tos/consent-management
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.group.has_users
  service: iam
  resource: group
  requirement: Has Users
  scope: iam.group.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Groups Contain Active Users
  rationale: IAM groups without users can lead to misconfigurations and potential security gaps, as they may be overlooked during audits and reviews. Empty groups can accumulate over time, resulting in unnecessary complexity and the risk of inappropriate access if mistakenly assigned policies. Ensuring that groups have active users aligns with security best practices and compliance requirements, reducing the attack surface and maintaining a clear overview of access control within the organization.
  description: This rule checks if IAM groups in GCP contain at least one active user. Groups should not be empty to ensure they serve a functional purpose and are properly monitored. Administrators should regularly audit IAM groups and remove any that no longer have users or are not in use. Remediation involves reviewing group membership through the GCP Console or gcloud CLI and adding users where necessary or deleting obsolete groups.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#identity_management
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0 - 1.1 Ensure that corporate login credentials are used
  - 'NIST SP 800-53 AC-2: Account Management'
  - ISO/IEC 27001:2013 - A.9 Access Control
- rule_id: gcp.iam.group.identity_access_rbac_attached_policies_not_admin_star
  service: iam
  resource: group
  requirement: Identity Access RBAC Attached Policies Not Admin Star
  scope: iam.group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Admin Role in Group IAM Policies
  rationale: Assigning broad admin permissions to IAM groups can lead to unchecked access across the GCP environment, increasing the risk of privilege escalation and accidental or malicious misuse of resources. This poses significant security threats and could lead to non-compliance with standards like PCI-DSS and ISO 27001, which require least privilege access controls.
  description: This rule checks that IAM policies attached to groups do not include overly permissive roles, such as 'roles/*', which grant admin-level access. To verify, review IAM policies for groups and ensure no policies include wildcard roles. Remediation involves narrowing permissions by using predefined roles or custom roles that align with the principle of least privilege, thereby reducing risk exposure.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis#section_1.1
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
- rule_id: gcp.iam.group.identity_access_rbac_external_sharing_restricted_w_supported
  service: iam
  resource: group
  requirement: Identity Access RBAC External Sharing Restricted W Supported
  scope: iam.group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict External Sharing of IAM Group Access
  rationale: Restricting external sharing of IAM group access is critical to prevent unauthorized access to sensitive data and resources. External sharing without proper controls can lead to exposure of confidential information, non-compliance with data protection regulations, and increased risk of security breaches. Ensuring that only authorized identities have access minimizes the risk of insider threats and data leaks.
  description: This rule checks whether IAM groups have restrictions in place to prevent unauthorized external sharing of access rights. It ensures that IAM group configurations do not allow identities outside the organization to gain unintended access. Verification involves reviewing IAM policies and group memberships for compliance with organizational policies. Remediation includes configuring IAM policies to limit access to trusted identities and regularly auditing group memberships to ensure compliance.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
  - 'CIS GCP Benchmark: 1.6 Ensure that IAM policies are not permissive'
  - 'NIST SP 800-53: AC-6 Least Privilege'
  - 'ISO 27001: A.9 Access Control'
  - https://cloud.google.com/security/best-practices/identity-access-management
- rule_id: gcp.iam.group.identity_access_rbac_no_inline_policies
  service: iam
  resource: group
  requirement: Identity Access RBAC No Inline Policies
  scope: iam.group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Avoid Inline Policies for IAM Group Access Control
  rationale: Using inline policies for IAM groups can lead to fragmented and hard-to-manage permissions, increasing the risk of unauthorized access. This practice can complicate audits and compliance efforts, as inline policies are less visible and harder to track compared to managed policies. Adhering to this rule helps maintain clear and consistent access control, reducing potential security breaches and ensuring compliance with frameworks such as ISO 27001 and NIST.
  description: This rule checks that IAM groups do not use inline policies, which should be replaced with managed policies for better visibility and control. Inline policies are directly attached to a resource, complicating permission management and auditing. To verify, inspect IAM group policies and ensure they reference managed policies only. Remediation involves detaching inline policies and applying equivalent managed policies instead.
  references:
  - https://cloud.google.com/iam/docs/policies-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/iam-best-practices
- rule_id: gcp.iam.key.90_days
  service: iam
  resource: key
  requirement: 90 Days
  scope: iam.key.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Keys Are Rotated Every 90 Days
  rationale: Rotating IAM keys every 90 days minimizes the risk of key compromise. Stale or long-lived keys increase the attack surface, making it easier for malicious actors to exploit outdated credentials. Regular rotation of keys is a best practice that supports compliance with regulations such as NIST and ISO 27001, which mandate strict access control mechanisms.
  description: This rule checks that IAM keys in your GCP environment are rotated at least every 90 days. To verify, review the 'last_used' and 'created' timestamps of IAM keys and ensure no key exceeds the 90-day threshold. If a key is older than 90 days, it's recommended to create and distribute new keys, then revoke the old ones. Use the 'gcloud iam service-accounts keys list' command to audit key ages and manage rotations programmatically.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-managing-service-accounts
- rule_id: gcp.iam.key.access_key_90_days
  service: iam
  resource: key
  requirement: Access Key 90 Days
  scope: iam.key.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Rotate IAM Access Keys Every 90 Days
  rationale: Regularly rotating access keys mitigates the risk of credential exposure and reduces the impact of a compromised key. Non-rotated keys are more susceptible to unauthorized access, which could lead to data breaches and non-compliance with security regulations such as PCI-DSS and ISO 27001.
  description: This rule checks if IAM access keys have been rotated within the last 90 days. Keys that have not been rotated may expose your resources to security vulnerabilities. To verify, review the 'last used' date of each key in the IAM console. Remediate by deleting old keys and creating new ones, updating any dependent systems to use the new keys to maintain operational continuity.
  references:
  - https://cloud.google.com/iam/docs/managing-access-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/docs/security/key-policies
- rule_id: gcp.iam.key.key_configured
  service: iam
  resource: key
  requirement: Key Configured
  scope: iam.key.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure IAM Keys are Properly Configured
  rationale: Improperly configured IAM keys can lead to unauthorized access, data breaches, and potential non-compliance with regulatory standards such as PCI-DSS and ISO 27001. Ensuring that IAM keys are securely configured helps protect sensitive information and maintain system integrity, minimizing the risk of exploitation by malicious actors.
  description: This rule checks that all IAM service account keys are configured with strong security settings, such as expiration dates and restricted permissions. Ensure keys are rotated regularly and audit logs are enabled to track usage. Remediation involves setting expiration dates, regularly reviewing key permissions, and removing unused keys to reduce the attack surface.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/service-accounts#best_practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.iam.policy.attached_only_to_group_or_roles
  service: iam
  resource: policy
  requirement: Attached Only To Group Or Roles
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Are Attached Only to Groups or Roles
  rationale: Attaching IAM policies directly to individual users increases the risk of excessive permissions, which can lead to unauthorized access and potential data breaches. By limiting policy attachments to groups or roles, organizations can manage permissions more effectively and ensure compliance with least privilege principles. This approach also facilitates easier audits and compliance with standards like ISO 27001 and SOC2.
  description: This rule checks that IAM policies are not directly attached to individual user accounts but are instead associated with groups or roles. To verify, review IAM policies in the GCP Console under the IAM & Admin section and ensure no policies are directly assigned to users. Remediation involves reassigning these policies to appropriate groups or roles, ensuring that permissions are granted through group membership or role assumption, which can be managed via the IAM & Admin interface or by using the gcloud CLI.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/manage-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.iam.policy.audit_logs_enabled
  service: iam
  resource: policy
  requirement: Audit Logs Enabled
  scope: iam.policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure IAM Audit Logs Are Enabled for All Services
  rationale: Enabling audit logs for IAM services is crucial for tracking access and modifications to resources, which helps in identifying unauthorized actions or potential security breaches. This practice supports regulatory compliance by providing a detailed record of activities, aiding in forensic investigations and demonstrating adherence to security policies.
  description: This rule checks whether audit logging is enabled for Google Cloud IAM policies, ensuring that all read, write, and administrative activities are recorded. To verify, navigate to the Google Cloud Console, select Logging > Logs Configuration, and confirm that audit logs are active for all IAM services. If not enabled, configure audit logging by setting up appropriate log sinks and permissions. This ensures comprehensive monitoring and accountability of IAM-related actions.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.policy.cloud_asset_inventory_enabled
  service: iam
  resource: policy
  requirement: Cloud Asset Inventory Enabled
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Cloud Asset Inventory is Enabled for IAM Policies
  rationale: Enabling Cloud Asset Inventory is crucial for maintaining visibility over your IAM policies and understanding resource access. Without it, organizations face risks such as unauthorized access and non-compliance with regulations like GDPR and CCPA. This can lead to data breaches, financial penalties, and loss of customer trust.
  description: This rule checks whether Cloud Asset Inventory is enabled in your GCP environment, specifically for tracking IAM policies. Cloud Asset Inventory provides a detailed inventory of your cloud assets, which helps in monitoring and managing access controls effectively. To verify, navigate to the Cloud Console, ensure the Cloud Asset API is enabled, and check that asset history is being recorded. Remediate by enabling the API and configuring asset types to include IAM policies, ensuring a comprehensive view of your access policy landscape.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/overview
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/
- rule_id: gcp.iam.policy.compliance
  service: iam
  resource: policy
  requirement: Compliance
  scope: iam.policy.compliance
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Meet Compliance Standards
  rationale: Adhering to IAM policy compliance is crucial for protecting sensitive data and maintaining operational integrity. Non-compliance can lead to unauthorized access, data breaches, and regulatory fines. Organizations must ensure their IAM policies reflect the least privilege principle and comply with industry standards to reduce security risks and meet regulatory obligations.
  description: This rule checks if IAM policies are configured to comply with defined security standards and best practices. It involves verifying that policies enforce the least privilege, do not include overly permissive roles, and are regularly reviewed and updated. Remediation includes auditing IAM policies, removing unnecessary permissions, and utilizing predefined roles where possible. Automated tools or scripts can assist in continuous compliance monitoring.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://hipaa.jotform.com/what-is-hipaa/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.policy.console_mfa_enabled
  service: iam
  resource: policy
  requirement: Console MFA Enabled
  scope: iam.policy.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Multi-Factor Authentication for Console Access
  rationale: Enabling Multi-Factor Authentication (MFA) for GCP console access significantly reduces the risk of unauthorized access due to compromised credentials. MFA protects sensitive resources and data from being accessed by attackers exploiting weak or stolen passwords, thus safeguarding against potential data breaches and financial losses. Additionally, it helps meet compliance requirements for standards such as PCI-DSS, NIST, and ISO 27001 that mandate strong authentication mechanisms.
  description: This rule checks that all users accessing the GCP console have MFA enabled on their accounts. To verify, review the IAM policies and ensure that MFA is required for all users. Remediation involves configuring IAM settings to enforce MFA, which can be done via the Google Cloud Console under the 'Security' section or by using the gcloud command-line tool. It is critical to communicate and enforce this policy across all teams to ensure compliance and security.
  references:
  - https://cloud.google.com/identity-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.policy.domain_compliance_configured
  service: iam
  resource: policy
  requirement: Domain Compliance Configured
  scope: iam.policy.compliance
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Domain Compliance for IAM Policies
  rationale: Configuring domain compliance in IAM policies is crucial to mitigate unauthorized access risks and ensure that only approved domains can authenticate and access resources. This is particularly important for protecting sensitive data and maintaining compliance with industry regulations such as GDPR and HIPAA, which mandate strict access controls and data protection measures.
  description: This rule checks whether IAM policies are configured to enforce domain compliance, ensuring that only identities from specified domains can be granted access. To verify, inspect the IAM policy bindings for the presence of domain-restricted conditions. Remediation involves updating IAM policies to include domain compliance conditions, using the 'member' field with a valid domain format (e.g., 'user:example.com'). This configuration helps prevent unauthorized access from external domains.
  references:
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.policy.identity_access_rbac_conditions_used_where_applicable
  service: iam
  resource: policy
  requirement: Identity Access RBAC Conditions Used Where Applicable
  scope: iam.policy.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Use of Conditional Access in IAM Policies
  rationale: Conditional access in IAM policies allows for fine-grained control over resource access, reducing the risk of over-permissioned roles. It mitigates potential unauthorized access by enforcing conditions like time-based access or IP restrictions. This aligns with compliance frameworks requiring least privilege and adaptive access controls, thereby reducing the attack surface and enhancing overall security posture.
  description: This rule checks if IAM policies utilize RBAC conditions where applicable, such as using 'conditions' in role bindings to specify context-aware access controls. Verifying implementation involves reviewing IAM policy bindings for conditional logic. Remediation includes updating IAM policies to incorporate conditions, like resource attributes, request attributes, or environment attributes, to enforce contextual access controls.
  references:
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.iam.policy.identity_access_rbac_no_action_star_on_resource_star
  service: iam
  resource: policy
  requirement: Identity Access RBAC No Action Star On Resource Star
  scope: iam.policy.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict IAM Action Star on Resource Star
  rationale: Allowing all actions on all resources can lead to excessive permissions, increasing the risk of unauthorized access or data breaches. This setup can violate compliance requirements such as PCI-DSS and SOC2, which mandate least privilege access. Misuse of such permissions could result in data loss, service disruption, and financial penalties.
  description: 'This rule checks for IAM policies where any identity has been granted wildcard (*) permissions on all resources (*). To verify, review IAM policies in the Google Cloud Console or using gcloud CLI for any entries with ''action: *'' and ''resource: *''. Remediation involves revising these policies to specify exact actions and resources required, aligning with the principle of least privilege.'
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 1.6
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
- rule_id: gcp.iam.policy.identity_access_rbac_no_policy_allows_without_constraints
  service: iam
  resource: policy
  requirement: Identity Access RBAC No Policy Allows Without Constraints
  scope: iam.policy.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict IAM Policies Without Constraints
  rationale: Allowing IAM policies without constraints can lead to excessive permissions, increasing the risk of unauthorized access and data breaches. This can affect business continuity and result in non-compliance with standards like ISO 27001 and PCI-DSS, potentially leading to financial penalties and reputational damage.
  description: This rule checks for IAM policies that grant permissions without specifying constraints such as conditions. Such lax policies can inadvertently provide more access than intended. To remediate, review and update IAM policies to include necessary conditions, ensuring access is restricted to specific circumstances. Verification involves inspecting IAM policy bindings for the presence of conditions.
  references:
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.iam.policy.identity_access_rbac_resource_constraints_present
  service: iam
  resource: policy
  requirement: Identity Access RBAC Resource Constraints Present
  scope: iam.policy.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure RBAC Resource Constraints are Defined in IAM Policies
  rationale: Defining RBAC resource constraints in IAM policies is crucial to minimize over-privileged access, reducing the risk of unauthorized data access and potential compliance violations. It helps maintain the principle of least privilege by ensuring that IAM roles are applied only to necessary resources, thereby mitigating internal and external threats.
  description: This rule checks that Identity and Access Management (IAM) policies in GCP have clearly defined RBAC resource constraints. Without these constraints, roles may have excessive access to resources, leading to potential security risks. To verify, review IAM policies for the presence of resource-level constraints. Remediation involves updating IAM policies to include specific resource constraints, ensuring roles are limited to only the necessary resources required for their function.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org
  - https://www.isaca.org/resources/cobit
- rule_id: gcp.iam.policy.minimum_length_14
  service: iam
  resource: policy
  requirement: Minimum Length 14
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce IAM Policy Password Minimum Length of 14 Characters
  rationale: Ensuring a minimum password length of 14 characters significantly enhances security by making it more difficult for attackers to use brute force or dictionary attacks to compromise user accounts. Longer passwords exponentially increase the number of possible combinations, thus reducing the risk of unauthorized access. Adhering to this requirement helps meet compliance standards such as NIST SP 800-63 for password policies, which many organizations are obligated to follow.
  description: This rule verifies that all IAM policies enforce a minimum password length of 14 characters. Administrators should configure Google Cloud IAM settings to set password policies that adhere to this length requirement. To verify, check the IAM settings under the Google Cloud Console or use gcloud CLI commands to list current policies. Remediation includes updating the policy settings to ensure the minimum length criteria is met, thus strengthening account security against unauthorized access.
  references:
  - https://cloud.google.com/iam/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://pages.nist.gov/800-63-3/sp800-63b.html
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.policy.no_administrative_privileges
  service: iam
  resource: policy
  requirement: No Administrative Privileges
  scope: iam.policy.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict IAM Policies to Prevent Excessive Privileges
  rationale: Limiting administrative privileges in IAM policies reduces the risk of unauthorized access and potential data breaches. Excessive permissions can lead to accidental or malicious misuse, compromising the integrity and confidentiality of cloud resources. Ensuring least privilege aligns with regulatory requirements and helps protect sensitive information from internal and external threats.
  description: This rule checks IAM policies to ensure that no user or service account is granted administrative roles, such as Owner or Editor, unless absolutely necessary. Review and modify IAM policies to follow the principle of least privilege by assigning roles with only the required permissions. Use the 'IAM Recommender' tool to get suggestions for role adjustments. Regularly audit IAM policies and update them to remove unnecessary permissions.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.iam.policy.overly_permissive
  service: iam
  resource: policy
  requirement: Overly Permissive
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Identify and Mitigate Overly Permissive IAM Policies
  rationale: Overly permissive IAM policies can lead to unauthorized access, resulting in potential data breaches and compliance violations. Such policies increase the risk of privilege escalation and data exfiltration, impacting the organization's security posture and regulatory requirements under frameworks like PCI-DSS and ISO 27001.
  description: This rule checks for IAM policies that grant excessive permissions, such as roles with wildcard ('*') permissions or broad access across resources. To verify, review IAM policy bindings and ensure roles are assigned on a principle of least privilege basis. Remediation involves auditing and refining IAM roles to limit permissions to only what is necessary for users to perform their job functions.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.iam.policy.policy_configured
  service: iam
  resource: policy
  requirement: Policy Configured
  scope: iam.policy.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policy is Correctly Configured
  rationale: Proper configuration of IAM policies is crucial to prevent unauthorized access to resources, reducing the risk of data breaches and supporting compliance with regulations like GDPR and HIPAA. Misconfigured policies can lead to privilege escalation and unauthorized data manipulation, impacting business operations and reputational standing.
  description: This rule checks if IAM policies are correctly configured, ensuring that least privilege principles are applied and no overly permissive roles are assigned. Verify that all IAM policies are documented and reviewed regularly, with roles assigned only as needed. Remediation involves auditing current policies, removing unnecessary permissions, and utilizing custom roles where appropriate to minimize access.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.iam.policy.root_hardware_mfa_enabled
  service: iam
  resource: policy
  requirement: Root Hardware MFA Enabled
  scope: iam.policy.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Root Account Hardware MFA is Enabled
  rationale: Enabling hardware MFA for the root account significantly reduces the risk of unauthorized access by adding an additional layer of security. This is critical because the root account has extensive permissions and control over the entire GCP environment. Compromise of this account can lead to full exposure of data and resources, resulting in severe business and financial impacts. Compliance with standards such as PCI-DSS and ISO 27001 also necessitates robust authentication controls.
  description: This rule checks if the root account in GCP has hardware MFA enabled. To verify, ensure that the root user has a hardware security key or other approved MFA device configured. Remediation involves accessing the Google Cloud Console, navigating to the 'Security' section of the account settings, and adding a hardware MFA device to the root account. This enhancement prevents unauthorized access and aligns with best practices for securing privileged accounts.
  references:
  - https://cloud.google.com/iam/docs/hardware-security-key
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/docs/security-overview
- rule_id: gcp.iam.policy.root_mfa_enabled
  service: iam
  resource: policy
  requirement: Root MFA Enabled
  scope: iam.policy.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Root Account MFA Must Be Enabled
  rationale: Enabling MFA for the root account significantly reduces the risk of unauthorized access, which can lead to data breaches and compromise of critical resources. Root accounts have extensive privileges, making them a prime target for attackers. Ensuring MFA is enabled aligns with security best practices and compliance with standards like NIST SP 800-53 and PCI-DSS, protecting organizational integrity and data confidentiality.
  description: This rule checks if multi-factor authentication (MFA) is enabled for the root account in Google Cloud Platform. MFA adds an additional layer of security, requiring not only a password and username but also something that only the user has on them, i.e., a piece of information only they should know or have immediately to hand. To verify and enable MFA, navigate to the IAM & Admin section in the GCP Console, select the root account, and ensure MFA is configured. Remediation involves setting up an authenticator application or enabling SMS-based verification if not already configured.
  references:
  - https://cloud.google.com/docs/authentication/mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.policy.strong
  service: iam
  resource: policy
  requirement: Strong
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Enforce Strong Authentication
  rationale: Implementing strong authentication mechanisms prevents unauthorized access to critical cloud resources, reducing the risk of data breaches and ensuring compliance with security regulations such as NIST and PCI-DSS. Without strong authentication, attackers could exploit weak credentials or identity vulnerabilities, leading to potential data loss and financial penalties.
  description: This rule checks if IAM policies enforce strong authentication by requiring multi-factor authentication (MFA) for all users with access to sensitive resources. To verify, audit IAM policies to ensure MFA is enabled and enforced. Remediation involves configuring IAM roles and policies to mandate the use of MFA for high-privilege accounts, and regularly reviewing access logs for compliance.
  references:
  - https://cloud.google.com/iam/docs/multi-factor-authentication
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/iam
  - https://cloud.google.com/blog/products/identity-security/how-to-enhance-your-security-with-multi-factor-authentication
- rule_id: gcp.iam.role.identity_access_attached_policies_not_admin_star
  service: iam
  resource: role
  requirement: Identity Access Attached Policies Not Admin Star
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Admin Role Assignments in IAM Policies
  rationale: Allowing roles with wildcard permissions (e.g., 'roles/*') can lead to excessive privileges and potential misuse by malicious actors. Properly scoped permissions reduce the risk of privilege escalation and data breaches, and ensure compliance with least privilege principles required by regulatory standards such as ISO 27001 and SOC2.
  description: This rule checks IAM roles to ensure that no policy grants overly broad permissions using 'roles/*'. Review and update policies to adhere to the principle of least privilege by assigning only specific, necessary roles. Use IAM Recommender to identify and remove unused permissions. Regular audits and policy reviews are essential to maintain a secure and compliant environment.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/recommender-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.iam.role.identity_access_max_session_duration_reasonable
  service: iam
  resource: role
  requirement: Identity Access Max Session Duration Reasonable
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Role Session Duration is Configured Appropriately
  rationale: Setting a reasonable session duration for IAM roles reduces the risk of credential misuse by limiting the time an identity can remain active without re-authentication. This helps mitigate potential unauthorized access in case of compromised sessions, aligning with compliance requirements such as ISO 27001 and NIST SP 800-53. Ensuring appropriate session durations enhances security by enforcing periodic re-authentication, which is critical for maintaining the integrity of sensitive operations.
  description: This rule checks that the maximum session duration for IAM roles in GCP is set to a reasonable value. By default, roles can have a session duration of up to 12 hours, but it is recommended to configure this setting to a shorter period, such as 1 hour, to minimize security risks. To verify, inspect the IAM role settings in the Google Cloud Console or use the `gcloud` command-line tool to check the `maxSessionDuration` attribute. Remediation involves adjusting this configuration to a lower value, balancing operational needs with security best practices.
  references:
  - https://cloud.google.com/iam/docs/roles-best-practices
  - https://cloud.google.com/iam/docs/creating-managing-roles
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.role.identity_access_no_inline_policies
  service: iam
  resource: role
  requirement: Identity Access No Inline Policies
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Avoid Inline Policies in IAM Roles
  rationale: Inline policies in IAM roles can lead to complex and fragmented access management, increasing the risk of privilege escalation and unauthorized access. They hinder the ability to uniformly enforce security policies and audit access controls. Adopting managed policies instead supports compliance with standards like ISO 27001 and facilitates easier policy management and auditing.
  description: This rule checks for the presence of inline policies attached directly to IAM roles in GCP. Inline policies create tightly coupled configurations that are harder to manage and audit. To verify, examine IAM roles and ensure policies are defined as standalone entities and not embedded directly in the role. Remediation involves migrating inline policies to standalone IAM policies and attaching them to roles, improving manageability and security oversight.
  references:
  - https://cloud.google.com/iam/docs/policies#inline_policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.iam.role.identity_access_rbac_attached_policies_not_admin_star
  service: iam
  resource: role
  requirement: Identity Access RBAC Attached Policies Not Admin Star
  scope: iam.role.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure IAM Roles Avoid Admin * Permissions
  rationale: Assigning overly permissive roles with wildcard permissions like '*' can lead to unauthorized access and potential data breaches. This exposure may violate compliance standards such as PCI-DSS and SOC2, and can result in significant financial and reputational damage if exploited by malicious actors.
  description: This rule checks for IAM roles that have attached policies with overly broad permissions such as 'admin *'. These permissions should be avoided because they grant more access than necessary, contradicting the principle of least privilege. Review and modify the IAM roles to ensure they only include necessary permissions. This can be verified by listing all IAM roles and inspecting their associated policies, ensuring no wildcards are present in critical permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/least-privilege
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.iam.role.identity_access_rbac_no_inline_policies
  service: iam
  resource: role
  requirement: Identity Access RBAC No Inline Policies
  scope: iam.role.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Avoid Inline Policies in IAM Roles
  rationale: Inline policies attached directly to IAM roles can lead to complex policy management and potential security risks, as they may not be centrally managed or easily reviewed. This poses a risk of unintended privilege escalation or unauthorized access, impacting compliance with security standards such as NIST and ISO 27001.
  description: This rule checks for inline policies directly attached to IAM roles in GCP. Inline policies should be avoided in favor of using managed policies, which offer better control and auditing capabilities. To verify, inspect IAM roles for any inline policies and migrate them to managed policies. This enhances security posture by simplifying policy management and ensuring consistent access controls.
  references:
  - https://cloud.google.com/iam/docs/policies#overview
  - https://cloud.google.com/iam/docs/creating-managing-policies
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 1.6
  - NIST SP 800-53 Rev. 5
  - ISO/IEC 27001:2013
- rule_id: gcp.iam.role.identity_access_rbac_session_duration_reasonable__applicable
  service: iam
  resource: role
  requirement: Identity Access RBAC Session Duration Reasonable Applicable
  scope: iam.role.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Reasonable IAM Role Session Duration
  rationale: Restricting IAM role session duration reduces the risk of unauthorized access and minimizes the impact of credential theft. Shorter sessions limit the time window in which compromised credentials can be exploited, aligning with regulatory requirements and best practices for secure identity management.
  description: This rule checks for IAM roles with session durations exceeding recommended limits. Session durations should be set to a reasonable timeframe, typically not exceeding an hour, to balance usability and security. Administrators can verify and adjust session durations through the IAM settings in the Google Cloud Console or via gcloud commands. Reducing session duration is a key step in mitigating extended unauthorized access risks.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 1.11
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.iam.role.identity_access_trust_principals_allowlist_only
  service: iam
  resource: role
  requirement: Identity Access Trust Principals Allowlist Only
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict IAM Role Trust Principals to Allowlist
  rationale: Restricting IAM role trust principals to a predefined allowlist reduces the risk of unauthorized access by ensuring that only approved and vetted identities can assume roles. This control mitigates the threat of privilege escalation and data breaches, aligning with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks if IAM roles in GCP have trust policies that only allow identities from a specified allowlist to assume them. Verify that role bindings are limited to known and necessary service accounts or identities. Remediation involves auditing existing IAM roles and updating their trust policies to include only approved principals, preventing unauthorized access.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/allow-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.role.identity_access_trust_requires_external_id_or_audi_supported
  service: iam
  resource: role
  requirement: Identity Access Trust Requires External Id Or Audi Supported
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Identity Access Trust Uses External Id or Audi
  rationale: Enforcing the use of External Id or Audi ensures that trust relationships with external identities or services are explicit and verified, preventing unauthorized access. This protects sensitive resources from impersonation attacks and meets compliance requirements for secure identity management, such as those outlined in ISO 27001 and SOC 2.
  description: This rule checks if roles requiring identity access trust have either an External Id or Audi configuration. External Ids and Audi claims facilitate secure and verified connections between GCP resources and external identities or services. To verify, review IAM role configurations for the presence of these identifiers. Remediation involves updating IAM role trust policies to include External Id or Audi claims, strengthening your security posture against unauthorized access.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/docs/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.role.kms_enforce_separation_of_duties
  service: iam
  resource: role
  requirement: KMS Enforce Separation Of Duties
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure KMS Roles Enforce Separation of Duties
  rationale: Implementing separation of duties in KMS roles mitigates the risk of unauthorized access and data breaches by distributing responsibilities among multiple individuals. This approach reduces the potential for insider threats and aligns with compliance requirements, such as PCI-DSS and ISO 27001, promoting secure key management practices.
  description: This rule checks that roles associated with Google Cloud Key Management Service (KMS) are configured to enforce separation of duties by ensuring that critical permissions are not concentrated within a single role. Verify that roles are defined to separate key creation, rotation, and usage tasks among different users or service accounts. To remediate, review IAM policies and adjust role assignments to comply with this principle, ensuring no single role has excessive permissions.
  references:
  - https://cloud.google.com/kms/docs/separation-of-duties
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.nist.gov/itl/ssd/software-quality-group/separation-duties
- rule_id: gcp.iam.role.least_privilege
  service: iam
  resource: role
  requirement: Least Privilege
  scope: iam.role.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure IAM Roles Follow Least Privilege Principle
  rationale: Implementing the least privilege principle minimizes the risk of unauthorized access and potential data breaches, which can lead to financial losses and compliance penalties. By restricting roles to only the permissions necessary for their tasks, organizations reduce the attack surface and protect sensitive data from malicious actors.
  description: This rule checks if IAM roles in GCP are configured with permissions that adhere to the least privilege principle. Review roles to ensure they do not have more permissions than necessary for their intended purpose. Remediation involves auditing role permissions and adjusting them to align with specific job functions, ensuring they are not overly permissive. Verification can be conducted via the IAM section in the GCP Console or using gcloud CLI commands to list and review role permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.iam.role.sa_enforce_separation_of_duties
  service: iam
  resource: role
  requirement: Sa Enforce Separation Of Duties
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Separation of Duties for Service Accounts
  rationale: Implementing separation of duties for service accounts in GCP minimizes the risk of privilege misuse and unauthorized access. By ensuring that critical tasks are divided among different accounts, organizations can prevent a single point of failure and reduce insider threats. This practice supports compliance with regulatory standards such as ISO 27001 and NIST SP 800-53, which require strict access control measures.
  description: This rule checks that GCP service accounts are configured to enforce separation of duties by restricting permissions to necessary roles only. Ensure service accounts do not have excessive or overlapping privileges by regularly auditing and adjusting IAM roles. Remediation involves reviewing service account roles via the IAM console or CLI and removing any unnecessary permissions, aligning roles with the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/service-accounts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.iam.service_account.access_approval_enabled
  service: iam
  resource: service_account
  requirement: Access Approval Enabled
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Access Approval is Enabled for Service Accounts
  rationale: Enabling Access Approval for service accounts mitigates unauthorized access by requiring explicit approval before access is granted to sensitive resources. This reduces the risk of data breaches and unauthorized operations, aligning with compliance requirements such as GDPR and SOC2, which mandate strict access controls and audit capabilities.
  description: This rule verifies that Access Approval is enabled for all Google Cloud service accounts to enforce a secondary approval process before granting access to sensitive resources. Administrators need to configure Access Approval in the Google Cloud Console under 'Security' settings for each service account. Regularly review and audit Access Approval configurations to ensure compliance with organizational security policies and standards.
  references:
  - https://cloud.google.com/access-approval/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/service-accounts
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.service_account.account_key_rotation_90_days_enforced
  service: iam
  resource: service_account
  requirement: Account Key Rotation 90 Days Enforced
  scope: iam.service_account.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Enforce 90-Day Rotation for Service Account Keys
  rationale: Regularly rotating service account keys helps mitigate the risk of key compromise, which could lead to unauthorized access to resources. This practice aligns with security best practices and compliance mandates such as PCI-DSS and ISO 27001, reducing the window of opportunity for malicious actors exploiting stale keys.
  description: This rule checks for service accounts in GCP that do not have enforced key rotation within 90 days. Service account keys should be rotated regularly to prevent unauthorized access due to key exposure. Verify key rotation policies in the IAM settings and update any service accounts to ensure keys are rotated every 90 days. Remediation involves using the Google Cloud Console or gcloud command-line tool to implement and manage key rotation policies.
  references:
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys#best_practices
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.iam.service_account.account_role_check_admin_privileges
  service: iam
  resource: service_account
  requirement: Account Role Check Admin Privileges
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Service Accounts Lack Admin Privileges
  rationale: Service accounts with excessive privileges pose a severe security risk, as they can be exploited to gain unauthorized access to resources, escalate privileges, and compromise the entire cloud environment. Adhering to the principle of least privilege is crucial for minimizing attack surfaces and meeting compliance requirements such as PCI-DSS and NIST.
  description: This rule checks that service accounts do not possess admin roles or equivalent high-privilege roles, which could lead to unauthorized access and control over GCP resources. To verify, review IAM policies and ensure that service accounts are granted only the necessary roles for their intended function. Remediation involves auditing current permissions, removing unnecessary admin roles, and using custom roles to enforce least privilege.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.service_account.account_sod_configured
  service: iam
  resource: service_account
  requirement: Account Sod Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Separation of Duties for Service Accounts
  rationale: Separation of duties (SoD) for service accounts mitigates the risk of privilege misuse by ensuring that no single service account has excessive permissions that could compromise security. Misconfigured service accounts can lead to unauthorized access and data breaches. Properly configured SoD aligns with compliance requirements such as PCI-DSS and ISO 27001, reducing the risk of non-compliance penalties.
  description: This rule checks whether service accounts are configured with separation of duties to minimize security risks. It verifies that no single service account has overly broad permissions that could allow unauthorized actions. To remediate, review permissions for each service account, and ensure roles such as 'editor' and 'owner' are not assigned to service accounts unnecessarily. Implement least privilege by assigning only essential roles and monitor changes using audit logs.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/iam/docs/iam-best-practices
- rule_id: gcp.iam.service_account.account_user_managed_keys_absent
  service: iam
  resource: service_account
  requirement: Account User Managed Keys Absent
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Absence of User-Managed Keys for Service Accounts
  rationale: User-managed keys for service accounts pose a security risk as they can be easily exposed, leading to unauthorized access. This exposure can result in data breaches or service disruptions, violating compliance with standards like PCI-DSS and ISO 27001. Ensuring keys are only managed by Google mitigates these risks and aligns with best practices for secure identity management.
  description: This rule checks for the presence of user-managed keys for Google Cloud Platform service accounts. User-managed keys should be avoided because they require manual management, increasing the risk of mismanagement or accidental exposure. To verify, list the service account keys and ensure all keys are Google-managed. If user-managed keys are detected, delete them and recreate the access using Google-managed keys to enhance security and compliance.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/docs/identity/users-managed-keys-best-practices
- rule_id: gcp.iam.service_account.administrative_privileges
  service: iam
  resource: service_account
  requirement: Administrative Privileges
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Admin Privileges for Service Accounts
  rationale: Service accounts with administrative privileges pose a significant security risk as they can be exploited by attackers to access sensitive resources and perform unauthorized actions. Limiting these privileges reduces the attack surface and helps prevent potential data breaches and compliance violations under frameworks like NIST and PCI-DSS.
  description: This rule checks for service accounts that have been granted excessive administrative privileges, which violate the principle of least privilege. It is crucial to audit permissions regularly and ensure service accounts have only the necessary permissions for their functions. Remediation involves reviewing roles attached to service accounts and removing unnecessary admin-level roles, such as 'roles/owner' or 'roles/editor', replacing them with more granular roles as needed.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.iam.service_account.identity_access_instance_profile_no_instance_profile_wi_star
  service: iam
  resource: service_account
  requirement: Identity Access Instance Profile No Instance Profile Wi Star
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Instance Profile Wildcard Permissions on Service Accounts
  rationale: Allowing wildcard permissions on IAM instance profiles for service accounts can lead to excessive privilege escalation. This misconfiguration may allow services to perform unauthorized actions and access sensitive data, increasing the risk of data breaches and non-compliance with standards like PCI-DSS and ISO 27001.
  description: This rule checks for service accounts that have IAM instance profiles with wildcard permissions ('*') that are overly permissive. To verify, review the IAM policies associated with each service account and ensure that specific permissions are granted rather than wildcards. Remediation involves refining IAM policies to grant only the necessary permissions required for the service account's intended tasks, following the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/service-accounts
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.iam.service_account.identity_access_instance_profile_role_attached
  service: iam
  resource: service_account
  requirement: Identity Access Instance Profile Role Attached
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Service Account Has No Instance Profile Role Attached
  rationale: Attaching an instance profile role to a service account can lead to privilege escalation, where unauthorized users may gain access to resources beyond their intended scope. This misconfiguration poses a security risk by potentially allowing lateral movement within the cloud infrastructure, which can lead to data breaches and non-compliance with regulations such as PCI-DSS and SOC2.
  description: This rule checks for service accounts with identity access to instance profile roles attached, which should be avoided to minimize risks of privilege escalation. Verify that no service accounts have roles granting excessive permissions by inspecting the IAM policy bindings. If such a role is found, detach it to limit access. Regularly audit IAM policies and utilize least privilege principles to ensure service accounts have only necessary permissions.
  references:
  - https://cloud.google.com/iam/docs/service-accounts
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.iam.service_account.identity_access_instance_profile_role_policies_lea_privilege
  service: iam
  resource: service_account
  requirement: Identity Access Instance Profile Role Policies Lea Privilege
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict Service Account Role to Least Privilege
  rationale: Allowing excessive permissions to service accounts can lead to unauthorized access and privilege escalation, posing significant security risks such as data breaches and compromised resources. Ensuring least privilege aligns with compliance requirements such as PCI-DSS and ISO 27001, reducing the attack surface and safeguarding sensitive operations.
  description: This rule checks for service accounts assigned with IAM roles that exceed the necessary permissions required for their function. It verifies that no service account has over-privileged roles such as 'Owner' or 'Editor'. To remediate, review and adjust IAM policies to ensure each service account has only the permissions necessary for its specific tasks. This can be done through the GCP Console or gcloud CLI by auditing current role assignments and modifying them accordingly.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/least-privilege
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.service_account.identity_access_keys_not_used_or_rotated_90_days_or_less
  service: iam
  resource: service_account
  requirement: Identity Access Keys Not Used Or Rotated 90 Days Or Less
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Service Account Keys Rotated Every 90 Days
  rationale: Regular rotation of service account keys mitigates the risk of unauthorized access due to key compromise. Stale or unused keys can be exploited by attackers to gain prolonged access to critical resources, potentially leading to data breaches or service disruptions. Compliance with regulatory frameworks often requires strict key management practices, ensuring the organization meets legal and industry standards.
  description: This rule checks for service account identity access keys in Google Cloud Platform that have not been used or rotated within the last 90 days. To verify compliance, review the 'Last Used' and 'Created' timestamps of all service account keys. Remediation involves disabling and deleting any stale keys and generating new ones as necessary, ensuring that key access is logged and monitored. Implement automated processes to enforce key rotation policies and reduce human error.
  references:
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.service_account.identity_access_no_user_managed_long_lived_keys
  service: iam
  resource: service_account
  requirement: Identity Access No User Managed Long Lived Keys
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Long-Lived Keys for User-Managed Service Accounts
  rationale: Long-lived keys associated with user-managed service accounts pose a significant security risk as they can be exposed or compromised, leading to unauthorized access. Ensuring that service accounts do not use long-lived keys enhances security by reducing the attack surface and aligning with compliance requirements such as PCI-DSS and ISO 27001, which mandate strict access control and key management practices.
  description: This rule checks for the presence of user-managed long-lived keys in service accounts, which should be avoided to minimize security risks. Instead, use short-lived keys or OAuth tokens for service account access, promoting the use of IAM roles for authentication. Remediation involves removing existing long-lived keys and transitioning to more secure authentication methods by following Google Cloud's best practices for service account key management.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/docs/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.iam.service_account.identity_access_scopes_or_roles_least_privilege
  service: iam
  resource: service_account
  requirement: Identity Access Scopes Or Roles Least Privilege
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Service Account Access Scopes/Roles
  rationale: Limiting access scopes and roles for service accounts is crucial to minimizing the risk of privilege escalation and unauthorized access to sensitive data or resources. Over-permissioned roles can lead to potential data breaches and non-compliance with regulations like GDPR and HIPAA. By adhering to least privilege principles, organizations can better protect their cloud environment and meet compliance requirements.
  description: This rule verifies that all service accounts are assigned the minimal set of access scopes or roles necessary for their function. It checks for overly broad permissions that exceed operational requirements. To ensure least privilege, audit all service accounts and adjust their roles to the minimum required. Regularly review and update these permissions in accordance with any changes in service account usage or organizational policy.
  references:
  - https://cloud.google.com/iam/docs/service-accounts
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - CIS Google Cloud Platform Foundation Benchmark, Section 1.6
  - NIST SP 800-53 Revision 5 - Access Control
  - 'PCI DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.service_account.identity_access_workload_identity_federation_used__supported
  service: iam
  resource: service_account
  requirement: Identity Access Workload Identity Federation Used Supported
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Use of Workload Identity Federation for Service Accounts
  rationale: Using Workload Identity Federation reduces the risk of long-lived credentials being compromised. It allows external identities to access GCP resources without using service account keys, aligning with best practices for secure identity management. This approach also aids in meeting compliance requirements by minimizing the attack surface associated with credential management.
  description: This rule checks whether Workload Identity Federation is used to authenticate workloads using external identities instead of traditional service account keys. To verify, ensure that workloads are set up to utilize Workload Identity Pools and Providers, allowing secure access management. Remediation involves configuring identity pools and updating workloads to authenticate using the federation, removing any direct service account key dependencies.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/identity-security/enabling-keyless-access-to-google-cloud
- rule_id: gcp.iam.service_account.level_service_account_role_configured
  service: iam
  resource: service_account
  requirement: Level Service Account Role Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Service Account Role Configured for Least Privilege
  rationale: Configuring service account roles with the principle of least privilege minimizes the attack surface by restricting permissions to only those necessary for tasks. This reduces the risk of privilege escalation and potential unauthorized access, which can lead to data breaches or service disruptions. Ensuring roles are appropriately configured is also crucial for compliance with standards such as NIST and ISO 27001, which require strict access controls.
  description: This rule checks if service accounts in GCP have been assigned roles that adhere to the principle of least privilege. It verifies that no over-privileged roles are configured, which could grant excessive permissions beyond what is necessary. To remediate, audit all service account roles and ensure each is appropriately scoped, removing any unnecessary permissions. Verification can be done via the GCP Console or gcloud CLI by examining role assignments and adjusting them to align with operational needs.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/iam-best-practices
- rule_id: gcp.iam.service_account.no_guest_accounts_with_permissions_configured
  service: iam
  resource: service_account
  requirement: No Guest Accounts With Permissions Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Guest Accounts from Having Service Account Permissions
  rationale: Assigning permissions to guest accounts poses a significant security risk as it can lead to unauthorized access to sensitive resources. Guest accounts often lack the scrutiny applied to regular user accounts, increasing the likelihood of malicious activity or data breaches. Ensuring that guest accounts do not have permissions aligns with compliance requirements such as NIST SP 800-53 and helps maintain a secure and controlled access environment.
  description: This rule checks for any IAM service accounts that have permissions configured for guest users, which are typically external and not managed by the organization. To verify, review the IAM policy bindings for each service account and ensure no bindings include external or guest users. For remediation, remove any permissions granted to guest accounts and reassign them to appropriate internal users or groups. This helps maintain strict access control and reduces the attack surface.
  references:
  - https://cloud.google.com/iam/docs/service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.iam.service_account.organization_essential_contacts_configured
  service: iam
  resource: service_account
  requirement: Organization Essential Contacts Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Org Essential Contacts for IAM Service Accounts
  rationale: Configuring Organization Essential Contacts for IAM Service Accounts helps ensure that critical notifications, such as security alerts or compliance issues, reach the right stakeholders promptly. This reduces the risk of missed alerts that could lead to data breaches or non-compliance with regulatory frameworks, potentially resulting in financial and reputational damage.
  description: This rule checks whether essential contacts are configured at the organization level for IAM service accounts. Essential contacts receive important notifications about security, privacy, and compliance issues. To verify, ensure that your organization has defined at least one essential contact via the Google Cloud Console under the Essential Contacts section. Remediation involves adding appropriate contacts to receive relevant alerts, ensuring they are kept up to date.
  references:
  - https://cloud.google.com/resource-manager/docs/essential-contacts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloudsecurityalliance.org/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.service_account.role_conflict_configured
  service: iam
  resource: service_account
  requirement: Role Conflict Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Detect and Resolve IAM Role Conflicts for Service Accounts
  rationale: Role conflicts for service accounts can lead to unauthorized access or privilege escalation, increasing the risk of data breaches and non-compliance with regulations. Properly configured roles help to enforce least privilege principles and ensure that service accounts have only the necessary permissions to perform their tasks, reducing the attack surface.
  description: This rule checks for role conflicts in GCP IAM service accounts where multiple roles may grant overlapping or excessive permissions. To verify, audit the roles assigned to service accounts and ensure no conflicting roles are configured. Remediation involves reviewing the roles and removing any redundant or excessive permissions, aligning them with the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/iam/docs/roles-best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.service_account.roles_at_project_level
  service: iam
  resource: service_account
  requirement: Roles At Project Level
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Service Account Roles at Project Level
  rationale: Assigning service account roles at the project level can inadvertently grant excessive permissions across all resources within the project, increasing the risk of privilege escalation and unauthorized access. This can lead to data breaches or service disruptions, affecting business operations and potentially leading to non-compliance with regulations such as PCI-DSS and SOC2.
  description: This rule checks for service accounts that have roles assigned at the project level, which may grant excessive permissions. Ensure that service accounts are only assigned the minimum necessary roles at the most granular level required, such as specific resources or services, to adhere to the principle of least privilege. Remediate by auditing service account permissions and adjusting roles to be more restrictive where possible.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
  - https://cloud.google.com/iam/docs/service-accounts
- rule_id: gcp.iam.service_account.sa_no_administrative_privileges_configured
  service: iam
  resource: service_account
  requirement: Sa No Administrative Privileges Configured
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Administrative Privileges for Service Accounts
  rationale: Service accounts with excessive administrative privileges pose significant security risks, including potential unauthorized access and data exfiltration. Limiting these privileges helps mitigate insider threats and reduces the attack surface. Compliance with frameworks like NIST and ISO 27001 often requires implementing least privilege principles, which is crucial for maintaining a secure cloud environment.
  description: This rule checks if Google Cloud service accounts have been assigned administrative privileges that are not necessary for their function. It verifies the IAM roles and permissions associated with each service account to ensure they adhere to the principle of least privilege. To remediate, review and adjust the IAM policies to remove unnecessary administrative roles from service accounts, ensuring they only have permissions essential to their tasks.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/roles-reference
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.service_account.service_account_read_only_image_repo_access
  service: iam
  resource: service_account
  requirement: Service Account Read Only Image Repo Access
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Read-Only Image Repo Access for Service Accounts
  rationale: Restricting service account permissions to read-only for image repositories reduces the risk of unauthorized image modification or deletion, which could lead to service disruptions or security vulnerabilities. This practice supports compliance with least privilege principles and helps prevent potential data exfiltration or corruption scenarios.
  description: This rule ensures that service accounts have read-only access to image repositories in Google Container Registry (GCR) or Artifact Registry. It checks for policies granting only the 'roles/storage.objectViewer' role to service accounts for these repositories. To verify, review IAM policies for service accounts and ensure no write or admin roles are granted. Remediation involves removing excessive permissions and assigning the correct read-only roles.
  references:
  - https://cloud.google.com/container-registry/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.iam.user.accesskey_unused
  service: iam
  resource: user
  requirement: Accesskey Unused
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Identify and Disable Unused GCP IAM Access Keys
  rationale: Unused IAM access keys pose a significant security risk as they can be exploited in unauthorized access attempts, leading to potential data breaches. Regularly auditing and disabling unused keys helps maintain a secure environment and ensures compliance with security frameworks such as PCI-DSS and ISO 27001, which require strict access controls.
  description: This rule checks for IAM user access keys in GCP that have not been used within a specified period. Identifying these keys is crucial as they may indicate stale or abandoned accounts that could be targeted for exploitation. To mitigate risk, unused access keys should be disabled and rotated regularly. Administrators can use the GCP Console or gcloud CLI to review key usage and take appropriate action.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/docs/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/best-practices-for-enterprise-organizations
- rule_id: gcp.iam.user.console_access_unused
  service: iam
  resource: user
  requirement: Console Access Unused
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Identify Unused IAM User Console Access
  rationale: Unused console access for IAM users can represent a significant security risk, as inactive accounts may be more vulnerable to compromise. Attackers may exploit these neglected accounts to gain unauthorized access to your GCP environment, potentially leading to data breaches or resource misuse. Additionally, maintaining inactive users could lead to non-compliance with various security frameworks that require regular review and deactivation of unused accounts.
  description: This rule checks for IAM users who have not accessed the Google Cloud Console for a specified period, indicating potentially unnecessary access permissions. Regularly reviewing and disabling unused console access helps minimize the attack surface by ensuring that only active users retain access. To verify, review the 'last sign-in' details under IAM users in the GCP Console and disable or delete accounts that have been inactive beyond your organization's defined threshold. Remediation steps include configuring automated alerts for inactive accounts and establishing a process for periodic access reviews.
  references:
  - https://cloud.google.com/iam/docs/manage-users
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.user.group_membership
  service: iam
  resource: user
  requirement: Group Membership
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Users are Assigned to Appropriate IAM Groups
  rationale: User group memberships in IAM are crucial for maintaining least privilege access, reducing the risk of unauthorized access and potential data breaches. Assigning users to the correct groups ensures that permissions are managed centrally and consistently, aligning with organizational security policies and regulatory compliance such as PCI-DSS and ISO 27001.
  description: This rule checks that all users in GCP are part of appropriate IAM groups, ensuring they have the correct access permissions. To verify, review the IAM settings and ensure that users are assigned to predefined groups that match their roles. Remediation involves assigning users to the appropriate group using the Google Cloud Console or gcloud CLI. This process helps streamline permission management and enforce security best practices.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/docs/identity-access-management
  - 'CIS GCP Benchmark: 1.1 Ensure that corporate login credentials are used'
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.user.in_group
  service: iam
  resource: user
  requirement: In Group
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure GCP IAM Users Belong to Appropriate Groups
  rationale: Grouping IAM users within GCP allows for centralized management of access permissions, reducing the risk of excessive privileges and potential data breaches. This practice supports compliance with principle of least privilege and aligns with regulatory standards like PCI-DSS and ISO 27001, ensuring users have only the necessary access for their roles.
  description: This rule checks if IAM users are assigned to appropriate groups within GCP, allowing for streamlined and secure management of permissions. Users not in groups may have unmanaged or over-privileged access. To verify, review IAM settings to ensure all users are correctly grouped, and align their permissions with organizational policies. Remediate by assigning users to relevant groups or creating groups that reflect access requirements.
  references:
  - https://cloud.google.com/iam/docs/groups-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.user.managed_key_rotate_90_days
  service: iam
  resource: user
  requirement: Managed Key Rotate 90 Days
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce 90-Day Rotation for User-Managed IAM Keys
  rationale: Frequent rotation of user-managed IAM keys mitigates the risk of compromised credentials being used for unauthorized access. Stale or long-lived keys can be exploited by attackers, leading to potential data breaches and non-compliance with security standards. Regular key rotation is essential for maintaining strong security posture and adhering to compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks if user-managed IAM keys are rotated within 90 days. It requires configuring IAM policies to enforce key rotation schedules and monitoring key age. To verify, audit IAM key last used dates and ensure automated alerts for keys nearing expiration. Remediation involves setting up automated scripts or using GCP tools to rotate keys and update dependent services, ensuring all keys comply with the 90-day rotation policy.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.user.managed_key_unused
  service: iam
  resource: user
  requirement: Managed Key Unused
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Detect and Manage Unused User-Managed IAM Keys
  rationale: Unused IAM keys can pose a significant security risk as they may become vulnerable to unauthorized access or misuse over time. Regularly auditing and removing unused keys reduces the attack surface and helps maintain compliance with security frameworks such as NIST, PCI-DSS, and ISO 27001, thereby protecting sensitive data and resources.
  description: This rule checks for user-managed IAM keys that have not been used for an extended period, indicating potential neglect or abandonment. To verify, examine the last used timestamp of IAM keys in the GCP console or via the gcloud CLI. If a key is identified as unused for over 90 days, it should be evaluated for necessity and either rotated or deleted to mitigate security risks. Ensuring keys are actively managed aligns with best practices for key management and access control.
  references:
  - https://cloud.google.com/iam/docs/managing-service-account-keys
  - https://cloud.google.com/security/compliance/cis/benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.iam.user.managed_keys
  service: iam
  resource: user
  requirement: Managed Keys
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure No User-Managed Keys in IAM for Enhanced Security
  rationale: User-managed keys introduce significant security risks as they are prone to loss, unauthorized access, and lack of lifecycle management controls. Using these keys can lead to potential data breaches and non-compliance with regulations like PCI-DSS and ISO 27001, which require robust access controls and key management practices.
  description: This rule checks for the presence of user-managed keys in IAM user accounts, as these keys are not centrally managed and can bypass security policies. To verify, audit IAM users for any non-service account keys and transition to using Google-managed keys or service accounts for automated processes. Remediation involves revoking user-managed keys and configuring IAM roles with least privilege using Google-managed keys.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.iam.user.mfa_enabled
  service: iam
  resource: user
  requirement: MFA Enabled
  scope: iam.user.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure MFA is Enabled for All IAM Users
  rationale: Enforcing Multi-Factor Authentication (MFA) for users enhances security by requiring an additional factor beyond just a password, thus protecting against unauthorized access. This is crucial for safeguarding sensitive data and maintaining trust. Non-compliance can lead to data breaches, financial loss, and legal penalties under frameworks like PCI-DSS and SOC2.
  description: This rule checks if MFA is enabled for all Google Cloud IAM users. To verify compliance, confirm that all user accounts have MFA configured using Cloud Identity. Remediation involves setting up and enforcing an MFA policy via the Google Admin console, ensuring that users register their devices and enable MFA. Regular audits should be performed to maintain compliance.
  references:
  - https://cloud.google.com/iam/docs/managing-mfa
  - https://cloud.google.com/security/compliance/cis#cis-gcp-foundations-benchmark-v1-1-0
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://cloud.google.com/security/best-practices/identity-access-management
- rule_id: gcp.iam.user.mfa_enabled_console_access
  service: iam
  resource: user
  requirement: MFA Enabled Console Access
  scope: iam.user.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure MFA for Console Access of IAM Users
  rationale: Multi-Factor Authentication (MFA) significantly enhances security by requiring additional verification beyond just a password, reducing the risk of unauthorized access. This is crucial for protecting sensitive resources and data from identity theft and account breaches, which can lead to financial loss, reputational damage, and non-compliance with standards like NIST SP 800-53 and PCI-DSS.
  description: This rule verifies that all IAM users with console access have MFA enabled. To check compliance, review user security settings in the Google Cloud Console under IAM & Admin > Users, ensuring MFA is configured. Remediation involves enabling MFA through the 'Security' section of the user's profile, requiring additional steps for login verification. This ensures that only authenticated users can access the console, mitigating potential unauthorized access risks.
  references:
  - https://cloud.google.com/iam/docs/mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.user.no_inline_policies
  service: iam
  resource: user
  requirement: No Inline Policies
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict User Inline Policies in IAM
  rationale: Inline policies increase complexity and risk of misconfiguration, leading to excessive permissions and potential unauthorized access. Centralized policy management helps maintain a consistent security posture, ensuring compliance with regulations like ISO 27001 and SOC 2 by reducing risk of privilege escalation.
  description: This rule checks for the presence of inline policies attached directly to IAM users. Inline policies can lead to inconsistent access controls and are harder to audit and manage. Instead, use IAM roles with managed policies to achieve a centralized and standardized permission model. To remediate, identify users with inline policies and migrate these permissions to IAM roles with predefined policies.
  references:
  - https://cloud.google.com/iam/docs/overview#policies
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/iso27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/iam/docs/best-practices
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.iam.user.policy_lowercase
  service: iam
  resource: user
  requirement: Policy Lowercase
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: IAM Policies Should Use Lowercase Identifiers
  rationale: Using lowercase identifiers in IAM policies reduces the risk of configuration errors caused by case sensitivity, preventing unauthorized access due to mismatches in policy definitions. This practice enhances consistency and aligns with best security practices, aiding in maintaining a clear audit trail and compliance with regulatory standards like SOC2 and ISO 27001.
  description: This rule checks IAM policies to ensure all identifiers are in lowercase format. This includes roles, permissions, and other policy components that are case sensitive. To verify, review IAM policy configurations in the GCP Console or via gcloud CLI, ensuring identifiers are uniformly lowercase. Remediation involves updating policy files and applying changes using the gcloud CLI or directly through the GCP Console.
  references:
  - https://cloud.google.com/iam/docs/policies
  - https://cloud.google.com/security/iam/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.user.policy_minimum_length_14
  service: iam
  resource: user
  requirement: Policy Minimum Length 14
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM User Passwords Minimum Length of 14 Characters
  rationale: Setting a minimum password length of 14 characters for IAM users helps mitigate the risk of unauthorized access through brute force attacks. Longer passwords significantly increase the computational effort required to crack them, enhancing the security of sensitive data and systems. This practice is essential for compliance with standards such as NIST SP 800-63 and supports the protection of organizational assets and user identities.
  description: This rule checks whether IAM user passwords adhere to a minimum length of 14 characters. To enforce this policy, configure the password policy within your identity management settings. Verifying compliance involves reviewing IAM password policies and updating them if necessary to meet the required length. Remediation includes accessing the Google Cloud Console, navigating to the IAM settings, and setting the password policy to enforce a minimum length of 14 characters.
  references:
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://cloud.google.com/security/best-practices
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.iam.user.policy_number
  service: iam
  resource: user
  requirement: Policy Number
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit IAM User Policy Bindings to Necessary Minimum
  rationale: Excessive IAM policy bindings for users can lead to unauthorized access and privilege escalation, increasing the risk of data breaches and non-compliance with regulations such as GDPR and ISO 27001. Limiting policy bindings helps maintain the principle of least privilege, reducing the attack surface and potential exploitation of elevated permissions.
  description: This rule checks the number of IAM policy bindings assigned to user accounts in GCP. It identifies users with an excessive number of roles and permissions, which should be minimized to only what is necessary for their job functions. To verify, review IAM policies in the Google Cloud Console under 'IAM & Admin' and remove any unnecessary roles. Remediation involves auditing current permissions and adjusting policy bindings to align with least privilege principles.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/iam/docs/roles-and-permissions
- rule_id: gcp.iam.user.policy_reuse_24
  service: iam
  resource: user
  requirement: Policy Reuse 24
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Unique IAM Policies for User Accounts
  rationale: Reusing IAM policies across multiple user accounts can lead to elevated security risks, including privilege escalation and unauthorized access. Distinct policies help maintain principle of least privilege, reducing the attack surface and fulfilling compliance requirements such as NIST SP 800-53 and ISO 27001. Ensuring unique policies also aids in more effective auditing and incident response.
  description: This rule checks for reused IAM policies among user accounts within GCP to ensure that each user has a unique set of permissions. Verify that IAM roles are not overly permissive and are tailored to the specific needs of each user. Remediation involves reviewing existing IAM policies, identifying shared policies, and creating customized roles to limit permissions according to job functions. Regular audits and updates to IAM policies help maintain compliance and security posture.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.iam.user.policy_reuse_24_gcp_cloud_build_project_no_secrets_i_logging
  service: iam
  resource: user
  requirement: Policy Reuse 24 Gcp Cloud Build Project No Secrets I Logging
  scope: iam.user.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Prevent Secrets in Cloud Build Logs
  rationale: Exposing secrets in Cloud Build logs can lead to unauthorized access and potential data breaches. This poses significant security risks as attackers could gain access to sensitive systems and data, violating compliance requirements such as PCI-DSS and HIPAA. Ensuring secrets are not logged helps protect the organization's infrastructure and maintains trust with clients and stakeholders.
  description: This rule checks Cloud Build configurations to ensure sensitive information such as API keys and passwords are not logged. Verify that steps in Cloud Build do not output secrets to logs by using environment substitutions and secret management best practices. Remediation involves reviewing build configurations for secret exposure and implementing secret manager or environment variables to securely handle sensitive information.
  references:
  - https://cloud.google.com/build/docs/securing-builds/use-secrets
  - https://cloud.google.com/iam/docs/using-iam-securely
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 7.4
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.user.policy_symbol
  service: iam
  resource: user
  requirement: Policy Symbol
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure User IAM Policies Do Not Use Symbols
  rationale: Using symbols in IAM policy names can lead to potential parsing errors and misconfiguration risks, impacting access control and security posture. Properly named policies enhance clarity, reduce human error, and support compliance with standards that require clear and auditable access controls.
  description: This rule checks that IAM policies for users do not contain symbols in their names. Symbols can complicate parsing and integration with other systems, leading to misconfigurations or security loopholes. It is recommended to use alphanumeric characters and underscores for policy names. To remediate, review and rename policies with symbols to a more standardized naming convention, ensuring consistent application of security policies.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.user.policy_uppercase
  service: iam
  resource: user
  requirement: Policy Uppercase
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Use Uppercase for User Identifiers
  rationale: Using consistent uppercase identifiers for IAM policies enhances readability and reduces the risk of human error in policy management. It helps prevent unauthorized access due to misconfiguration and supports compliance with best practices for identity governance. Consistent formatting aids in maintaining a secure, manageable cloud environment.
  description: This check ensures that user identifiers in IAM policies are in uppercase format. Verify policy configurations by reviewing IAM roles and bindings for case consistency. Remediate by updating policies to use uppercase identifiers, ensuring all IAM roles and user accounts comply with this standard. This practice prevents accidental privilege escalation and simplifies auditing processes.
  references:
  - https://cloud.google.com/iam/docs/manage-access
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.user.unused_credentials
  service: iam
  resource: user
  requirement: Unused Credentials
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Identify and Disable Unused GCP IAM User Credentials
  rationale: Unused IAM user credentials pose a significant security risk as they can be exploited by unauthorized users if compromised. These credentials increase the attack surface and can lead to unauthorized access, data breaches, and potential regulatory non-compliance. Regularly reviewing and disabling unused credentials helps mitigate these risks and ensures compliance with security best practices.
  description: This rule checks for IAM user credentials that have not been used within a specified timeframe, indicating they may no longer be necessary. To enhance security, credentials that have been inactive for 90 days or more should be disabled or removed. Administrators can review the 'Last Used' information in the IAM console or via the gcloud CLI to identify such credentials. Remediation involves either confirming the necessity of the credentials or disabling them to prevent unauthorized access.
  references:
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
  - https://cloud.google.com/iam/docs/audit-logging
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0 - 1.6 Ensure that IAM users with administrative privileges do not have API access keys
  - NIST SP 800-53 Rev. 5 - AC-2 Account Management
  - PCI-DSS v3.2.1 - 8.1.4 Remove/disable inactive user accounts within 90 days
  - ISO/IEC 27001:2013 - A.9.2.3 Management of privileged access rights
- rule_id: gcp.iam.workload_identity_pool.identity_access_oidc_allowed_client_ids_audiences_restricted
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Oidc Allowed Client Ids Audiences Restricted
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict OIDC Client IDs and Audiences in Workload Identity Pools
  rationale: Restricting OIDC allowed client IDs and audiences prevents unauthorized access and misuse of identity tokens within your environment. Failure to enforce such restrictions can lead to impersonation attacks and unauthorized access to sensitive resources, posing a significant security threat and potential non-compliance with regulations like NIST and PCI-DSS.
  description: This rule checks that OIDC allowed client IDs and audiences within workload identity pools are explicitly specified and restricted. To verify, ensure that the workload identity pool configuration only includes trusted client IDs and audiences. Remediation involves updating the identity pool settings to define a precise list of allowed client IDs and audiences, thereby minimizing the risk of token misuse.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/architecture/identity/access-management-best-practices#workload-identity-federation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/document_library
- rule_id: gcp.iam.workload_identity_pool.identity_access_oidc_issuer_https_and_matches_discovery
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Oidc Issuer HTTPS And Matches Discovery
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Workload Identity OIDC Issuer Uses HTTPS and Matches Discovery
  rationale: Using HTTPS for OIDC issuers ensures the confidentiality and integrity of identity tokens during transmission, mitigating risks of interception or tampering. Compliance with discovery document specifications helps prevent unauthorized entities from impersonating identity providers, reducing the risk of unauthorized access and data breaches.
  description: This rule verifies that the OIDC issuer URL for a workload identity pool uses HTTPS and matches the discovery document. To ensure secure communication, configure the issuer to use HTTPS and confirm it aligns with the discovery endpoint details. Remediation involves updating any non-compliant issuer URLs to use HTTPS and verifying they match the well-known discovery document hosted by the identity provider.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.locations.workloadIdentityPools
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63c.pdf
  - https://tools.ietf.org/html/rfc8414
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.workload_identity_pool.identity_access_oidc_thumbprints_or_jwks_pinning_configured
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Oidc Thumbprints Or Jwks Pinning Configured
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure OIDC Thumbprints or JWKs Pinning Configured for Workload Identity Pools
  rationale: Configuring OIDC thumbprints or JWKs pinning is crucial to preventing man-in-the-middle attacks and ensuring the integrity of identity tokens used for authentication. Without this configuration, attackers could intercept or tamper with identity assertions, leading to unauthorized access. This setup also aids in meeting compliance requirements by ensuring secure communication channels for identity verification.
  description: This rule checks whether OpenID Connect (OIDC) thumbprints or JSON Web Key Sets (JWKS) pinning is configured for Workload Identity Pools in GCP. To verify, ensure that the OIDC provider's thumbprints or JWKs are specified in the workload identity pool configuration. This can be done via the GCP Console or by using the gcloud CLI. Remediation involves updating the identity pool settings to include the correct thumbprints or JWKs from the identity provider.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/understanding-workload-identity-federation
  - https://csrc.nist.gov/publications/detail/sp/800-63/4/draft
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.workload_identity_pool.identity_access_oidc_token_lifetime_reasonable
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Oidc Token Lifetime Reasonable
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Reasonable OIDC Token Lifetime for Workload Identity Pools
  rationale: Setting a reasonable lifetime for OIDC tokens in Workload Identity Pools is crucial to minimize the risk of token misuse and unauthorized access. Short-lived tokens reduce the window of opportunity for attackers to exploit a compromised token, aligning with security best practices and compliance requirements such as NIST SP 800-63B and PCI-DSS. This control helps maintain a secure identity and access management posture, preventing potential data breaches and unauthorized resource access.
  description: This rule checks that the lifetime of OIDC tokens issued by Workload Identity Pools is set to a reasonable duration, typically no longer than one hour. To verify, ensure that the 'token_lifetime' property is configured within this threshold when creating or updating the identity pool. Remediation involves updating the pool configuration to reduce the token lifetime, thereby enhancing security by limiting token validity. Administrators can use the GCP Console or gcloud CLI to adjust these settings.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/creating-managing-workload-identity-pools
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/identity-access-management
- rule_id: gcp.iam.workload_identity_pool.identity_access_saml_assertion_lifetime_reasonable
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Saml Assertion Lifetime Reasonable
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Reasonable SAML Assertion Lifetime for Workload Identity Pools
  rationale: Setting a reasonable lifetime for SAML assertions in workload identity pools is crucial to minimize the risk of token misuse and unauthorized resource access. Shorter assertion lifetimes reduce the time window for attackers to exploit stolen tokens, thereby enhancing security posture. This practice aligns with compliance requirements such as NIST SP 800-63B, which advocates for stringent identity and authentication controls.
  description: This rule checks that the SAML assertion lifetime for GCP workload identity pools is set to a reasonable duration, typically not exceeding one hour. Administrators can verify this by inspecting the 'session_duration' attribute in the workload identity configuration. To remediate, configure the assertion lifetime using the GCP Console or gcloud CLI to ensure it meets security best practices, reducing potential attack vectors caused by long-lived tokens.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.locations.workloadIdentityPools
  - https://pages.nist.gov/800-63-3/sp800-63b.html
  - https://cloud.google.com/security/best-practices/identity
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
- rule_id: gcp.iam.workload_identity_pool.identity_access_saml_audience_restriction_configured
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Saml Audience Restriction Configured
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SAML Audience Restriction is Configured for Workload Identity Pools
  rationale: Configuring SAML Audience Restrictions for Workload Identity Pools is crucial to prevent unauthorized access by ensuring that tokens are only accepted by intended services. This reduces the risk of identity spoofing and helps organizations comply with regulatory requirements such as NIST SP 800-63B and ISO 27001, which demand strict identity and access management controls.
  description: This rule checks if SAML Audience Restrictions are configured for Workload Identity Pools in GCP. Proper configuration ensures that SAML assertions are only valid for specific intended recipients, mitigating the risk of token misuse. To verify, inspect the Workload Identity Pool's SAML configuration settings in the GCP Console or through the gcloud CLI. Remediation involves updating the SAML provider settings to include the correct audience restriction URIs.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.locations.workloadIdentityPools.providers#Saml
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.workload_identity_pool.identity_access_saml_certificates_not_expired
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Saml Certificates Not Expired
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SAML Certificates in Workload Identity Pools Are Valid
  rationale: Expired SAML certificates in workload identity pools can lead to authentication failures, disrupting service operations and potentially causing unauthorized access if not properly managed. This poses a security risk by allowing attackers to exploit expired certificates for malicious activities. Maintaining valid certificates is crucial for compliance with regulations like NIST and SOC2, which require robust identity and access management practices.
  description: This rule checks that all SAML certificates associated with GCP IAM Workload Identity Pools are valid and not expired. Expired certificates can prevent successful SSO authentication, leading to access issues for services relying on these identities. To verify, regularly audit your certificate expiration dates and replace or renew certificates before they expire. Remediation involves updating the SAML provider configurations with a valid certificate in the GCP Console or via gcloud CLI.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-pools
  - https://cloud.google.com/iam/docs/managing-workload-identity-pools
  - https://cloud.google.com/iam/docs/saml
  - https://www.nist.gov/cyberframework
  - https://www.isc2.org/Certifications/SSCP
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.iam.workload_identity_pool.identity_access_saml_idp_metadata_signed
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Saml Idp Metadata Signed
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SAML IdP Metadata is Signed for Workload Identity Pools
  rationale: Unsigned SAML IdP metadata can lead to security vulnerabilities such as unauthorized access, identity spoofing, and data breaches. Ensuring metadata is signed helps maintain integrity and authenticity, reducing the risk of man-in-the-middle attacks and ensuring compliance with security frameworks like NIST SP 800-63 and ISO 27001.
  description: This rule verifies that SAML IdP metadata associated with GCP Workload Identity Pools is signed, ensuring authenticity and integrity. To check compliance, review the metadata configuration in your identity provider settings. Remediate by configuring your identity provider to sign the SAML metadata, and update the GCP Workload Identity Pool to trust metadata only if it's signed by a trusted certificate authority (CA).
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/manage-workload-identity-pools
  - https://csrc.nist.gov/publications/detail/sp/800-63/4/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.logging.bucket.access_rbac_least_privilege
  service: logging
  resource: bucket
  requirement: Access RBAC Least Privilege
  scope: logging.bucket.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Logging Buckets Use Least Privilege RBAC
  rationale: Implementing least privilege access for logging buckets minimizes the risk of unauthorized data access and potential data breaches. It is crucial for preventing privilege escalation attacks and ensuring compliance with data protection regulations like GDPR and SOX. By enforcing strict access controls, organizations can safeguard sensitive log data that could be leveraged in threat detection and incident response.
  description: This check verifies that all IAM roles assigned to logging buckets adhere to the principle of least privilege. Specifically, it analyzes the roles associated with logging buckets to ensure they grant only the necessary permissions for users and services to perform their tasks. Remediation involves auditing IAM policies and adjusting roles to remove excessive permissions, typically by using predefined roles or creating custom roles tailored to specific needs.
  references:
  - https://cloud.google.com/logging/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/itl/nist-special-publication-800-53
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/resource-manager/docs/access-control-org
- rule_id: gcp.logging.bucket.encryption_at_rest_cmek
  service: logging
  resource: bucket
  requirement: Encryption At Rest Cmek
  scope: logging.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Logging Buckets Use CMEK for Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encryption at rest in logging buckets enhances data security by granting organizations full control over the encryption keys, thus mitigating risks associated with unauthorized access or data breaches. This is crucial for meeting compliance requirements with standards such as ISO 27001 and PCI-DSS, which mandate robust encryption practices to protect sensitive information.
  description: This rule checks if Google Cloud Logging buckets are configured to use Customer-Managed Encryption Keys (CMEK) for data encryption at rest. To verify, check the bucket's encryption settings in the Google Cloud Console or via the gcloud CLI to ensure a CMEK is specified. If not configured, set a CMEK by assigning a Cloud KMS key to the logging bucket, ensuring that the key is properly managed and access is restricted to authorized personnel only.
  references:
  - https://cloud.google.com/logging/docs/audit#encryption
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.logging.bucket.immutability_or_object_lock_where_supported
  service: logging
  resource: bucket
  requirement: Immutability Or Object Lock Where Supported
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Bucket Immutability or Object Lock for GCP Logging
  rationale: Ensuring data immutability helps protect against tampering and unauthorized alterations, enhancing data integrity and compliance with regulations like GDPR and CCPA. This is crucial for audit logs which serve as evidence during forensic investigations or audits, reducing the risk of data breaches and ensuring accountability.
  description: This rule checks if GCP logging buckets have immutability or object lock enabled, where supported. Immutability prevents any modification or deletion of log data for a specified retention period, ensuring data integrity. To verify, check bucket settings in the GCP Console or use the 'gsutil' command-line tool. Remediation involves configuring bucket policies to enable immutability or setting object lock policies to enforce write-once-read-many (WORM) compliance.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
- rule_id: gcp.logging.bucket.permission_changes_enabled
  service: logging
  resource: bucket
  requirement: Permission Changes Enabled
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for Bucket Permission Changes
  rationale: Monitoring permission changes in GCP storage buckets is crucial because unauthorized alterations can lead to data breaches and compliance violations. By logging these changes, organizations can detect and respond to unauthorized access attempts, ensuring data integrity and adherence to security frameworks like NIST and PCI-DSS.
  description: This rule checks if logging is enabled for permission changes on GCP storage buckets. It verifies that the appropriate audit logs are configured to capture IAM policy modifications. To ensure compliance, configure the logging settings in the GCP Console or through the gcloud CLI to track these changes. Remediation involves enabling 'Admin Activity' logs for the storage bucket, ensuring that all permission modifications are recorded and can be audited.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.logging.bucket.read_events_enabled
  service: logging
  resource: bucket
  requirement: Read Events Enabled
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Read Events Logging for GCP Storage Buckets
  rationale: Enabling read events logging for GCP Storage Buckets is crucial for maintaining visibility into data access patterns, detecting unauthorized access, and ensuring compliance with data protection regulations. This facilitates auditing and incident response by providing a detailed record of who accessed which data and when. Without read events logging, organizations risk undetected data exfiltration and non-compliance with regulatory frameworks such as GDPR and CCPA.
  description: This rule checks whether read access logging is enabled for all GCP Storage Buckets. To verify, ensure that Google Cloud Audit Logs are configured to capture 'dataRead' events on storage buckets. Remediation involves enabling Cloud Audit Logging for 'DATA_READ' events by setting up the required IAM permissions and configuring the appropriate log sinks to store these logs in a centralized location for analysis.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/storage/docs/access-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.logging.bucket.retention_days_minimum
  service: logging
  resource: bucket
  requirement: Retention Days Minimum
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Buckets Retain Logs for a Minimum Duration
  rationale: Setting a minimum retention period for logging buckets is crucial for ensuring that audit logs are available for security reviews, incident investigations, and compliance audits. Insufficient retention periods can lead to loss of critical data which may result in undetected security breaches, non-compliance with regulations like GDPR or HIPAA, and potential financial penalties.
  description: This rule checks whether logging buckets in GCP have a minimum retention period configured. Logs should be retained for at least 30 days to ensure they are available for analysis and compliance purposes. To verify, navigate to the Logging > Logs Storage section in the GCP Console and check the retention settings for each bucket. If a bucket's retention period is less than the required minimum, update it to comply with your organizational policies by using the 'gcloud logging buckets update' command.
  references:
  - https://cloud.google.com/logging/docs/buckets
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.hipaajournal.com/hipaa-compliance-requirements/
- rule_id: gcp.logging.bucket.server_access_logging_enabled
  service: logging
  resource: bucket
  requirement: Server Access Logging Enabled
  scope: logging.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Server Access Logging for GCP Storage Buckets
  rationale: Enabling server access logging for GCP storage buckets helps organizations track and analyze access patterns to their data, which is critical for identifying unauthorized access attempts and ensuring accountability. This visibility supports compliance with regulatory requirements such as GDPR and SOC 2, which mandate rigorous data access monitoring. Additionally, it aids in forensic investigations by providing detailed logs of read and write operations.
  description: This rule checks whether server access logging is enabled for Google Cloud Storage buckets. To ensure logging, configure the bucket to write logs to a designated logging bucket. This involves setting the target bucket and ensuring the necessary permissions are granted for log writing. Verification can be done by checking bucket settings in the GCP Console or using the gcloud CLI. Remediation involves enabling logging through the GCP Console under bucket settings or via the gsutil command line tool.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/architecture/using-storage-logs-for-security-and-compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 6.5
  - NIST SP 800-53 Rev. 5 - AU-2, AU-12
  - ISO/IEC 27001:2013 - A.12.4.1
  - https://cloud.google.com/storage/docs/gsutil/commands/logging
- rule_id: gcp.logging.bucket.write_events_enabled
  service: logging
  resource: bucket
  requirement: Write Events Enabled
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Buckets Have Write Events Enabled
  rationale: Enabling write events for logging buckets is crucial for maintaining an audit trail of changes and data writes, which is essential for forensic investigations and compliance audits. Without these logs, unauthorized data alterations or deletions may go undetected, increasing the risk of data breaches and non-compliance with regulations such as PCI-DSS and SOC2.
  description: This rule checks if the write events logging is enabled for all logging buckets within the GCP environment. To verify, ensure that bucket-level logging is configured to capture and store write operations. Remediation involves adjusting the logging configuration to include write events, which can be done via the GCP Console or using gcloud command-line tools. This ensures that any changes to the data are recorded, facilitating monitoring and compliance.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.log.access_rbac_least_privilege
  service: logging
  resource: log
  requirement: Access RBAC Least Privilege
  scope: logging.log.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure RBAC Least Privilege for Log Access
  rationale: Implementing least privilege access for logging ensures that only authorized personnel can view or manipulate logs, reducing the risk of data breaches and insider threats. This approach minimizes the attack surface and aids in meeting compliance requirements such as GDPR and HIPAA, which mandate strict access controls to sensitive information.
  description: This rule checks whether the IAM roles assigned to users accessing GCP logs adhere to the principle of least privilege. Specifically, it verifies that users do not have overly permissive roles like 'Owner' or 'Editor' when not necessary. Remediation involves reviewing IAM policies and assigning more restrictive roles such as 'Log Viewer' to limit access. Validation can be performed through the IAM section in the GCP Console or using gcloud CLI commands to audit current role assignments.
  references:
  - https://cloud.google.com/logging/docs/access-control
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/hipaa/
  - https://csrc.nist.gov/csrc/media/projects/risk-management/file/sp800-53-rev4-final.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.logging.log.logging_configuration_verification
  service: logging
  resource: log
  requirement: Logging Configuration Verification
  scope: logging.log.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Verify GCP Logging Configuration for Audit Logging
  rationale: Proper logging configuration is crucial for detecting unauthorized access and ensuring accountability. It helps organizations meet compliance requirements such as PCI-DSS and ISO 27001 by maintaining a tamper-proof log of security-related events. Failure to verify logging configurations can lead to undetected security incidents and non-compliance with regulatory standards.
  description: This rule checks that all necessary audit logs are enabled in Google Cloud Platform for critical services. Ensure that Data Access audit logs are activated for resources handling sensitive data and Admin Activity logs are on for all services. To verify, navigate to the 'Logging' section in the GCP Console and check the audit log settings for each service. Remediation involves enabling these logs through the GCP Console or using the gcloud CLI to ensure comprehensive logging.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.log.metric_custom_role_changes_alerts_enabled
  service: logging
  resource: log
  requirement: Metric Custom Role Changes Alerts Enabled
  scope: logging.log.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Alerts for GCP Custom Role Changes
  rationale: Monitoring changes to custom roles is critical as it helps detect unauthorized or malicious modifications that could lead to privilege escalation and potential data breaches. Ensuring alerts for these changes helps organizations maintain control over access management and comply with regulatory requirements such as PCI-DSS and ISO 27001, which mandate strict access controls.
  description: This rule verifies that alerts are configured for changes to custom IAM roles in GCP. It checks if log-based metrics are set up to trigger alerts when modifications to these roles occur. To ensure compliance, create a log-based metric on custom role changes in Cloud Logging and set up an alerting policy in Cloud Monitoring to notify security teams of any changes. This setup provides real-time awareness of potential security risks associated with IAM role modifications.
  references:
  - https://cloud.google.com/logging/docs/logs-based-metrics
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-custom-roles
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.logging.log.metric_filter_alerts_vpc_changes
  service: logging
  resource: log
  requirement: Metric Filter Alerts VPC Changes
  scope: logging.log.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Monitor VPC Configuration Changes via Log Metric Alerts
  rationale: Monitoring VPC configuration changes is crucial for identifying unauthorized network access or potential security breaches. Without alerts on such changes, organizations risk exposure to network misconfigurations that can lead to data leaks or compliance violations. Ensuring visibility into these changes supports compliance with standards like PCI-DSS and NIST, which require robust logging and monitoring of network activities.
  description: This rule ensures that a log metric filter is set up to monitor and alert on changes to the Virtual Private Cloud (VPC) configurations in GCP. Specifically, it checks for the presence of a metric filter that captures operations such as creation, deletion, and modification of VPC resources within Cloud Logging. To verify, navigate to the 'Logs-based metrics' page in GCP Logging, and confirm the presence of a metric filter with conditions for VPC changes. Remediation involves creating a logs-based metric and setting up an alerting policy to notify security teams of any unauthorized or suspicious VPC changes.
  references:
  - https://cloud.google.com/logging/docs/logs-based-metrics
  - https://cloud.google.com/vpc/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.logging.log.metric_project_ownership_assignments_monitored
  service: logging
  resource: log
  requirement: Metric Project Ownership Assignments Monitored
  scope: logging.log.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Monitor Project Ownership Assignment Changes in GCP Logs
  rationale: Monitoring project ownership assignments helps detect unauthorized changes that could lead to privilege escalation or data exfiltration. This is crucial for maintaining a secure and compliant environment, as ownership changes can impact access controls and auditability. Regular monitoring aligns with compliance standards like SOC2 and ISO 27001, which require tracking changes to critical GCP resources.
  description: This rule ensures that changes to project ownership are being logged and monitored in GCP, allowing for quick detection and response to unauthorized modifications. To verify, ensure that relevant audit logs are enabled in the Cloud Logging service and that a log metric is set up to capture ownership change events. Remediation involves configuring a log-based metric to trigger alerts when ownership assignments are altered, ensuring continuous oversight and rapid incident response.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/resource-manager/docs/project-move-log
  - CIS Google Cloud Platform Foundation Benchmark
  - NIST SP 800-53
  - PCI-DSS Requirement 10.2
  - ISO/IEC 27001:2013 Clause A.12.4
- rule_id: gcp.logging.log.privacy_audit_logs_centralized_and_encrypted
  service: logging
  resource: log
  requirement: Privacy Audit Logs Centralized And Encrypted
  scope: logging.log.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Centralized and Encrypted Privacy Audit Logs
  rationale: Centralizing and encrypting privacy audit logs is crucial for ensuring data integrity and confidentiality. This approach helps mitigate risks such as unauthorized access or tampering, which can lead to data breaches or compliance failures. It supports regulatory requirements like GDPR and helps in forensic investigations by providing a consistent and secure logging environment.
  description: This rule checks that all privacy audit logs in your GCP environment are centralized in a designated logging project and encrypted using Customer-Managed Encryption Keys (CMEK). Ensure that your logs are routed to a central Cloud Logging bucket with CMEK enabled, which provides you control over the encryption keys. Verify the setup by inspecting the log sinks, and ensure CMEK is applied to the destination bucket.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/cmek
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.logging.log.privacy_audit_retention_days_minimum
  service: logging
  resource: log
  requirement: Privacy Audit Retention Days Minimum
  scope: logging.log.audit_logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Minimum Retention for Privacy Audit Logs
  rationale: Retaining audit logs for a minimum duration is crucial for forensic analysis and compliance with regulatory requirements. Insufficient retention periods could lead to loss of critical audit data, hindering incident investigations and exposing organizations to compliance risks with frameworks like GDPR and HIPAA.
  description: This rule checks if the retention period for privacy audit logs in GCP is set to a minimum number of days. Audit logs should be retained for at least 365 days to ensure compliance and support security investigations. To verify, navigate to the GCP Console, access the Logs Viewer, and check the retention settings for audit logs. If necessary, update the retention period in the Logging configuration settings to meet or exceed the minimum requirement.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/document-1503
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/best-practices
- rule_id: gcp.logging.log.storage_encrypted
  service: logging
  resource: log
  requirement: Storage Encrypted
  scope: logging.log.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Log Storage is Encrypted at Rest
  rationale: Encrypting logs at rest helps protect sensitive information from unauthorized access and breaches, which can lead to significant financial and reputational damage. It is crucial for compliance with data protection laws and regulations such as GDPR, PCI-DSS, and HIPAA, which mandate encryption of stored data to safeguard privacy and confidentiality.
  description: This rule checks whether log data stored in Google Cloud Logging is encrypted at rest using Google-managed keys. To verify, ensure that the default encryption settings are enabled for your logging storage. Remediation involves configuring the logging service to use encryption by default, and optionally, leveraging Customer-Managed Encryption Keys (CMEK) for more granular control of encryption keys.
  references:
  - https://cloud.google.com/logging/docs/storage
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r3.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.metric.alert_policy_audit_configuration_changes
  service: logging
  resource: metric
  requirement: Alert Policy Audit Configuration Changes
  scope: logging.metric.audit_logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Alert on Audit Configuration Changes in Logging Metrics
  rationale: Monitoring audit configuration changes is vital to detect unauthorized modifications that could lead to misconfiguration or data breaches. Such changes may indicate a potential insider threat or a compromised account, impacting regulatory compliance and business integrity. Ensuring audit logs inform on these changes helps maintain accountability and facilitates incident response.
  description: This rule checks for any alterations to the audit logging configurations in GCP logging metrics. It ensures that any changes to the audit settings are logged and generate alerts to notify security teams promptly. To verify, configure alert policies in Cloud Monitoring to trigger when specific audit log entries are created, indicating changes to logging configurations. Remediation involves reviewing the change, identifying the cause, and reverting unauthorized modifications.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/monitoring/alerts
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Control 5.2
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.logging.metric.alert_vpc_firewall_rule_changes
  service: logging
  resource: metric
  requirement: Alert VPC Firewall Rule Changes
  scope: logging.metric.network_security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Alert on VPC Firewall Rule Modifications
  rationale: Monitoring changes to VPC firewall rules is critical as unauthorized alterations can expose sensitive data to unauthorized access and compromise network integrity. Such changes can lead to data breaches or service disruptions, violating compliance requirements like PCI-DSS and ISO 27001, which mandate strict access controls and monitoring.
  description: This rule checks for modifications in VPC firewall rules by leveraging Google Cloud's Logging service to create a metric filter that triggers alerts on any detected changes. Administrators should configure this alert to notify security teams of any rule alterations, enabling immediate investigation and response. Ensure the metric is properly set up in the Logs Explorer, and verify alerting configurations are active and tested. Remediation involves reviewing the change, reverting unauthorized modifications, and updating access controls as needed.
  references:
  - https://cloud.google.com/logging/docs/logs-based-metrics
  - https://cloud.google.com/firewalls/docs/rules-overview
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.metric.iam_permission_changes_storage
  service: logging
  resource: metric
  requirement: IAM Permission Changes Storage
  scope: logging.metric.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Log IAM Permission Changes for Storage Monitoring
  rationale: Monitoring IAM permission changes is crucial as unauthorized access or privilege escalations can lead to data breaches and compliance violations. By logging these changes, organizations can detect and respond to suspicious activities, ensuring data integrity and alignment with regulatory requirements such as GDPR and SOC2.
  description: This rule checks whether audit logs for IAM permission changes are enabled and stored for GCP Storage resources. To verify, ensure that logging is configured to capture 'admin_activity' logs and that logs are retained for an appropriate period. Remediation involves setting up a log sink to export these logs to a secure storage bucket, enabling alerts for any unauthorized changes.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.logging.metric.log_metric_filter_alarm_configured
  service: logging
  resource: metric
  requirement: Log Metric Filter Alarm Configured
  scope: logging.metric.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Log Metric Filter Alarms Are Configured
  rationale: Configuring alarms for log metric filters is crucial for proactive threat detection and timely response to potential security incidents. Without these alarms, critical events might go unnoticed, leading to data breaches, operational disruptions, or compliance violations. Properly configured alarms support compliance with standards like PCI-DSS and ISO 27001 by ensuring that anomalies are promptly identified and addressed.
  description: This rule checks whether alerting policies are configured for log metric filters in Google Cloud Logging. Alarms should be set up to notify administrators when specific audit logs indicate potential security threats or policy violations. To verify, review the list of log metrics in Cloud Monitoring and ensure that each critical metric has an associated alerting policy. Remediation involves creating alerting policies in Cloud Monitoring that trigger notifications based on log metrics thresholds or conditions.
  references:
  - https://cloud.google.com/logging/docs/logs-based-metrics
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.audit_logging_enabled
  service: logging
  resource: sink
  requirement: Audit Logging Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Audit Logging is Enabled for Log Sinks
  rationale: Enabling audit logging for log sinks is crucial for tracking access and changes to logging configurations, which can prevent unauthorized modifications and data exfiltration. Audit logs provide visibility into who accessed what resources and when, aiding in forensic analysis and regulatory compliance. Failure to enable audit logging can result in undetected malicious activity and non-compliance with standards like PCI-DSS and HIPAA.
  description: This rule checks whether audit logging is enabled for all configured log sinks in GCP. Audit logging must be activated to capture and store access logs and configuration changes. To verify, ensure that each sink has a corresponding audit log with appropriate permissions. Remediation involves configuring audit logs in the IAM & Admin section of the GCP Console and ensuring that all necessary permissions are granted for logging.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.sink.bucket_retention_policy_locked
  service: logging
  resource: sink
  requirement: Bucket Retention Policy Locked
  scope: logging.sink.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Bucket Retention Policy is Locked for Logging Sinks
  rationale: Locking the bucket retention policy for logging sinks ensures that log data is retained for the required period, protecting against accidental or malicious deletion. This is crucial for forensic analysis, compliance with data retention laws, and maintaining audit trails crucial for security investigations and regulatory requirements.
  description: This rule checks if the retention policy for buckets used by logging sinks is locked, preventing changes to the retention period. To verify, ensure that the 'isLocked' property of the bucket's retention policy is set to true. Remediation involves updating the bucket's IAM policy to lock the retention, ensuring it cannot be reduced or removed, thus preserving the integrity of the log data.
  references:
  - https://cloud.google.com/logging/docs/best-practices
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.logging.sink.changes_enabled
  service: logging
  resource: sink
  requirement: Changes Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Change Auditing for Logging Sinks
  rationale: Enabling change auditing for logging sinks is crucial for tracking modifications that could affect logging integrity and availability. Unauthorized or accidental changes to logging sinks could lead to loss of critical log data, impacting forensic investigations and compliance with regulatory requirements such as PCI-DSS and ISO 27001. Continuous auditing helps in identifying and mitigating potential security threats by maintaining a comprehensive change history.
  description: This rule checks whether change auditing is enabled for logging sinks in GCP. It ensures that any modifications to sink configurations are logged and monitored. To verify, ensure that Cloud Audit Logs are configured to capture 'Admin Activity' logs for logging sinks. Remediation involves setting up appropriate IAM permissions and configuring logging sinks to capture and store audit logs in a secure location, such as a dedicated Google Cloud Storage bucket or Pub/Sub topic.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/iam/docs/monitoring-iam
- rule_id: gcp.logging.sink.configuration_changes_enabled
  service: logging
  resource: sink
  requirement: Configuration Changes Enabled
  scope: logging.sink.configuration_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sink Configuration Changes Are Monitored
  rationale: Monitoring configuration changes to logging sinks is crucial for maintaining the integrity and reliability of audit logs. Unauthorized or accidental modifications to sink configurations can disrupt log ingestion, leading to potential blind spots in security monitoring and compliance reporting. Ensuring changes are logged helps in forensic analysis and demonstrates accountability, aligning with regulatory standards.
  description: This rule checks whether configuration changes to logging sinks are being monitored and logged. The Google Cloud Logging service should be configured to track changes to sinks, which are responsible for exporting logs to specified destinations. To verify this, ensure that audit logging is enabled for the Google Cloud Logging service, particularly focusing on 'ADMIN_READ' and 'ADMIN_WRITE' operations. Remediation involves enabling and configuring audit logging for these operations through the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/export/configure_export_v2
- rule_id: gcp.logging.sink.configuration_configured
  service: logging
  resource: sink
  requirement: Configuration Configured
  scope: logging.sink.configuration_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sinks Are Properly Configured
  rationale: Proper configuration of logging sinks in GCP is crucial for security and compliance as it ensures that all relevant logs are captured and routed to appropriate destinations for analysis and alerting. Misconfigured sinks can lead to gaps in log data, making it difficult to detect and respond to security incidents, potentially resulting in data breaches or compliance violations with frameworks such as PCI-DSS, HIPAA, or ISO 27001.
  description: This rule checks if logging sinks in Google Cloud Platform are configured to capture and route logs from all relevant resources. Verify that sinks are set up to export logs to Cloud Storage, BigQuery, or Pub/Sub for analysis and long-term storage. Ensure that the sinks are correctly filtering and including necessary log types like admin activity and data access logs. Remediation involves auditing current sink configurations and updating them to cover all critical resources and log types without gaps.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/index.html
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.logging.sink.cross_account_destinations_restricted
  service: logging
  resource: sink
  requirement: Cross Account Destinations Restricted
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict GCP Logging Sinks to Same Account Destinations
  rationale: Allowing logging sinks to export data to cross-account destinations can lead to unauthorized data exposure, violating data governance policies and increasing the risk of data breaches. Restricting this ensures that log data remains within the same GCP account, aligning with best practices for data protection and compliance with regulations such as GDPR, which mandate strict control over data transfers.
  description: This rule checks if logging sinks in GCP are configured to export logs only within the same account. Logs exported to cross-account destinations could be accessed by unauthorized users, leading to potential data leaks. To verify, inspect the 'destination' field in the sink configuration and ensure it points to a resource within the same account. Remediate by updating the sink destination to a compliant storage location such as a Cloud Storage bucket, Pub/Sub topic, or BigQuery dataset within the same GCP account.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/architecture/best-practices-for-logging
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0, Section 4.5
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.destination_encrypted
  service: logging
  resource: sink
  requirement: Destination Encrypted
  scope: logging.sink.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Logging Sink Destination is Encrypted
  rationale: Encrypting log sink destinations is crucial for protecting sensitive data from unauthorized access and ensuring compliance with data protection regulations. Without encryption, log data stored in destinations such as Cloud Storage or BigQuery could be exposed to potential data breaches, leading to financial loss, reputational damage, and non-compliance with standards like PCI-DSS and HIPAA.
  description: This rule checks that all logging sink destinations are encrypted using Google-managed or customer-managed encryption keys. To verify, ensure that encryption is enabled at the storage level of the destination, such as Cloud Storage buckets or BigQuery datasets. Remediation involves configuring these destinations to use encryption keys for data protection, which can be done via the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/security/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.sink.destination_endpoint_private_networking
  service: logging
  resource: sink
  requirement: Destination Endpoint Private Networking
  scope: logging.sink.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Logging Sink Uses Private Networking for Destination
  rationale: Routing logs through private networking minimizes exposure to public internet threats, reducing the risk of data interception and unauthorized access. This configuration aligns with compliance requirements such as PCI-DSS and HIPAA, which mandate secure transmission of sensitive data. Additionally, it helps maintain the integrity and confidentiality of log data critical for auditing and forensic analysis.
  description: This rule checks whether logging sinks in GCP are configured to route their destination endpoints via private networking, such as VPC Service Controls. To verify, ensure that the destination endpoint of your logging sink is a private IP or within a configured private network. Remediation involves updating the logging sink configuration to use a private IP address or ensuring it communicates through a VPC connector. This setup enhances security by preventing data from being exposed to the public internet.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/vpc-service-controls/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.logging.sink.destination_iam_policy_least_privilege
  service: logging
  resource: sink
  requirement: Destination IAM Policy Least Privilege
  scope: logging.sink.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege on Logging Sink IAM Policies
  rationale: Applying the principle of least privilege to logging sink IAM policies minimizes security risks by ensuring that only authorized users have access to sensitive logging data. Without proper access controls, there's potential for unauthorized data exposure, leading to compliance violations with standards like GDPR or HIPAA, and increased risk of data breaches.
  description: This rule checks if IAM policies attached to logging sinks grant only the necessary permissions for the intended operations. It ensures that roles like 'roles/logging.viewer' or 'roles/logging.privateLogViewer' are not overly permissive. Verify by reviewing IAM policies on each sink in the Cloud Logging section of the GCP Console and remove any roles that provide more access than necessary. Implement least privilege by granting the minimal set of permissions required for each user or service account interacting with the logging sink.
  references:
  - https://cloud.google.com/logging/docs/access-control
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.nist.gov/cyberframework
- rule_id: gcp.logging.sink.destination_least_privilege
  service: logging
  resource: sink
  requirement: Destination Least Privilege
  scope: logging.sink.least_privilege
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: high
  title: Ensure Sink Destination Uses Least Privilege Principle
  rationale: Applying the least privilege principle to logging sink destinations is critical to minimize potential security breaches, data leaks, or unauthorized data access. Overprivileged access could lead to unintentional data modification or exposure, threatening compliance with standards such as PCI-DSS and HIPAA. Implementing strict access controls helps protect sensitive data and ensures adherence to regulatory requirements.
  description: This rule checks that all logging sinks in GCP are configured with the least privilege principle, ensuring that only necessary permissions are granted to the sink destination. To verify, review the IAM roles assigned to the sink destination and remove any superfluous permissions. Remediation involves aligning permissions with specific data access and processing needs, and periodically auditing these permissions to ensure ongoing compliance.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2#destination
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.destination_tls_min_1_2_enforced
  service: logging
  resource: sink
  requirement: Destination TLS Min 1 2 Enforced
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure TLS 1.2+ for Logging Sink Destinations
  rationale: Enforcing TLS 1.2 or higher for logging sink destinations ensures data is securely transmitted, protecting against man-in-the-middle attacks and unauthorized data access. This compliance measure is vital for maintaining data integrity and confidentiality, aligning with industry standards and regulatory frameworks such as PCI-DSS and HIPAA.
  description: This rule checks that all logging sinks in GCP enforce a minimum of TLS 1.2 when transmitting logs to their destinations. To verify, review the sink configurations in the Google Cloud Console or via gcloud CLI to ensure TLS settings specify version 1.2 or higher. Remediation involves updating the sink configuration to enforce TLS 1.2+; this can be done by modifying the sink destination's security settings to meet the required TLS version.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.logging.sink.instance_configuration_change_alert_enabled
  service: logging
  resource: sink
  requirement: Instance Configuration Change Alert Enabled
  scope: logging.sink.configuration_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Alerts for Instance Configuration Changes in Logging Sinks
  rationale: Configuring alerts for instance configuration changes in logging sinks is crucial for maintaining system integrity and preventing unauthorized changes. Unmonitored changes can lead to potential security breaches, data loss, or compliance violations, exposing the organization to significant financial and reputational risks. Alerts ensure that security teams are promptly informed of any suspicious activities, facilitating rapid response and compliance with regulatory requirements.
  description: This rule checks whether alerts are enabled for configuration changes in GCP logging sinks. To verify, ensure that audit logs are configured to capture configuration changes and that notifications are set up to alert the security team. Remediation involves enabling audit logging for the sink resource and configuring alert policies in Cloud Monitoring to notify stakeholders of any changes. This setup helps in maintaining a secure and compliant cloud environment by tracking alterations to logging configurations.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - CIS GCP Foundation Benchmark v1.3.0 - Section 4.2
  - NIST SP 800-53 Rev. 5 - AU-12 Audit Generation
  - https://cloud.google.com/monitoring/alerts
  - PCI-DSS Requirement 10.2 - Implement automated audit trails
- rule_id: gcp.logging.sink.kms_encryption_enabled
  service: logging
  resource: sink
  requirement: KMS Encryption Enabled
  scope: logging.sink.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption is Enabled for Logging Sinks
  rationale: Enabling KMS encryption for logging sinks is crucial to protect sensitive log data at rest. This prevents unauthorized access and tampering, mitigates the risk of data breaches, and ensures compliance with data protection laws such as GDPR and CCPA. It also aligns with security best practices and industry standards, reducing potential financial and reputational harm.
  description: This rule checks that Cloud Logging sinks have CMEK (Customer-Managed Encryption Keys) enabled. To verify, ensure that each logging sink is configured with a KMS key by reviewing the 'destination' and 'filter' fields in the sink configuration. Remediation involves updating or creating a sink to specify a KMS key for encryption in the 'kmsKeyName' parameter. This enhances the security of log data by leveraging GCP's encryption capabilities.
  references:
  - https://cloud.google.com/logging/docs/export/configure_storage
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0 - 7.1
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.logging.sink.log_analysis_enabled
  service: logging
  resource: sink
  requirement: Log Analysis Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Log Analysis is Enabled for Logging Sinks
  rationale: Enabling log analysis for logging sinks helps organizations monitor and audit activities in their GCP environment. This ensures timely detection and response to potential security incidents, supports forensic investigations, and fulfills regulatory compliance requirements such as PCI-DSS and SOC2, which mandate comprehensive logging and monitoring practices.
  description: This rule checks whether log analysis is enabled for all logging sinks in GCP. Logging sinks must be configured to export logs to BigQuery, Pub/Sub, or Cloud Storage for further analysis. To verify, ensure that the sink's destination is correctly set and that all required logs are being captured. Remediation involves configuring the sink in the Cloud Console or using gcloud CLI to direct logs to a supported analysis destination.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/socforserviceorganizations.html
  - https://cloud.google.com/architecture/logging
  - https://cloud.google.com/security
- rule_id: gcp.logging.sink.log_file_validation_enabled
  service: logging
  resource: sink
  requirement: Log File Validation Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Log File Validation for Logging Sinks
  rationale: Enabling log file validation ensures the integrity of logs by detecting unauthorized changes. This is critical for compliance with regulations like PCI-DSS and SOC2, which require audit trails to be tamper-proof. Without validation, malicious actors could alter logs to cover their tracks, leading to undetected breaches and financial penalties.
  description: This rule checks if log file validation is enabled for logging sinks in GCP. Log validation uses cryptographic hashes to confirm that log entries have not been altered, ensuring data integrity. To verify, ensure that the `logFileValidation` field is set to true in the sink configuration. Remediation involves configuring your logging sinks to enable log validation, which can be done through the Console or CLI.
  references:
  - https://cloud.google.com/logging/docs/reference/tools/gcloud-logging
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/logging/docs/best-practices
- rule_id: gcp.logging.sink.log_metric_filter_and_alert_for_audit_configuration__enabled
  service: logging
  resource: sink
  requirement: Log Metric Filter And Alert For Audit Configuration Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Log Metric Filter and Alert for Audit Config Changes
  rationale: Monitoring audit configuration changes is crucial for ensuring the integrity and accountability of logging practices in GCP. Unauthorized or unmonitored changes can lead to loss of critical security data, impacting compliance with standards like PCI-DSS and SOC2. This rule helps detect and respond promptly to potential misconfigurations or malicious activities.
  description: This rule checks if a log metric filter and alert is configured for audit configuration changes in GCP. By setting up a filter for 'auditConfig' changes within the logging sink, you can ensure that any alterations to logging configurations are captured and alerted upon. Verify this by checking the existence of a metric filter for 'protoPayload.methodName="SetIamPolicy"' and configure alerts to notify security teams of any changes. To remediate, create a log metric for these changes and set up an alerting policy in GCP Monitoring.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/logging/docs/view/logging-query-language
- rule_id: gcp.logging.sink.log_metric_filter_and_alert_for_project_ownership_ch_enabled
  service: logging
  resource: sink
  requirement: Log Metric Filter And Alert For Project Ownership Ch Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Alerting for Project Ownership Changes via Log Metric Filter
  rationale: Monitoring project ownership changes helps detect unauthorized access and prevent potential misuse of administrative privileges. Such changes can indicate an insider threat or credential compromise, impacting data integrity and compliance with regulations like GDPR and ISO 27001. Proactive alerting ensures timely response to suspicious activities, minimizing business risk.
  description: This rule checks for the presence of a log metric filter and corresponding alert that monitors changes to project ownership. Ensure that a logging sink is configured to capture IAM policy changes, particularly those affecting owners. A metric should be created to filter these logs, and an alerting policy must notify security personnel of such changes. Remediation involves setting up the logging, metric, and alerting configurations in the GCP Console or via gcloud commands.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.logging.sink.logging_enabled
  service: logging
  resource: sink
  requirement: Logging Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Logging is Enabled for GCP Logging Sinks
  rationale: Enabling logging for GCP logging sinks is critical to maintain visibility over data flows and access patterns, which helps identify unauthorized access and potential security breaches. It supports compliance with regulations such as PCI-DSS and GDPR, which mandate detailed audit logging for data access and modification. Without logging, organizations may face challenges in forensic investigations and risk significant penalties for non-compliance.
  description: This rule checks if logging is enabled for all configured logging sinks in GCP projects. Ensuring that logging sinks have logging enabled captures all data flowing through the sinks, which is essential for audit purposes and incident response. To verify, navigate to the GCP Console, access the 'Logging' service, and ensure that log sinks are configured with appropriate logging destinations. Remediate by enabling or configuring logging for any sinks that lack it, using the gcloud logging sinks update command or through the GCP Console.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.multi_region_enabled
  service: logging
  resource: sink
  requirement: Multi Region Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sinks Use Multi-Region Storage
  rationale: Enabling multi-region storage for logging sinks ensures data redundancy and availability across multiple geographical locations, reducing the risk of data loss due to regional outages. This is crucial for maintaining business continuity and meeting compliance requirements that mandate data availability and resilience. Furthermore, it mitigates the impact of localized failures, enhancing the overall reliability of logging systems.
  description: This rule checks if logging sinks are configured to use multi-region storage buckets, which distribute data across multiple regions. Multi-region buckets provide higher availability and durability compared to single-region buckets. To verify, inspect the sink's destination bucket settings to ensure it is configured as a multi-region bucket. Remediation involves updating the sink to point to an appropriate multi-region bucket, which can be done via the Google Cloud Console or using gcloud CLI commands.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/storage/docs/locations#available-locations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/identity-security/google-cloud-security-best-practices-center
- rule_id: gcp.logging.sink.resource_enabled
  service: logging
  resource: sink
  requirement: Resource Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sink Resource is Enabled
  rationale: Enabling logging sinks is crucial for capturing and exporting logs that provide visibility into system activities, which helps in detecting anomalies and unauthorized access. It supports compliance with regulatory requirements by ensuring that audit logs are retained and accessible for auditing purposes. Without enabled logging sinks, organizations risk missing critical security incidents and failing to meet legal obligations.
  description: This rule checks whether logging sinks are enabled within your GCP environment. Logging sinks must be configured to export logs to external destinations such as Cloud Storage, BigQuery, or Pub/Sub for long-term storage and analysis. Verification involves reviewing sink configurations in the GCP Console or using gcloud commands to ensure they are active. Remediation requires setting up new sinks or enabling existing ones to ensure all necessary logs are captured and exported.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/centralized-logging
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/logging/docs/reference/tools/gcloud-logging
- rule_id: gcp.logging.sink.route_change_alert_configured
  service: logging
  resource: sink
  requirement: Route Change Alert Configured
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Route Change Alert Configured for Logging Sinks
  rationale: Configuring alerts for route changes in logging sinks is crucial to detect unauthorized modifications that could divert log data, impacting incident detection and response. This helps maintain the integrity and availability of audit logs, which are essential for forensic investigations, compliance with standards like PCI-DSS, and maintaining trust in system operations.
  description: This rule verifies that alerts are configured for route changes in logging sinks to ensure any unauthorized changes trigger an immediate response. To verify, ensure that alert policies in Cloud Monitoring are set up to monitor sink configurations for changes. Remediate by creating alert policies that monitor logging sinks and configure notifications to the appropriate security teams. This will help in promptly addressing any suspicious activities.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/monitoring/alerts
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.route_changes_enabled
  service: logging
  resource: sink
  requirement: Route Changes Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sinks Route Changes are Enabled
  rationale: Enabling logging for route changes is critical for maintaining network integrity and security. It allows for monitoring of unauthorized or incorrect routing modifications that could lead to data breaches or loss of connectivity. Compliance with regulations such as PCI-DSS and NIST requires auditing of network changes, making it essential for both security and regulatory adherence.
  description: This rule checks if logging sinks are configured to capture route changes within Google Cloud Platform. Logging these changes helps in tracking configuration alterations, facilitating incident response, and maintaining a historical record for audit purposes. To verify, ensure that your logging sinks have filters set up to capture route modification events. Remediation involves configuring or updating logging sinks to include route change events by using the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/logging/docs/routing/overview
  - https://cloud.google.com/logging/docs/reference/tools/gcloud-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/nistpubs/800-53/sp800-53.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.sink.rule_changes_enabled
  service: logging
  resource: sink
  requirement: Rule Changes Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Audit Logs for Sink Rule Changes
  rationale: Enabling audit logs for sink rule changes is critical to ensure that any modifications to logging sinks are tracked and monitored. This helps in identifying unauthorized changes that could lead to data exfiltration or loss of critical logging data, impacting incident response and forensic investigations. Compliance with regulations such as PCI-DSS and SOC2 often requires detailed logging of configuration changes to maintain data security and integrity.
  description: This rule checks if audit logs for modifications to logging sinks are enabled in GCP. It ensures that any changes to sink configurations are logged, providing a trail for security and compliance audits. To verify, check the 'Audit Config' in the IAM settings of your project to ensure 'ADMIN_READ' logs are enabled for the logging service. Remediation involves updating the IAM policy to include the necessary audit log configurations.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/audit-logging
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 6.5
  - NIST SP 800-53 Rev. 5 AU-2 Audit Events
  - PCI-DSS Requirement 10.2.2
  - SOC 2 Trust Services Criteria CC6.1
- rule_id: gcp.logging.sink.run_task_definition_logging_enabled
  service: logging
  resource: sink
  requirement: Run Task Definition Logging Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Task Definition Logging is Enabled for Logging Sinks
  rationale: Enabling task definition logging for logging sinks is crucial for maintaining visibility into the operations and security posture of your GCP environment. It helps in detecting unauthorized access and misconfigurations, which could lead to data breaches or service disruptions. This control also supports compliance with regulatory requirements by ensuring that all task execution details are logged and auditable.
  description: This rule checks whether logging sinks have task definition logging enabled, which records detailed information about task executions. To verify, ensure that each logging sink in your GCP environment is configured to capture task definition logs by setting the appropriate logging parameters. Remediation involves updating the sink configuration to include task definition logging, thereby improving audit capabilities and traceability.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - 'CIS GCP Benchmark: https://www.cisecurity.org/benchmark/google_cloud_computing_platform/'
  - 'NIST SP 800-53: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf'
  - 'PCI-DSS Requirement 10: https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf'
  - 'ISO/IEC 27001: https://www.iso.org/isoiec-27001-information-security.html'
- rule_id: gcp.logging.sink.sink_created_configured
  service: logging
  resource: sink
  requirement: Sink Created Configured
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sinks Are Properly Configured
  rationale: Configuring logging sinks correctly ensures that critical logs are captured and sent to appropriate destinations for analysis and retention. This helps in detecting unauthorized access, tracking changes, and fulfilling compliance requirements such as PCI-DSS and NIST. Misconfigured sinks could lead to loss of important audit logs, impacting incident response and forensic investigations.
  description: This rule checks that logging sinks in Google Cloud are properly created and configured to export logs to specified destinations like Cloud Storage, BigQuery, or Pub/Sub. It verifies that sinks are not left in a default or incomplete state, which could prevent logs from being exported correctly. To remedy misconfigurations, review the sink settings in the GCP Console or via gcloud command-line tool, ensuring destinations are correctly specified and IAM permissions are set to allow writing logs to those destinations.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.sink.stream_encryption_at_rest_enabled
  service: logging
  resource: sink
  requirement: Stream Encryption At Rest Enabled
  scope: logging.sink.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Logging Sink Encryption At Rest is Enabled
  rationale: Enabling encryption at rest for logging sinks protects sensitive data from unauthorized access and potential breaches, ensuring compliance with regulatory requirements such as PCI-DSS and HIPAA. Without encryption, sensitive log data could be exposed, leading to data leaks, reputational damage, and financial penalties.
  description: This rule checks that all logging sinks in your GCP environment have encryption at rest enabled. This ensures that the data written to any storage destination is encrypted using Google-managed or customer-managed encryption keys. To verify, inspect the logging sink configurations for the 'writerIdentity' field and ensure it is set to use an appropriate key. Remediate by enabling Cloud KMS for managing encryption keys and updating sink configurations accordingly.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/security/compliance/cis#gcp_cis_v1.3_2.8
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
- rule_id: gcp.logging.sink.stream_ingestion_auth_required
  service: logging
  resource: sink
  requirement: Stream Ingestion Auth Required
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Authenticated Access for Logging Sink Streaming
  rationale: Authenticated access to logging sinks is crucial to prevent unauthorized data ingestion, which could lead to data leakage or manipulation, compromising the integrity of audit logs. Ensuring authentication aligns with compliance mandates such as PCI-DSS and ISO 27001, reducing the risk of exposure to malicious activities and regulatory penalties.
  description: This rule checks that all logging sinks configured for streaming ingestion require authenticated access. It verifies that IAM policies are set to enforce identity verification for any entity attempting to write logs. To remediate, review the IAM permissions for the logging sink and ensure that only authorized identities can access it. Modify the IAM policy to include only necessary roles with the least privilege principle.
  references:
  - https://cloud.google.com/logging/docs/sinks
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.logging.sink.stream_retention_days_minimum
  service: logging
  resource: sink
  requirement: Stream Retention Days Minimum
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Minimum Retention Days for Logging Sinks
  rationale: Enforcing a minimum retention period for logging sinks helps ensure that logs are available for analysis and compliance auditing. Retaining logs for an adequate duration is crucial to detect and investigate security incidents, meet regulatory requirements, and support forensic investigations. Insufficient log retention can lead to gaps in security monitoring and hinder incident response efforts.
  description: This rule verifies that all logging sinks in your GCP environment have a minimum retention period set for log entries. It checks the configuration of each logging sink to ensure that log data is retained for at least the specified minimum number of days. To remediate, configure your logging sinks to retain logs for the required period by adjusting the sink's retention settings in the Google Cloud Console or using the gcloud command line tool.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf
  - https://cloud.google.com/architecture/best-practices-for-logging-monitoring-and-alerting
- rule_id: gcp.monitoring.alert_policy.acls_alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Acls Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Policies Have ACLs Alarm Configured
  rationale: Configuring ACLs alarms for alert policies in GCP ensures that unauthorized access attempts to alter alert configurations are promptly detected. This protects sensitive systems from potential breaches and aids in maintaining compliance with security standards. Failing to monitor these changes can lead to undetected alterations that compromise system integrity and data security.
  description: This rule checks whether ACLs alarms are configured for alert policies within Google Cloud Monitoring. It verifies that changes to access control lists trigger alerts, enabling timely responses to unauthorized modifications. To ensure compliance, review your alert policies and configure alarms for ACL changes using the Google Cloud Console or gcloud CLI. Remediation involves setting up notification channels and defining conditions for ACL change alerts.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security-command-center/docs/how-to-alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Alert Policies Must Have Alarms Configured
  rationale: Configuring alarms in alert policies is crucial to detect and respond to potential security incidents in a timely manner. Without proper alarms, critical events might go unnoticed, leading to data breaches or service disruptions. This practice supports compliance with standards like NIST and ISO 27001, which emphasize proactive monitoring and incident response capabilities.
  description: This rule checks whether alarm conditions are configured for alert policies in Google Cloud Monitoring. An alert policy should include conditions and notification channels to ensure that stakeholders are informed of incidents promptly. To verify, review the alert policy configurations in the GCP console or use the gcloud command-line tool. If alarms are not configured, update the alert policies to include relevant conditions and notification channels, ensuring coverage for all critical metrics.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/monitoring/docs/alerting/alerts-overview
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 7.3 Ensure that alerts are configured for important metrics
  - NIST SP 800-53 Rev. 5 - SI-4 Information System Monitoring
  - ISO/IEC 27001:2013 - A.12.4 Logging and monitoring
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.monitoring.alert_policy.alert_alarm_actions_configured
  service: monitoring
  resource: alert_policy
  requirement: Alert Alarm Actions Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Policies Have Configured Alarm Actions
  rationale: Configuring alarm actions for alert policies is crucial to ensure timely notifications and responses to incidents, minimizing potential business disruptions and security risks. Without proper alarm actions, critical alerts might be overlooked, leading to prolonged exposure to threats and non-compliance with standards that require incident detection and response capabilities.
  description: This rule checks whether alert policies in Google Cloud Monitoring have appropriate alarm actions configured, which might include sending notifications via email, SMS, or integrating with incident management systems. To verify, examine each alert policy for configured notification channels or automated actions. Remediation involves configuring these actions in the alert policy settings through the Google Cloud Console or using the gcloud CLI to ensure alerts trigger predefined responses.
  references:
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 7.2
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloud.google.com/architecture/security-best-practices
- rule_id: gcp.monitoring.alert_policy.alert_critical_alarms_enabled
  service: monitoring
  resource: alert_policy
  requirement: Alert Critical Alarms Enabled
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Critical Alert Policies are Enabled
  rationale: Enabling critical alert policies is essential to detect and respond to significant security incidents in a timely manner. Failure to configure these alerts can lead to undetected breaches, prolonged downtime, and non-compliance with regulatory frameworks such as PCI-DSS and ISO 27001, which mandate regular monitoring and alerting on security events.
  description: This rule checks if critical alert policies are configured and enabled in GCP's Monitoring service. Specifically, it verifies the presence of alert policies that notify relevant stakeholders when critical thresholds are breached. To ensure compliance, configure alert policies via the Google Cloud Console under Monitoring by specifying conditions, notification channels, and severity levels. Regularly review and update these policies to align with evolving security requirements.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.alert_destinations_authenticated
  service: monitoring
  resource: alert_policy
  requirement: Alert Destinations Authenticated
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Destinations Are Authenticated
  rationale: Unauthenticated alert destinations may lead to unauthorized data access or manipulation, posing a security risk. Authenticating alert destinations ensures that only verified endpoints receive monitoring alerts, which is crucial for maintaining data integrity and meeting compliance standards like ISO 27001 and SOC2.
  description: This rule checks that all alert destinations configured in GCP Monitoring's alert policies are authenticated. Ensure that destinations use secure methods such as OAuth 2.0 or service account keys. Verification can be done by reviewing the alert policy configuration in the Google Cloud Console or using the gcloud command-line tool. Remediation involves updating alert policies to include only authenticated destinations, ensuring secure communication.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/service-accounts
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/sdk/gcloud/reference/monitoring/alert-policies
- rule_id: gcp.monitoring.alert_policy.anomaly_alerts_configured
  service: monitoring
  resource: alert_policy
  requirement: Anomaly Alerts Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Anomaly Alerts Are Configured in GCP Monitoring
  rationale: Configuring anomaly alerts in GCP Monitoring enables proactive detection of unusual patterns in metrics, which can indicate potential security incidents or system failures. This is crucial for maintaining service reliability and minimizing downtime, aligning with compliance requirements for timely incident detection and response.
  description: This check verifies that anomaly alerts are configured within the GCP Monitoring service, which involves setting up alert policies that automatically trigger notifications when metrics deviate significantly from expected patterns. To verify, ensure that alert policies with anomaly detection conditions are active. Remediation involves defining anomaly detection conditions in alert policies based on critical metrics, such as CPU usage or network traffic, and setting up notifications to alert relevant teams.
  references:
  - https://cloud.google.com/monitoring/alerts/concepts-alerting-rules
  - https://cloud.google.com/monitoring/docs/alerting/alert-policy-anomalies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.anomaly_detectors_enabled
  service: monitoring
  resource: alert_policy
  requirement: Anomaly Detectors Enabled
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Anomaly Detection in Monitoring Alert Policies
  rationale: Enabling anomaly detection in monitoring alert policies helps identify unusual patterns in metrics that could indicate security breaches or operational issues. This proactive measure mitigates risks by alerting administrators to potential threats or system failures, ensuring timely response. It also assists in maintaining compliance with standards requiring continuous monitoring and threat detection, such as PCI-DSS and ISO 27001.
  description: This rule checks if anomaly detection is enabled in GCP Monitoring alert policies. Anomaly detection uses AI to automatically learn your system's normal behavior and alerts on deviations. To verify, ensure your alert policies in the Cloud Monitoring console have anomaly detection configured. Remediate by accessing the GCP Console, navigating to 'Monitoring', selecting 'Alerting', and enabling anomaly detection on relevant policies. This enhances your capability to detect and respond to incidents promptly.
  references:
  - https://cloud.google.com/monitoring/alerts/concepts-indepth
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.monitoring.alert_policy.anomaly_training_data_sources_approved
  service: monitoring
  resource: alert_policy
  requirement: Anomaly Training Data Sources Approved
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Approved Data Sources for Anomaly Detection
  rationale: Approving anomaly training data sources is crucial for maintaining the integrity of monitoring systems by preventing unauthorized access and data manipulation. This minimizes the risk of false positives or negatives in alerts, which could lead to overlooked threats or unnecessary operational disruptions. Compliance with industry standards often requires auditable and controlled environments for data processing and analysis.
  description: This rule ensures that only approved data sources are used for training anomaly detection models within GCP Monitoring. Verify that data sources are listed in the approved sources registry and follow organizational guidelines for data access. Remediation involves reviewing data source configurations and updating alert policies to exclude unapproved sources, ensuring they align with the organization's security policies and compliance requirements.
  references:
  - https://cloud.google.com/monitoring/alerts/configuring-alert-policies
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.monitoring.alert_policy.capacity_alerts_configured
  service: monitoring
  resource: alert_policy
  requirement: Capacity Alerts Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Capacity Alerts are Configured for GCP Monitoring
  rationale: Proper configuration of capacity alerts in GCP Monitoring is crucial to proactively manage resource utilization and prevent service disruptions. It helps identify potential capacity issues before they impact business operations, reducing downtime and maintaining service availability. Configuring these alerts also supports compliance with industry standards that require monitoring of system resources.
  description: This rule checks if capacity alerts are configured in GCP Monitoring alert policies. To verify, review the alert policies under Monitoring in the Google Cloud Console to ensure alerts are set for critical resources like CPU, memory, and storage usage. Remediation involves creating or updating alert policies to include thresholds for capacity metrics, ensuring notifications are promptly sent to the appropriate teams. This proactive approach helps maintain operational efficiency and continuity.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/monitoring/docs/alerts/concepts-indepth
  - https://cloud.google.com/monitoring/settings/notification-options
- rule_id: gcp.monitoring.alert_policy.changes_to_vpcs_alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Changes To Vpcs Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Monitor VPC Changes with Alert Policies
  rationale: Monitoring changes to VPCs is crucial for detecting unauthorized modifications that could expose sensitive data or disrupt services. Configuring alerts for VPC changes helps organizations quickly identify and respond to potential security incidents, ensuring network integrity and compliance with frameworks like NIST and PCI-DSS.
  description: This rule checks if alert policies are configured in Google Cloud Monitoring to track changes to Virtual Private Clouds (VPCs). Ensure that an alert policy exists which triggers notifications for actions such as creation, deletion, or modification of VPCs. To verify, access the Google Cloud Console, navigate to Monitoring, and ensure an alert policy targets 'vpc.googleapis.com' with appropriate conditions. Remediation involves creating or updating the alert policy to include VPC change detection, setting notification channels for timely responses.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.monitoring.alert_policy.cpu_utilization_alert_configured
  service: monitoring
  resource: alert_policy
  requirement: Cpu Utilization Alert Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure CPU Utilization Alert is Configured in Monitoring
  rationale: Monitoring CPU utilization is crucial for maintaining optimal performance and cost management in GCP environments. Failure to configure alerts can lead to undetected performance bottlenecks, resulting in downtime, financial loss, and potential non-compliance with industry standards that require availability and operational monitoring.
  description: This check ensures that an alert policy for CPU utilization is configured in GCP Monitoring. Specifically, it verifies that there is a policy set to trigger alerts when CPU usage exceeds a specified threshold, indicating potential resource strain. To verify, review the alert policies in the Monitoring section of the GCP Console and ensure that a CPU utilization alert is present. Remediation involves creating an alert policy that specifies conditions for CPU usage and sets appropriate notifications.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/monitoring/docs/alerting/alerts
  - https://cloud.google.com/blog/products/management-tools/how-to-set-up-stackdriver-alerting-policies
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.monitoring.alert_policy.data_analytics_admin_activity_logging_enabled
  service: monitoring
  resource: alert_policy
  requirement: Data Analytics Admin Activity Logging Enabled
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Ensure Data Analytics Admin Activity Logging is Enabled
  rationale: Enabling admin activity logging for Data Analytics services is crucial for maintaining an audit trail of administrative actions, which assists in detecting unauthorized access and supports forensic investigations in case of a security breach. This logging is also necessary for compliance with regulations such as PCI-DSS and ISO 27001, which require detailed records of user activities.
  description: This rule checks that logging of admin activities for Data Analytics services is enabled in Google Cloud's Monitoring Alert Policies. To verify, ensure that audit logs are set to capture all admin activities in the Google Cloud Console under Stackdriver Logging. If not enabled, configure audit logs by navigating to 'IAM & Admin' > 'Audit Logs' in the Cloud Console and activate the necessary log types for Data Analytics services. This ensures all pertinent admin actions are recorded and reviewed.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.data_analytics_logs_retention_days_minimum
  service: monitoring
  resource: alert_policy
  requirement: Data Analytics Logs Retention Days Minimum
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Minimum Retention for Data Analytics Logs in Alert Policies
  rationale: Retaining data analytics logs for a minimum period is crucial to ensure that any anomalies or security incidents can be thoroughly investigated. Insufficient retention may lead to data loss, making it difficult to perform forensic analysis and comply with industry regulations such as GDPR and HIPAA, which mandate specific data retention periods.
  description: This rule checks that the retention period for data analytics logs within GCP alert policies is set to a minimum of 30 days. This ensures that logs are available for auditing and forensic purposes. To verify, review the alert policy settings in the GCP Console under 'Logging' and ensure the retention period is configured correctly. If not, update the policy settings to extend the log retention period to meet compliance requirements.
  references:
  - https://cloud.google.com/logging/docs/alerting/logging-alerts
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.data_analytics_query_access_logging_enabled
  service: monitoring
  resource: alert_policy
  requirement: Data Analytics Query Access Logging Enabled
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Data Analytics Query Access Logging
  rationale: Enabling data analytics query access logging is crucial for detecting unauthorized access and usage patterns. This enhances visibility into data access activities, which is essential for compliance with regulations such as GDPR and HIPAA, and helps in identifying potential data breaches or insider threats.
  description: This rule checks if query access logging is enabled for data analytics services in GCP. It involves configuring Cloud Audit Logs to capture all query activities, ensuring that any access to data is logged for auditing purposes. To verify and remediate, ensure that logging is enabled via the GCP Console or gcloud CLI for the relevant services, such as BigQuery. Regularly review these logs to identify any suspicious activities and maintain compliance with industry standards.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.monitoring.alert_policy.dr_alert_destinations_configured
  service: monitoring
  resource: alert_policy
  requirement: DR Alert Destinations Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Policies Have DR Alert Destinations Configured
  rationale: Configuring disaster recovery (DR) alert destinations is essential to ensure that critical alerts reach key personnel during incidents, thereby minimizing downtime and potential data loss. Without proper alert routing, organizations risk delayed responses to critical events, increasing the potential for business disruptions and non-compliance with industry standards such as ISO 27001 and NIST SP 800-53.
  description: This rule checks whether GCP alert policies have configured destinations for disaster recovery alerts, ensuring they are routed to appropriate communication channels like email, SMS, or webhook during incidents. To verify, inspect the alert policies in the Cloud Monitoring dashboard and confirm that notification channels are set for all critical alerts. Remediation involves configuring these channels via the GCP Console under Monitoring > Alerting > Edit the specific alert policy > Notification channels.
  references:
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - https://cloud.google.com/security/compliance/iso-27001
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/monitoring/alerts/concepts-indepth
- rule_id: gcp.monitoring.alert_policy.dr_alerts_for_backup_failures_configured
  service: monitoring
  resource: alert_policy
  requirement: DR Alerts For Backup Failures Configured
  scope: monitoring.alert_policy.backup_recovery
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Configure Alerts for Backup Failures in GCP Monitoring
  rationale: Configuring disaster recovery (DR) alerts for backup failures is crucial to ensure timely response to potential data loss incidents, which can lead to significant operational disruptions and financial losses. It helps maintain business continuity by enabling quick remediation of backup issues, thus safeguarding critical data assets. Additionally, having such alerts supports compliance with data protection regulations that mandate regular backup and recovery processes.
  description: This rule checks if alert policies are configured in GCP Monitoring to notify administrators of backup failures. It requires setting up specific alert conditions that monitor backup operations and trigger notifications when failures occur. To verify, review the alert policies in the Monitoring console to ensure they cover all critical backup jobs. To remediate, create or update alert policies with conditions that detect backup failures and define notification channels for immediate alerting.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/solutions/dr-scenarios-planning-guide
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/backup-and-dr
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.dr_alerts_for_replication_lag_configured
  service: monitoring
  resource: alert_policy
  requirement: DR Alerts For Replication Lag Configured
  scope: monitoring.alert_policy.replication
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Configure DR Alerts for Replication Lag in GCP Monitoring
  rationale: Configuring alerts for replication lag ensures timely identification of data synchronization issues in disaster recovery (DR) scenarios, reducing potential data loss and service downtime. This is crucial for maintaining business continuity and meeting regulatory requirements related to data integrity and availability.
  description: This rule verifies that Google Cloud Monitoring alert policies are configured to detect replication lag for disaster recovery systems. Ensure that alert conditions are set for key metrics associated with replication lag and that notifications are sent to responsible parties. To remediate, create or update alert policies in GCP Monitoring to include conditions for replication lag metrics, specifying appropriate thresholds and notification channels.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/solutions/disaster-recovery-cookbook
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 7.6
  - 'NIST SP 800-53 Rev. 5 CP-10: System Recovery and Restoration'
  - ISO/IEC 27001:2013 - A.17.2.1 Availability of Information Processing Facilities
  - https://cloud.google.com/architecture/best-practices-for-building-enterprise-grade-apps
- rule_id: gcp.monitoring.alert_policy.dr_alerts_for_rpo_rto_breach_configured
  service: monitoring
  resource: alert_policy
  requirement: DR Alerts For Rpo Rto Breach Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Configure DR Alerts for RPO RTO Breach in GCP Monitoring
  rationale: Configuring disaster recovery (DR) alerts for RPO (Recovery Point Objective) and RTO (Recovery Time Objective) breaches is critical for ensuring business continuity and minimizing data loss during incidents. Without these alerts, organizations may face extended downtimes and potential financial losses, impacting customer trust and compliance with regulations like ISO 27001 and SOC2.
  description: This rule checks if alert policies are configured in GCP Monitoring to notify administrators of DR breaches related to RPO and RTO. Ensure alert policies are set to trigger when specific thresholds indicating potential data loss or unacceptable recovery times are breached. Verify by reviewing the configurations in GCP Monitoring and adjusting thresholds as necessary to align with business continuity plans.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/architecture/dr-scenarios-planning-guide
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
- rule_id: gcp.monitoring.alert_policy.gateways_alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Gateways Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Monitoring Alert Policies for Gateways are Configured
  rationale: Configuring alert policies for gateways is essential to promptly detect and respond to potential security breaches or system failures. This helps maintain continuous operation and protects sensitive data from being exposed due to gateway vulnerabilities. Compliance with security frameworks often requires proactive monitoring and alerts to mitigate risks associated with gateway misconfigurations.
  description: This rule checks whether alert policies are configured for monitoring gateways, which are critical components for managing data flow and security. Verify that alert policies are set to trigger notifications for anomalies or performance issues. Remediation involves accessing the GCP Console, navigating to Monitoring, and creating or updating alert policies to include gateway-specific metrics and conditions. Ensure alerts are directed to responsible teams for quick action.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/architecture/security-foundations
  - CIS Google Cloud Computing Foundations Benchmark v1.0.0 - Section 9.1
  - NIST SP 800-53 Rev. 5 - Security and Privacy Controls for Information Systems
  - ISO/IEC 27001:2013 - Information Security Management Systems
  - https://cloud.google.com/monitoring/support/notification-options
- rule_id: gcp.monitoring.alert_policy.group_retention_policy_specific_days_enabled_gcp_sto_enabled
  service: monitoring
  resource: alert_policy
  requirement: Group Retention Policy Specific Days Enabled Gcp Sto Enabled
  scope: monitoring.alert_policy.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Specific Days for Group Retention in Alert Policies
  rationale: Setting specific days for group retention in alert policies helps organizations manage data retention effectively, reducing the risk of unnecessary data exposure and ensuring compliance with data protection regulations. By specifying retention periods, businesses can optimize storage costs and safeguard sensitive information against unauthorized access.
  description: This rule checks whether specific days are set for group retention in GCP Monitoring alert policies. Ensuring that retention policies are configured to retain alert data only for the necessary period is crucial for data management and security. To verify, review the alert policy configurations in the GCP Console to confirm that the 'Group Retention Policy' specifies retention days. If not configured, update the policy to define a retention schedule that aligns with your organization's data retention standards.
  references:
  - https://cloud.google.com/monitoring/alerts/policies
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.monitoring.alert_policy.logging_metric_filter_console_root_login_detected_fi_present
  service: monitoring
  resource: alert_policy
  requirement: Logging Metric Filter Console Root Login Detected Fi Present
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Detect Console Root Logins Using Logging Metric Filter
  rationale: Monitoring console root logins is crucial for preventing unauthorized access and potential privilege escalation. Root account usage should be minimal and carefully audited due to the associated high-level privileges, which can lead to severe security breaches if misused. Compliance requirements often mandate the logging and monitoring of root account activities to ensure accountability and traceability.
  description: This rule checks for the presence of a logging metric filter that detects console logins using the root account. To verify, ensure a log-based metric is configured to filter and capture 'gcloud' or 'console' login events where the user is 'root'. Remediation involves creating or updating logging metrics in Google Cloud Logging to capture such events, and setting up alert policies in Cloud Monitoring to notify security teams of any detected root logins.
  references:
  - https://cloud.google.com/logging/docs/logs-based-metrics
  - https://cloud.google.com/monitoring/alerts
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Control 4.6
  - 'NIST SP 800-53 Rev. 5, AC-2: Account Management'
  - 'PCI-DSS 3.2.1 Requirement 10.2: Logging and Monitoring'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.logging_metric_filter_iam_policy_change_detected_fil_present
  service: monitoring
  resource: alert_policy
  requirement: Logging Metric Filter IAM Policy Change Detected Fil Present
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Alert on IAM Policy Changes via Logging Metric Filter
  rationale: Monitoring IAM policy changes is crucial as unauthorized alterations can lead to privilege escalation and data breaches. Detecting these changes promptly helps mitigate risks by ensuring only authorized modifications occur, aligning with security best practices and compliance mandates like PCI-DSS and ISO 27001, which require strict access controls.
  description: This rule checks for the presence of a logging metric filter that detects IAM policy changes. The filter should trigger an alert when a policy modification occurs, allowing for immediate investigation. To verify, ensure a logging metric exists that captures 'SetIamPolicy' or 'ModifyIamPolicy' operations within audit logs. Remediate by creating a filter in Google Cloud Logging and associating it with an alert policy in Monitoring, ensuring real-time notifications of policy changes.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/monitoring-iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/monitoring/alerts
- rule_id: gcp.monitoring.alert_policy.logging_metric_filter_kms_key_deletion_or_disable_de_present
  service: monitoring
  resource: alert_policy
  requirement: Logging Metric Filter KMS Key Deletion Or Disable De Present
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: high
  title: Alert on KMS Key Deletion or Disablement
  rationale: Monitoring for the deletion or disablement of KMS keys is critical as these keys protect sensitive data. Unauthorized key deletion or disablement can lead to data exposure, compliance violations, and operational disruptions, impacting business continuity and reputation. This rule helps in identifying potential security threats and ensures adherence to regulatory requirements like PCI-DSS and HIPAA.
  description: This rule checks for the existence of a logging metric filter for detecting KMS key deletion or disablement in Google Cloud's Monitoring service. Ensure the alert policy is configured to trigger an alert when a key is deleted or disabled to promptly respond to unauthorized activities. To verify, navigate to the Monitoring section in GCP Console and check alert policies for filters set on KMS key operations. Remediation involves creating or updating alert policies to include metric filters for KMS key lifecycle events.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kms/docs/key-rotation
  - https://cloud.google.com/monitoring/alerts
- rule_id: gcp.monitoring.alert_policy.logging_metric_filter_network_acl_or_sg_change_detec_present
  service: monitoring
  resource: alert_policy
  requirement: Logging Metric Filter Network ACL Or Sg Change Detec Present
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Monitor Network ACL & Security Group Changes via Logging Metrics
  rationale: Monitoring changes to Network ACLs and Security Groups is crucial because unauthorized modifications can lead to potential exposure of sensitive data or unauthorized access to cloud resources. These changes can indicate possible security breaches or misconfigurations that could compromise the organization's security posture. Compliance frameworks like NIST and PCI-DSS require monitoring such changes to ensure ongoing security and integrity of cloud environments.
  description: This rule checks for the presence of a logging metric filter and an associated alert policy that detects changes to Network ACLs (Access Control Lists) and Security Groups. The configuration should be set to create logs whenever changes occur and trigger alerts to notify administrators immediately. To verify, ensure that a logging metric filter is configured for these changes and that an alert policy is linked to notify on any log entries. Remediation involves setting up the required logging metric filter and alert policy using the GCP Cloud Monitoring and Logging services.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.monitoring.alert_policy.memory_utilization_alert_configured
  service: monitoring
  resource: alert_policy
  requirement: Memory Utilization Alert Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Memory Utilization Alert is Configured
  rationale: Configuring memory utilization alerts helps prevent service disruptions by providing timely notifications of potential memory exhaustion. This enables proactive resource management, reducing the risk of application downtime and ensuring consistent performance. Additionally, it assists in aligning with compliance frameworks that mandate monitoring of critical resource metrics for operational integrity.
  description: This rule checks if an alert policy is configured in GCP Monitoring to track memory utilization. Proper configuration involves setting thresholds that trigger alerts when memory usage exceeds defined limits, allowing for timely intervention. To verify, review your alert policies in the GCP Console under Monitoring, ensure thresholds are appropriately defined, and adjust as necessary to fit application needs. Remediation involves creating or updating alert policies to include memory utilization metrics with actionable thresholds.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/monitoring/quotas
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#monitoring
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.monitoring.alert_policy.metric_filter_policy_changes_gcp_iam_password_policy__unused
  service: monitoring
  resource: alert_policy
  requirement: Metric Filter Policy Changes Gcp IAM Password Policy Unused
  scope: monitoring.alert_policy.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Monitor Unused IAM Password Policy Change Alerts
  rationale: Monitoring IAM password policy changes is crucial as it helps detect unauthorized or unintended modifications that could weaken security controls, increasing the risk of unauthorized access. Regularly assessing and responding to these changes ensures compliance with security standards and mitigates potential insider threats or account compromises.
  description: This rule checks for the presence and activity of alert policies that monitor changes to IAM password policies. If such policies are unused, it indicates a gap in monitoring critical configuration changes. Administrators should verify the existence of alert policies that trigger on password policy changes and ensure they are active. Remediation involves creating or enabling alert policies to monitor these changes and integrating them into the organization's incident response process.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/iam/docs/audit-logging
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - 'NIST SP 800-53 Rev. 4: CM-6 Configuration Settings'
  - 'PCI-DSS Requirement 10.6: Review Logs and Security Events'
  - ISO/IEC 27001:2013 A.12.4 Logging and Monitoring
- rule_id: gcp.monitoring.alert_policy.route_tables_alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Route Tables Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Route Table Alert Policies Are Configured
  rationale: Configuring alert policies for route tables is crucial in detecting unauthorized or unintended changes that might affect network traffic flow, potentially leading to security vulnerabilities or service disruptions. It helps organizations mitigate risks associated with network misconfigurations and supports compliance with security standards that require continuous monitoring of network configuration changes.
  description: This rule checks whether alert policies are configured for route table changes in GCP. It involves verifying that appropriate alerting mechanisms are in place to notify security teams when modifications to route tables occur. To ensure compliance, define alert policies in Google Cloud Monitoring with conditions that track changes to route tables and specify notification channels. Remediation involves setting up these alerts if they are missing, ensuring that the monitoring configuration aligns with organizational security policies.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/solutions/best-practices-vpc-design
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.sampling_rule_access_rbac_least_privilege
  service: monitoring
  resource: alert_policy
  requirement: Sampling Rule Access RBAC Least Privilege
  scope: monitoring.alert_policy.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure RBAC Least Privilege for Monitoring Alert Policies
  rationale: Implementing least privilege access for alert policies minimizes the risk of unauthorized modifications, which could lead to missed critical alerts, data breaches, or system downtime. This practice supports compliance with regulations like GDPR and NIST by ensuring that only authorized personnel can modify alert settings that protect sensitive information.
  description: This rule checks that roles assigned to users or groups for accessing GCP Monitoring Alert Policies adhere to the principle of least privilege. Specifically, it verifies that users have only the necessary permissions to perform their job functions, such as 'roles/monitoring.alertPolicyEditor' or 'roles/monitoring.alertPolicyViewer', rather than overly broad roles like 'Owner'. To remediate, review and adjust IAM policies to ensure appropriate roles are assigned, removing any excessive permissions.
  references:
  - https://cloud.google.com/monitoring/access-control
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/docs/iam-best-practices
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/monitoring/alerts
- rule_id: gcp.monitoring.alert_policy.sampling_rule_rules_present
  service: monitoring
  resource: alert_policy
  requirement: Sampling Rule Rules Present
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Sampling Rules in Alert Policies are Configured
  rationale: Properly configured sampling rules in alert policies can help reduce noise by ensuring alerts are triggered only for significant events, which enhances operational efficiency and ensures critical alerts are not missed. Without sampling rules, organizations may face alert fatigue, leading to potential oversight of important security incidents. Additionally, effective alerting is essential for maintaining compliance with regulations that require prompt detection and response to security events.
  description: This check ensures that alert policies in GCP Monitoring have sampling rules defined, which are necessary to filter out non-critical alerts and focus on events that require attention. Verify that each alert policy includes appropriately configured sampling rules by reviewing the alert policy configurations in the GCP Console or using the gcloud command-line tool. To remediate, define sampling rules that match your organization's security and operational thresholds, ensuring they align with your critical incident response requirements.
  references:
  - https://cloud.google.com/monitoring/alerts/concepts-indepth
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf
  - https://cloud.google.com/monitoring/support
- rule_id: gcp.monitoring.alert_policy.sampling_rule_storage_encrypted
  service: monitoring
  resource: alert_policy
  requirement: Sampling Rule Storage Encrypted
  scope: monitoring.alert_policy.encryption
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Encryption for Sampling Rule Storage in Alert Policies
  rationale: Encrypting sampling rule storage in alert policies ensures that sensitive monitoring data is protected against unauthorized access and data breaches. This is crucial for maintaining confidentiality and integrity of logging information, reducing the risk of data exposure in case of compromised accounts or services. Compliance with regulations such as GDPR, HIPAA, and others often mandates encryption of sensitive data, making this a key aspect of regulatory adherence.
  description: This rule checks that all sampling rule storage associated with GCP alert policies is encrypted using Customer-Managed Encryption Keys (CMEK) or similar mechanisms. To verify, inspect the configuration of alert policies in the GCP Console or via the gcloud CLI to ensure that encryption is enabled. Remediation involves updating the alert policy settings to enable encryption, leveraging CMEK when possible, to enhance data security and meet compliance requirements.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.monitoring.dashboard.public_embeds_disabled
  service: monitoring
  resource: dashboard
  requirement: Public Embeds Disabled
  scope: monitoring.dashboard.public_access
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Disable Public Embeds on Monitoring Dashboards
  rationale: Allowing public embeds on GCP Monitoring Dashboards can expose sensitive operational metrics to unauthorized users, leading to potential data breaches and security incidents. This exposure can also result in non-compliance with regulations such as GDPR and HIPAA, which mandate strict controls over data access and sharing. Unauthorized access to dashboard data could be leveraged by attackers to gain insights into system vulnerabilities or operational weaknesses.
  description: This rule checks if public embeds are enabled on GCP Monitoring Dashboards, which could allow unauthorized users to access sensitive monitoring data. It is critical to ensure that only authenticated and authorized personnel have access to these dashboards to maintain data integrity and confidentiality. To remediate, audit all GCP Monitoring Dashboards and disable public embeds by setting access controls to authenticated users only. This involves configuring IAM policies to restrict public access and verifying permissions periodically to ensure compliance with security best practices.
  references:
  - https://cloud.google.com/monitoring/dashboards
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.dashboard.sharing_restricted_to_org
  service: monitoring
  resource: dashboard
  requirement: Sharing Restricted To Org
  scope: monitoring.dashboard.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Dashboard Sharing to Organization Members
  rationale: Limiting dashboard access to organization members reduces the risk of unauthorized data exposure and potential breaches. This practice ensures that sensitive monitoring data is only accessible to trusted personnel, thereby protecting intellectual property and meeting compliance requirements with standards such as SOC2 and ISO 27001.
  description: This rule checks if monitoring dashboards are shared only with members of the organization, preventing external access. Review each dashboard's sharing settings in Google Cloud Console to ensure access is limited to organizational emails. Remediation involves adjusting IAM policies to restrict access to organization members, using predefined roles like 'roles/viewer' or custom roles with restricted permissions.
  references:
  - https://cloud.google.com/monitoring/dashboards/api-dashboard#sharing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
- rule_id: gcp.monitoring.dashboard.sso_required
  service: monitoring
  resource: dashboard
  requirement: Sso Required
  scope: monitoring.dashboard.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure SSO is Enabled for GCP Monitoring Dashboards
  rationale: Requiring SSO for accessing GCP Monitoring Dashboards reduces the risk of unauthorized access by enforcing stronger authentication mechanisms. This minimizes potential security breaches and helps in complying with industry regulations that mandate robust access controls to sensitive monitoring data.
  description: This rule checks if Single Sign-On (SSO) is enforced for accessing Google Cloud Monitoring Dashboards. To verify, ensure that IAM permissions for dashboard access are configured to require authentication through a centralized identity provider. Remediation involves integrating with an identity provider that supports SSO and updating IAM policies to enforce its use. This practice enhances security by leveraging centralized identity management.
  references:
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.monitoring.group.kms_encryption_enabled
  service: monitoring
  resource: group
  requirement: KMS Encryption Enabled
  scope: monitoring.group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption is Enabled for Monitoring Groups
  rationale: Enabling KMS encryption for Monitoring Groups ensures that sensitive data is protected at rest, minimizing the risk of data breaches. This is crucial for maintaining data privacy and meeting compliance requirements such as GDPR and CCPA, which mandate robust data protection measures. Without KMS encryption, unauthorized access to data can occur, leading to potential financial losses and reputational damage.
  description: This rule checks whether Cloud Monitoring Groups in GCP have Key Management Service (KMS) encryption enabled. KMS encryption ensures that data stored within monitoring groups is encrypted using customer-managed keys, providing enhanced security control over data access. To verify, review the encryption settings for each monitoring group and enable KMS encryption if not already configured. This can be done through the GCP Console by navigating to the Monitoring Groups section and applying the necessary encryption settings.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/monitoring/docs
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
- rule_id: gcp.monitoring.group.retention
  service: monitoring
  resource: group
  requirement: Retention
  scope: monitoring.group.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Monitoring Group Retention Policies are Configured
  rationale: Proper retention policies for monitoring groups are vital to maintain visibility into historical data, which aids in incident response and forensic investigations. Without adequate retention, organizations risk losing critical information needed to detect and respond to security incidents, potentially leading to compliance violations and financial losses.
  description: This rule audits the retention settings for Google Cloud Monitoring Groups to ensure they align with organizational policies and compliance requirements. It checks the configuration to verify that logs and metrics are retained for an appropriate duration. Organizations should configure retention settings in the Monitoring Groups to meet both business needs and regulatory requirements. Remediation involves adjusting retention policies through the GCP Console or API to ensure data is retained according to specified policies.
  references:
  - https://cloud.google.com/monitoring/docs/overview
  - https://cloud.google.com/monitoring/alerts/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.monitoring.group.retention_policy_specific_days_enabled
  service: monitoring
  resource: group
  requirement: Retention Policy Specific Days Enabled
  scope: monitoring.group.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Specific Days Retention Policy for Monitoring Groups
  rationale: Enabling a specific days retention policy for monitoring groups ensures that log data is retained according to organizational compliance requirements. This mitigates the risk of data loss and supports forensic investigations and auditing by retaining crucial monitoring information for a defined period.
  description: This rule checks whether a specific days retention policy is enabled for Google Cloud Monitoring groups. To verify, ensure the retention policy is set in the group's configuration settings, specifying the number of days logs should be retained. Remediation involves configuring the retention policy to meet your organization's data retention needs using the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/logging/docs/routing/overview
- rule_id: gcp.monitoring.notification_channel.dr_communication_channels_configured
  service: monitoring
  resource: notification_channel
  requirement: DR Communication Channels Configured
  scope: monitoring.notification_channel.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure DR Communication Channels in Monitoring
  rationale: Configuring Disaster Recovery (DR) communication channels in GCP Monitoring is vital to ensure timely alerts during incidents, minimizing downtime and potential data loss. It helps in maintaining business continuity and meets compliance requirements for incident response mechanisms as per standards like ISO 27001 and NIST. Without proper configuration, crucial alerts may be missed, leading to delayed incident response and increased risk of business disruption.
  description: This rule checks whether notification channels for disaster recovery are properly configured in the GCP Monitoring service. Verify that all necessary communication channels such as email, SMS, or webhook are set up to receive alerts for critical incidents. Remediation involves accessing the Monitoring section of the GCP console, navigating to Notification Channels, and ensuring that appropriate channels are configured and tested for functionality. This ensures that alerts reach the right personnel promptly during a disaster.
  references:
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/disaster-recovery-planning-guide
- rule_id: gcp.monitoring.notification_channel.dr_communication_destinations_access_least_privilege
  service: monitoring
  resource: notification_channel
  requirement: DR Communication Destinations Access Least Privilege
  scope: monitoring.notification_channel.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for DR Communication Destinations
  rationale: Granting excessive permissions to disaster recovery (DR) communication channels can lead to data exposure or unauthorized access in the event of a breach. This misconfiguration increases the risk of sensitive information being leaked or misused, potentially violating compliance frameworks such as PCI-DSS and SOC2, and could result in significant financial and reputational damage.
  description: This rule verifies that access to Notification Channels used for DR communication is restricted to the minimum necessary permissions. It checks IAM policies to ensure that only authorized users or groups have access to create, modify, or delete these channels. To remediate, review and update IAM policies to limit permissions to specific roles and service accounts that require access, following the principle of least privilege.
  references:
  - https://cloud.google.com/monitoring/alerts/using-channels-api
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.monitoring.notification_channel.dr_communication_no_public_webhooks_without_auth
  service: monitoring
  resource: notification_channel
  requirement: DR Communication No Public Webhooks Without Auth
  scope: monitoring.notification_channel.public_access
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Prevent Unauthenticated Public Webhook Usage in Monitoring
  rationale: Allowing public webhooks without authentication in disaster recovery (DR) communications can expose sensitive alert data to unauthorized entities. This poses a risk of data breaches and unauthorized access to system alerts, potentially leading to non-compliance with regulatory standards such as GDPR and HIPAA, and impacting the organization's ability to respond to incidents effectively.
  description: This rule checks if public webhooks used as notification channels in GCP Monitoring are configured with authentication. Ensure that all public webhooks require a secure authentication method, such as OAuth 2.0, to prevent unauthorized access. Review the configuration of each notification channel and update any public webhooks lacking authentication. Remediation involves modifying the webhook settings to enforce authentication or replacing them with secure alternatives.
  references:
  - https://cloud.google.com/monitoring/support/notification-options#webhooks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.notification_channel.incident_escalation_change_audit_logging_enabled
  service: monitoring
  resource: notification_channel
  requirement: Incident Escalation Change Audit Logging Enabled
  scope: monitoring.notification_channel.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for Incident Escalation Changes
  rationale: Enabling audit logging for incident escalation changes in Google Cloud Monitoring is crucial for maintaining a reliable audit trail, which helps organizations detect unauthorized changes, meet compliance requirements such as SOC2 and ISO 27001, and improve incident response capabilities. Without proper logging, malicious or accidental modifications to notification channels may go unnoticed, potentially leading to delayed incident response and increased security risks.
  description: This rule checks whether audit logging is enabled for changes to notification channel configurations in Google Cloud Monitoring, specifically focusing on incident escalation settings. Administrators should verify that logging is configured to capture 'write' and 'admin' activity on notification channels by reviewing Cloud Audit Logs settings. Remediation involves enabling the appropriate log sinks and ensuring that audit logs are sent to a secure storage location for analysis and retention.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/monitoring/support/notification-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.monitoring.notification_channel.incident_escalation_oncall_contacts_verified
  service: monitoring
  resource: notification_channel
  requirement: Incident Escalation Oncall Contacts Verified
  scope: monitoring.notification_channel.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Verify Oncall Contacts for Incident Escalation
  rationale: Ensuring that oncall contacts are verified for incident escalation is crucial for timely response to security incidents, reducing potential downtime and mitigating damage. This verification helps prevent unauthorized access and ensures that alerts reach the intended personnel, supporting compliance with standards like ISO 27001 and SOC2.
  description: This rule checks that the notification channels used for incident escalation in GCP Monitoring have verified oncall contacts. Verification requires checking that contact details are accurate and the communication path is tested. To remediate, review the notification channel settings in the GCP Console, verify the contact information, and ensure that test notifications are successfully received and acknowledged by oncall personnel.
  references:
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/compliance/iso-27001/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/monitoring/support/notification-options#verify
- rule_id: gcp.monitoring.notification_channel.incident_escalation_policy_exists_for_critical_severity
  service: monitoring
  resource: notification_channel
  requirement: Incident Escalation Policy Exists For Critical Severity
  scope: monitoring.notification_channel.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Critical Severity Incidents Have Escalation Policies
  rationale: Having an incident escalation policy for critical severity alerts ensures that potential security breaches or system failures are promptly addressed, minimizing downtime and potential data loss. This is critical for maintaining operational integrity and meeting compliance standards, as delays in response could result in significant business disruptions and financial penalties.
  description: This rule verifies that all notification channels used for monitoring have an associated incident escalation policy for alerts marked as critical severity. Without such policies, critical alerts may not be addressed in a timely manner, increasing risk exposure. To remediate, configure escalation policies in Google Cloud Monitoring by specifying notification channels and defining escalation paths for critical alerts. Ensure that the policies are tested and verified regularly to maintain effectiveness.
  references:
  - https://cloud.google.com/monitoring/alerts/using-alerting-policies
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
- rule_id: gcp.monitoring.notification_channel.incident_notification_destinations_configured
  service: monitoring
  resource: notification_channel
  requirement: Incident Notification Destinations Configured
  scope: monitoring.notification_channel.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Configure Incident Notification Destinations in GCP Monitoring
  rationale: Having incident notification destinations configured ensures timely alerts for potential issues, enabling rapid response and mitigation of risks. This is crucial for minimizing downtime and maintaining service integrity, ultimately protecting business operations and data integrity. Compliance with frameworks such as ISO 27001, which emphasize incident management, is supported by this configuration.
  description: This rule checks that incident notification destinations are properly configured in GCP Monitoring. It verifies whether notification channels like email, SMS, or webhooks are set up to receive alerts for incidents. To remediate, ensure that all critical incidents are mapped to appropriate notification channels in the Google Cloud Console under Monitoring > Alerting > Notification channels. This setup guarantees that alerts are managed efficiently by the relevant personnel.
  references:
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/monitoring/alerts/using-channels-api
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.monitoring.notification_channel.incident_notification_endpoints_authenticated
  service: monitoring
  resource: notification_channel
  requirement: Incident Notification Endpoints Authenticated
  scope: monitoring.notification_channel.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Incident Notification Endpoints Are Authenticated
  rationale: Authenticating incident notification endpoints is crucial to prevent unauthorized access to sensitive alert data, which can lead to information leakage or manipulation. This measure helps maintain the integrity of alerting mechanisms and ensures compliance with security regulations like PCI-DSS and ISO 27001, which mandate secure communication channels.
  description: This rule checks that all notification channels configured for incident alerts in GCP Monitoring are authenticated, which typically involves using secure protocols such as HTTPS. To verify, ensure that all notification channels have endpoints configured with authentication methods like OAuth2 or API keys. If any channel lacks authentication, update its configuration to include a secure authentication method to protect the integrity and confidentiality of alert data.
  references:
  - https://cloud.google.com/monitoring/alerts/using-channels-api
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.monitoring.notification_channel.incident_notification_message_encryption_in_transit
  service: monitoring
  resource: notification_channel
  requirement: Incident Notification Message Encryption In Transit
  scope: monitoring.notification_channel.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption for Incident Notification Messages in Transit
  rationale: Encrypting incident notification messages in transit is critical to protect sensitive data from interception and unauthorized access during transmission. Failure to encrypt these messages can lead to data breaches, compromising incident response processes and violating compliance with regulations such as HIPAA and PCI-DSS. This practice safeguards confidentiality and integrity, reducing the risk of exposure to cyber threats and ensuring trust in the monitoring system.
  description: This rule checks that all incident notification messages sent via GCP Monitoring notification channels are encrypted in transit using TLS. Verify that the notification channels, such as email, SMS, or webhooks, are configured to use secure protocols. Remediation involves updating any non-compliant notification channels to enforce TLS encryption, ensuring that all data transmitted is protected against eavesdropping and tampering.
  references:
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
- rule_id: gcp.monitoring.notification_channel.incident_notification_no_public_webhooks
  service: monitoring
  resource: notification_channel
  requirement: Incident Notification No Public Webhooks
  scope: monitoring.notification_channel.public_access
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Prevent Public Webhooks for Incident Notifications
  rationale: Allowing public webhooks for incident notifications can expose sensitive information to unauthorized parties, leading to potential data breaches or misuse of data. This practice can violate compliance requirements such as PCI-DSS and HIPAA, resulting in legal and financial repercussions. Ensuring notifications are sent to secure endpoints mitigates these risks and safeguards organizational data integrity.
  description: This rule checks for the presence of public webhooks in the notification channels configured for incident alerts in GCP Monitoring. Public webhooks can be misused if exposed, thus it is imperative to use secure, authenticated channels. To verify, review the notification channel settings in the GCP console and ensure webhooks are restricted to internal or trusted endpoints. Remediation involves updating any public webhook configurations to secure channels, ideally leveraging OAuth or similar authentication mechanisms.
  references:
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0 - 6.5
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#security
  - NIST SP 800-53 Rev. 5 - SI-5
  - PCI-DSS Requirement 10.6
- rule_id: gcp.multi.region.region_enabled
  service: multi
  resource: region
  requirement: Region Enabled
  scope: multi.region.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure GCP Regions are Enabled for Resource Distribution
  rationale: Enabling multiple GCP regions enhances disaster recovery capabilities and supports compliance with regulatory requirements for data residency. It mitigates the risk of service disruptions by distributing resources across different geographic locations, thereby increasing business continuity and resilience against regional failures.
  description: This rule checks whether multiple regions are enabled within your GCP environment to ensure resource distribution and redundancy. Verify that your resources are not confined to a single region by navigating to the Google Cloud Console, selecting your project, and reviewing the list of enabled regions under the 'Compute Engine' settings. Remediate by enabling additional regions through the console or CLI to distribute critical workloads and improve fault tolerance.
  references:
  - https://cloud.google.com/compute/docs/regions-zones
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/best-practices-for-architecting-on-gcp
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.notebooks.instance.data_governance_ai_notebook_encrypted
  service: notebooks
  resource: instance
  requirement: Data Governance Ai Notebook Encrypted
  scope: notebooks.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Notebook Instances are Encrypted at Rest
  rationale: Encrypting AI Notebook instances protects sensitive data from unauthorized access and data breaches, which can lead to significant business losses and compliance violations. This is crucial for safeguarding intellectual property and meeting regulatory requirements such as GDPR and HIPAA, which mandate data protection measures.
  description: This rule checks that AI Notebook instances in GCP are configured to use encryption at rest, leveraging Google-managed encryption keys or customer-managed encryption keys (CMEK) for enhanced security. To verify, ensure that the 'encryptionConfig' field is set within the instance's configuration. Remediation involves enabling encryption by configuring the notebook instance to use either default Google-managed keys or specifying a CMEK during instance creation or update.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0 - 5.8
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.notebooks.instance.data_governance_ai_notebook_execution_role_least_privilege
  service: notebooks
  resource: instance
  requirement: Data Governance Ai Notebook Execution Role Least Privilege
  scope: notebooks.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Notebook Execution Role
  rationale: Applying the principle of least privilege to AI Notebook execution roles minimizes the risk of unauthorized access and data exfiltration. Misconfigured roles can lead to excessive permissions, exposing sensitive data and potentially violating compliance mandates like GDPR and HIPAA. This is critical for protecting intellectual property and maintaining customer trust.
  description: This rule checks that AI Notebook execution roles are configured with only the necessary permissions to perform their tasks. To verify, review attached IAM policies for roles assigned to notebook instances and ensure no additional permissions are granted beyond what is strictly needed. Remediate by adjusting the IAM policies to remove any superfluous permissions, and implement role-based access controls (RBAC) to further restrict access.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/notebooks/docs/security#restricting-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.notebooks.instance.data_governance_ai_notebook_idle_shutdown_configured
  service: notebooks
  resource: instance
  requirement: Data Governance Ai Notebook Idle Shutdown Configured
  scope: notebooks.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Configure Idle Shutdown for AI Notebook Instances
  rationale: Ensuring that idle AI Notebook instances are automatically shut down helps reduce unnecessary resource consumption and minimizes the risk of data exposure. Unattended instances may lead to unauthorized access or data leaks, impacting business operations and compliance with data protection regulations such as GDPR or HIPAA.
  description: This rule checks whether AI Notebook instances in GCP have idle shutdown configurations enabled. To mitigate security risks and optimize resource use, configure the idle shutdown feature by setting an appropriate timeout. Verification involves ensuring that the 'idle_shutdown' setting is enabled and configured with a reasonable duration. Remediate by accessing the AI Notebook instance settings and setting an idle shutdown timeout under the 'Environment' options in the GCP Console.
  references:
  - https://cloud.google.com/vertex-ai/docs/workbench/user-managed/enable-idle-shutdown
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/best-practices
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.notebooks.instance.data_governance_ai_notebook_no_public_access
  service: notebooks
  resource: instance
  requirement: Data Governance Ai Notebook No Public Access
  scope: notebooks.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to AI Notebooks
  rationale: Allowing public access to AI Notebooks can lead to unauthorized data exposure, potential data breaches, and misuse of computational resources. This poses significant risks, especially in environments handling sensitive data, as it can lead to non-compliance with regulations like GDPR, HIPAA, or PCI-DSS, resulting in financial penalties and reputational damage.
  description: This rule checks for AI Notebook instances in GCP that are publicly accessible over the internet. Public access should be disabled by setting appropriate IAM policies that restrict access only to authorized users within the organization. Ensure firewall rules are configured to block public IPs and that access is granted through VPNs or private endpoints. Regularly audit access logs for any unauthorized access attempts.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/security
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.notebooks.instance.data_privacy_ai_notebook_encrypted
  service: notebooks
  resource: instance
  requirement: Data Privacy Ai Notebook Encrypted
  scope: notebooks.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Notebooks Have Encryption at Rest Enabled
  rationale: Encrypting data at rest in AI Notebooks reduces the risk of unauthorized access and data breaches. It provides a layer of protection against threats such as data theft and accidental exposure, and helps comply with regulations like GDPR, HIPAA, and ISO 27001 that mandate data protection and privacy.
  description: This rule checks if AI Notebook instances in GCP have encryption at rest enabled. Ensure that all sensitive data stored in notebooks is encrypted using either Google-managed keys or customer-managed keys (CMEK) for additional control. To verify, check the encryption configuration in the AI Notebook's settings. Remediate by enabling encryption at rest through the GCP Console or using the gcloud CLI, specifying the appropriate key management option.
  references:
  - https://cloud.google.com/ai-platform-notebooks/docs/security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - 'CIS GCP Benchmark: Section 7.2'
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.notebooks.instance.data_privacy_ai_notebook_execution_role_least_privilege
  service: notebooks
  resource: instance
  requirement: Data Privacy Ai Notebook Execution Role Least Privilege
  scope: notebooks.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Notebook Execution Roles
  rationale: Implementing least privilege for AI Notebook execution roles minimizes the risk of data exposure and unauthorized access. Overly permissive roles can lead to data breaches, non-compliance with privacy regulations such as GDPR, and potential financial loss due to data misuse. Ensuring roles have only necessary permissions protects sensitive data and aligns with best practices in access management.
  description: This rule checks that AI Notebook instances in GCP are assigned execution roles with the minimum set of permissions needed to perform their functions. Review current role assignments on notebook instances and adjust the IAM policies to restrict permissions to only those necessary. Remediation involves using IAM policy bindings to assign roles at the granularity level required, avoiding overly broad roles like 'Owner' or 'Editor'. Verify by auditing the IAM roles attached to notebook instances and ensuring compliance with the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/best-practices/iam
  - https://cloud.google.com/compute/docs/access/identity-access-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/notebooks/docs
- rule_id: gcp.notebooks.instance.data_privacy_ai_notebook_idle_shutdown_configured
  service: notebooks
  resource: instance
  requirement: Data Privacy Ai Notebook Idle Shutdown Configured
  scope: notebooks.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Configure Idle Shutdown for AI Notebooks to Protect Data
  rationale: Configuring idle shutdown for AI Notebooks helps mitigate the risk of unauthorized access and reduces cost by ensuring that resources are not left running unnecessarily. This is critical in protecting sensitive data, as idle notebooks can be exploited if left unattended. Compliance with data protection regulations, such as GDPR and HIPAA, necessitates minimizing the exposure of data through idle compute resources.
  description: This check verifies that AI Notebooks have idle shutdown configured to automatically power down after a period of inactivity. To ensure this, set the 'idle_shutdown' and 'idle_shutdown_timeout' parameters within the notebook's metadata. Verification involves reviewing the notebook's metadata settings for these parameters. Remediation includes specifying an appropriate timeout duration in the GCP Console or using gcloud commands to update existing notebooks.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/configure-instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.notebooks.instance.data_privacy_ai_notebook_no_public_access
  service: notebooks
  resource: instance
  requirement: Data Privacy Ai Notebook No Public Access
  scope: notebooks.instance.public_access
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: critical
  title: Disable Public Access to AI Notebooks for Data Privacy
  rationale: Allowing public access to AI Notebooks can expose sensitive data and intellectual property to unauthorized users, increasing the risk of data breaches and non-compliance with data protection regulations such as GDPR and CCPA. Publicly accessible notebooks may inadvertently allow attackers to execute arbitrary code, leading to potential data exfiltration or service disruption.
  description: This rule checks if AI Notebooks have public IP addresses assigned, which can expose them to the internet. Ensure that AI Notebooks are configured with private IP addresses or behind a secure VPN. Verify by reviewing the notebook's network settings via the GCP console or using the `gcloud` CLI. Remediate by updating the notebook's network configuration to restrict access to trusted networks only.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.notebooks.instance.machine_learning_notebook_encrypted
  service: notebooks
  resource: instance
  requirement: Machine Learning Notebook Encrypted
  scope: notebooks.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of Machine Learning Notebooks at Rest
  rationale: Encrypting machine learning notebooks at rest protects sensitive data from unauthorized access, minimizing the risk of data breaches. This is crucial for maintaining customer trust and complying with data protection regulations such as GDPR and HIPAA. Failure to encrypt these assets could result in exposure of proprietary algorithms and datasets, leading to significant business and reputational damage.
  description: This rule checks if Machine Learning Notebooks in GCP are encrypted at rest using Customer Managed Encryption Keys (CMEK) or Google-managed keys. To verify, ensure the 'encryptionConfig' field is set for each notebook instance. Remediation involves configuring encryption settings during instance creation or updating existing instances to enforce encryption policies. This step is vital to safeguard data against unauthorized access and meet compliance requirements.
  references:
  - https://cloud.google.com/notebooks/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
- rule_id: gcp.notebooks.instance.machine_learning_notebook_execution_role_least_privilege
  service: notebooks
  resource: instance
  requirement: Machine Learning Notebook Execution Role Least Privilege
  scope: notebooks.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for ML Notebook Execution Roles
  rationale: Applying the principle of least privilege to machine learning notebook execution roles minimizes the risk of unauthorized access and data breaches. With excessive privileges, compromised accounts can lead to lateral movement and data exfiltration. This practice is essential for meeting compliance requirements such as PCI-DSS and ISO 27001, which mandate strict access controls.
  description: This rule checks whether machine learning notebook execution roles in GCP are configured with the least privilege necessary. Review the roles assigned to instances to ensure they only have permissions required for their specific tasks. Remediate by auditing IAM policies and removing unnecessary permissions, following the principle of least privilege. Validate configurations using the IAM section of the GCP Console or via the gcloud command-line tool.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/notebooks/docs/security
  - https://cloud.google.com/iam/docs/best-practices
- rule_id: gcp.notebooks.instance.machine_learning_notebook_idle_shutdown_configured
  service: notebooks
  resource: instance
  requirement: Machine Learning Notebook Idle Shutdown Configured
  scope: notebooks.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Configure Idle Shutdown for ML Notebooks
  rationale: Unattended machine learning notebooks can lead to unnecessary cloud resource consumption, increased costs, and potential exposure to unauthorized access. Configuring idle shutdown helps mitigate these risks by ensuring that notebooks are not left running without supervision, aligning with cost management and security best practices.
  description: This rule checks whether machine learning notebooks on Google Cloud Platform have idle shutdown configured. Idle shutdown ensures that notebooks automatically stop after a specified period of inactivity, reducing the attack surface and optimizing resource usage. Verify by checking the notebook instance settings and ensure the 'idle shutdown' feature is enabled with an appropriate timeout period. Remediation involves accessing the notebook's configuration and setting the 'idle shutdown' parameter to a suitable timeout value based on organizational policies.
  references:
  - https://cloud.google.com/vertex-ai/docs/general/notebooks
  - https://cloud.google.com/vertex-ai/docs/workbench/managed/protection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.notebooks.instance.machine_learning_notebook_no_public_access
  service: notebooks
  resource: instance
  requirement: Machine Learning Notebook No Public Access
  scope: notebooks.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Access for ML Notebooks Instances
  rationale: Allowing public access to Machine Learning Notebooks can expose sensitive data and computational resources to unauthorized users, increasing the risk of data breaches and misuse of resources. Ensuring notebooks are not publicly accessible helps comply with security best practices and regulatory requirements, such as GDPR and HIPAA, by protecting data confidentiality and integrity.
  description: This rule checks if any Google Cloud Machine Learning Notebook instances are configured with public access. To secure the environment, verify that all notebook instances are restricted to private IPs or specific trusted networks. Remediation involves modifying the instance's network settings to disable public IPs and using IAM to restrict access to authorized users only.
  references:
  - https://cloud.google.com/notebooks/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/nist-sp-800-53-revision-5
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.osconfig.guest_policy.patch_compliance_configured
  service: osconfig
  resource: guest_policy
  requirement: Patch Compliance Configured
  scope: osconfig.guest_policy.compliance
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: medium
  title: Ensure OS Config Patch Compliance is Configured
  rationale: Unpatched systems can be exploited by attackers to gain unauthorized access or disrupt services. Configuring patch compliance ensures vulnerabilities are addressed promptly, reducing the risk of breaches and downtime. This is crucial for meeting compliance requirements such as NIST SP 800-53 and maintaining customer trust.
  description: This rule checks whether patch compliance is configured in Google Cloud's OS Config service for guest policies. It evaluates if patch configurations are specified to ensure that systems are regularly updated. To verify, review your guest policy settings in the Google Cloud Console under 'OS Config' and ensure that patch compliance settings are active. Remediation involves defining a patch compliance policy that specifies update schedules and critical patch installations.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/managing-instances
- rule_id: gcp.osconfig.patch_deployment.compliant_patching
  service: osconfig
  resource: patch_deployment
  requirement: Compliant Patching
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Deployment Compliance in OS Config
  rationale: Regular patch deployment is crucial to protect systems from vulnerabilities that could be exploited by attackers, leading to data breaches or service disruptions. Ensuring compliant patching helps maintain system integrity, reduces the risk of security incidents, and aligns with regulatory standards such as PCI-DSS and NIST, which require timely application of security patches.
  description: This rule checks if patch deployments in OS Config are configured to apply updates within a defined schedule, ensuring that all instances are patched in compliance with security policies. Verify that patch deployments are set up with correct schedules and target the intended instances. To remediate, configure patch deployments with appropriate settings using the Google Cloud Console or gcloud CLI to ensure that all compute instances receive necessary security updates promptly.
  references:
  - https://cloud.google.com/compute/docs/os-patch-management
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/document-800-53
  - https://cloud.google.com/blog/products/identity-security/implementing-patch-management-strategy-with-google-cloud
- rule_id: gcp.osconfig.patch_deployment.config_managed_compliant_patching_configured
  service: osconfig
  resource: patch_deployment
  requirement: Config Managed Compliant Patching Configured
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Managed Compliant Patching is Configured for OS Config
  rationale: Properly configured compliant patching ensures that all instances are up-to-date with security patches, reducing the risk of vulnerabilities and potential exploits. This is critical for maintaining the integrity of systems, preventing unauthorized access, and complying with regulatory standards such as PCI-DSS and HIPAA. Inadequate patch management can lead to breaches and data loss, impacting business continuity and reputation.
  description: This rule checks that Google Cloud OS Config is configured to manage compliant patching for all patch deployments. Specifically, it verifies that patch deployments have a defined configuration for automated and regular security patches. To verify, ensure that patch deployments include a patch configuration that aligns with your organization's policies. Remediation involves setting up a patch deployment schedule in OS Config and specifying the required patch compliance settings to automate security updates.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-securing-your-google-cloud
- rule_id: gcp.osconfig.patch_deployment.managed_compliant_patching
  service: osconfig
  resource: patch_deployment
  requirement: Managed Compliant Patching
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Deployments Are Managed and Compliant
  rationale: Regular patch management is critical to protect systems against vulnerabilities that can be exploited by attackers, potentially leading to data breaches or system compromise. Managed compliant patching helps maintain system integrity, reduces the risk of exposure to known threats, and supports regulatory compliance with standards like PCI-DSS and HIPAA.
  description: This rule checks whether patch deployments on Google Cloud Platform are configured for compliant patch management using OS Configuration. Ensure that patch deployment schedules are defined and that compliance reports are regularly reviewed to verify successful patching. Remediation involves configuring patch deployments via the GCP Console or API to automate updates and setting alerts for failed patches.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.hhs.gov/sites/default/files/hipaa-security-series-4-technical-safeguards.pdf
  - https://cloud.google.com/compute/docs/os-patch-management
  - https://cloud.google.com/security/compliance
- rule_id: gcp.osconfig.patch_deployment.managed_compliant_patching_gcp_gke_cluster_uses_a_su_enabled
  service: osconfig
  resource: patch_deployment
  requirement: Managed Compliant Patching Gcp Gke Cluster Uses A Su Enabled
  scope: osconfig.patch_deployment.patch_management
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Ensure SU Enabled for GKE Cluster Patch Deployments
  rationale: Enabling SU for GKE cluster patch deployments ensures that patches are applied under elevated privileges, reducing the risk of unauthorized access and potential exploitation. This enhances the security posture by ensuring that critical security updates are applied promptly and correctly, mitigating vulnerabilities that could be exploited by threat actors. Compliance with regulatory standards often requires such stringent patch management practices to protect sensitive data and maintain system integrity.
  description: This rule checks if the GKE clusters utilize an SU-enabled patch deployment configuration within the Google Cloud OS Config service. It ensures that patches are deployed with superuser privileges, which is critical for applying certain types of updates that require elevated permissions. To verify compliance, audit your patch deployment configurations and ensure that SU is enabled. Remediation involves updating patch deployment settings to enable SU, which can be done via the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-upgrade
  - https://cloud.google.com/security/compliance/cis-gcp-1-0
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.osconfig.patch_deployment.patch_deployment_exists_configured
  service: osconfig
  resource: patch_deployment
  requirement: Patch Deployment Exists Configured
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Deployment Configured for OS Patching
  rationale: Unpatched systems are vulnerable to security exploits, which can lead to unauthorized access and data breaches. Configuring patch deployments ensures that systems are regularly updated, reducing exposure to known vulnerabilities and maintaining compliance with standards like PCI-DSS and ISO 27001. Automated patch management minimizes downtime and operational disruptions.
  description: This rule checks if patch deployment configurations are set up in GCP OS Config to manage system updates automatically. Verify that patch deployments exist and are scheduled to run at appropriate intervals. Remediation involves creating patch deployment schedules that include approved patches, specifying target VM instances, and setting maintenance windows to minimize impact on operations. Ensure that all critical and security patches are applied in a timely manner.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/blog/products/identity-security/securing-your-vms-with-os-patch-management
- rule_id: gcp.osconfig.patch_deployment.vuln_baseline_exemptions_have_expiry_and_owner
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Baseline Exemptions Have Expiry And Owner
  scope: osconfig.patch_deployment.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Patch Deployment Exemptions Have Expiry and Owner
  rationale: Without expiry dates and ownership assignment for vulnerability baseline exemptions, organizations risk leaving systems unpatched indefinitely, increasing exposure to security threats. This oversight can lead to non-compliance with industry regulations and standards, potentially resulting in financial penalties and reputational damage. Assigning ownership ensures accountability and timely review of exemptions.
  description: This rule checks that all vulnerability baseline exemptions in patch deployments have a specified expiry date and an assigned owner. These configurations help ensure that exemptions are temporary and reviewed periodically. To verify, examine the patch deployment configurations in the GCP Console or via gcloud CLI to ensure 'expiryDate' and 'owner' fields are populated. Remediate by updating configurations to include these fields where missing.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.osconfig.patch_deployment.vuln_baseline_required_controls_present
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Baseline Required Controls Present
  scope: osconfig.patch_deployment.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Vulnerability Baseline Controls in Patch Deployments
  rationale: Insufficient patch management can leave systems vulnerable to exploits, leading to potential data breaches, service disruptions, and non-compliance with regulatory frameworks such as PCI-DSS and HIPAA. Establishing a vulnerability baseline ensures that all instances meet security standards, reducing the risk of attack vectors through unpatched software.
  description: This rule checks that all patch deployments in Google Cloud OS Config include required vulnerability baseline controls. It verifies that patch deployments are configured to address critical and high-severity vulnerabilities. To comply, ensure that patch deployment configurations specify required baseline criteria and that patches are applied within the defined maintenance window. Remediation involves updating patch deployment configurations to meet security baseline requirements and scheduling regular patch assessments.
  references:
  - https://cloud.google.com/compute/docs/os-patch-management
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-40r3.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-compliance-guide/
- rule_id: gcp.osconfig.patch_deployment.vuln_maintenance_execution_roles_least_privilege
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Maintenance Execution Roles Least Privilege
  scope: osconfig.patch_deployment.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Patch Deployment Roles
  rationale: Properly scoped IAM roles prevent unauthorized access and modifications, minimizing the risk of privilege escalation and data breaches. Ensuring least privilege aligns with regulatory requirements such as ISO 27001 and PCI-DSS, reducing the attack surface and protecting sensitive workloads from potential exploitation during patch management operations.
  description: This rule checks that IAM roles assigned for executing patch deployments in the osconfig service are minimized to the least privilege necessary. Verify that roles such as 'osconfig.patchDeploymentRunner' have only the permissions needed for their specific tasks without additional access rights. To remediate, review current IAM policies for patch deployment and adjust them to restrict access based on the principle of least privilege, removing unnecessary permissions and ensuring proper role assignment.
  references:
  - https://cloud.google.com/osconfig/docs
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/pdf/cis-gcp-foundations-benchmark-v1.3.0.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.osconfig.patch_deployment.vuln_maintenance_window_defined
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Maintenance Window Defined
  scope: osconfig.patch_deployment.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Define Vulnerability Maintenance Window for Patch Deployment
  rationale: Defining a maintenance window for patch deployment ensures timely application of security patches, reducing exposure to vulnerabilities. This minimizes the risk of exploitation by threat actors and maintains system integrity. Proper scheduling also aligns with business operation times, minimizing disruption and meeting compliance with security standards.
  description: This rule checks whether a maintenance window is defined for each patch deployment in the GCP OS Config service. A well-defined maintenance window ensures that security patches are applied consistently and at times that minimize business impact. To verify, inspect the patch deployment configuration for a specified maintenance window. Remediate by adding a maintenance window to the patch deployment settings through the GCP Console or via API.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.osconfig.patch_deployment.vuln_patch_approval_rules_defined
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Patch Approval Rules Defined
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Define Vulnerability Patch Approval Rules for Patch Deployments
  rationale: Defining vulnerability patch approval rules in GCP's OS Config ensures that all patches, especially critical security updates, are reviewed and approved before deployment. This reduces the risk of deploying untested patches that might cause system instability or downtime. It also helps in meeting regulatory requirements and maintaining a secure and compliant cloud environment.
  description: This rule checks whether vulnerability patch approval rules are defined for patch deployments in Google Cloud's OS Config service. Ensuring these rules are set up means that any patches, particularly those addressing known vulnerabilities, are subjected to an approval process before being applied to instances. To verify compliance, review the patch deployment configurations in your GCP Console and ensure that approval rules are specified. Remediation involves defining approval rules that align with your organizationâ€™s security policies and updating the patch deployment settings accordingly.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - NIST SP 800-53 Rev. 5 CM-5
  - PCI-DSS v3.2.1 Requirement 6.3
  - https://cloud.google.com/security/best-practices
  - ISO/IEC 27001:2013 A.12.6.1
- rule_id: gcp.osconfig.patch_deployment.vuln_patch_baseline_defined_for_families
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Patch Baseline Defined For Families
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Vulnerability Patch Baseline for OS Families in GCP
  rationale: Defining a vulnerability patch baseline for operating system families is crucial for maintaining system integrity and reducing the risk of exploitation through known vulnerabilities. Organizations face threats like data breaches and service disruptions when systems are left unpatched. Ensuring a baseline helps comply with standards like PCI-DSS and ISO 27001, which mandate timely patch management.
  description: This rule checks that a vulnerability patch baseline is defined for each operating system family in the Google Cloud Platform using the OS Config service. It ensures that patch deployments are configured to apply security patches to all instances, reducing exposure to vulnerabilities. Verification involves reviewing the OS Config patch deployment settings to confirm that vulnerability patching is enabled for all relevant OS families. Remediation requires updating the patch deployment to include a defined baseline for security updates.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/instances/patch-management
- rule_id: gcp.osconfig.patch_deployment.vuln_patch_maintenance_windows_configured
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Patch Maintenance Windows Configured
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Maintenance Windows Are Configured for Vulnerability Patching
  rationale: Properly configuring patch maintenance windows for vulnerability management ensures that critical and high-severity patches are applied in a timely manner, reducing the risk of exploitation. This practice helps maintain system integrity and availability, minimizes downtime during patching, and supports compliance with security standards such as ISO 27001 and NIST SP 800-53.
  description: This rule checks if patch deployments are configured with maintenance windows in Google Cloud's OS Config service. A maintenance window specifies when a patch deployment can be executed, which is crucial for planning and minimizing disruption. To verify, ensure that each patch deployment has a defined schedule for maintenance windows. Remediation involves configuring the patch deployment with a suitable maintenance window through the OS Config API or Cloud Console, ensuring alignment with operational needs.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/compute/docs/osconfig-management/create-patch-deployments
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.pubsub.subscription.data_analytics_event_cross_account_sharing_restricted
  service: pubsub
  resource: subscription
  requirement: Data Analytics Event Cross Account Sharing Restricted
  scope: pubsub.subscription.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Sharing for Pub/Sub Data Analytics Events
  rationale: Ensuring that Pub/Sub subscriptions do not allow cross-account sharing of data analytics events is crucial to prevent unauthorized access and data breaches. This restriction is essential for maintaining data privacy and adhering to compliance requirements such as GDPR and CCPA. Unrestricted sharing can lead to violation of data protection standards, resulting in legal penalties and damage to an organization's reputation.
  description: This rule verifies that Pub/Sub subscriptions are configured to restrict cross-account sharing of data analytics events. It ensures that only authorized accounts have access to sensitive data. To comply, review the IAM policies associated with your Pub/Sub subscriptions and ensure that only trusted accounts have access. Remediation involves auditing current permissions and revoking any unnecessary cross-account access. Use the GCP Console or gcloud CLI to update IAM policies appropriately.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.1
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/pubsub/docs/quickstart-console
- rule_id: gcp.pubsub.subscription.data_analytics_event_destination_encrypted
  service: pubsub
  resource: subscription
  requirement: Data Analytics Event Destination Encrypted
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Analytics Destinations are Encrypted
  rationale: Encrypting data at rest for Pub/Sub analytics event destinations is crucial to protect sensitive information from unauthorized access and data breaches, especially in environments handling regulated data. This measure helps satisfy compliance requirements such as GDPR and HIPAA, and mitigates risks associated with data leakage and cyber threats.
  description: This rule checks if Pub/Sub subscriptions for data analytics events have encryption at rest enabled using customer-managed keys (CMKs) or Google-managed encryption keys. To verify, ensure that the 'encryptionConfig' field within your Pub/Sub subscription configuration is defined and points to the correct Cloud KMS key. Remediation involves updating the subscription settings to specify an appropriate encryption key using the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/kms/docs
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 4.2
  - https://cloud.google.com/security/compliance
  - NIST SP 800-57 Part 1 - Recommendation for Key Management
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.pubsub.subscription.data_analytics_event_destination_least_privilege
  service: pubsub
  resource: subscription
  requirement: Data Analytics Event Destination Least Privilege
  scope: pubsub.subscription.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Pub/Sub Subscriptions Use Least Privilege for Data Analytics
  rationale: Implementing least privilege for Pub/Sub subscription permissions minimizes the risk of unauthorized access to sensitive data. Over-permissioned roles can lead to data breaches, compliance violations, and increased exposure to insider threats, especially in environments handling critical data analytics. Adhering to least privilege is essential for meeting compliance with standards like NIST and GDPR.
  description: This rule checks that IAM permissions for Pub/Sub subscriptions used in data analytics are configured with the principle of least privilege. It ensures that only necessary roles and permissions are granted to service accounts or users interacting with these subscriptions. Verify by reviewing IAM policies and removing excessive permissions that are not required for intended tasks. Remediate by using predefined roles specific to Pub/Sub and limiting access to essential permissions only.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance/cis#section-5.3
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/docs/iam
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.pubsub.subscription.data_warehouse_event_cross_account_sharing_restricted
  service: pubsub
  resource: subscription
  requirement: Data Warehouse Event Cross Account Sharing Restricted
  scope: pubsub.subscription.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Sharing for Pub/Sub Subscriptions
  rationale: Restricting cross-account sharing for Pub/Sub subscriptions helps prevent unauthorized access to sensitive data events, reducing the risk of data breaches. This is crucial for maintaining data privacy and meeting compliance requirements such as GDPR and CCPA, which mandate data protection across boundaries. Unauthorized access could lead to misuse or exposure of sensitive data, affecting business reputation and incurring potential legal penalties.
  description: This rule checks if any Pub/Sub subscriptions associated with data warehouse events are shared across different accounts unnecessarily. Ideally, subscriptions should be limited to accounts within the same organization to ensure data is not exposed to unauthorized parties. To verify, review the IAM policies attached to Pub/Sub subscriptions for non-organizational accounts and adjust permissions accordingly. Remediation involves updating IAM roles to ensure only authorized accounts have access, following the principle of least privilege.
  references:
  - https://cloud.google.com/pubsub/docs/security
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0 - Recommendation 6.3
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.pubsub.subscription.data_warehouse_event_destination_encrypted
  service: pubsub
  resource: subscription
  requirement: Data Warehouse Event Destination Encrypted
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Subscription Event Destination is Encrypted
  rationale: Encryption of data at rest is crucial for protecting sensitive information from unauthorized access and potential data breaches. Inadequately encrypted data can lead to exposure of confidential business information, violating compliance with standards such as PCI-DSS and HIPAA. Encrypting data within Pub/Sub ensures that event data sent to data warehouses is secure, maintaining data integrity and privacy.
  description: This rule checks if the Google Cloud Pub/Sub subscription used as a data warehouse event destination is encrypted at rest. Specifically, it verifies that Cloud KMS keys are used to encrypt the data. To ensure compliance, configure the subscription to use customer-managed encryption keys (CMEK) via the Pub/Sub console or gcloud CLI. Remediation involves setting the appropriate CMEK during the subscription creation or updating the configuration to include encryption settings.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/overview
  - https://cloud.google.com/security/encryption-at-rest/
- rule_id: gcp.pubsub.subscription.data_warehouse_event_destination_least_privilege
  service: pubsub
  resource: subscription
  requirement: Data Warehouse Event Destination Least Privilege
  scope: pubsub.subscription.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Pub/Sub Data Warehouse Subscriptions
  rationale: Granting excessive permissions to Pub/Sub subscriptions for data warehouse events can lead to unauthorized data access and potential data breaches. Ensuring least privilege helps mitigate the risk of data exposure and supports compliance with data protection standards such as PCI-DSS and ISO 27001, which require stringent access controls.
  description: This rule verifies that Pub/Sub subscriptions used as event destinations for data warehouses have the minimal set of permissions necessary to function. It checks the IAM policies attached to these subscriptions to ensure they are not overly permissive. To remediate, audit the IAM roles associated with the subscription and adjust them to restrict access to only what is necessary for the required operations.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.pubsub.subscription.streaming_consumer_access_least_privilege
  service: pubsub
  resource: subscription
  requirement: Streaming Consumer Access Least Privilege
  scope: pubsub.subscription.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Pub/Sub Subscription Consumers
  rationale: Implementing least privilege for Pub/Sub subscription consumers minimizes the risk of data breaches by reducing the attack surface. Over-permissioned roles can lead to unauthorized access to sensitive messages, potentially violating compliance with standards such as PCI-DSS and HIPAA. Ensuring least privilege supports secure operations and protects organizational data integrity.
  description: This rule checks that IAM roles granted to streaming consumers on Pub/Sub subscriptions have only the necessary permissions to perform their tasks. Verify and adjust roles to ensure consumers cannot perform actions beyond what is needed for their specific function, such as removing 'roles/pubsub.viewer' if 'roles/pubsub.subscriber' suffices. Remediation involves auditing IAM policies and using the principle of least privilege to restrict permissions accordingly.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.pubsub.subscription.streaming_consumer_auth_required
  service: pubsub
  resource: subscription
  requirement: Streaming Consumer Auth Required
  scope: pubsub.subscription.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Pub/Sub Streaming Consumers Use Authenticated Connections
  rationale: Requiring authentication for Pub/Sub streaming consumers mitigates the risk of unauthorized data access and potential data leakage. It helps maintain data integrity and confidentiality, ensuring that only authorized entities can consume sensitive messages. This is critical for meeting compliance requirements and protecting intellectual property.
  description: This rule checks that all Pub/Sub subscriptions configured for streaming pull operations require authentication using service accounts. To verify, ensure that IAM roles granting subscriber permissions are only assigned to trusted service accounts. Remediation involves auditing current IAM policies and updating them to restrict unauthorized access by enforcing service account usage for streaming consumers.
  references:
  - https://cloud.google.com/pubsub/docs/authentication
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - 'CIS GCP Benchmark: Ensure Pub/Sub topics and subscriptions are configured with appropriate IAM policies'
  - 'NIST SP 800-53 Rev. 5: AC-6 Least Privilege'
- rule_id: gcp.pubsub.subscription.streaming_destination_encrypted
  service: pubsub
  resource: subscription
  requirement: Streaming Destination Encrypted
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Streaming Destination Encryption at Rest
  rationale: Encrypting Pub/Sub subscription data at rest is crucial for protecting sensitive information from unauthorized access and meeting compliance mandates such as GDPR and HIPAA. Without encryption, data is vulnerable to breaches, potentially leading to financial loss and reputational damage. It is essential for organizations to ensure that all data stored in Google Cloud Pub/Sub is encrypted to mitigate these risks.
  description: This rule checks that all Google Cloud Pub/Sub subscriptions have their streaming data destinations encrypted at rest using Customer-Managed Encryption Keys (CMEK) or Google-managed encryption. Verify that the `kmsKeyName` is set for subscriptions requiring CMEK. If not configured, apply the necessary encryption settings via the Google Cloud Console or gcloud CLI to ensure compliance with data protection policies.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.pubsub.subscription.streaming_destination_least_privilege
  service: pubsub
  resource: subscription
  requirement: Streaming Destination Least Privilege
  scope: pubsub.subscription.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Pub/Sub Subscriptions Use Least Privilege
  rationale: Implementing least privilege for Pub/Sub subscriptions minimizes the risk of unauthorized data access and potential data breaches. Misconfigured permissions can lead to privilege escalation, allowing malicious actors to exploit data streams. Ensuring least privilege aligns with compliance standards like PCI-DSS and ISO 27001, protecting sensitive data and maintaining trust.
  description: This rule checks that IAM roles assigned to Pub/Sub subscriptions grant only the necessary permissions for their specific task. Review and adjust the IAM policies associated with your subscriptions to ensure they follow the principle of least privilege. Verify roles via Google Cloud Console or gcloud CLI and limit permissions to only those required for the subscription's intended function. Regular audits and updates to these roles are recommended to adapt to evolving security needs.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/best-practices
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.pubsub.subscription.streaming_encryption_at_rest_enabled
  service: pubsub
  resource: subscription
  requirement: Streaming Encryption At Rest Enabled
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Pub/Sub Subscription Encryption At Rest is Enabled
  rationale: Enabling encryption at rest for Pub/Sub subscriptions protects sensitive data from unauthorized access and potential breaches. This is crucial for maintaining data confidentiality and integrity, especially for organizations handling sensitive or regulated information. Failure to encrypt data at rest can lead to severe legal penalties, reputational damage, and financial loss in the event of a data breach.
  description: This rule verifies that Google Cloud Pub/Sub subscriptions have streaming encryption at rest enabled to protect stored messages. To ensure compliance, users should verify that a Customer-Managed Encryption Key (CMEK) or Google-managed key is applied to all subscriptions. If encryption is not enabled, configure the subscription to use a CMEK via the Google Cloud Console, CLI, or API, thus enhancing data security and meeting regulatory requirements.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.pubsub.subscription.streaming_logging_enabled
  service: pubsub
  resource: subscription
  requirement: Streaming Logging Enabled
  scope: pubsub.subscription.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Pub/Sub Subscription Has Streaming Logging Enabled
  rationale: Enabling streaming logging for Pub/Sub subscriptions is critical for security and operational visibility. It allows tracking of message delivery and access patterns, which helps in detecting unauthorized use, troubleshooting issues, and complying with data governance policies. Without logging, organizations may face increased risk of undetected data breaches and non-compliance with standards like PCI-DSS and SOC 2.
  description: This rule checks whether streaming logging is enabled for Google Cloud Pub/Sub subscriptions. Streaming logs capture detailed information about message delivery activities and access events. Organizations should enable this feature through the Google Cloud Console or via the gcloud command-line tool. To remediate, configure the subscription to send logs to a Cloud Logging sink, ensuring real-time monitoring and alerting capabilities are active.
  references:
  - https://cloud.google.com/pubsub/docs/monitoring
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/pubsub/docs/admin
- rule_id: gcp.pubsub.subscription.streaming_private_network_only_where_supported
  service: pubsub
  resource: subscription
  requirement: Streaming Private Network Only Where Supported
  scope: pubsub.subscription.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Pub/Sub Subscriptions Use Private Network for Streaming
  rationale: Configuring Pub/Sub subscriptions to use a private network minimizes exposure to the public internet, reducing the risk of unauthorized access and data breaches. This aligns with best practices for network segmentation and fulfills compliance requirements for data protection, especially in regulated industries such as finance and healthcare.
  description: This rule checks if Google Cloud Pub/Sub subscriptions that support private access are configured to use a private network for streaming. To verify, ensure that the 'usePrivateEndpoint' setting is enabled. Remediation involves configuring the Pub/Sub subscription to use VPC Service Controls, restricting network access to trusted internal IP addresses, and ensuring that the 'usePrivateEndpoint' flag is set to true.
  references:
  - https://cloud.google.com/pubsub/docs/private-access-options
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/document-2670
  - https://cloud.google.com/vpc-service-controls/docs/
- rule_id: gcp.pubsub.subscription.streaming_video_encryption_at_rest_enabled
  service: pubsub
  resource: subscription
  requirement: Streaming Video Encryption At Rest Enabled
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Pub/Sub Subscriptions Encrypt Streaming Video At Rest
  rationale: Encrypting streaming video data at rest in Pub/Sub mitigates the risk of unauthorized data access and potential data breaches, which can result in substantial financial losses and damage to reputation. Compliance with standards such as PCI-DSS and HIPAA requires that sensitive data is protected through encryption, thus reducing the likelihood of regulatory fines and ensuring trust with customers and stakeholders.
  description: This rule verifies that Google Cloud Pub/Sub subscriptions handling streaming video data are configured to use Customer-Managed Encryption Keys (CMEK) for encryption at rest. Check that each subscription is associated with a CMEK in the subscription's settings. Remediate by updating the subscription configuration to include a reference to a valid CMEK, ensuring that sensitive video data is encrypted according to your security requirements.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/kms/docs/cmek
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.pubsub.subscription.streaming_video_encryption_in_transit_required
  service: pubsub
  resource: subscription
  requirement: Streaming Video Encryption In Transit Required
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption In Transit for Pub/Sub Video Streams
  rationale: Encryption in transit protects streaming video data from interception and unauthorized access during transmission. This is crucial for safeguarding sensitive content, maintaining user privacy, and meeting compliance requirements such as GDPR and CCPA. Failure to encrypt data in transit can lead to data breaches, reputational damage, and financial penalties.
  description: This rule checks that all Pub/Sub subscriptions handling streaming video data have encryption in transit enabled. Verify that the subscription's client uses TLS 1.2 or higher to encrypt data during transmission. Remediation involves configuring the client library to establish secure connections and updating any existing subscriptions that do not use TLS. Regular audits and monitoring should ensure continuous compliance.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.pubsub.subscription.streaming_video_retention_days_minimum_configured
  service: pubsub
  resource: subscription
  requirement: Streaming Video Retention Days Minimum Configured
  scope: pubsub.subscription.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Minimum Retention for Streaming Video Subscriptions in Pub/Sub
  rationale: Setting a minimum retention period for streaming video data in Pub/Sub is crucial to ensure that data is available for auditing, troubleshooting, and compliance purposes. Insufficient retention can lead to data loss, impacting business operations and regulatory compliance, especially in industries with strict data governance requirements.
  description: This rule checks that Google Cloud Pub/Sub subscriptions associated with streaming video data have a minimum retention period configured. Specifically, it ensures that the 'ackDeadlineSeconds' and 'retainAckedMessages' settings are appropriately configured to prevent premature data deletion. To verify, inspect the subscription configuration in the GCP Console or via the gcloud command-line tool. If not compliant, update the settings to meet the required retention period.
  references:
  - https://cloud.google.com/pubsub/docs/subscriber
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/pubsub/docs/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.pubsub.subscription.topic_subscription_configured
  service: pubsub
  resource: subscription
  requirement: Topic Subscription Configured
  scope: pubsub.subscription.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Pub/Sub Subscription Has Associated Topic
  rationale: Having a topic associated with a Pub/Sub subscription is crucial for message delivery. Without it, the subscription cannot receive messages, potentially leading to loss of important data or delayed processing. This configuration is essential for maintaining operational integrity and adhering to compliance requirements that mandate reliable data processing and transmission.
  description: This rule checks whether a Pub/Sub subscription is properly configured with an associated topic. A subscription without an associated topic cannot receive messages, which may result in data loss and operational inefficiencies. To verify, ensure each subscription has a 'topic' field specified in its configuration. Remediation involves configuring the 'topic' field in the subscription settings through the GCP Console or gcloud CLI, linking it to the required Pub/Sub topic.
  references:
  - https://cloud.google.com/pubsub/docs/subscriber
  - https://cloud.google.com/pubsub/docs/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/pubsub/docs/admin
- rule_id: gcp.pubsub.topic.data_protection_storage_queue_auth_required
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Queue Auth Required
  scope: pubsub.topic.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Topics Require Authentication
  rationale: Enforcing authentication for Pub/Sub topics is crucial to prevent unauthorized access and data breaches. Without proper authentication, sensitive data can be exposed to malicious actors, leading to potential data loss, financial damage, and non-compliance with regulatory frameworks such as GDPR and HIPAA.
  description: This rule checks that all Pub/Sub topics have authentication configured to ensure only authorized entities can publish or subscribe to messages. Ensure that the 'requireAuthentication' setting is enabled for each topic. Use IAM roles and policies to manage access and verify configurations through the GCP Console or gcloud command-line tool. To remediate, apply IAM policies that restrict access to required users and service accounts.
  references:
  - https://cloud.google.com/pubsub/docs/security-overview
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - NIST SP 800-53
  - CIS GCP Benchmark v1.2.0
- rule_id: gcp.pubsub.topic.data_protection_storage_queue_cross_account_send__restricted
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Queue Cross Account Send Restricted
  scope: pubsub.topic.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Data Sent via Pub/Sub Topics
  rationale: Restricting cross-account data sends in Pub/Sub topics is critical to prevent unauthorized access and data leakage between different projects or organizations. This control helps mitigate risks associated with data exfiltration and ensures adherence to data protection regulations such as GDPR and CCPA, which require stringent controls over data access and sharing.
  description: This rule checks that Pub/Sub topics are configured to restrict cross-account data sends, ensuring that only authorized entities within the same account can publish messages. Verify that the IAM policies on Pub/Sub topics do not allow 'roles/pubsub.publisher' or similar permissions to external accounts. Remediate by reviewing and refining IAM policies, ensuring only necessary internal roles have publishing capabilities, and use VPC Service Controls to further restrict data movement.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance/cis#gcp_pubsub
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/vpc-service-controls/docs/overview
- rule_id: gcp.pubsub.topic.data_protection_storage_queue_encryption_at_rest_enabled
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Queue Encryption At Rest Enabled
  scope: pubsub.topic.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Pub/Sub Topic Encryption at Rest is Enabled
  rationale: Encrypting data at rest is crucial for protecting sensitive information from unauthorized access and potential data breaches. It helps mitigate risks associated with data theft and ensures compliance with regulatory standards such as GDPR and HIPAA, which mandate the protection of personal and sensitive data. Failure to encrypt can lead to significant financial and reputational damage in the event of data compromise.
  description: This rule verifies that all Pub/Sub topics have encryption at rest enabled using either Google-managed or customer-managed encryption keys. Ensure that the topic is configured to use Cloud KMS keys if custom encryption is preferred. To verify, check the encryption properties of each Pub/Sub topic in the GCP Console or using the gcloud command-line tool. Remediation involves configuring the topic to use the desired encryption key, ensuring data remains protected.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.pubsub.topic.data_protection_storage_queue_private_network_only_supported
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Queue Private Network Only Supported
  scope: pubsub.topic.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Pub/Sub Topics Use Private Network for Enhanced Security
  rationale: Restricting Pub/Sub topic access to private networks minimizes exposure to unauthorized access and data breaches. It ensures data confidentiality by confining data flow within trusted network boundaries, which is crucial for compliance with regulations such as GDPR and HIPAA. This practice reduces the risk of data interception or misuse by malicious actors.
  description: This rule checks if Pub/Sub topics are configured to only allow access from private networks. Ensure that the topics are not exposed to the public internet by using VPC Service Controls or private IP addresses. Verification involves checking network settings in the Google Cloud Console under the Pub/Sub section and configuring the network to restrict access. Remediation includes adjusting the IAM policies and network settings to limit access to trusted internal networks only.
  references:
  - https://cloud.google.com/pubsub/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc-service-controls/docs
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2_1.pdf
- rule_id: gcp.pubsub.topic.data_protection_storage_stream_consumer_auth_required
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Stream Consumer Auth Required
  scope: pubsub.topic.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Auth on Pub/Sub Topic Stream Consumers
  rationale: Requiring authentication for Pub/Sub topic stream consumers ensures that only authorized applications can consume sensitive data. This mitigates risks of unauthorized data access, which can lead to data breaches, regulatory non-compliance, and potential financial and reputational damage.
  description: This rule checks if authentication is enforced for consumers of a Pub/Sub topic. Ensure that only authenticated identities are granted permissions to consume messages from a topic. Verify IAM policies associated with the topic to ensure 'roles/pubsub.subscriber' is only assigned to authenticated identities. To remediate, update IAM policies to restrict access to trusted service accounts or identities.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/pubsub/docs/security-best-practices
- rule_id: gcp.pubsub.topic.data_protection_storage_stream_encryption_at_rest_enabled
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Stream Encryption At Rest Enabled
  scope: pubsub.topic.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Pub/Sub Topic Encryption at Rest is Enabled
  rationale: Enabling encryption at rest for Google Cloud Pub/Sub topics ensures that sensitive data is protected from unauthorized access, reducing the risk of data breaches. It supports compliance with regulations such as GDPR and HIPAA, which mandate data protection measures. Failure to encrypt data could lead to financial penalties and reputational damage.
  description: This rule checks if Cloud Pub/Sub topics have server-side encryption enabled using Google-managed keys. To verify, inspect the topic settings in the GCP Console or use the `gcloud` command-line tool. Remediate by ensuring that encryption is enabled during topic creation or update the topic configuration to use Google-managed encryption keys.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis#section_5.6
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.pubsub.topic.data_protection_storage_stream_private_network_onl_supported
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Stream Private Network Onl Supported
  scope: pubsub.topic.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Pub/Sub Topics Use Private Network for Data Streaming
  rationale: Using a private network to stream data to Pub/Sub topics minimizes the exposure of sensitive information to potential attackers. This approach reduces the risk of data breaches, unauthorized access, and man-in-the-middle attacks, which can have severe business impacts and violate compliance with regulations like GDPR and HIPAA.
  description: This rule checks whether data streaming to Pub/Sub topics is restricted to a private network, ensuring that the data exchange between producers and Pub/Sub happens securely over a Virtual Private Cloud (VPC). Verify that all Pub/Sub topics are configured to use VPC Service Controls. To remediate, configure your Pub/Sub topic to enforce VPC Service Controls by using the 'private service connect' feature, which restricts access to your VPC network.
  references:
  - https://cloud.google.com/pubsub/docs/vpc
  - https://cloud.google.com/vpc-service-controls/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/network-connectivity/docs/psc/configure
- rule_id: gcp.pubsub.topic.streaming_analytics_checkpoints_and_outputs_encrypted
  service: pubsub
  resource: topic
  requirement: Streaming Analytics Checkpoints And Outputs Encrypted
  scope: pubsub.topic.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Topics for Streaming Analytics Are Encrypted
  rationale: Encrypting Pub/Sub topics that handle streaming analytics data prevents unauthorized access and ensures data integrity. This is crucial for protecting sensitive information and complying with regulations such as GDPR and HIPAA that mandate data encryption. Failure to encrypt data at rest can lead to data breaches, financial loss, and reputational damage.
  description: This rule checks if Pub/Sub topics used in streaming analytics pipelines have encryption enabled. Verify that Cloud KMS keys are used for encrypting Pub/Sub topic data at rest. To remediate, configure the Pub/Sub topic to use a customer-managed encryption key (CMEK) by specifying the KMS key in the topic settings. This enhances data protection and aligns with best practices for encryption.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/cmek
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.pubsub.topic.streaming_analytics_network_private_only
  service: pubsub
  resource: topic
  requirement: Streaming Analytics Network Private Only
  scope: pubsub.topic.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Pub/Sub Topics are Accessible via Private Networks Only
  rationale: Restricting Pub/Sub topics to private network access minimizes exposure to unauthorized access and data breaches, which can lead to data leakage and potential business disruptions. It supports compliance with data protection regulations such as GDPR and aligns with best practices for securing sensitive data in cloud environments.
  description: This rule checks if Pub/Sub topics are configured to be accessed only from private networks, such as VPCs, by ensuring that the IAM policies do not allow public access. Verify that service accounts and users with access to these topics are within your organization's private network. Remediation involves updating IAM policies to restrict access to specific IP ranges or VPCs, and employing Private Google Access for secure communication.
  references:
  - https://cloud.google.com/pubsub/docs/overview
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.pubsub.topic.streaming_analytics_role_least_privilege
  service: pubsub
  resource: topic
  requirement: Streaming Analytics Role Least Privilege
  scope: pubsub.topic.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Pub/Sub Streaming Analytics Roles
  rationale: Implementing least privilege for Pub/Sub topics minimizes the risk of unauthorized data access and potential data breaches. Excessive permissions can lead to inadvertent data exposure or malicious actions by insiders or compromised accounts. Ensuring that roles only have the necessary permissions supports compliance with regulations like PCI-DSS and GDPR, which mandate strict access controls.
  description: This rule checks that IAM roles assigned to Pub/Sub topics used in streaming analytics are configured with the least privilege principle. It verifies that roles do not include excessive permissions that are not required for their specific tasks. To remediate, audit the IAM roles associated with these topics and adjust permissions to include only those necessary for the task, following the principle of least privilege. Regularly review and update the roles to adapt to any changes in business processes or team responsibilities.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.resourcemanager.folder.governance_ou_auto_tag_policies_enabled_where_supported
  service: resourcemanager
  resource: folder
  requirement: Governance Ou Auto Tag Policies Enabled Where Supported
  scope: resourcemanager.folder.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Auto Tag Policies for Governance in Folders
  rationale: Enabling auto tag policies for governance on GCP folders ensures that resources are consistently tagged across the organization, facilitating better resource management, cost allocation, and compliance monitoring. This minimizes the risk of unmanaged resources and supports regulatory requirements by maintaining clear visibility and traceability of resources.
  description: This rule checks if auto tag policies are enabled on supported folders within GCP's Resource Manager service. Auto tag policies automatically apply labels to resources within a folder, helping to enforce organizational tagging standards. To verify, inspect the folder's tag policies via the GCP Console under Resource Manager settings or use the gcloud CLI. Remediation involves configuring tag policies to apply automatically using the 'gcloud resource-manager tags bindings create' command.
  references:
  - https://cloud.google.com/resource-manager/docs/tags/tags-overview
  - https://cloud.google.com/docs/enterprise/cis-google-cloud-platform-foundations-benchmark#2.9
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.folder.governance_ou_no_overly_permissive_exceptions
  service: resourcemanager
  resource: folder
  requirement: Governance Ou No Overly Permissive Exceptions
  scope: resourcemanager.folder.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Prevent Overly Permissive Folder IAM Policies
  rationale: Overly permissive IAM policies on GCP folders can lead to unauthorized access and potential data breaches. This compromises the principle of least privilege, increasing the risk of insider threats and accidental data exposure. Adhering to strict IAM controls is crucial for meeting compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks for any IAM policies on GCP folders that grant broad permissions to users or service accounts, which could lead to security vulnerabilities. It specifically looks for roles with excessive permissions that are not justified by business needs. To remediate, review the IAM policies on affected folders, remove any overly permissive roles, and ensure roles are aligned with the least privilege principle. Use GCP IAM Policy Analyzer to identify and adjust permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/resource-manager/docs/creating-managing-folders
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/iam-policy-analyzer
- rule_id: gcp.resourcemanager.folder.governance_ou_required_scps_attached
  service: resourcemanager
  resource: folder
  requirement: Governance Ou Required Scps Attached
  scope: resourcemanager.folder.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Compliance Policies Attached to GCP Folders
  rationale: Attaching Security Control Policies (SCPs) to GCP folders helps enforce organizational governance, ensuring folders adhere to defined security and compliance policies. This prevents unauthorized actions and misconfigurations that could lead to data breaches or non-compliance with standards such as PCI-DSS or SOC2.
  description: This rule checks if all GCP folders have appropriate SCPs attached, ensuring that organizational policies are enforced consistently. Verify by navigating to the GCP Console under 'IAM & Admin' > 'Folders' and checking for attached policies. Remediation involves creating and attaching SCPs that align with your organization's security and compliance requirements.
  references:
  - https://cloud.google.com/resource-manager/docs/creating-managing-folders
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.ssae-16.com/soc-2/
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/policies
- rule_id: gcp.resourcemanager.organization.governance_org_all_accounts_enrolled_in_org
  service: resourcemanager
  resource: organization
  requirement: Governance Org All Accounts Enrolled In Org
  scope: resourcemanager.organization.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure All GCP Accounts Are Enrolled in the Organization
  rationale: Enrolling all Google Cloud accounts in the organization's governance structure ensures consistent application of security policies, access controls, and auditing across the organization. This minimizes the risk of shadow IT, unauthorized access, and non-compliance with regulatory standards, safeguarding sensitive data and resources from potential breaches.
  description: This rule checks whether all Google Cloud Platform accounts are properly enrolled under the organization's governance. To verify, ensure that each project is linked to the organization node and that identity and access management (IAM) policies are consistently applied. Remediation involves reviewing organizational policies, enrolling any standalone or unlinked accounts, and configuring IAM roles to enforce security standards uniformly across all accounts.
  references:
  - https://cloud.google.com/resource-manager/docs/creating-managing-organization
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/docs/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/resource-manager/docs/access-control-org
- rule_id: gcp.resourcemanager.organization.governance_org_block_leave_org_by_policy
  service: resourcemanager
  resource: organization
  requirement: Governance Org Block Leave Org By Policy
  scope: resourcemanager.organization.policy_management
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce Org Membership via Policy Constraints
  rationale: Blocking the ability to leave an organization via policy is crucial for maintaining governance and preventing unauthorized or accidental departures that could lead to compliance violations, data loss, or weakened security posture. This policy ensures ongoing accountability and control over resource management within the organization, aligning with regulatory requirements.
  description: This rule checks for the enforcement of organization policies that prevent users from leaving the organization without proper authorization. It involves setting constraints in the Resource Manager to control this action. Administrators should verify that the organizational policy 'constraints/resourcemanager.organizationPolicy' is set to disallow leaving the organization. Remediation involves applying the necessary constraints through the GCP Console or gcloud CLI to ensure compliance with governance protocols.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0
  - https://nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.organization.governance_org_restrict_region_usage_by_policy_wher_required
  service: resourcemanager
  resource: organization
  requirement: Governance Org Restrict Region Usage By Policy Wher Required
  scope: resourcemanager.organization.policy_management
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict GCP Region Usage via Organization Policies
  rationale: Restricting region usage mitigates risks related to data sovereignty, regulatory compliance, and potential data breaches by ensuring data and resources are only deployed in approved regions. This control helps organizations avoid legal penalties and maintain trust with stakeholders by adhering to geographic data handling requirements.
  description: This rule checks if organization policies are in place to restrict usage of Google Cloud regions as required by governance standards. The organization must configure policies under `constraints/gcp.resourceLocations` to limit the locations where resources can be deployed. To verify, review the policy settings in the Google Cloud Console under Organization Policy Management. If necessary, update these settings to align with compliance mandates and security requirements by defining allowed locations.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/blog/topics/public-cloud/achieving-pci-dss-compliance-on-google-cloud
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.organization.governance_org_scps_enabled
  service: resourcemanager
  resource: organization
  requirement: Governance Org Scps Enabled
  scope: resourcemanager.organization.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Organization SCPs for Governance Control
  rationale: Enabling Organization Service Control Policies (SCPs) is critical for enforcing consistent security policies across your Google Cloud organization. Without SCPs, individual projects may implement inconsistent policies, increasing the risk of misconfigurations and unauthorized access. This is particularly important for meeting compliance requirements such as HIPAA, PCI-DSS, and others that mandate strict access and policy controls.
  description: This rule checks whether Service Control Policies (SCPs) are enabled at the organization level in GCP, ensuring that all projects adhere to predefined security and compliance policies. To verify, ensure that SCPs are configured and applied to your organization's resources by navigating to the Cloud Console's 'IAM & Admin' section and reviewing policy attachments. Remediation involves setting up SCPs to enforce necessary security controls across your organization and ensuring they are properly applied to all relevant resources.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_lockout_threshold_defined
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Lockout Threshold Defined
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Define Lockout Threshold in Identity Access Password Policy
  rationale: Defining a lockout threshold in password policies mitigates the risk of unauthorized access due to brute force attacks. Without a lockout policy, repeated unauthorized login attempts could compromise sensitive organizational data and access. This control is crucial for regulatory compliance with standards such as NIST and PCI-DSS, which mandate strong authentication mechanisms.
  description: This rule checks if a lockout threshold is defined in the Identity Access Management password policy for the organization in GCP. To verify, ensure the password policy includes a 'lockoutThreshold' setting that specifies the number of failed login attempts before an account is temporarily locked. Remediation involves configuring the 'lockoutThreshold' in the Google Cloud Console under IAM settings, which can be done by navigating to Security > Identity > Password Policies.
  references:
  - https://cloud.google.com/iam/docs/reference/rest/v1/organizations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloud.google.com/security/best-practices/identity-and-access-management
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_max_age_90_days_or_less
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Max Age 90 Days Or Less
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce 90-Day Maximum Password Age for GCP IAM Users
  rationale: Implementing a maximum password age of 90 days or less helps mitigate the risk of unauthorized access due to stale or compromised credentials. Regularly updating passwords reduces the window of opportunity for attackers to exploit compromised accounts, aligning with compliance standards such as NIST SP 800-53 and PCI-DSS.
  description: This rule ensures that the maximum password age for Google Cloud Platform Identity and Access Management (IAM) users is configured to 90 days or less. To verify, navigate to the GCP Console, access the IAM section, and review the password policy settings. Remediation involves setting an organizational policy that enforces a password expiration policy of 90 days or less by using the gcloud command-line tool or the GCP Console under the 'Security Settings'.
  references:
  - https://cloud.google.com/iam/docs/password-policy
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_min_length_14
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Min Length 14
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Password Policy Min Length of 14 for IAM
  rationale: Setting a minimum password length of 14 characters reduces the risk of successful brute force attacks and enhances the overall security posture of the organization by making passwords harder to guess. This aligns with compliance frameworks such as NIST and ISO 27001, which recommend robust password standards to protect sensitive data and access.
  description: This rule checks whether the identity access password policy for an organization in GCP is configured with a minimum length of 14 characters. To verify, navigate to the GCP Console, access IAM & Admin, and inspect the organization's password policy settings. To remediate, update the password policy to enforce a minimum length of 14 characters through the 'gcloud' command-line tool or via the Console's 'Security settings'.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/creating-managing-policies
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.serviceAccounts/setIamPolicy
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_prevent_reuse_last_24
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Prevent Reuse Last 24
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Password Policy to Prevent Reuse of Last 24 Passwords
  rationale: Preventing password reuse is crucial to mitigate the risk of unauthorized access due to credential stuffing attacks and compromised accounts. Reusing passwords increases vulnerability to attacks where stolen passwords from other services are used. Compliance with security standards like NIST SP 800-63B that recommend against password reuse enhances organizational security posture.
  description: This rule checks if the organization's password policy is configured to prevent the reuse of the last 24 passwords. It requires setting the 'passwordPolicy.passwordReusePrevention' field to '24' in the IAM policy. Verify this setting in the Google Cloud Console or by using the gcloud CLI. Remediation involves updating the IAM policy to enforce this password history setting, ensuring users cannot reuse any of their last 24 passwords.
  references:
  - https://cloud.google.com/iam/docs/reference/rest/v1/organizations/getIamPolicy
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 1.1
  - https://pages.nist.gov/800-63-3/sp800-63b.html
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/overview/whitepapers
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_require_upper_lower__special
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Require Upper Lower Special
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Password Complexity with Upper, Lower, and Special Characters
  rationale: Implementing a robust password policy is crucial to mitigate risks of unauthorized access due to weak passwords. Passwords that require a combination of upper, lower case letters, and special characters significantly reduce the likelihood of successful brute force attacks. This measure is often required to comply with regulatory frameworks such as PCI-DSS and ISO 27001, ensuring that sensitive data is protected against unauthorized access.
  description: This rule checks if the organization-level password policy in GCP mandates the use of upper case, lower case, and special characters. To verify compliance, review the organization's IAM password policy settings. To enforce this, navigate to 'IAM & Admin' in the GCP Console, select 'Security Settings', and configure the password policy to require upper, lower case letters, and special characters. This ensures that all user accounts adhere to strong password requirements.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://cloud.google.com/security/compliance/cis#gcp_cis_1.0
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/best-practices-for-managing-iam-policies
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_api_access_keys_root_or_ow_disallowed
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant API Access Keys Root Or Ow Disallowed
  scope: resourcemanager.organization.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Root or Owner API Access Keys in GCP Organizations
  rationale: Allowing root or owner access keys for Identity Access Tenant API poses significant security risks, such as unauthorized access, privilege escalation, and potential data breaches. It is crucial to enforce least privilege principles to mitigate threats and adhere to compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks whether any identity access tenant API access keys are assigned root or owner permissions in your GCP organization. Such permissions should be avoided to prevent excessive access rights that can lead to security vulnerabilities. To remediate, review IAM policies and ensure that API keys are assigned the minimum necessary permissions. Regularly audit key usage and rotate keys to maintain security hygiene.
  references:
  - https://cloud.google.com/resource-manager/docs/access-control-org
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/best-practices-for-managing-service-accounts
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_break_glass_accounts_mfa_enforced
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Break Glass Accounts MFA Enforced
  scope: resourcemanager.organization.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA for Break Glass Accounts in GCP Organizations
  rationale: Break glass accounts are critical for emergency access and bypass usual security controls. Without enforcing Multi-Factor Authentication (MFA), these accounts are susceptible to unauthorized access, posing a significant risk to organizational security. Ensuring MFA is enabled mitigates risks such as identity theft and unauthorized administrative actions, aligning with compliance mandates like NIST and PCI-DSS.
  description: This rule checks if Multi-Factor Authentication is enforced for break glass accounts within GCP organizations. Break glass accounts must have MFA enabled to provide an additional layer of security. Administrators should verify that all such accounts have MFA settings configured in the GCP Identity and Access Management (IAM) policies and ensure compliance by implementing MFA via Google Cloud Console or using API configurations. Remediation involves enabling MFA for these accounts and regularly auditing their settings.
  references:
  - https://cloud.google.com/iam/docs/best-practices-for-iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/identity-access-management/
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_console_mfa_required_org_wide
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Console MFA Required Org Wide
  scope: resourcemanager.organization.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA for GCP Console Access Across Organization
  rationale: Requiring multi-factor authentication (MFA) for access to the GCP Console significantly reduces the risk of unauthorized access due to compromised credentials. This control helps protect sensitive data and resources by ensuring that even if passwords are stolen, access cannot be gained without the second authentication factor. Compliance with standards like PCI-DSS and ISO 27001 often mandates MFA for administrative access, thus supporting regulatory adherence and enhancing overall security posture.
  description: This rule checks if multi-factor authentication is enforced for all users accessing the GCP Console within the organization. To verify, ensure that MFA is set up in the Identity and Access Management (IAM) settings under 'Security Settings' in the Google Cloud Console. Remediation involves configuring and enforcing MFA in the IAM policy settings, ensuring that all users are required to authenticate with an additional factor beyond just a password.
  references:
  - https://cloud.google.com/iam/docs/managing-mfa
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-63/rev-3/final
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_inactive_user_disable_thre_configured
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Inactive User Disable Thre Configured
  scope: resourcemanager.organization.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Inactive User Auto-Disable is Configured
  rationale: Configuring auto-disable for inactive users mitigates the risk of unauthorized access through orphaned accounts, which could be exploited by malicious actors. This control helps maintain a clean and secure identity environment, reducing potential attack surfaces and ensuring compliance with identity management best practices.
  description: This rule checks if the organization has configured automatic disabling of inactive user accounts. Ensure that Identity and Access Management (IAM) policies include automated processes to identify and disable user accounts that have not been used for a predefined period. This can be verified by reviewing the IAM policies and organization settings in the GCP Console. Remediation involves setting up appropriate lifecycle management policies to automatically suspend inactive accounts, using tools like Google Cloud's Identity Platform.
  references:
  - https://cloud.google.com/iam/docs/configuring-inactive-user-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/docs/enterprise-best-practices-for-iam
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_password_policy_compliant
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Password Policy Compliant
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Organization Password Policy Compliance
  rationale: Enforcing a strong password policy is crucial to prevent unauthorized access to GCP resources and protect sensitive data. Weak or insufficient password policies can lead to compromised accounts, data breaches, and potential financial and reputational damage. Compliance with password policy standards is also often required by regulatory frameworks such as NIST, PCI-DSS, and ISO 27001.
  description: This rule checks that your organization's password policy meets recommended security standards. It requires configuring the Identity and Access Management (IAM) settings to enforce strong password requirements, such as minimum length, complexity, and expiration period. To verify compliance, review the IAM password policy settings in the Google Cloud Console under Security > Identity & Organization. Remediate by adjusting the password policy to meet or exceed the recommended security guidelines.
  references:
  - https://cloud.google.com/iam/docs/best-practices-for-managing-identities
  - https://cloud.google.com/iam/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/document_library
  - https://www.iso.org/iso-iec-27001-information-security.html
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_sso_federation_configured_w_supported
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Sso Federation Configured W Supported
  scope: resourcemanager.organization.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SSO Federation is Configured for Identity Access Tenant
  rationale: Configuring SSO federation for your organization's identity access tenant enhances security by centralizing user authentication and reducing the risk of unauthorized access. It ensures compliance with industry standards by allowing integration with established identity providers, thus mitigating potential security breaches and protecting sensitive organizational data.
  description: This rule checks that Single Sign-On (SSO) federation is configured for your GCP organization's identity access tenant, ensuring that only authorized identity providers can authenticate users. To verify, review the IAM settings in the GCP Console under Security > Identity and Access Management, confirming that SSO is enabled with supported providers. Remediation involves setting up a federation with a trusted identity provider, following GCP's guidelines to ensure proper configuration and security.
  references:
  - https://cloud.google.com/identity/docs/overview
  - https://cloud.google.com/iam/docs/manage-iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf
  - https://cloud.google.com/identity/docs/security
- rule_id: gcp.resourcemanager.project.configuration_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Project Configuration Management is Enabled
  rationale: Enabling configuration management for GCP projects is crucial for maintaining consistent and secure infrastructure deployments. It helps in minimizing misconfigurations that can lead to vulnerabilities, thereby reducing the risk of unauthorized access or data breaches. Proper configuration management supports compliance with standards like NIST and ISO 27001 by enforcing structured control over resource settings.
  description: This rule verifies that configuration management processes are enabled and actively monitoring GCP project settings. It checks for the use of tools like Google Cloud Deployment Manager to manage infrastructure as code, ensuring configurations are consistent and deviations are flagged. Remediation involves setting up deployment scripts and integrating them with CI/CD pipelines to automate and audit configurations. Regular reviews and updates should be scheduled to align with best practices.
  references:
  - https://cloud.google.com/deployment-manager/docs
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.project.configuration_management_aggregation_auto_enroll_ne_accounts
  service: resourcemanager
  resource: project
  requirement: Configuration Management Aggregation Auto Enroll Ne Accounts
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Auto-enroll New Accounts in Config Management Aggregation
  rationale: Automatically enrolling new accounts in configuration management aggregation ensures that all resources are consistently monitored and managed from inception, reducing the risk of misconfiguration and security gaps. This proactive approach mitigates threats associated with unsecured cloud resources and aids in maintaining compliance with industry standards by ensuring all project configurations are aligned with organizational security policies.
  description: This rule verifies that new accounts within a GCP project are automatically enrolled in configuration management aggregation. It ensures that any new resources are immediately subject to the same configuration checks and policies as existing accounts. To verify, check the project's settings in the Google Cloud Console under Resource Manager and ensure that auto-enrollment is enabled for configuration management. Remediation involves enabling auto-enrollment by adjusting the project configuration settings to include all new accounts.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.resourcemanager.project.configuration_management_aggregation_org_aggregator_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Aggregation Org Aggregator Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Org Aggregator is Enabled for Configuration Management
  rationale: Enabling the Organization Aggregator is crucial for consolidating configuration data across all projects under an organization. This facilitates comprehensive visibility into configuration settings, enabling proactive risk management and ensuring compliance with regulatory standards such as NIST and ISO 27001. Without aggregation, organizations may face challenges in identifying misconfigurations that could lead to security vulnerabilities.
  description: This rule checks if the Configuration Management Aggregation using an Organization Aggregator is enabled for all projects within an organization. Verify that the aggregator is set up in the Google Cloud Console under Organization Settings, ensuring it collects configuration data across projects. To remediate, enable the Organization Aggregator by navigating to the Configuration Management Settings and selecting 'Enable Aggregation'. This action allows centralized monitoring and auditing of security configurations.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/resource-manager/docs/creating-managing-organization
- rule_id: gcp.resourcemanager.project.configuration_management_baseline_conformance_pack_deployed
  service: resourcemanager
  resource: project
  requirement: Configuration Management Baseline Conformance Pack Deployed
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Deploy Configuration Management Baseline Conformance Pack
  rationale: Implementing a Configuration Management Baseline Conformance Pack helps to ensure that all cloud resources comply with defined security best practices and organizational policies, reducing the risk of misconfigurations that could lead to vulnerabilities. It supports compliance with industry standards and regulatory requirements, thereby safeguarding data integrity and privacy.
  description: This check verifies whether a Configuration Management Baseline Conformance Pack is deployed across GCP projects. The conformance pack provides a predefined set of security controls and configuration checks, ensuring that instances meet baseline security requirements. To verify, review the deployment status of the conformance pack in the GCP Console under the Resource Manager section. Remediation involves deploying the pack using the Cloud Deployment Manager or Terraform scripts provided by Google.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/solutions/best-practices-for-enterprise-organizations
  - https://cloud.google.com/deployment-manager/docs/configuration/samples
- rule_id: gcp.resourcemanager.project.configuration_management_baseline_parameters_set
  service: resourcemanager
  resource: project
  requirement: Configuration Management Baseline Parameters Set
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Configuration Management Baseline Parameters Are Set
  rationale: Establishing a configuration management baseline is crucial for maintaining consistent security postures across projects, minimizing misconfigurations, and ensuring compliance with regulatory standards. Without a baseline, projects are vulnerable to inconsistent policies, leading to potential security breaches and compliance violations.
  description: This rule checks whether a configuration management baseline has been set for projects in GCP. It involves reviewing project-level settings to ensure they adhere to predefined security policies and configuration standards. Verification can be done by examining project configurations via the GCP Console or CLI. Remediation involves implementing configuration management tools such as Google Cloud Deployment Manager or third-party solutions to maintain and enforce baseline configurations across all projects.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.project.configuration_management_compliance_evidence_expo_configured
  service: resourcemanager
  resource: project
  requirement: Configuration Management Compliance Evidence Expo Configured
  scope: resourcemanager.project.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Project Compliance Evidence Configuration
  rationale: Ensuring that configuration management compliance evidence is properly configured is essential for maintaining audit readiness and minimizing security risks. Misconfiguration can lead to unauthorized changes, increasing the potential for data breaches and non-compliance with regulatory standards such as SOC2 and ISO 27001.
  description: This rule verifies that projects within the Google Cloud Platform have configuration management compliance evidence explicitly configured. It checks for the presence of settings that document and track changes to project configurations, ensuring traceability and accountability. To remediate, enable Cloud Audit Logs for configuration changes and integrate with a centralized logging solution to store and review evidence.
  references:
  - https://cloud.google.com/resource-manager/docs/creating-managing-projects
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/logging/docs/audit/configure-data-access
- rule_id: gcp.resourcemanager.project.configuration_management_compliance_reporting_dashbo_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Compliance Reporting Dashbo Enabled
  scope: resourcemanager.project.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable GCP Configuration Management Compliance Dashboard
  rationale: Enabling the Configuration Management Compliance Dashboard in GCP helps organizations monitor and ensure that their resource configurations align with internal policies and external compliance requirements. This is critical in identifying misconfigurations that could lead to security vulnerabilities, data breaches, and non-compliance with standards such as PCI-DSS and ISO 27001. By having visibility into configuration compliance, organizations can proactively address potential risks and maintain a robust security posture.
  description: This rule checks whether the Configuration Management Compliance Dashboard is enabled for a GCP project, allowing for comprehensive compliance monitoring across resources. To verify, ensure that the dashboard is activated through the Google Cloud Console under the Security Command Center settings, and that necessary permissions are granted to view compliance reports. Remediate by enabling the dashboard and configuring it to track relevant compliance benchmarks applicable to your organization.
  references:
  - https://cloud.google.com/security-command-center/docs/compliance-management
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/quickstart-security-command-center
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - NIST SP 800-53
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.project.configuration_management_delivery_destination_acce_privilege
  service: resourcemanager
  resource: project
  requirement: Configuration Management Delivery Destination Acce Privilege
  scope: resourcemanager.project.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict Access to Configuration Management Destinations
  rationale: Misconfigured access privileges to configuration management delivery destinations can lead to unauthorized access or data exposure, resulting in potential data breaches and non-compliance with regulatory frameworks. Ensuring least privilege access helps mitigate insider threats and reduces the attack surface, aligning with compliance requirements like NIST and ISO 27001.
  description: This rule checks that access permissions for configuration management delivery destinations in GCP projects are set according to the principle of least privilege. Verify that IAM roles assigned to users or service accounts have only the necessary permissions to perform their tasks. Remediation involves auditing IAM policies, removing unnecessary roles, and using predefined roles where possible to limit access.
  references:
  - https://cloud.google.com/resource-manager/docs/access-control-org
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.configuration_management_delivery_kms_encryption_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Delivery KMS Encryption Enabled
  scope: resourcemanager.project.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Config Management Delivery
  rationale: Enabling KMS encryption for configuration management delivery protects sensitive configuration data from unauthorized access and potential breaches. This encryption ensures that data at rest is secure, aligning with compliance requirements such as GDPR and HIPAA, and mitigating risks associated with data exposure and unauthorized disclosure.
  description: This rule checks whether Google Cloud's configuration management delivery processes are secured using Google Cloud Key Management Service (KMS) encryption. To verify, ensure that the delivery of configuration data within the project is encrypted with a customer-managed key (CMK) from KMS. Remediation involves creating or selecting a KMS key and configuring the project to use this key for encrypting configuration management delivery data.
  references:
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.resourcemanager.project.configuration_management_delivery_secure_destinat_configured
  service: resourcemanager
  resource: project
  requirement: Configuration Management Delivery Secure Destinat Configured
  scope: resourcemanager.project.configuration_management
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Secure Configuration Management Delivery in GCP Projects
  rationale: Configuring secure delivery for configuration management in GCP projects is critical to prevent unauthorized access and data leaks. Without secure delivery methods, sensitive configuration data could be intercepted or tampered with, leading to potential security breaches and non-compliance with standards like PCI-DSS and ISO 27001. Properly securing these deliveries ensures data integrity and confidentiality, supporting overall trust in cloud infrastructure.
  description: This rule checks whether secure delivery methods, such as HTTPS, are configured for configuration management operations in GCP projects. It ensures that all configuration data is transmitted securely to prevent interception by unauthorized parties. Verification involves reviewing project settings in the Resource Manager to confirm the use of secure protocols. Remediation includes updating configuration settings to use secure transport mechanisms, such as enabling HTTPS and using encryption for data in transit.
  references:
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.resourcemanager.project.configuration_management_drift_alerts_configured
  service: resourcemanager
  resource: project
  requirement: Configuration Management Drift Alerts Configured
  scope: resourcemanager.project.configuration_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Configuration Drift Alerts are Set for GCP Projects
  rationale: Configuration drift can lead to security vulnerabilities by allowing unauthorized changes that go unnoticed, potentially violating compliance standards and exposing your organization to risk. Proactively monitoring configuration changes helps maintain the integrity and security of your cloud environment, ensuring adherence to organizational policies and regulatory requirements such as PCI-DSS, HIPAA, and SOC2.
  description: This rule verifies that configuration drift alerts are enabled for GCP projects, ensuring that any unauthorized changes to project configurations are quickly detected and addressed. To configure drift alerts, set up monitoring and alerting using Google Cloud's Monitoring and Logging services to track configuration changes. Remediation involves enabling audit logging for Cloud Resource Manager and setting up alerting policies to notify administrators of any anomalies in project configurations.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
  - https://cloud.google.com/monitoring
- rule_id: gcp.resourcemanager.project.configuration_management_drift_critical_resources_dr_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Drift Critical Resources DR Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Configuration Drift for Critical Resources
  rationale: Enabling configuration management drift detection for critical resources ensures that any unauthorized or unintended changes are promptly identified and addressed. This reduces the risk of configuration errors that could lead to security vulnerabilities or compliance violations. It is crucial for maintaining the integrity and availability of critical workloads and aligning with regulatory requirements such as NIST and ISO 27001.
  description: This rule checks whether configuration management drift detection is enabled for critical resources within GCP projects. It involves verifying that tools like Google Cloud Config Connector or Terraform are configured to monitor and report any changes to resource configurations. To remediate, ensure that critical resources are tagged appropriately and that drift detection is enabled and actively monitored. Implement automated alerts and corrective actions to address detected drifts promptly.
  references:
  - https://cloud.google.com/config-connector/docs/overview
  - https://cloud.google.com/docs/terraform
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.configuration_management_recorder_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Recorder Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Configuration Management Recorder for Projects
  rationale: Enabling Configuration Management Recorder ensures that any changes to project configurations are tracked, reducing the risk of unauthorized modifications and helping meet compliance with regulations like NIST and ISO 27001. This capability provides visibility into configuration changes, allowing for quick identification and response to potential security incidents.
  description: This rule checks if the Configuration Management Recorder is enabled for GCP projects, which is crucial for maintaining an audit trail of configuration changes. To verify, ensure that Cloud Asset Inventory is enabled, which can be done via the GCP Console or gcloud command-line tool. Remediation involves enabling the Cloud Asset Inventory API and ensuring logs are being captured and reviewed regularly.
  references:
  - https://cloud.google.com/resource-manager/docs/cloud-asset-inventory/overview
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.resourcemanager.project.configuration_management_recorder_global_resource_ty_tracked
  service: resourcemanager
  resource: project
  requirement: Configuration Management Recorder Global Resource Ty Tracked
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Track Global Resources with Configuration Management Recorder
  rationale: Ensuring that all global resources are tracked using a Configuration Management Recorder reduces the risk of unmanaged changes that could lead to security vulnerabilities or compliance issues. This is crucial for maintaining an accurate inventory of resources and configurations, which is a fundamental part of security operations and audits. It helps in quickly identifying unauthorized changes and facilitates incident response and forensic analysis.
  description: This rule checks if the Configuration Management Recorder is set to track global resources within a Google Cloud project. To verify, ensure that Cloud Asset Inventory is enabled and properly configured to capture all resources, including global types. Remediation involves enabling Cloud Asset Inventory and configuring it to monitor all necessary resources, ensuring comprehensive visibility and control over your project's configuration state.
  references:
  - https://cloud.google.com/resource-manager/docs/cloud-asset-inventory/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security-overview
  - https://cloud.google.com/resource-manager/docs/monitoring-changes
- rule_id: gcp.resourcemanager.project.configuration_management_remediation_auto_remediation_e_high
  service: resourcemanager
  resource: project
  requirement: Configuration Management Remediation Auto Remediation E High
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Auto Remediation for High-Security Configurations
  rationale: Auto remediation for configuration management in GCP projects ensures that security misconfigurations are automatically corrected, reducing the risk of exposure to threats. This is crucial for maintaining a strong security posture and meeting compliance requirements, as it minimizes human error and accelerates response times to potential vulnerabilities.
  description: This rule checks that automatic remediation is enabled for high-severity configuration management issues in GCP projects. It ensures that any detected misconfiguration related to instance configuration is automatically corrected to align with security best practices. To verify, review the project settings in the GCP Console under the 'Configuration Management' section and ensure the auto-remediation feature is activated. Remediation steps include enabling auto-remediation policies via the Google Cloud Console or using gcloud commands for automation.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/compute/docs/instances
- rule_id: gcp.resourcemanager.project.configuration_management_remediation_manual_runbook_attached
  service: resourcemanager
  resource: project
  requirement: Configuration Management Remediation Manual Runbook Attached
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Attach Manual Runbooks for Configuration Remediation
  rationale: Attaching a manual runbook for configuration management remediation ensures that any misconfiguration can be swiftly addressed, reducing downtime and potential security breaches. This is critical for maintaining compliance with regulatory standards and minimizing risk exposure by providing a clear, documented process for remediation.
  description: This rule checks if a manual runbook is attached to each project for configuration management remediation. It requires documenting the steps necessary to address configuration issues, ensuring that these can be followed by any team member. To verify, ensure that each project has an associated runbook and that it's regularly updated to reflect current infrastructure and policies. Remediation involves creating a comprehensive runbook and integrating it with project management tools.
  references:
  - https://cloud.google.com/resource-manager/docs/project-setup
  - https://www.cisecurity.org/benchmark/google_cloud_computing/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.resourcemanager.project.configuration_management_rule_remediation_targets_bound
  service: resourcemanager
  resource: project
  requirement: Configuration Management Rule Remediation Targets Bound
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Project Remediation Targets Are Correctly Configured
  rationale: Properly binding configuration management rule remediation targets ensures that corrective actions are automatically applied to misconfigured resources, reducing the risk of security breaches and compliance violations. This practice helps maintain organizational security posture by preventing unauthorized changes and ensuring consistency across cloud environments.
  description: This rule checks if projects within GCP are configured to have remediation targets bound to configuration management rules, enabling automated corrections for identified misconfigurations. Verify that each project has the necessary settings to enforce remediation actions by checking the configuration management policy bindings. To remediate, ensure that the remediation targets are correctly defined in your configuration management tools and that they are associated with the appropriate rules and policies.
  references:
  - https://cloud.google.com/resource-manager/docs/creating-managing-projects
  - https://cloud.google.com/security-command-center/docs/concepts-and-features-overview
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://cloud.google.com/iam/docs/understanding-roles
  - NIST SP 800-53 Rev. 5
  - https://cloud.google.com/security
- rule_id: gcp.resourcemanager.project.configuration_management_rule_required_rules_present
  service: resourcemanager
  resource: project
  requirement: Configuration Management Rule Required Rules Present
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Required Configuration Management Rules in Projects
  rationale: Having required configuration management rules ensures that project resources are consistently and securely configured, reducing the risk of misconfigurations that can lead to data breaches or non-compliance with industry standards. This is crucial for maintaining a secure cloud environment and meeting regulatory requirements such as GDPR or HIPAA.
  description: This rule checks for the presence of essential configuration management rules within a GCP project. It ensures that projects adhere to predefined security configurations, thereby enforcing best practices for secure instance deployment. To verify compliance, audit your project's resource configurations against the set rules and apply any missing rules using configuration management tools like Terraform or Deployment Manager. Remediation involves defining and implementing missing rules to align with organizational security policies.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://cloud.google.com/architecture/security-foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/terraform
- rule_id: gcp.resourcemanager.project.configuration_management_rule_rules_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Rule Rules Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Configuration Management Rules are Enabled on Projects
  rationale: Enabling configuration management rules on GCP projects helps ensure that resources are consistently and securely configured, reducing the risk of misconfigurations that could lead to security vulnerabilities. This practice supports compliance with regulatory requirements and prevents unauthorized access or data breaches by maintaining a secure baseline configuration.
  description: This rule checks whether configuration management rules are enabled for GCP projects. It involves verifying that policies are in place to automatically enforce security configurations across all resources within the project. Remediation involves reviewing and enabling configuration management rules using the GCP Console or command-line tools to ensure all instances adhere to the defined security policies.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.project.governance_config_rule_remediation_targets_bound
  service: resourcemanager
  resource: project
  requirement: Governance Config Rule Remediation Targets Bound
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Project Remediation Targets are Properly Configured
  rationale: Proper configuration of remediation targets is crucial to ensure that any non-compliance issues are automatically addressed, minimizing security risks and maintaining adherence to compliance standards. Unbound or incorrectly configured targets can lead to unmanaged risks and potential breaches, impacting business continuity and regulatory compliance.
  description: This rule checks that all projects within Google Cloud have their governance configuration rules associated with correctly specified remediation targets. It ensures that any violations trigger appropriate automated actions to rectify the situation. Verify the configuration by reviewing the project settings in the Google Cloud Console or using Cloud Shell with gcloud commands. To remediate, bind governance configuration rules to suitable Cloud Functions or other automation scripts that can address the violations.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.governance_config_rule_required_rules_present
  service: resourcemanager
  resource: project
  requirement: Governance Config Rule Required Rules Present
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Required Governance Config Rules Are Present
  rationale: Having required governance configuration rules in place is crucial for maintaining visibility and control over project resources, reducing security risks such as unauthorized access and data breaches. It also ensures compliance with regulatory standards and internal governance policies, mitigating potential legal and financial repercussions for the organization.
  description: This check verifies that all necessary governance configuration rules are implemented within a GCP project. It ensures that specified security and compliance controls are consistently applied, enabling effective monitoring and management of resources. To verify, review the project settings in the GCP Console and ensure compliance rules are active. Remediation involves configuring these rules through Cloud Asset Inventory and IAM policies to enforce governance standards.
  references:
  - https://cloud.google.com/resource-manager/docs/overview
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.governance_config_rule_rules_enabled
  service: resourcemanager
  resource: project
  requirement: Governance Config Rule Rules Enabled
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Governance Config Rules for Project Monitoring
  rationale: Enabling governance config rules is crucial for continuous monitoring of project configurations and compliance with organizational policies. This helps in identifying misconfigurations early, reducing the risk of potential security breaches and ensuring adherence to regulatory standards like ISO 27001 and NIST. Failure to enable these rules can lead to undetected vulnerabilities and non-compliance issues, impacting business operations and reputation.
  description: This rule checks whether governance configuration rules are enabled for projects in GCP. These rules ensure that project settings adhere to defined security and compliance standards. To verify, navigate to the GCP Console, access the 'IAM & Admin' section, and ensure that governance policies are configured correctly under 'Org Policies'. Remediation includes setting up automated alerts and enabling necessary rules within the 'Config Management' dashboard to enforce compliance consistently.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/anthos-config-management/docs/how-to/installing-config-sync
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
- rule_id: gcp.resourcemanager.project.governance_editors_rbac_least_privilege
  service: resourcemanager
  resource: project
  requirement: Governance Editors RBAC Least Privilege
  scope: resourcemanager.project.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Limit Editor Role Use to Minimize Security Risks
  rationale: Excessive permissions can lead to unintended access, increasing the risk of data breaches and non-compliance with regulatory frameworks such as GDPR and HIPAA. By ensuring that governance editors have only the necessary permissions, organizations can mitigate insider threats and prevent misuse of sensitive resources, thus protecting business-critical operations and maintaining customer trust.
  description: This rule checks for the assignment of the 'Editor' role within GCP projects to ensure adherence to the principle of least privilege. It identifies accounts with more permissions than necessary, which could lead to excessive access to project resources. Remediation involves reviewing and adjusting IAM policies to assign only the necessary roles and permissions, using custom roles if necessary to precisely tailor access rights. Verification can be done through the IAM policy analysis tool in the GCP console or by using `gcloud` commands to list and audit current role assignments.
  references:
  - https://cloud.google.com/resource-manager/docs/access-control-proj
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/iam/docs/creating-custom-roles
- rule_id: gcp.resourcemanager.project.governance_no_public_admin_roles
  service: resourcemanager
  resource: project
  requirement: Governance No Public Admin Roles
  scope: resourcemanager.project.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Admin Roles on GCP Projects
  rationale: Allowing public access to admin roles on GCP projects poses a significant risk of unauthorized access, leading to potential data breaches, service disruptions, and non-compliance with regulations like GDPR and PCI-DSS. Such exposure can allow malicious actors to escalate privileges, manipulate resources, or exfiltrate sensitive information, impacting operational integrity and trust.
  description: This rule checks for any public bindings of admin roles within a GCP project, ensuring that no user outside the organization is granted administrative access. To verify, review the IAM policy bindings for the presence of 'allUsers' or 'allAuthenticatedUsers' with roles such as 'roles/owner', 'roles/editor', or 'roles/admin'. Remediation involves removing these bindings and restricting access to trusted identities only, utilizing IAM conditions or VPC Service Controls for enhanced security.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/best-practices
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.project.governance_no_wildcard_admin_actions
  service: resourcemanager
  resource: project
  requirement: Governance No Wildcard Admin Actions
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: critical
  title: Prevent Wildcard Admin Roles in GCP Projects
  rationale: Granting wildcard admin roles can expose projects to excessive access, increasing the risk of unauthorized activities, data breaches, and compliance violations. Ensuring roles are specific and limited helps mitigate insider threats and conforms to least privilege principles, aligning with regulatory requirements like NIST and ISO 27001.
  description: This rule checks for IAM policy bindings in GCP projects that grant overly permissive roles, such as 'roles/*admin', to any user or service account. To verify, review the IAM policies of your projects in the GCP Console or via the gcloud command-line tool. Remediation involves replacing wildcard roles with more specific permissions tailored to the user's needs, following the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-using-iam-securely
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.resourcemanager.project.governance_required_guardrail_policies_attached
  service: resourcemanager
  resource: project
  requirement: Governance Required Guardrail Policies Attached
  scope: resourcemanager.project.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Guardrail Policies are Attached to Projects
  rationale: Attaching governance guardrail policies to GCP projects is crucial for ensuring resilience and disaster recovery. It helps in mitigating potential security risks by enforcing organizational policies that prevent unauthorized changes and promote compliance with industry standards. This reduces the likelihood of data loss or service disruptions, thereby protecting business continuity and regulatory compliance.
  description: This rule checks whether all GCP projects have the required guardrail policies attached to enforce governance controls. These policies should include constraints that align with your organization's security and compliance requirements, such as restrictions on resource modifications and data handling practices. To verify compliance, review the IAM policy bindings and ensure the necessary policies are applied. Remediation involves using the gcloud command-line tool or Cloud Console to attach missing policies to the respective projects.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.resourcemanager.project.governance_scp_mandatory_block_disabling_guardrail_services
  service: resourcemanager
  resource: project
  requirement: Governance Scp Mandatory Block Disabling Guardrail Services
  scope: resourcemanager.project.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Block Disabling Critical Guardrail Services in GCP Projects
  rationale: Disabling essential guardrail services such as identity and access management or logging can lead to increased security vulnerabilities and compliance failures. These guardrails are crucial for maintaining operational integrity and ensuring resilience against disruptions. Regulatory frameworks often require continuous monitoring and management of security controls, making this configuration vital for organizational compliance and risk management.
  description: This rule checks that critical guardrail services are not disabled in GCP projects, such as IAM roles and audit logging. It verifies that these services are configured to prevent accidental or malicious changes that could compromise security. Remediation involves enabling and enforcing appropriate policies through Cloud Identity and Access Management (IAM) and Audit Logs, ensuring they are continuously active. Administrators should regularly audit these configurations to ensure compliance with organizational policies.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.governance_scp_mandatory_deny_iam_star_admin
  service: resourcemanager
  resource: project
  requirement: Governance Scp Mandatory Deny IAM Star Admin
  scope: resourcemanager.project.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent IAM Wildcard Admin Roles on Projects
  rationale: Allowing IAM roles with wildcard permissions poses a critical security risk as it grants excessive privileges, potentially leading to data breaches, unauthorized access, and non-compliance with regulatory frameworks such as PCI-DSS and SOC2. This rule helps ensure that IAM policies adhere to the principle of least privilege, minimizing the attack surface and fortifying the project's security posture.
  description: This rule verifies that IAM policies on GCP projects do not include wildcard ('*') permissions for administrative roles. To comply, ensure IAM roles are defined with specific permissions rather than using broad 'roles/*' grants. Remediation involves auditing current IAM roles, identifying wildcard permissions, and replacing them with roles that have tailored permissions matching the project's specific needs.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.resourcemanager.project.governance_scp_mandatory_require_mfa_for_console_w_supported
  service: resourcemanager
  resource: project
  requirement: Governance Scp Mandatory Require MFA For Console W Supported
  scope: resourcemanager.project.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA for GCP Console Access via Governance SCP
  rationale: Requiring MFA for console access significantly mitigates the risk of unauthorized access due to compromised credentials. This control is critical given the increased threat landscape where phishing attacks are prevalent. Implementing MFA aligns with compliance mandates such as NIST SP 800-63, PCI-DSS, and others that require strong authentication measures.
  description: This rule ensures that all users accessing the GCP Console must authenticate using Multi-Factor Authentication (MFA). The policy is enforced through Service Control Policies (SCPs) at the project level. To verify, check that IAM policies include MFA requirements for all users. Remediate by configuring IAM roles with conditions that require MFA and update organizational policies to enforce these requirements.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/identity-platform/docs/mfa-overview
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, section 1.1
  - NIST SP 800-63B Digital Identity Guidelines
- rule_id: gcp.resourcemanager.project.governance_scp_no_allow_star_on_star_resources
  service: resourcemanager
  resource: project
  requirement: Governance Scp No Allow Star On Star Resources
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Wildcard Permissions on All GCP Resources
  rationale: Allowing wildcard permissions on all resources ('*' on '*') in GCP can expose your cloud environment to unauthorized access and potential data breaches. This configuration violates the principle of least privilege, increasing the risk of accidental or malicious actions that can disrupt operations or result in data loss. Ensuring specific, limited permissions helps align with compliance frameworks such as ISO 27001 and PCI-DSS, which mandate stringent access controls.
  description: This rule checks for and restricts the use of wildcard permissions ('*' on '*') in GCP Identity and Access Management (IAM) policies at the project level. Specifically, it ensures that no IAM policy grants overly permissive access across all resources. To verify, examine IAM policy bindings for projects and identify any entries with 'roles/*' or 'members/*'. Remediate by replacing such entries with specific roles and members that follow the principle of least privilege, ensuring users and services have only the necessary permissions.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.resourcemanager.project.governance_security_services_mandatory_enabled
  service: resourcemanager
  resource: project
  requirement: Governance Security Services Mandatory Enabled
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Mandatory Security Services Are Enabled on Projects
  rationale: Enabling mandatory security services on GCP projects reduces the risk of unauthorized access and data breaches. It ensures compliance with regulatory standards such as PCI-DSS and SOC2, and provides proactive monitoring capabilities to detect and respond to potential threats efficiently.
  description: This rule checks if mandatory security services, such as Security Command Center and Cloud Audit Logs, are enabled on GCP projects. Enabling these services allows for continuous security monitoring and logging of user activities and configurations. To verify, ensure that Security Command Center is set to at least 'Standard' tier, and that Cloud Audit Logs are configured to capture Admin Activity and Data Access logs. Remediation involves activating these services via the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/security-command-center/docs/overview
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.resourcemanager.project.governance_versioning_and_change_audit_enabled
  service: resourcemanager
  resource: project
  requirement: Governance Versioning And Change Audit Enabled
  scope: resourcemanager.project.audit_logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Governance Versioning and Change Audit for Projects
  rationale: Enabling governance versioning and change auditing for projects is crucial to track modifications, detect unauthorized changes, and ensure accountability. This practice mitigates risks such as data breaches and unauthorized access, supporting compliance with standards like PCI-DSS and ISO 27001 which require detailed audit trails for security events.
  description: This rule checks whether governance versioning and change auditing is enabled for GCP projects. Ensure that Audit Logging is configured to record 'ADMIN_READ', 'DATA_READ', and 'DATA_WRITE' activities in the Cloud Audit Logs for critical services. To verify, review the IAM policy for the project and confirm that necessary roles are assigned to capture audit logs. Remediation involves adjusting permissions and enabling logs in the Cloud Console or via gcloud commands.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/resource-manager/docs/access-control-proj
  - CIS GCP Benchmark 1.0.0 - Section 4.1
  - 'NIST SP 800-53 Rev. 5 - AU-3: Content of Audit Records'
  - 'PCI DSS v3.2.1 - Requirement 10: Track and monitor all access to network resources and cardholder data'
  - 'ISO/IEC 27001:2013 - A.12.4: Logging and monitoring'
- rule_id: gcp.resourcemanager.project.part_of_organizations
  service: resourcemanager
  resource: project
  requirement: Part Of Organizations
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Project is Associated with an Organization
  rationale: Associating GCP projects with an organization allows for centralized management and governance, reducing the risk of resource sprawl and potential security misconfigurations. It enforces consistent security policies, access control, and billing practices across all projects, which is critical for compliance with corporate and regulatory standards.
  description: This rule checks if a GCP project is associated with an organization. Projects not linked to an organization may bypass organization-wide security policies, leading to potential vulnerabilities. Verify that projects are part of an organization by using the GCP Console or gcloud CLI. To remediate, attach the project to the appropriate organization or ensure new projects are created under an organization.
  references:
  - https://cloud.google.com/resource-manager/docs/creating-managing-folders
  - https://cloud.google.com/resource-manager/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.resourcemanager.project.recorder_all_regions_enabled
  service: resourcemanager
  resource: project
  requirement: Recorder All Regions Enabled
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Cloud Audit Logging Enabled for All Regions
  rationale: Enabling Cloud Audit Logging across all regions is crucial for maintaining a comprehensive security posture. It allows organizations to track access and modifications to resources, providing critical insights into potential unauthorized activities. This is essential for meeting compliance requirements such as GDPR and PCI-DSS, which mandate thorough audit logging and monitoring.
  description: This rule checks if Cloud Audit Logging is enabled in all regions for projects within the Google Cloud Platform. Audit logs provide details of 'admin activity' and 'data access' actions performed in the system, which are necessary for security monitoring and forensic analysis. To verify, ensure that the Logging API is enabled and logs are being collected for all applicable services. Remediate by configuring the logging settings in the Cloud Console under Logging > Logs Explorer, and ensure the necessary IAM permissions are in place for log access.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.secretmanager.secret.compliance
  service: secretmanager
  resource: secret
  requirement: Compliance
  scope: secretmanager.secret.compliance
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure Secret Manager Secrets Meet Compliance Standards
  rationale: Proper management and compliance of secrets in GCP Secret Manager are crucial for protecting sensitive data and maintaining trust. Non-compliance can lead to unauthorized access, data breaches, and significant financial and reputational damage. Regulatory frameworks such as PCI-DSS and HIPAA mandate strict key management practices to safeguard data integrity and confidentiality.
  description: This rule checks that secrets stored in GCP Secret Manager adhere to defined compliance policies, including encryption standards, access controls, and expiration dates. Verification involves ensuring that secrets are encrypted with customer-managed encryption keys (CMEK) and access permissions are reviewed regularly. Remediation includes configuring CMEK for secrets and implementing role-based access control (RBAC) to limit exposure.
  references:
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://cloud.google.com/security/compliance/cis#cis-google-cloud-computing-foundations-benchmark-v1-1-0
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.secretmanager.secret.compliant_patching
  service: secretmanager
  resource: secret
  requirement: Compliant Patching
  scope: secretmanager.secret.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Secret Manager Secrets Are Patched Regularly
  rationale: Regular patching of secrets in GCP's Secret Manager is critical to protect sensitive data. Unpatched vulnerabilities can be exploited by attackers to gain unauthorized access, leading to data breaches and non-compliance with regulations like PCI-DSS and GDPR. Ensuring secrets are updated mitigates these risks and maintains organizational trust.
  description: This rule checks if the secrets stored in GCP Secret Manager are subject to a regular patch management process. It verifies that secrets are kept up-to-date with the latest security patches and best practices. To remediate, implement automated workflows that regularly update and rotate secrets, ensuring alignment with your organization's patch management policy.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/secrets-management
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.secretmanager.secret.manager_enabled
  service: secretmanager
  resource: secret
  requirement: Manager Enabled
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure Secret Manager is Enabled for Secrets
  rationale: Enabling Secret Manager for your secrets in GCP is crucial for maintaining the confidentiality and integrity of sensitive information such as API keys, passwords, and certificates. Without proper management, secrets risk being exposed to unauthorized access, leading to potential data breaches, compliance violations, and financial losses. Compliance with frameworks like PCI-DSS and SOC2 often requires stringent management of sensitive data.
  description: This rule checks that the Secret Manager service is enabled for storing and managing secrets within your GCP environment. It verifies the presence of the Secret Manager API and that secrets are actively managed using this service. To remediate, ensure that the Secret Manager API is enabled in your GCP project and that all sensitive information is stored and managed through this API. Regularly audit and rotate your secrets to maintain secure access.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.soc2.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.secretmanager.secret.patching
  service: secretmanager
  resource: secret
  requirement: Patching
  scope: secretmanager.secret.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Secret Manager Secrets Are Regularly Patched
  rationale: Regular patching of secrets in GCP Secret Manager is crucial to mitigate vulnerabilities that can be exploited by attackers to gain unauthorized access to sensitive information. Unpatched secrets can lead to data breaches, non-compliance with regulatory requirements such as GDPR or HIPAA, and potential financial and reputational damage to the organization.
  description: This rule checks that secrets stored in GCP Secret Manager are regularly updated and patched to ensure they are protected against known vulnerabilities. This involves verifying that the latest updates and patches are applied to secret values and configurations. Remediation includes setting up automated patch management processes and ensuring secrets are rotated and updated in adherence to the organization's security policy.
  references:
  - https://cloud.google.com/secret-manager/docs/managing-secrets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.secretmanager.secret.secret_rotation_enabled
  service: secretmanager
  resource: secret
  requirement: Secret Rotation Enabled
  scope: secretmanager.secret.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Rotation is Enabled for GCP Secrets
  rationale: Enabling secret rotation reduces the risk of exposure by ensuring that secrets are regularly updated, thereby minimizing the window of opportunity for unauthorized access. This practice is crucial for maintaining the confidentiality and integrity of sensitive data, aligning with compliance requirements such as PCI-DSS and HIPAA which mandate regular updates to access credentials.
  description: This rule checks whether automatic secret rotation is enabled for secrets stored in Google Cloud Secret Manager. Secrets without rotation can become stale and vulnerable to unauthorized access. To verify, check the rotation policy for each secret and ensure it is configured to rotate automatically at a regular interval. Remediation involves setting a rotation schedule via the GCP Console or using the gcloud command-line tool to update the secret's configuration.
  references:
  - https://cloud.google.com/secret-manager/docs/managing-secrets
  - https://cloud.google.com/secret-manager/docs/rotation
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/secret-manager/docs/best-practices
- rule_id: gcp.secretmanager.secret.secrets_access_rbac_least_privilege
  service: secretmanager
  resource: secret
  requirement: Secrets Access RBAC Least Privilege
  scope: secretmanager.secret.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Secret Manager Access
  rationale: Adopting the principle of least privilege minimizes the risk of unauthorized access to sensitive information stored in GCP Secret Manager. Inadequate access controls can lead to data breaches, compromising business integrity and violating compliance requirements such as GDPR and HIPAA. Implementing strict RBAC policies helps protect critical business secrets from internal and external threats.
  description: This rule checks that access to secrets in GCP Secret Manager is granted based on the principle of least privilege. It verifies that only necessary roles and permissions are assigned to users and service accounts. Remediation involves reviewing IAM policies to ensure that roles like 'Secret Manager Admin' or 'Secret Manager Secret Accessor' are assigned judiciously, and removing any excessive permissions not required for the user's function. Regular audits and monitoring of access logs are recommended to maintain secure access controls.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.secretmanager.secret.secrets_alias_points_to_active_key
  service: secretmanager
  resource: secret
  requirement: Secrets Alias Points To Active Key
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Alias Points to Active Encryption Key
  rationale: Using an active encryption key for secrets is crucial to maintain data confidentiality and ensure that secrets are protected with the latest security controls. An inactive or deleted key can lead to unauthorized access or data loss, posing significant risks to business operations and compliance with standards such as PCI-DSS and HIPAA.
  description: This rule checks whether the alias of a secret in Google Cloud Secret Manager points to an active encryption key. An alias pointing to an inactive key indicates that the key cannot be used for encryption or decryption, leading to potential service disruptions. To remediate, ensure that all secret aliases are updated to reference active keys by accessing the Secret Manager in the GCP Console, verifying key statuses, and updating aliases as necessary.
  references:
  - https://cloud.google.com/secret-manager/docs/managing-secrets
  - https://cloud.google.com/secret-manager/docs/security-best-practices
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.secretmanager.secret.secrets_ca_access_rbac_least_privilege
  service: secretmanager
  resource: secret
  requirement: Secrets Ca Access RBAC Least Privilege
  scope: secretmanager.secret.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Secret Access in Secret Manager
  rationale: Implementing least privilege for accessing secrets is crucial to minimizing the risk of unauthorized data exposure and potential data breaches. By restricting access rights to the minimum necessary, organizations reduce the attack surface available to malicious actors, thus protecting sensitive data and complying with regulatory requirements like GDPR and HIPAA.
  description: This rule checks if IAM roles granted to users or service accounts for accessing secrets in Secret Manager are limited to the least privilege necessary. It ensures that roles such as 'Secret Manager Viewer' are only assigned where absolutely needed and that sensitive roles like 'Secret Manager Admin' are not overly permissive. To verify, review IAM policies and ensure only necessary roles are granted. Remediation involves auditing and adjusting IAM policies to align with the principle of least privilege.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://gdpr-info.eu/art-32-gdpr/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.secretmanager.secret.secrets_ca_key_in_hsm_where_supported
  service: secretmanager
  resource: secret
  requirement: Secrets Ca Key In Hsm Where Supported
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets CA Key Uses HSM Where Supported
  rationale: Storing CA keys in a Hardware Security Module (HSM) enhances the protection of cryptographic keys by using dedicated hardware appliances, reducing the risk of key exposure and unauthorized access. This is crucial for maintaining the integrity and confidentiality of secrets, and it aligns with compliance frameworks like PCI-DSS and NIST that mandate strong key management practices.
  description: This rule checks whether the CA keys for secrets in Google Cloud Secret Manager are stored in an HSM, where supported. An HSM provides robust physical and logical protection for cryptographic keys, ensuring they remain secure even if a software vulnerability is exploited. To verify, inspect the key management settings of your secrets and configure them to use HSM-backed keys when possible. Remediation involves migrating or creating keys in an HSM environment through GCP's Key Management Service (KMS).
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/overview/whitepaper#key_management
  - https://cloud.google.com/kms/docs/hsm
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r4.pdf
- rule_id: gcp.secretmanager.secret.secrets_crl_or_ocsp_configured
  service: secretmanager
  resource: secret
  requirement: Secrets Crl Or Ocsp Configured
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure CRL or OCSP is Configured for Secrets
  rationale: Configuring Certificate Revocation Lists (CRL) or Online Certificate Status Protocol (OCSP) for secrets ensures that any compromised certificates are rapidly invalidated, reducing the risk of unauthorized access to sensitive data. This is crucial for maintaining the integrity and confidentiality of secrets managed within Google Secret Manager. It also supports compliance with industry regulations that mandate effective key management processes.
  description: This rule checks whether CRL or OCSP is configured for secrets stored in Google Secret Manager. Proper configuration involves setting up mechanisms that automatically check the validity of certificates associated with secrets. To verify, ensure that secret access policies include certificate revocation settings, and configure the necessary infrastructure for CRL or OCSP. Remediation involves updating secret configurations to include CRL or OCSP endpoints, and ensuring regular audits of these settings.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://docs.microsoft.com/en-us/security/benchmark/azure/best-practices
- rule_id: gcp.secretmanager.secret.secrets_key_has_alias
  service: secretmanager
  resource: secret
  requirement: Secrets Key Has Alias
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Manager Secrets Have Key Alias
  rationale: Aliases for keys in Secret Manager provide an additional layer of abstraction and security by allowing key rotation without changing the actual key ID used by applications. This practice helps mitigate the risk of key exposure and ensures easier management of cryptographic keys, thereby supporting compliance with security standards such as PCI-DSS and ISO 27001.
  description: This rule checks whether secrets stored in GCP Secret Manager have an associated alias for their encryption keys. Ensuring that each key has an alias simplifies key rotation and management. To verify, navigate to the Secret Manager in the GCP Console, select a secret, and check the key settings for an alias. If none exists, create an alias through Cloud KMS and configure the secret to use it. This enhances security posture by decoupling key identity from application usage.
  references:
  - https://cloud.google.com/secret-manager/docs/set-secret-version-alias
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r4.pdf
- rule_id: gcp.secretmanager.secret.secrets_key_length_minimum
  service: secretmanager
  resource: secret
  requirement: Secrets Key Length Minimum
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Key Length Is At Least 256 Bits
  rationale: Secrets with insufficient key length are vulnerable to brute force attacks, potentially leading to unauthorized access and data breaches. Ensuring a minimum key length supports robust encryption, protecting sensitive information and aligning with compliance mandates like PCI-DSS and ISO 27001.
  description: This rule checks that all keys managed within GCP Secret Manager have a minimum length of 256 bits. Keys below this threshold are considered weak and can be compromised more easily. To verify, inspect the key length settings in your GCP Secret Manager and update any keys below 256 bits using the GCP Console or CLI. Implementing this change strengthens your security posture by enhancing the encryption of sensitive data.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
- rule_id: gcp.secretmanager.secret.secrets_kms_constraints_present
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Constraints Present
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Use KMS Encryption Constraints
  rationale: Implementing KMS constraints on secrets in Secret Manager is critical to ensuring that sensitive data is encrypted using organization-approved keys, mitigating the risk of unauthorized access and data breaches. This approach aligns with regulatory requirements and supports key management best practices by enforcing strong encryption policies and ensuring centralized control over encryption keys.
  description: This rule checks if Secret Manager secrets have KMS constraints applied to ensure encryption with customer-managed keys (CMKs). Without these constraints, secrets may be encrypted with default keys, increasing exposure to potential security threats. To verify, inspect the secret's configuration for KMS constraints and ensure they reference a valid CMK. Remediation involves configuring secrets to use organization-approved KMS keys by setting the appropriate constraints in the Secret Manager settings.
  references:
  - https://cloud.google.com/secret-manager/docs/encrypting-secrets
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/kms/docs/key-policies
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/security/compliance
- rule_id: gcp.secretmanager.secret.secrets_kms_encryption_enabled
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Encryption Enabled
  scope: secretmanager.secret.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Secrets Use Customer-Managed Encryption Keys
  rationale: Utilizing Customer-Managed Encryption Keys (CMEK) for secrets enhances security by allowing organizations to control the encryption process and manage encryption keys centrally. This reduces the risk of unauthorized access and helps comply with regulatory standards such as PCI-DSS and HIPAA, which require strong encryption practices. Failure to properly encrypt secrets can lead to data breaches and exposure of sensitive information, impacting business reputation and incurring compliance fines.
  description: This rule checks if secrets stored in Google Cloud Secret Manager are encrypted using customer-managed keys (CMEK) rather than Google-managed keys. To verify, ensure the secret has the 'replication.policy.userManaged' configuration with a valid KMS key ID. Remediation involves configuring your secrets to use CMEK by specifying the customer-managed key during secret creation or updating existing secrets to use a CMEK. This provides stronger security controls over encryption operations.
  references:
  - https://cloud.google.com/secret-manager/docs/secret-manager-overview
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.secretmanager.secret.secrets_kms_grantee_principal_valid
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Grantee Principal Valid
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Valid KMS Grantee Principal for Secret Access
  rationale: Validating the KMS grantee principal for secrets is crucial to prevent unauthorized access to sensitive data. Improperly configured principals can lead to data breaches, exposing critical secrets to unauthorized users. Compliance standards such as PCI-DSS and ISO 27001 require strict access controls on key management systems to safeguard sensitive information.
  description: This rule checks whether the KMS key used for encrypting secrets in Secret Manager has valid grantee principals. Ensure that only authorized identities and service accounts are granted access to these keys. To verify, inspect the IAM policies associated with KMS keys and confirm that only necessary entities are included. Remediation involves adjusting IAM policies to remove unnecessary or invalid principals.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/kms/docs/resource-hierarchy-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57pt1r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.secretmanager.secret.secrets_kms_key_configured
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Key Configured
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Use Customer-Managed Encryption Keys
  rationale: Using Customer-Managed Encryption Keys (CMEK) for secrets in Secret Manager enhances security by allowing organizations to have control over the encryption keys used to protect their sensitive data. This reduces the risk of unauthorized access and aligns with compliance requirements that demand stringent encryption controls, such as GDPR and HIPAA. In the event of a breach, having CMEK configured can prevent data exposure by revoking access to the key, thereby safeguarding the secrets.
  description: This rule checks if a Customer-Managed Encryption Key (CMEK) is configured for secrets stored in GCP Secret Manager. By default, Google Cloud encrypts secrets using Google-managed keys, but configuring CMEK provides enhanced security control. To ensure compliance with this rule, verify that CMEK is set for your secrets by checking the encryption settings in the Secret Manager console or via the gcloud CLI. If CMEK is not configured, update the secret's settings to use a specific Cloud KMS key for encryption. This involves selecting an appropriate key ring and key in Cloud KMS and applying it to the secret.
  references:
  - https://cloud.google.com/secret-manager/docs/encrypting-secrets
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/docs/security-overview
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.secretmanager.secret.secrets_kms_lessthan_wildcard_permissions
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Lessthan Wildcard Permissions
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Limit KMS Permissions for Secrets in Secret Manager
  rationale: Excessive permissions on KMS keys used by Secret Manager can lead to unauthorized access and potential data breaches. By restricting permissions to specific actions, organizations minimize the risk of unauthorized key usage, aligning with compliance requirements such as PCI-DSS and protecting sensitive information from exposure.
  description: This rule checks that the Cloud KMS key permissions associated with secrets in Secret Manager do not include wildcard permissions, which permit all actions. Ensure that permissions are explicitly defined to limit actions to only those necessary for operational purposes. To remediate, review IAM policies on KMS keys and replace wildcard permissions with specific actions like 'encrypt', 'decrypt', 'getCryptoKeyVersion'.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/reference/permissions-and-roles
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/cyberframework
- rule_id: gcp.secretmanager.secret.secrets_no_plaintext_exposed_in_policy_or_tags
  service: secretmanager
  resource: secret
  requirement: Secrets No Plaintext Exposed In Policy Or Tags
  scope: secretmanager.secret.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Exposure of Secrets in Policies or Tags
  rationale: Exposing plaintext secrets in IAM policies or resource tags increases the risk of unauthorized access and data breaches. Malicious actors or unintentional internal accesses can exploit these exposures to compromise sensitive information, leading to financial losses and regulatory non-compliance with standards like PCI-DSS and HIPAA.
  description: This check ensures that sensitive information, such as secrets managed by Secret Manager, is not stored in plaintext within IAM policy bindings or resource tags. Verify that no secrets are hardcoded or referenced in these areas by reviewing IAM policies and tag configurations. To remediate, remove any plaintext secrets and utilize Secret Manager's access controls to manage and access secrets securely.
  references:
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.secretmanager.secret.secrets_not_expired
  service: secretmanager
  resource: secret
  requirement: Secrets Not Expired
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets in Secret Manager Have Expiration Dates
  rationale: Secrets without expiration dates pose a security risk as they may be used indefinitely, increasing the chance of unauthorized access and potential data breaches. Expired secrets also help in minimizing the risk of exposure from unused or forgotten credentials, which can be exploited by attackers. Implementing expiration policies is critical for compliance with security frameworks like NIST and PCI-DSS, which require that secrets are rotated and managed properly.
  description: This rule checks if secrets stored in Google Cloud Secret Manager are configured with expiration dates. To verify, review each secret's settings to ensure an expiration date is set. If not, update the secret configuration to include an appropriate expiration policy. Regularly rotate secrets and establish a lifecycle management policy to automatically handle secret expiration and renewal.
  references:
  - https://cloud.google.com/secret-manager/docs/managing-secrets
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.secretmanager.secret.secrets_path_length_constraints_set
  service: secretmanager
  resource: secret
  requirement: Secrets Path Length Constraints Set
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Path Length Constraints in Secret Manager
  rationale: Defining strict path length constraints for secrets in Secret Manager helps mitigate risks of unauthorized access and accidental exposure by ensuring that secret paths are not easily guessable. This practice is crucial for maintaining confidentiality and integrity of sensitive data, aligning with compliance requirements such as PCI-DSS and ISO 27001 which mandate robust access controls and data protection measures.
  description: This rule checks that the path length for secrets stored in Google Secret Manager adheres to defined security policies, ensuring it is neither too short nor too complex to avoid predictability and manageability issues. To verify compliance, review your secret naming conventions and ensure they follow set constraints. Remediation involves updating secret paths to meet the length criteria, utilizing tools like Terraform or gcloud CLI for efficient management.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.secretmanager.secret.secrets_rotation_configured_where_applicable
  service: secretmanager
  resource: secret
  requirement: Secrets Rotation Configured Where Applicable
  scope: secretmanager.secret.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Rotation Configured in Secret Manager
  rationale: Regular rotation of secrets is crucial to minimize the risk of exposure and unauthorized access. Unrotated secrets can be compromised, leading to potential data breaches, legal liabilities, and non-compliance with standards such as PCI-DSS and ISO 27001. Automated rotation reduces human error and enhances the security posture by ensuring secrets are regularly updated.
  description: This rule checks whether secrets stored in GCP Secret Manager have rotation policies configured. A rotation policy automatically updates the secret value at specified intervals, mitigating the risk of long-term exposure. To verify, inspect each secret's configuration for a 'rotation' setting and ensure it is enabled with a defined schedule. Remediate by setting or updating the rotation policy in the Secret Manager API or Console to meet security best practices.
  references:
  - https://cloud.google.com/secret-manager/docs/rotation
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://www.pcisecuritystandards.org/pci_security
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.secretmanager.secret.secrets_rotation_enabled_for_rotatable_secrets
  service: secretmanager
  resource: secret
  requirement: Secrets Rotation Enabled For Rotatable Secrets
  scope: secretmanager.secret.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Rotation is Enabled for All Rotatable Secrets
  rationale: Rotating secrets regularly minimizes the risk of unauthorized access due to secret compromise. It's critical for maintaining the confidentiality and integrity of sensitive information managed by the Secret Manager. Non-compliance can lead to data breaches and regulatory penalties under frameworks such as PCI-DSS and HIPAA, which require stringent data protection measures.
  description: This rule verifies that all secrets stored in GCP Secret Manager, which are capable of being rotated, have rotation policies enabled. To ensure secrets are rotated frequently, configure a rotation schedule or use automated rotation. Verify by checking the 'rotation' configuration of each secret. Remediate by setting a rotation policy using the Google Cloud Console or gcloud CLI, specifying the rotation period based on your security policy.
  references:
  - https://cloud.google.com/secret-manager/docs/managing-secrets#secret-rotation
  - https://cloud.google.com/secret-manager/docs/reference/rest/v1/projects.secrets#Secret
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.secretmanager.secret.secrets_secure_string_type_for_sensitive
  service: secretmanager
  resource: secret
  requirement: Secrets Secure String Type For Sensitive
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Manager Uses Secure String Type for Sensitive Data
  rationale: Using Secure String type for secrets in GCP ensures that sensitive information is encrypted and securely managed. Failure to use secure string types can result in unauthorized access, data breaches, and non-compliance with regulatory standards such as PCI-DSS and HIPAA, potentially leading to significant financial and reputational damage.
  description: This rule checks that all secrets stored in GCP Secret Manager are using the Secure String type. It verifies that sensitive data is not stored in plaintext or less secure formats. To remediate, ensure all secrets are updated to use the Secure String type by utilizing the `gcloud secrets versions add` command with proper encryption settings. Regular audits and updates should be conducted to maintain compliance and security.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.secretmanager.secret.secrets_trusted_issuer
  service: secretmanager
  resource: secret
  requirement: Secrets Trusted Issuer
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets have Trusted Issuer in Secret Manager
  rationale: Managing secrets with a trusted issuer is crucial to prevent unauthorized access and data breaches. Misconfigured secret issuers can lead to unauthorized decryption and misuse of sensitive data, potentially violating compliance requirements such as PCI-DSS and SOC2. Ensuring secrets are managed by trusted issuers reduces the risk of compromised credentials and enhances the overall security posture.
  description: This rule checks if all secrets stored in GCP Secret Manager are issued by trusted entities. It verifies the issuer's identity to ensure that secrets are not exposed to unverified sources, which could lead to unauthorized access. To comply, configure the Secret Manager to accept only specific, trusted issuers through IAM policies or organizational policies. Remediate any discrepancies by updating the issuer configurations and regularly reviewing issuer trust lists.
  references:
  - https://cloud.google.com/secret-manager/docs/managing-secrets
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security
- rule_id: gcp.securitycenter.automation.vuln_automation_change_audit_logging_enabled
  service: securitycenter
  resource: automation
  requirement: Vuln Automation Change Audit Logging Enabled
  scope: securitycenter.automation.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for Vulnerability Automation Changes
  rationale: Enabling audit logging for vulnerability automation changes is crucial to ensure traceability of actions affecting the security posture. Without these logs, unauthorized or accidental changes may go undetected, increasing the risk of exposure to vulnerabilities. Furthermore, audit logs support compliance with regulatory standards such as PCI-DSS and ISO 27001, which require detailed records of security-related activities.
  description: This rule checks if audit logging is enabled for changes made to vulnerability automation configurations in Google Cloud Security Command Center (SCC). To verify, ensure that the 'Admin Activity' logs are activated for the Security Command Center, capturing any modifications to vulnerability settings. Remediation involves navigating to the 'Logging' section in the GCP Console, selecting the appropriate project, and enabling 'Admin Activity' logs for SCC under 'Audit Logs'.
  references:
  - https://cloud.google.com/security-command-center/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.securitycenter.automation.vuln_automation_execution_roles_least_privilege
  service: securitycenter
  resource: automation
  requirement: Vuln Automation Execution Roles Least Privilege
  scope: securitycenter.automation.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Vulnerability Automation Roles
  rationale: Implementing least privilege for vulnerability automation roles reduces the risk of unauthorized access and potential exploitation. Excessive permissions can lead to data breaches or disruptions in service if compromised. Adhering to least privilege aligns with compliance requirements such as NIST and PCI-DSS, ensuring only necessary permissions are granted.
  description: This rule checks for adherence to the principle of least privilege in roles assigned to vulnerability automation tasks in Security Command Center. Review and adjust IAM roles to ensure they only include permissions essential for automation execution. Remediate by auditing existing roles and modifying or creating custom roles with minimal necessary permissions. Verification can be done via the GCP Console IAM & Admin section or using Google Cloud CLI commands.
  references:
  - https://cloud.google.com/security-command-center/docs/automating-tasks
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - 'NIST SP 800-53: AC-6 Least Privilege'
  - 'PCI-DSS v3.2.1: 7.1 Limit Access to System Components'
  - https://cloud.google.com/iam/docs/creating-custom-roles
  - https://cloud.google.com/security-command-center/docs/best-practices
- rule_id: gcp.securitycenter.finding.center_enabled
  service: securitycenter
  resource: finding
  requirement: Center Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Command Center is Enabled
  rationale: Enabling the Security Command Center (SCC) is crucial for maintaining visibility into cloud resources and identifying vulnerabilities and threats in your GCP environment. It helps mitigate risks by providing threat intelligence and insights into potential security misconfigurations, which is essential for protecting sensitive data and maintaining compliance with regulations such as PCI-DSS and HIPAA.
  description: This rule checks whether the Security Command Center is enabled on your GCP projects. SCC is a centralized security and risk management platform that aggregates security findings from various GCP services. To verify, navigate to the Security Command Center in the Google Cloud Console and ensure that it is activated for all relevant projects. If not enabled, follow the setup guide to activate SCC and configure it to suit your organization's security needs.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-compliance-requirements/
- rule_id: gcp.securitycenter.finding.center_is_enabled
  service: securitycenter
  resource: finding
  requirement: Center Is Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Command Center is Enabled
  rationale: Enabling Security Command Center is crucial for gaining visibility into your Google Cloud resources and identifying potential security threats. Without it, organizations may be unaware of vulnerabilities, leading to increased risk of data breaches and non-compliance with regulatory standards such as GDPR or PCI-DSS. This can result in financial penalties and damage to company reputation.
  description: This rule checks whether the Security Command Center is enabled, which is essential for monitoring and managing security findings across Google Cloud resources. To verify, navigate to the Security Command Center in the Google Cloud Console and ensure it is activated for your organization. If not enabled, activate it via the console or use the gcloud command-line tool. This will help in proactively detecting, analyzing, and mitigating security threats.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart-security-command-center
  - https://cloud.google.com/security-command-center/docs/how-to-api-findings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/document_library
- rule_id: gcp.securitycenter.finding.center_no_high_severity_findings
  service: securitycenter
  resource: finding
  requirement: Center No High Severity Findings
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure No High Severity Findings in Security Command Center
  rationale: High severity findings in the Security Command Center indicate potential vulnerabilities or misconfigurations that could lead to data breaches, unauthorized access, or service disruptions. Addressing these findings promptly helps maintain the integrity of the cloud environment, protects sensitive data, and ensures compliance with industry regulations and standards such as ISO 27001 and PCI-DSS.
  description: This rule checks for the presence of high severity findings within the Security Command Center. High severity findings typically indicate significant security issues that require immediate attention. To verify compliance, regularly review the findings list in the Security Command Center and prioritize remediation of high severity issues. Implement automated alerts and workflows to ensure timely resolution of these findings.
  references:
  - https://cloud.google.com/security-command-center/docs/finding-types
  - https://cloud.google.com/security-command-center/docs/how-to-hub-findings
  - https://cloud.google.com/security-command-center/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.securitycenter.finding.command_center_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Security Command Center for Robust Threat Detection
  rationale: Enabling Security Command Center is crucial for identifying and mitigating security risks in real-time, providing a centralized platform for monitoring and managing threats across GCP resources. This reduces potential security breaches and data loss, ensuring compliance with industry standards and regulations such as PCI-DSS and NIST. Effective threat detection helps organizations maintain customer trust and protect sensitive data.
  description: This rule checks if the Security Command Center is enabled, which is essential for comprehensive security monitoring on GCP. To verify, ensure that the Security Command Center API is activated in your GCP project and configured to continuously monitor and analyze security findings. Remediation involves enabling the Security Command Center through the GCP Console or using the gcloud command line tool, followed by setting up appropriate notifications and response actions.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security-command-center/docs/how-to-enable-scc
  - https://cloud.google.com/security-command-center/docs/concepts-vulnerability-management
- rule_id: gcp.securitycenter.finding.command_center_guardduty_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Guardduty Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Command Center is Enabled for Threat Detection
  rationale: Enabling Security Command Center is crucial for identifying and mitigating threats across GCP environments. Without it, potential security incidents such as unauthorized access or data breaches may go unnoticed, leading to financial loss or reputational damage. It also aids in meeting compliance requirements by providing continuous monitoring and alerting capabilities.
  description: This rule checks whether the Security Command Center is enabled for threat detection on GCP projects. Specifically, it verifies the activation of Security Health Analytics, which includes threat intelligence and anomaly detection. To remediate, ensure that the Security Command Center is enabled and configured correctly in the GCP Console. This involves setting up appropriate permissions and integrating with existing security workflows for effective monitoring.
  references:
  - https://cloud.google.com/security-command-center/docs
  - https://cloud.google.com/security-command-center/docs/concepts-security-health-analytics
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.securitycenter.finding.command_center_is_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Is Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Command Center is Enabled
  rationale: Enabling Security Command Center is crucial for monitoring and managing security risks across your Google Cloud environment. It provides visibility into threats and vulnerabilities, helping to identify misconfigurations and security incidents. Without it, organizations may face increased risk of data breaches and non-compliance with regulatory standards such as PCI-DSS or ISO 27001.
  description: This rule checks if Security Command Center is activated within your GCP environment. To verify, navigate to the Security Command Center in the Google Cloud Console and ensure it is enabled for continuous monitoring. Remediation involves activating the service from the Console, which will allow you to leverage its threat intelligence and analysis capabilities to maintain a strong security posture.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart-security-command-center
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/best-practices
- rule_id: gcp.securitycenter.finding.command_center_no_high_severity_findings_configured
  service: securitycenter
  resource: finding
  requirement: Command Center No High Severity Findings Configured
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure High Severity Findings Are Configured in Security Command Center
  rationale: Proper configuration of high severity findings in Security Command Center is critical for identifying and mitigating significant security risks that could lead to data breaches or service disruptions. Without these configurations, organizations may overlook critical vulnerabilities, leading to potential non-compliance with regulatory frameworks such as PCI-DSS and NIST, resulting in legal penalties and reputational damage.
  description: This rule checks if high severity findings are configured in the Google Cloud Security Command Center to ensure that critical vulnerabilities are identified and addressed promptly. Administrators should enable and configure high severity findings to receive alerts on potential security threats. Verification involves reviewing the Security Command Center settings to ensure high severity findings are active and appropriately configured. Remediation includes accessing the Security Command Center, navigating to the 'Findings' settings, and enabling notifications for high severity issues.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-findings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloud.google.com/security-command-center/docs/best-practices
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-using-security-command-center
- rule_id: gcp.securitycenter.finding.command_center_security_center_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Security Center Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Cloud Security Command Center for Enhanced Monitoring
  rationale: Enabling Security Command Center is crucial for centralized security management, helping detect and mitigate threats across your GCP environment. Without it, organizations may face delayed threat detection, leading to potential data breaches and non-compliance with regulatory standards such as GDPR and HIPAA. Proactive monitoring reduces the risk of financial loss and reputation damage.
  description: This rule checks whether the Security Command Center is enabled in your GCP environment, ensuring all security findings are captured and addressed. Verify by navigating to the Security Command Center in the GCP Console and ensuring it is active. To enable, go to the Security Command Center page, select your organization, and activate the service. Regularly review and respond to security findings to maintain a robust security posture.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart
  - https://cloud.google.com/security-command-center/docs/overview
  - CIS GCP Foundation Benchmark v1.3.0 - Section 7.1
  - NIST SP 800-53 Rev. 5 - CA-7 Continuous Monitoring
  - ISO/IEC 27001:2013 A.12.6.1 - Monitoring
  - PCI-DSS v3.2.1 - Requirement 10.6
- rule_id: gcp.securitycenter.finding.command_center_vulnerability_assessment_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Vulnerability Assessment Enabled
  scope: securitycenter.finding.vulnerability_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Enable Command Center Vulnerability Assessment
  rationale: Enabling vulnerability assessments in Security Command Center is vital for identifying potential security weaknesses in your GCP resources. This proactive approach helps prevent exploitation by attackers, assists in maintaining a strong security posture, and supports compliance with security frameworks like NIST and PCI-DSS, which require regular vulnerability assessments.
  description: This rule checks if the vulnerability assessment feature in Google Cloud's Security Command Center is enabled. This capability scans your environment for vulnerabilities across various resources such as VM instances and containers, providing actionable insights and recommendations for remediation. To verify, ensure that Security Command Center is set up in your GCP environment with the premium tier that includes vulnerability scanning. Remediation involves enabling the vulnerability assessment feature and regularly reviewing the findings to mitigate identified risks.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart-vm-scanning
  - https://cloud.google.com/security-command-center/docs/concepts-vulnerabilities
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.securitycenter.finding.incident_automation_artifacts_encrypted_and_private
  service: securitycenter
  resource: finding
  requirement: Incident Automation Artifacts Encrypted And Private
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Incident Artifacts Are Encrypted and Private
  rationale: Encrypting and securing incident automation artifacts is crucial to prevent unauthorized access and data breaches. Unencrypted or publicly accessible artifacts can expose sensitive information, leading to compliance violations with regulations like GDPR and HIPAA. This rule mitigates risks of data exposure during automated incident handling processes, ensuring both data integrity and privacy.
  description: This rule checks that incident automation artifacts within Security Command Center are both encrypted at rest and not publicly accessible. Ensure that all buckets storing these artifacts have encryption enabled and appropriate IAM policies set to restrict public access. Verify encryption settings in the Google Cloud Console and audit IAM policies to confirm they adhere to the principle of least privilege. Remediate by enabling default bucket encryption and reviewing IAM roles and permissions.
  references:
  - https://cloud.google.com/security-command-center/docs/security-command-center-overview
  - https://cloud.google.com/storage/docs/encryption
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Control 4.1
  - https://cloud.google.com/security-compliance
  - 'NIST SP 800-53 Rev. 5: AC-19, SC-28'
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.securitycenter.finding.incident_automation_change_audit_logging_enabled
  service: securitycenter
  resource: finding
  requirement: Incident Automation Change Audit Logging Enabled
  scope: securitycenter.finding.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Audit Logging for Incident Automation Changes
  rationale: Audit logging for incident automation changes is crucial for tracking alterations in security configurations, which can help detect unauthorized modifications and potential security breaches. This ensures accountability and supports compliance with regulations such as PCI-DSS and ISO 27001 by providing a clear audit trail of changes.
  description: This rule checks that audit logging is enabled for changes to incident automation within Google Cloud Security Command Center. Audit logs should capture who made changes, what changes were made, and when, ensuring visibility into configuration alterations. To verify, navigate to the Google Cloud Console, go to 'Security Command Center', and ensure logging settings are configured to capture all change activities. Remediate by enabling logging for all incident automation changes in the Security Command Center settings.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.securitycenter.finding.incident_automation_execution_roles_least_privilege
  service: securitycenter
  resource: finding
  requirement: Incident Automation Execution Roles Least Privilege
  scope: securitycenter.finding.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Incident Automation Roles
  rationale: Implementing least privilege for incident automation roles minimizes the risk of unauthorized access and potential data breaches. Over-permissive roles can lead to unintended actions, malicious exploitation, or data exfiltration, which can damage business reputation and lead to non-compliance with regulatory standards such as NIST and ISO 27001.
  description: This rule checks that roles assigned for incident automation in Google Cloud Security Command Center are operating under the principle of least privilege. It verifies the permissions granted are strictly necessary for task execution. To remediate, audit the roles and adjust permissions to only include necessary actions, using predefined roles where possible, and regularly review role assignments. This helps reduce the attack surface and ensures compliance with security policies.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-manage-findings
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.securitycenter.finding.incident_response_access_rbac_least_privilege
  service: securitycenter
  resource: finding
  requirement: Incident Response Access RBAC Least Privilege
  scope: securitycenter.finding.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Incident Response Roles
  rationale: Implementing least privilege access for incident response roles minimizes the risk of unauthorized access to sensitive security findings, which could lead to data breaches or misuse of critical information. This approach is essential to comply with regulatory requirements such as PCI-DSS and ISO 27001, which mandate strict access control measures to protect data integrity and confidentiality.
  description: This rule checks if the roles assigned for incident response in Security Command Center adhere to the principle of least privilege by ensuring only necessary permissions are granted. Review IAM policies to verify that roles such as 'Incident Responder' or 'Security Viewer' are not overprivileged. To remediate, adjust IAM policies to align with least privilege principles by removing unnecessary permissions and regularly auditing access logs for security findings.
  references:
  - https://cloud.google.com/security-command-center/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.securitycenter.finding.incident_response_storage_encrypted_and_private
  service: securitycenter
  resource: finding
  requirement: Incident Response Storage Encrypted And Private
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Incident Response Storage is Encrypted and Private
  rationale: Encrypting and securing incident response storage in GCP protects sensitive data from unauthorized access, mitigating risks such as data breaches and unauthorized data manipulation. This is critical for maintaining trust and compliance with regulations like GDPR and HIPAA, which mandate data protection measures to safeguard personal and sensitive information.
  description: This rule verifies that storage used for incident response data is encrypted using Google-managed or customer-managed encryption keys and is configured to prevent public access. To ensure compliance, check that Cloud Storage buckets used for incident response have uniform bucket-level access enabled, encryption configured, and IAM policies that restrict access to authorized personnel only. Remediation involves configuring encryption settings in the GCP Console and reviewing IAM policies to ensure only authorized users have access.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/access-control/iam-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.securitycenter.finding.incident_response_versioning_and_immutability_enabled
  service: securitycenter
  resource: finding
  requirement: Incident Response Versioning And Immutability Enabled
  scope: securitycenter.finding.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Incident Response Versioning & Immutability in Security Center
  rationale: Enabling versioning and immutability for incident response findings in GCP Security Center is critical to maintain data integrity and facilitate forensic investigations. Without versioning, historical data could be altered or lost, impacting the ability to analyze incidents accurately. This practice supports regulatory requirements for data protection and incident handling, such as those outlined in frameworks like NIST and ISO 27001.
  description: This rule checks if versioning and immutability are enabled for findings within GCP Security Center. By configuring these settings, you ensure that all changes to incident data are tracked and that previous versions are preserved. To verify, navigate to the GCP Console, access Security Center settings, and confirm that versioning is switched on and immutability policies are configured. Remediation involves enabling these features in the Security Center settings and regularly reviewing configurations to align with best practices.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-configure-versioning
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 8.1
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/best-practices
- rule_id: gcp.securitycenter.finding.is_enabled
  service: securitycenter
  resource: finding
  requirement: Is Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Security Findings in Security Command Center
  rationale: Enabling security findings in Google Cloud's Security Command Center is crucial for maintaining visibility over potential security threats. Without enabled findings, organizations may miss critical alerts that could lead to breaches or non-compliance with regulations such as PCI-DSS or ISO 27001. This not only increases the risk of data exposure but can also result in financial and reputational damage.
  description: This rule checks whether security findings are enabled within the Security Command Center. To verify, ensure that the Security Command Center is correctly configured to detect and report findings for all relevant resources. Remediation involves navigating to the Security Command Center settings in the GCP Console and enabling findings for each necessary security project. This ensures continuous monitoring and timely detection of security threats.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-enable-and-configure
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.securitycenter.finding.privacy_breach_detection_alert_destinations_configured
  service: securitycenter
  resource: finding
  requirement: Privacy Breach Detection Alert Destinations Configured
  scope: securitycenter.finding.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Configure Privacy Breach Alert Destinations in Security Command Center
  rationale: Properly configuring alert destinations for privacy breaches ensures timely notifications, enabling rapid response to potential data leaks. This reduces the risk of data exposure and helps meet compliance requirements such as GDPR and HIPAA, protecting both the organization's reputation and financial standing.
  description: This rule verifies that Google Cloud's Security Command Center is configured with alert destinations for privacy breach detections. Ensure that appropriate channels, such as Cloud Pub/Sub or email notifications, are set up for real-time alerts. To verify, check the Security Command Center settings under the 'Notifications' section. Remediate by configuring alert destinations to enable prompt incident response.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-notifications
  - https://cloud.google.com/security-command-center/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.securitycenter.finding.privacy_breach_detection_integrations_authenticated
  service: securitycenter
  resource: finding
  requirement: Privacy Breach Detection Integrations Authenticated
  scope: securitycenter.finding.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Privacy Breach Detection Integrations are Authenticated
  rationale: Authenticating integrations with privacy breach detection services is crucial to prevent unauthorized access to sensitive data and ensure compliance with privacy regulations such as GDPR and HIPAA. Unauthenticated or improperly configured integrations could lead to data leaks, reputational damage, and legal penalties by allowing unauthorized data access.
  description: This rule verifies that all integrations with privacy breach detection services within Google Cloud Security Command Center are properly authenticated. It checks for the presence of secure authentication methods such as OAuth 2.0 or service accounts for API access. To remediate, ensure that each integration uses a secure, OAuth 2.0 token-based mechanism or a service account with the least privilege required. Regularly audit these configurations to maintain security and compliance.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-api-authentication
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/privacy/
  - https://cloud.google.com/docs/security-overview
- rule_id: gcp.securitycenter.finding.resource_enabled
  service: securitycenter
  resource: finding
  requirement: Resource Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Findings Are Enabled in Security Command Center
  rationale: Enabling resource monitoring in Security Command Center helps organizations identify and mitigate potential security threats promptly. This capability is essential for maintaining a robust security posture by ensuring that all security findings are captured and addressed, reducing the risk of data breaches and ensuring compliance with industry standards such as ISO 27001 and NIST SP 800-53.
  description: This rule checks whether security findings are enabled within the Security Command Center for all relevant resources. Ensuring that findings are enabled allows for continuous monitoring of security events and incidents. To verify, navigate to the Security Command Center in the GCP Console and ensure that all necessary resources have findings enabled. Remediate by configuring the Security Command Center to monitor all critical resources and review findings regularly to address any security gaps.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-use-security-command-center
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/iso-27001/
- rule_id: gcp.securitycenter.finding.threat_finding_alert_destinations_configured
  service: securitycenter
  resource: finding
  requirement: Threat Finding Alert Destinations Configured
  scope: securitycenter.finding.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Configure Threat Finding Alert Destinations
  rationale: Configuring alert destinations for threat findings in Security Command Center enables timely notifications for incidents, facilitating rapid response and mitigation. Without proper alerting, organizations may face delayed threat detection, increasing the risk of data breaches and non-compliance with standards like PCI-DSS and GDPR, which mandate timely incident response.
  description: This rule checks if Google Cloud Security Command Center has alert destinations configured for threat findings. Proper configuration includes setting up Pub/Sub topics or email notifications to ensure that security teams are promptly informed about potential threats. To verify, review the Security Command Center settings and ensure alert policies are in place. Remediation involves configuring alerting mechanisms through the Google Cloud Console or API to route findings to appropriate destinations.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-notifications
  - https://cloud.google.com/security-command-center/docs/how-to-enable-and-configure-notifications
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.securitycenter.finding.threat_finding_archival_export_encrypted
  service: securitycenter
  resource: finding
  requirement: Threat Finding Archival Export Encrypted
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt Threat Finding Archival Exports
  rationale: Encrypting threat finding archival exports is crucial for protecting sensitive security data from unauthorized access, reducing the risk of data breaches and ensuring compliance with data protection regulations. It prevents potential attackers from accessing or tampering with security findings, which could lead to undetected threats and vulnerabilities within the cloud environment.
  description: This rule checks if threat finding archival exports from Security Command Center are encrypted at rest using Customer-managed encryption keys (CMEK). To verify, ensure that the Cloud Storage buckets used for storing exports are configured with CMEK. Remediation involves setting up CMEK for the relevant storage buckets and updating export configurations to use these encrypted buckets. This protects exported security data by leveraging advanced encryption keys managed through Cloud Key Management Service (KMS).
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-findings#exporting_findings
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/kms/docs
- rule_id: gcp.securitycenter.finding.threat_finding_suppression_rules_documented_and_scoped
  service: securitycenter
  resource: finding
  requirement: Threat Finding Suppression Rules Documented And Scoped
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Document and Scope Threat Finding Suppression Rules
  rationale: Documenting and scoping threat finding suppression rules ensures that only legitimate findings are suppressed, reducing the risk of ignoring critical security alerts. Proper documentation aids in compliance with regulations such as PCI-DSS and ISO 27001, and supports auditability and accountability in security operations.
  description: This check verifies that all threat finding suppression rules in Google Cloud Security Command Center are documented and scoped appropriately. It ensures that rules are not overly broad, which could lead to critical security findings being ignored. To verify, review the Security Command Center settings to ensure suppression rules are clearly documented with justifications and scope limitations. Remediate by updating the rules to include detailed documentation and by narrowing their scope as necessary.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-suppress-findings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security-command-center/docs/concepts-findings
- rule_id: gcp.securitycenter.finding.vuln_assessment_policy_store_encrypted
  service: securitycenter
  resource: finding
  requirement: Vuln Assessment Policy Store Encrypted
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Vuln Assessment Policies are Encrypted at Rest
  rationale: Encrypting vulnerability assessment policies at rest protects sensitive information from unauthorized access and potential data breaches. This is crucial for maintaining the confidentiality and integrity of security policies, reducing the risk of exploitation by attackers, and ensuring compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks that all vulnerability assessment policies stored within Google Cloud Security Command Center are encrypted using Google-managed encryption keys. To verify, ensure that the policies are stored in a manner that leverages Cloud KMS for encryption. Remediation involves configuring the storage of these policies to use encryption at rest by default, which can be done by enabling Cloud KMS integration through the Google Cloud Console or CLI.
  references:
  - https://cloud.google.com/security-command-center/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 4.5
  - 'NIST SP 800-57 Part 1: Recommendation for Key Management'
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.securitycenter.finding.vuln_assessment_roles_least_privilege
  service: securitycenter
  resource: finding
  requirement: Vuln Assessment Roles Least Privilege
  scope: securitycenter.finding.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Vulnerability Assessment Roles
  rationale: Implementing least privilege for vulnerability assessment roles minimizes the risk of unauthorized access and potential data breaches, safeguarding sensitive information and maintaining compliance with regulations like PCI-DSS and SOC2. Excessive permissions can lead to exploitation, allowing attackers to escalate privileges or access confidential data, resulting in financial and reputational damage.
  description: This rule checks that roles assigned for vulnerability assessment in GCP Security Command Center are configured with the minimum permissions necessary to perform their tasks. It involves reviewing IAM policies to ensure that no unnecessary permissions are granted. Remediation includes auditing current roles and permissions, adjusting them to align with the principle of least privilege by removing any permissions that are not essential for specific vulnerability assessment tasks.
  references:
  - https://cloud.google.com/security-command-center/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/document_library
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.securitycenter.finding.vuln_scan_agents_or_scanners_deployed
  service: securitycenter
  resource: finding
  requirement: Vuln Scan Agents Or Scanners Deployed
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Vulnerability Scanners are Properly Deployed
  rationale: Deploying vulnerability scanners helps identify security weaknesses early, reducing the risk of exploitation. Regular scanning is crucial for compliance with standards like PCI-DSS, which mandate proactive vulnerability management. It also aligns with best practices for maintaining a robust security posture by enabling prompt remediation of identified vulnerabilities.
  description: This rule verifies the deployment of vulnerability scanning agents across your GCP environment. It checks for the presence of security tools such as Google Cloud Security Command Center's built-in scanners or third-party scanning solutions. Proper deployment involves ensuring these tools are authorized, regularly updated, and cover all applicable resources. To remediate, deploy scanners on all critical assets, configure regular scan schedules, and review findings promptly.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-vulnerability-scanning
  - https://cloud.google.com/security-command-center/docs/how-to-security-health-analytics
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-115.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.securitycenter.finding.vuln_scan_results_export_destination_encrypted
  service: securitycenter
  resource: finding
  requirement: Vuln Scan Results Export Destination Encrypted
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Vulnerability Scan Results Exports Are Encrypted at Rest
  rationale: Encrypting vulnerability scan results exported to storage ensures that sensitive data about system vulnerabilities is protected against unauthorized access. This helps prevent data breaches, protects organizational reputation, and meets compliance requirements for data protection such as those outlined by NIST and ISO 27001. It mitigates the risk of exposure of potential attack vectors to malicious actors.
  description: This rule checks whether the destination where vulnerability scan results are exported is encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that the storage bucket or database where the results are stored has encryption enabled in its configuration settings. Remediation involves enabling encryption on the storage destination either by using Google Cloud's default encryption or by applying a customer-managed encryption key (CMEK) for enhanced security control.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-export-findings
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices/data-encryption
- rule_id: gcp.securitycenter.finding.vuln_scan_scope_includes_all_asset_groups
  service: securitycenter
  resource: finding
  requirement: Vuln Scan Scope Includes All Asset Groups
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Vulnerability Scan Scope Covers All Asset Groups
  rationale: Ensuring vulnerability scans include all asset groups is critical for identifying and mitigating potential security risks across your entire Google Cloud environment. Without comprehensive coverage, vulnerabilities in unscanned asset groups could lead to security breaches or compliance failures, affecting business continuity and data protection. This is particularly important for adhering to regulatory standards such as PCI-DSS and ISO 27001, which mandate comprehensive risk assessments.
  description: This rule checks whether the vulnerability scanning scope in Google Cloud Security Command Center includes all asset groups. A complete scan ensures no asset group is overlooked, minimizing the risk of undetected vulnerabilities. To verify, ensure that Security Command Center's configuration encompasses every asset group. If gaps are identified, adjust the scan scope settings to include all asset groups, ensuring ongoing monitoring and remediation of vulnerabilities.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-vuln-mgt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/quickstart-vuln-mgt
- rule_id: gcp.securitycenter.source.threat_custom_identifier_source_trusted
  service: securitycenter
  resource: source
  requirement: Threat Custom Identifier Source Trusted
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Trusted Sources for Custom Threat Identifiers
  rationale: Having trusted sources for custom threat identifiers is crucial to prevent the ingestion of unreliable or malicious threat data, which could lead to false positives, missed threats, or even system compromise. Ensuring the integrity and trustworthiness of threat data sources supports compliance with security frameworks that mandate accurate threat detection and response mechanisms.
  description: This rule checks whether the custom threat identifiers in Security Command Center are sourced from trusted entities. Organizations should configure their threat intelligence sources to ensure data integrity and reliability. Verification involves checking the configuration of custom threat sources in Security Command Center and aligning them with trusted threat intelligence providers. Remediation includes removing untrusted sources and adding verified and reputable ones.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-use
  - https://cloud.google.com/security-command-center/docs/concepts-threat-intelligence
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.securitycenter.source.threat_custom_identifier_storage_encrypted
  service: securitycenter
  resource: source
  requirement: Threat Custom Identifier Storage Encrypted
  scope: securitycenter.source.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption for Threat Custom Identifier Storage
  rationale: Encrypting threat custom identifier storage mitigates the risk of unauthorized access to sensitive threat intelligence data, protecting against data breaches and ensuring privacy. This practice supports compliance with standards such as NIST and PCI-DSS, which require data protection measures for sensitive information.
  description: This rule checks if threat custom identifier storage in Google Cloud Security Command Center (SCC) is encrypted at rest. It requires enabling and configuring encryption settings to use customer-managed encryption keys (CMEK) for enhanced security. Administrators should verify that encryption is enabled for SCC sources and configure CMEK through the Google Cloud Console or via gcloud commands.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/encrypting-data
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://docs.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.securitycenter.source.threat_detector_enabled_in_all_regions
  service: securitycenter
  resource: source
  requirement: Threat Detector Enabled In All Regions
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Threat Detection Enabled Across All GCP Regions
  rationale: Enabling threat detection in all regions is crucial to maintain comprehensive visibility into potential security threats and anomalies across your GCP environment. This helps in early identification and mitigation of risks, ensuring that no regional deployment is left unmonitored. It also supports compliance with various regulatory standards that mandate continuous monitoring and incident detection capabilities.
  description: This rule checks whether threat detection is active in all available GCP regions for the Security Command Center. To verify and enable threat detection, access the Security Command Center settings in the Google Cloud Console and ensure that the threat detectors are configured and operating in each region where your resources are deployed. Enabling this feature provides continuous monitoring and alerting for suspicious activities, thus reinforcing your security posture.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-enable-and-configure
  - https://cloud.google.com/security-command-center/docs/concepts-threat-detection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.securitycenter.source.threat_detector_finding_export_encrypted_destination
  service: securitycenter
  resource: source
  requirement: Threat Detector Finding Export Encrypted Destination
  scope: securitycenter.source.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Threat Detector Findings Export with Encryption
  rationale: Encrypting threat detector findings is critical to protect sensitive data from unauthorized access during storage and transmission. Unencrypted exports could expose vulnerabilities and sensitive information to malicious actors, potentially leading to data breaches or non-compliance with regulations such as GDPR and HIPAA.
  description: This rule checks whether findings from the Threat Detector are exported to a destination with encryption enabled at rest. It ensures that all exported data is encrypted using Cloud Storage bucket-level encryption settings. To verify, confirm that the destination bucket has a default encryption key configured. Remediation involves setting a default encryption key on the Cloud Storage bucket used for exports, which can be done via the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-export-findings
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.securitycenter.source.threat_hub_auto_enroll_new_accounts
  service: securitycenter
  resource: source
  requirement: Threat Hub Auto Enroll New Accounts
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Auto Enrollment of New Accounts in Security Command Center
  rationale: Automatically enrolling new accounts in the Google Cloud Security Command Center ensures continuous security monitoring and compliance from the onset. This reduces the risk of security misconfigurations and vulnerabilities in newly added accounts, which can lead to unauthorized access or data breaches. It aligns with compliance requirements by ensuring consistent security posture management across all accounts.
  description: This rule checks whether new projects are automatically enrolled in the Google Cloud Security Command Center (SCC). To verify, access the SCC settings in your GCP Console and ensure the 'Auto-enroll new projects' option is enabled. If not configured, modify the SCC settings to automatically include new projects, ensuring they are immediately subject to security scans and threat analysis. This proactive measure helps maintain a robust security posture across your GCP environment.
  references:
  - https://cloud.google.com/security-command-center/docs/enabling-security-command-center
  - https://cloud.google.com/architecture/security-command-center-best-practices
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0 - 2.6
  - 'NIST SP 800-53 Rev. 5 - CA-2: Security Assessments'
  - ISO/IEC 27001:2013 - A.12.4.1 Event Logging
- rule_id: gcp.securitycenter.source.threat_hub_master_member_configured
  service: securitycenter
  resource: source
  requirement: Threat Hub Master Member Configured
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Threat Hub Master Member Configured in Security Command Center
  rationale: Configuring a master member in Google Cloud's Security Command Center (SCC) is crucial for centralized threat intelligence management, allowing for effective threat detection and response. Without proper configuration, organizations may face fragmented threat monitoring, increasing the risk of missed alerts and inefficient incident response, which can lead to regulatory non-compliance and data breaches.
  description: This rule checks if the Threat Hub master member is properly configured within the Security Command Center (SCC). A master member acts as a central point for managing threat intelligence across multiple projects, enabling streamlined security operations. Verify configuration by accessing the SCC settings in the Google Cloud Console and ensuring the master member is set. Remediation involves assigning or configuring a master member, which can be done through the console or using gcloud commands.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-security-command-center
  - https://cloud.google.com/security-command-center/docs/set-up
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-enterprise-security-in-google-cloud
  - https://cloud.google.com/security-command-center/docs/reference/rest/v1/organizations.sources
- rule_id: gcp.securitycenter.source.threat_ip_set_sources_trusted
  service: securitycenter
  resource: source
  requirement: Threat Ip Set Sources Trusted
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Threat IP Set Sources Are Trusted
  rationale: Ensuring that only trusted IP sources are included in your threat IP set is crucial for minimizing false positives and enhancing the accuracy of threat detection. Untrusted or incorrectly configured IP sources can lead to unnecessary alerts, wasting resources and potentially causing critical threats to be overlooked. This practice supports compliance with security frameworks that require accurate and efficient threat monitoring.
  description: This check verifies that all IP sources included in your Security Command Center's threat intelligence feed are from trusted origins. It involves reviewing the configured threat IP sets and ensuring they are only populated with sources vetted by your organization's security policy. Remediation involves updating threat IP sets to exclude any sources deemed untrusted or irrelevant, and regularly auditing these settings to maintain their integrity.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-threat-intelligence
  - https://cloud.google.com/security-command-center/docs/how-to-use-threat-intelligence
  - https://cloud.google.com/security-command-center/docs/reference/rest/v1/organizations.sources
  - CIS Google Cloud Computing Foundation Benchmark (v1.2.0) - Section 5.1
  - NIST SP 800-53 Rev. 5 - SI-4 Information System Monitoring
  - ISO/IEC 27001:2013 - A.12.4 Logging and monitoring
- rule_id: gcp.securitycenter.source.threat_ip_set_storage_encrypted
  service: securitycenter
  resource: source
  requirement: Threat Ip Set Storage Encrypted
  scope: securitycenter.source.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Threat IP Set Storage is Encrypted at Rest
  rationale: Encrypting threat IP set storage protects sensitive data from unauthorized access and potential data breaches, which can lead to financial loss and reputational damage. It is crucial for meeting compliance requirements such as GDPR and HIPAA, which mandate the protection of personal and sensitive data. Encryption helps mitigate risks from threat actors who may gain access to storage devices.
  description: 'This rule checks that all threat IP set data stored within Security Command Center sources are encrypted at rest using Google-managed keys. Verify that encryption is enabled by reviewing the encryption settings in the Google Cloud Console under Security Command Center. Remediate by enabling encryption for any unencrypted IP sets, ensuring compliance with data protection policies. Use the ''gcloud'' command line tool to confirm encryption settings: `gcloud security-center sources describe [SOURCE_ID] --format=''value(encryptionKey)''`.'
  references:
  - https://cloud.google.com/security-command-center/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.securitycenter.source.threat_ip_set_used_by_detectors
  service: securitycenter
  resource: source
  requirement: Threat Ip Set Used By Detectors
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Threat Intelligence IP Sets Are Applied in Security Center
  rationale: Applying threat intelligence IP sets in Google Cloud Security Center enhances your organization's ability to detect and respond to potential cybersecurity threats. This practice helps minimize risks by proactively identifying and blocking malicious IP addresses, thereby protecting sensitive data and maintaining compliance with security standards such as NIST and ISO 27001.
  description: This rule checks if threat intelligence IP sets are being utilized by detectors in Google Cloud Security Center. Ensuring these IP sets are applied allows for enhanced monitoring and alerting on suspicious activities associated with known malicious IP addresses. To verify, navigate to the Security Center's settings and confirm that threat intelligence IP sets are configured and active. Remediation involves enabling these IP sets under the Security Center configuration and ensuring they are linked to relevant detectors.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-threat-intelligence
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/how-to-use
  - https://cloud.google.com/security-command-center/docs/best-practices
- rule_id: gcp.services.service.keys_active_service_configured
  service: services
  resource: service
  requirement: Keys Active Service Configured
  scope: services.service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Service Account Keys Are Properly Managed
  rationale: Improper management of service account keys can lead to unauthorized access to critical resources, data breaches, and compliance violations. Ensuring that keys are actively managed and configured minimizes the risk of exposure and aligns with data protection requirements under frameworks such as PCI-DSS and SOC2.
  description: This rule checks whether service account keys are actively managed and configured according to best practices, ensuring encryption at rest and adherence to a minimal access principle. It verifies that keys are rotated regularly and unused keys are deleted. Remediation involves auditing existing keys, setting up key rotation policies, and using Google Cloud KMS for key management.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
- rule_id: gcp.services.service.keys_restrictions_enforced
  service: services
  resource: service
  requirement: Keys Restrictions Enforced
  scope: services.service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enforce Restrictions on Service Account Keys
  rationale: Enforcing restrictions on service account keys is crucial to minimize the risk of unauthorized access to sensitive data and services. Unrestricted keys pose a significant security risk as they can be exploited by attackers to gain entry into your cloud environment, leading to potential data breaches and compliance violations with standards like PCI-DSS and ISO 27001.
  description: This rule checks that restrictions such as IP address and time-based restrictions are enforced on service account keys. To verify, ensure that every service account key has conditional IAM policies implemented. Remediation involves auditing existing keys, applying necessary restrictions, and rotating keys regularly to mitigate risk. Configure these settings using the Google Cloud Console or gcloud CLI for effective access control.
  references:
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/blog/products/identity-security/enforcing-least-privilege-best-practices-in-gcp
- rule_id: gcp.services.service.keys_rotation_90_days_configured
  service: services
  resource: service
  requirement: Keys Rotation 90 Days Configured
  scope: services.service.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Enforce 90-Day Key Rotation for GCP Service Keys
  rationale: Regular rotation of service keys mitigates the risk of key compromise, reducing the window of opportunity for unauthorized access. This practice aligns with security policies and compliance requirements such as PCI-DSS and ISO 27001, which mandate regular key rotation to protect sensitive data.
  description: This rule checks if service keys for GCP services are configured to rotate every 90 days. Regular rotation helps prevent the risks associated with long-term key exposure, such as unauthorized access and data breaches. To verify, review the key management settings in the GCP Console and ensure automated rotation is enabled. Remediation involves setting up a Cloud Scheduler job or using Cloud Functions to automate the key rotation process.
  references:
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0#2.2
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/key-management
- rule_id: gcp.spanner.instance.backup_enabled
  service: spanner
  resource: instance
  requirement: Backup Enabled
  scope: spanner.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup is Enabled for Spanner Instances
  rationale: Enabling backups for Cloud Spanner instances is crucial for data resilience and disaster recovery. Without regular backups, data loss can occur due to accidental deletions, corruption, or incidents like system failures, affecting business continuity and potentially leading to significant financial impacts. Compliance with standards such as ISO 27001 and SOC2 often requires effective backup strategies to protect sensitive data.
  description: This rule checks whether Cloud Spanner instances have automated backups enabled. To verify, ensure that each Spanner instance has a backup schedule configured in the GCP Console or through the gcloud command-line tool. Remediation involves setting up automated backups using the 'gcloud spanner backups create' command or through the GCP Console by navigating to the Spanner instance settings and configuring a backup schedule.
  references:
  - https://cloud.google.com/spanner/docs/backup
  - https://cloud.google.com/security/compliance/cis-gcp-1-5-0
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/spanner/pricing#backup-pricing
- rule_id: gcp.storage.bucket.bucket_object_versioning
  service: storage
  resource: bucket
  requirement: Bucket Object Versioning
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Object Versioning for GCP Storage Buckets
  rationale: Enabling object versioning in GCP Storage Buckets is essential for data integrity and recovery, providing a mechanism to restore previous versions of objects in case of accidental deletion or overwriting. It mitigates risks associated with data loss and supports compliance with data protection regulations by ensuring data availability and resilience.
  description: This rule checks if object versioning is enabled for GCP Storage Buckets, a feature that keeps a history of modifications to objects, allowing you to retrieve deleted or overwritten data. To enable versioning, navigate to the Cloud Console, select your bucket, and enable versioning under the 'Versioning' tab. This can also be set via the gcloud command-line tool using `gsutil versioning set on gs://[BUCKET_NAME]`. Regularly review and manage versions to optimize storage costs.
  references:
  - https://cloud.google.com/storage/docs/using-object-versioning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.bucket_policy_only_enforced
  service: storage
  resource: bucket
  requirement: Bucket Policy Only Enforced
  scope: storage.bucket.policy_management
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Bucket Policy Only is Enforced on GCP Storage Buckets
  rationale: Enforcing Bucket Policy Only on GCP Storage Buckets ensures that all access is controlled exclusively through Cloud Identity and Access Management (IAM) policies. This reduces the risk of unauthorized access through less secure, legacy ACLs, aligning with modern security practices and compliance requirements like CIS and ISO 27001.
  description: This rule checks that GCP Storage Buckets have 'Bucket Policy Only' enabled, meaning that the bucket's access is governed solely by IAM policies, not ACLs. To verify, examine the bucket's settings in the GCP Console or use the gcloud command-line tool. Remediation involves updating the bucket's configuration to enable Bucket Policy Only, ensuring access control is centralized and consistent.
  references:
  - https://cloud.google.com/storage/docs/bucket-policy-only
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/storage/docs/access-control/iam
- rule_id: gcp.storage.bucket.cross_region_replication_encryption_enabled
  service: storage
  resource: bucket
  requirement: Cross Region Replication Encryption Enabled
  scope: storage.bucket.encryption
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: high
  title: Ensure Cross Region Replication Encryption for GCP Buckets
  rationale: Enabling encryption for cross-region replication in GCP storage buckets is crucial to protect sensitive data from unauthorized access during transit between regions. This mitigates the risk of data breaches and unauthorized disclosures, which can lead to significant financial loss, reputational damage, and non-compliance with regulatory standards such as GDPR and HIPAA.
  description: This rule checks if cross-region replication for GCP storage buckets is encrypted to secure data during transit. To verify, ensure that the 'replication' configuration in the bucket settings includes 'data encryption' enabled. If not configured, enable encryption by updating the bucket replication policy to include encryption options, utilizing either Google-managed or customer-managed encryption keys to meet compliance and security requirements.
  references:
  - https://cloud.google.com/storage/docs/replication
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/bucket-lock
- rule_id: gcp.storage.bucket.data_governance_enforced_on_sensitive_datasets
  service: storage
  resource: bucket
  requirement: Data Governance Enforced On Sensitive Datasets
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Data Governance on Sensitive Storage Buckets
  rationale: Enforcing data governance on sensitive datasets is crucial to prevent unauthorized access and data breaches, which can lead to financial loss and reputational damage. It ensures compliance with regulations such as GDPR and CCPA, which mandate strict data protection and privacy measures. Without proper governance, organizations risk non-compliance, leading to potential fines and legal repercussions.
  description: This rule checks whether data governance policies are enforced on storage buckets containing sensitive datasets. It involves verifying that access controls, encryption settings, and audit logging are properly configured. To remedy any deficiencies, ensure that IAM policies limit access to authorized users only, enable Object Versioning to preserve data integrity, and use Customer-Managed Encryption Keys (CMEK) for enhanced data protection. Regularly review and update access permissions and audit logs to ensure ongoing compliance.
  references:
  - https://cloud.google.com/storage/docs/data-governance
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/storage/docs/access-control/iam
- rule_id: gcp.storage.bucket.data_governance_expiration_rules_defined
  service: storage
  resource: bucket
  requirement: Data Governance Expiration Rules Defined
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Bucket Lifecycle Rules for Data Expiration are Configured
  rationale: Defining data expiration rules for GCP storage buckets supports effective data lifecycle management, reducing storage costs and minimizing risks of unauthorized data access. Without expiration rules, organizations may retain data longer than necessary, increasing exposure to data breaches and complicating compliance with regulations such as GDPR and CCPA.
  description: This rule checks if Google Cloud Storage buckets have lifecycle policies configured to automatically delete or archive data after a specified period. Ensure that each bucket has a well-defined lifecycle policy that aligns with data retention requirements. To verify, review the bucket's lifecycle configuration in the Cloud Console or via the gcloud CLI. To remediate, define lifecycle rules that specify conditions for data expiration and actions to take, such as deleting or transitioning objects.
  references:
  - https://cloud.google.com/storage/docs/lifecycle
  - https://cloud.google.com/storage/docs/best-practices#lifecycle
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.data_governance_immutable_retention_locked_where_required
  service: storage
  resource: bucket
  requirement: Data Governance Immutable Retention Locked Where Required
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Bucket Retention Policies for Immutable Data
  rationale: Implementing immutable retention policies for storage buckets is crucial for protecting sensitive data from unauthorized modifications and deletions, which is vital for maintaining data integrity and compliance with regulations like GDPR and HIPAA. Immutable retention ensures that once data is stored, it cannot be altered, mitigating risks from insider threats and accidental data loss.
  description: This rule checks if Google Cloud Storage buckets have an immutable retention policy locked, which prevents any changes to the data until the retention period expires. To verify, ensure that the 'retentionPolicy' is set and locked on the bucket configuration. If not configured, apply a policy using the GCP Console or gcloud CLI by setting 'retention-policy' and locking it to prevent modifications. This action ensures long-term data integrity and adherence to data governance standards.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://cloud.google.com/storage/docs/using-bucket-lock
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.2
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.storage.bucket.data_governance_policies_defined
  service: storage
  resource: bucket
  requirement: Data Governance Policies Defined
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Data Governance Policies for GCS Buckets
  rationale: Defining and enforcing data governance policies for Google Cloud Storage (GCS) buckets is critical to protect sensitive data from unauthorized access and ensure compliance with regulatory standards. Without these policies, organizations risk data breaches, potential financial penalties, and damage to reputation due to non-compliance with standards such as GDPR and HIPAA.
  description: This rule checks whether data governance policies have been defined for GCS buckets, focusing on encryption settings and access controls. Administrators should ensure that all buckets have defined policies that specify encryption at rest using Customer Managed Encryption Keys (CMEK) and enforce least privilege access. Remediation involves setting bucket policies in the GCP Console or using the gcloud command-line tool to apply encryption and access controls.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/storage/docs/security
- rule_id: gcp.storage.bucket.data_governance_protected_from_public_override
  service: storage
  resource: bucket
  requirement: Data Governance Protected From Public Override
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Access to Sensitive Storage Buckets
  rationale: Unrestricted public access to storage buckets can lead to unauthorized data exposure, resulting in potential data breaches, financial loss, and reputational damage. This is critical for compliance with data protection regulations such as GDPR and CCPA, which mandate strict access controls to safeguard personal and sensitive information.
  description: This rule checks for any storage buckets that allow public overrides, ensuring that data governance policies are enforced and sensitive data is not exposed to public access. Specifically, it verifies that the 'allUsers' and 'allAuthenticatedUsers' entities are not granted access to buckets. Remediation involves reviewing and adjusting bucket IAM policies to restrict public access and implementing appropriate access controls to meet compliance requirements.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.data_governance_versioning_enabled_where_supported
  service: storage
  resource: bucket
  requirement: Data Governance Versioning Enabled Where Supported
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Versioning is Enabled for GCP Storage Buckets
  rationale: Enabling versioning on GCP storage buckets is crucial for data recovery and integrity. It helps protect against accidental deletion or modification of objects, ensuring that previous versions can be restored if needed. This practice mitigates risks associated with data loss and supports compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks if versioning is enabled on GCP storage buckets. Versioning allows you to keep multiple variants of an object in the same bucket and retrieve previous versions when necessary. To verify and enable versioning, navigate to the Google Cloud Console, select the storage bucket, and check the 'Versioning' status under the 'Lifecycle' settings. If not enabled, activate versioning through the console or by using the 'gsutil' command-line tool with the command 'gsutil versioning set on gs://[BUCKET_NAME]'.
  references:
  - https://cloud.google.com/storage/docs/object-versioning
  - https://cloud.google.com/storage/docs/gsutil/commands/versioning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.data_protection_access_logging_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Access Logging Enabled
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Access Logging for Google Cloud Storage Buckets
  rationale: Enabling access logging for Google Cloud Storage buckets is crucial for tracking and analyzing who accesses the data and how often. This helps in identifying unauthorized access attempts, understanding usage patterns, and meeting compliance requirements such as PCI-DSS and HIPAA, which mandate detailed logging of data access and usage.
  description: This rule checks whether access logging is enabled for all Google Cloud Storage buckets. Enabling logging involves configuring the bucket to write access logs to a specified destination bucket. To verify, check the bucket's properties for a logging configuration. Remediation involves using the Google Cloud Console or CLI to set a logging destination bucket, ensuring that logs are retained and monitored regularly.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/storage/docs/audit-logging
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.storage.bucket.data_protection_block_public_access_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Block Public Access Enabled
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure Block Public Access on Storage Buckets is Enabled
  rationale: Public access to storage buckets can lead to data breaches, exposing sensitive information to unauthorized users. Enabling block public access is critical to prevent inadvertent exposure of data, which can have severe business impacts, including financial losses, reputational damage, and non-compliance with regulations such as GDPR and PCI-DSS.
  description: This rule checks if the 'Block Public Access' setting is enabled on Google Cloud Storage buckets to ensure no public access is permitted. To verify, navigate to the Google Cloud Console, select the Storage service, and review the bucket's permissions settings. Remediation involves setting 'Block Public Access' to 'on' for all applicable buckets through the console or using gcloud CLI to update bucket IAM policies, ensuring that no 'allUsers' or 'allAuthenticatedUsers' permissions are granted.
  references:
  - https://cloud.google.com/storage/docs/access-control/block-public-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.storage.bucket.data_protection_bucket_deny_insecure_transport
  service: storage
  resource: bucket
  requirement: Data Protection Bucket Deny Insecure Transport
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Buckets Deny Insecure Transport
  rationale: Allowing insecure transport to storage buckets can expose sensitive data to interception and unauthorized access. This poses risks such as data breaches and non-compliance with regulations like PCI-DSS, which mandate secure data transmission. Enforcing secure transport ensures data integrity and confidentiality, protecting organizational and customer data.
  description: This rule checks if GCP storage buckets are configured to deny requests made over insecure transport protocols. Buckets should enforce HTTPS by setting the 'Enforce TLS' option, ensuring that data is transmitted securely. To verify, check the bucket's configuration for 'Enforce HTTPS' and ensure it is enabled. If disabled, update the bucket's settings to enforce HTTPS to mitigate the risk of data interception.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/authentication
  - https://cloud.google.com/storage/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.storage.bucket.data_protection_bucket_deny_unencrypted_puts
  service: storage
  resource: bucket
  requirement: Data Protection Bucket Deny Unencrypted Puts
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Encryption for GCP Storage Bucket PUT Requests
  rationale: Preventing unencrypted data from being uploaded to storage buckets is crucial for protecting sensitive information from unauthorized access and potential data breaches. It mitigates risks associated with data leakage and ensures compliance with regulatory requirements like GDPR, HIPAA, and PCI-DSS, which mandate encryption of data at rest.
  description: This rule checks if GCP Storage Buckets are configured to deny PUT requests that do not specify server-side encryption. To verify, ensure that the bucket's IAM policy includes a condition that requires the 'x-goog-encryption-algorithm' header for PUT operations. Remediation involves updating the bucket policy to enforce this condition, ensuring all data is encrypted at rest using customer-managed keys or Google-managed encryption.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
- rule_id: gcp.storage.bucket.data_protection_bucket_no_public_principals
  service: storage
  resource: bucket
  requirement: Data Protection Bucket No Public Principals
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Access to GCP Storage Buckets
  rationale: Publicly accessible storage buckets can lead to unauthorized data exposure, resulting in data breaches or compliance violations. Ensuring no public principals have access to storage buckets protects sensitive data and helps meet regulatory requirements such as HIPAA and PCI-DSS, which mandate strict access controls.
  description: This rule checks for any public principals (e.g., allUsers, allAuthenticatedUsers) that have access to storage buckets, which can be verified by examining the bucket's IAM policy. To remediate, remove public access by updating the bucket's IAM policy to restrict access to only authorized users or service accounts. This involves carefully auditing and modifying the bucket policy using the Google Cloud Console or the gcloud CLI to ensure compliance with data protection policies.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/sites/default/files/hipaa-for-professionals.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.data_protection_bucket_no_wildcards_on_actions_or_principals
  service: storage
  resource: bucket
  requirement: Data Protection Bucket No Wildcards On Actions Or Principals
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Disallow Wildcards for Storage Bucket Actions/Principals
  rationale: Using wildcards in IAM policies for actions or principals can lead to unauthorized access and data exposure by inadvertently granting overly broad permissions. This practice increases the attack surface and risks violating compliance standards such as PCI-DSS or HIPAA, potentially leading to data breaches and financial penalties.
  description: This rule checks for IAM policies on GCP storage buckets that use wildcard characters in action or principal specifications, which can grant unintended access. To verify, review the bucket's IAM policy and ensure specific actions and principals are defined. Remediation involves replacing wildcards with explicit permissions, ensuring that only necessary roles are assigned to specific users or service accounts.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.storage.bucket.data_protection_cmk_cmek_configured
  service: storage
  resource: bucket
  requirement: Data Protection CMK Cmek Configured
  scope: storage.bucket.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure CMEK Configured for Storage Bucket Encryption
  rationale: Encrypting data at rest using Customer-Managed Encryption Keys (CMEK) enhances data security by giving organizations full control over the encryption keys, reducing the risk of unauthorized access. This is crucial for meeting compliance requirements such as GDPR and HIPAA, which mandate robust data protection measures. Failure to implement CMEK could lead to data breaches and regulatory penalties.
  description: This rule checks if Google Cloud Storage buckets are configured to use Customer-Managed Encryption Keys (CMEK) for data encryption at rest. To verify, check the bucket's configuration in the GCP Console or via the gcloud command-line tool to ensure a CMEK is specified. Remediation involves navigating to the 'Storage' section in the GCP Console, selecting the bucket, and configuring it to use a CMEK from Cloud Key Management Service (KMS).
  references:
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/kms/docs/cmek
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.bucket.data_protection_cross_region_copy_encrypted
  service: storage
  resource: bucket
  requirement: Data Protection Cross Region Copy Encrypted
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region Bucket Copies are Encrypted
  rationale: Encrypting cross-region bucket copies is crucial for protecting sensitive data against unauthorized access, especially during transit over potentially insecure networks. This helps mitigate risks such as data breaches, which can lead to financial loss, reputational damage, and non-compliance with regulations like GDPR and HIPAA that mandate data protection.
  description: This rule checks if cross-region copies of Cloud Storage buckets are configured to use encryption. Ensure that all bucket transfers between regions use either Google-managed or customer-managed encryption keys. Verify this by inspecting bucket settings in the GCP Console or using the gcloud CLI. Remediation involves updating bucket settings to enforce encryption during cross-region data transfers.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/cross-region-replication
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.storage.bucket.data_protection_encrypted
  service: storage
  resource: bucket
  requirement: Data Protection Encrypted
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud Storage Buckets Use Customer-Managed Encryption Keys
  rationale: Encrypting data at rest in Cloud Storage buckets with customer-managed encryption keys (CMEK) provides enhanced data control and security. It mitigates the risk of unauthorized access by allowing organizations to manage their own encryption keys. This practice also aids in meeting stringent compliance requirements such as PCI-DSS and HIPAA by ensuring sensitive data is protected and access is logged.
  description: This rule checks whether Cloud Storage buckets are configured to use CMEK for data encryption. It verifies that a Cloud KMS key is associated with each bucket. To remediate, navigate to the GCP Console, select the desired bucket, and update its encryption settings to use a key managed in Cloud KMS. This can also be automated using gcloud commands or Terraform scripts to enforce encryption policies consistently across your organization.
  references:
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kms/docs/
  - https://cloud.google.com/storage/docs/json_api/v1/buckets/setIamPolicy
- rule_id: gcp.storage.bucket.data_protection_encrypted_at_rest
  service: storage
  resource: bucket
  requirement: Data Protection Encrypted At Rest
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud Storage Buckets are Encrypted at Rest
  rationale: Encrypting data at rest in Cloud Storage is crucial for protecting sensitive information from unauthorized access and potential data breaches. This security measure helps mitigate risks associated with compromised credentials and unauthorized access to storage infrastructure. It also supports compliance with regulatory requirements such as GDPR, HIPAA, and PCI-DSS, which mandate data protection and privacy.
  description: This rule checks if Cloud Storage buckets have server-side encryption enabled by default. It verifies that encryption keys, either Google-managed or customer-managed, are used to encrypt data at rest. To ensure compliance, review bucket settings in the GCP Console or using gcloud CLI, and enable default encryption if not already configured. This can be done by specifying a default key or allowing Google to manage the encryption keys automatically.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/json_api/v1/buckets#resource
- rule_id: gcp.storage.bucket.data_protection_encryption_at_rest_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Encryption At Rest Enabled
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Storage Bucket Encryption At Rest
  rationale: Encrypting data at rest minimizes the risk of unauthorized data access and is crucial for protecting sensitive information stored in Google Cloud Storage. Without encryption, data breaches can lead to severe financial and reputational damage. Compliance with standards like PCI-DSS and HIPAA often mandates encryption to protect personal and financial information.
  description: This rule checks if Cloud Storage buckets have server-side encryption enabled to protect data at rest. By default, Google Cloud encrypts all data using Google-managed keys, but you can enhance security by using Customer-Managed Encryption Keys (CMEK) for added control. Verify encryption settings in the Google Cloud Console under the 'Encryption' section of the bucket settings, and ensure that CMEK is configured if required by your organization's policy.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-4/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.bucket.data_protection_fileshare_encryption_at_rest_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Fileshare Encryption At Rest Enabled
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Encryption at Rest for GCP Storage Buckets
  rationale: Encryption at rest protects sensitive data from unauthorized access, ensuring that even if physical media is compromised, the data remains secure. It is crucial for maintaining customer trust, achieving compliance with regulations such as GDPR, HIPAA, and PCI-DSS, and mitigating risks associated with data breaches and insider threats.
  description: This rule checks whether encryption at rest is enabled for GCP storage buckets, ensuring that all data is automatically encrypted using Google-managed keys or customer-supplied keys. Verification involves checking bucket settings for encryption configuration. To remediate, configure bucket settings to enable encryption at rest, preferably using customer-managed encryption keys (CMEK) for greater control. This can be done via the GCP Console, CLI, or API.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
- rule_id: gcp.storage.bucket.data_protection_fileshare_kms_key_policy_least_privilege
  service: storage
  resource: bucket
  requirement: Data Protection Fileshare KMS Key Policy Least Privilege
  scope: storage.bucket.policy_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Least Privilege on KMS Key Policies for Storage Buckets
  rationale: Implementing least privilege on KMS key policies reduces the risk of unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, especially when handling sensitive information subject to compliance regulations such as PCI-DSS and HIPAA. Over-permissive access can lead to data exposure and significant financial and reputational damage.
  description: This rule checks if Cloud Key Management Service (KMS) key policies used by Cloud Storage buckets adhere to the principle of least privilege. It ensures that only necessary roles and identities have access to encryption keys, minimizing the attack surface. Verify the IAM policy on your KMS keys to confirm that only specific service accounts and users have the required permissions. Remediation involves reviewing and updating IAM policies to restrict permissions to the smallest scope necessary.
  references:
  - https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.data_protection_fileshare_private_network_only
  service: storage
  resource: bucket
  requirement: Data Protection Fileshare Private Network Only
  scope: storage.bucket.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure GCP Storage Buckets Use Private Network Access
  rationale: Restricting GCP Storage Buckets to private network access minimizes exposure to unauthorized entities, lowering the risk of data breaches and ensuring compliance with data protection regulations. This control is crucial for protecting sensitive data from public internet exposure and aligns with principles of least privilege and network segmentation, which are vital for meeting regulatory standards like GDPR and HIPAA.
  description: This rule verifies that GCP Storage Buckets are configured to allow access only from private networks, specifically by implementing VPC Service Controls. To verify compliance, ensure that your Storage Bucket access is restricted using private IPs and that the 'public access prevention' feature is enabled. Remediation involves configuring your VPC network settings to include only trusted subnets and utilizing Identity and Access Management (IAM) policies to enforce this constraint.
  references:
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security
- rule_id: gcp.storage.bucket.data_protection_fileshare_snapshots_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Fileshare Snapshots Enabled
  scope: storage.bucket.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Fileshare Snapshots are Enabled for GCP Storage Buckets
  rationale: Enabling fileshare snapshots for GCP storage buckets enhances data resiliency by providing point-in-time recovery options. This feature mitigates the risk of data loss due to accidental deletion, data corruption, or ransomware attacks. It supports compliance with data protection regulations by maintaining data integrity and availability.
  description: This rule checks if fileshare snapshots are enabled for GCP storage buckets, which are crucial for backup and recovery processes. To verify, ensure that the snapshot schedule is configured and active for each bucket. Remediation involves accessing the Google Cloud Console, navigating to the relevant bucket, and setting up a snapshot schedule under the 'Backups' section. This ensures that data snapshots are regularly captured and stored securely.
  references:
  - https://cloud.google.com/storage/docs/snapshots
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/soc2
- rule_id: gcp.storage.bucket.data_protection_not_public
  service: storage
  resource: bucket
  requirement: Data Protection Not Public
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure GCS Buckets Do Not Allow Public Access
  rationale: Public access to Google Cloud Storage buckets can lead to unauthorized data exposure, potentially resulting in data breaches and compliance violations. Protecting sensitive data is crucial for maintaining customer trust and adhering to regulations such as GDPR, HIPAA, and PCI-DSS. Unauthorized access can lead to financial losses and damage to organizational reputation.
  description: This rule checks if any Google Cloud Storage buckets are publicly accessible. Publicly accessible buckets may allow unauthorized users to read, write, or delete data. Ensure that the bucket's IAM policy does not grant 'allUsers' or 'allAuthenticatedUsers' roles, and review ACL settings to restrict access. Remediation involves updating bucket permissions to limit access to trusted identities only, using the GCP Console or CLI.
  references:
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/storage/docs/public-access-prevention
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.storage.bucket.data_protection_not_publicly_readable
  service: storage
  resource: bucket
  requirement: Data Protection Not Publicly Readable
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure Storage Buckets Are Not Publicly Readable
  rationale: Publicly readable storage buckets can lead to unauthorized data access, exposing sensitive information to anyone on the internet. This can result in data breaches, financial loss, reputational damage, and non-compliance with regulations like GDPR, HIPAA, and PCI-DSS. Ensuring buckets are not publicly readable mitigates these risks and protects organizational data integrity.
  description: This rule checks if any Google Cloud Storage buckets are configured to allow public read access. Public access can be verified through the bucket's permissions settings, specifically by examining the IAM policy bindings and ensuring that 'allUsers' or 'allAuthenticatedUsers' are not granted 'roles/storage.objectViewer' or higher. To remediate, remove any public access permissions and implement granular access controls based on the principle of least privilege.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.storage.bucket.data_protection_object_lock_or_immutability_enable_supported
  service: storage
  resource: bucket
  requirement: Data Protection Object Lock Or Immutability Enable Supported
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Object Lock or Immutability for Storage Buckets
  rationale: Enabling Object Lock or immutability on storage buckets prevents accidental or malicious data deletion or alteration, ensuring data integrity and compliance with regulations such as SEC Rule 17a-4(f) and financial data retention policies. This not only mitigates risks of data breaches and loss but also aids in meeting legal and compliance requirements for data preservation.
  description: This rule checks whether Object Lock or immutability is enabled on GCP storage buckets. Object Lock allows you to configure a bucket's data retention settings to prevent any changes to objects during a specified retention period. To verify, inspect bucket settings in the GCP Console or through gcloud CLI for Object Lock configuration. Remediation involves enabling Object Lock via the Console under bucket settings or using appropriate gcloud commands to set retention policies.
  references:
  - https://cloud.google.com/storage/docs/object-lock
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/system/files/documents/2017/06/05/nist-sp800-88-rev1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.data_protection_policy_denies_public_and_insecure
  service: storage
  resource: bucket
  requirement: Data Protection Policy Denies Public And Insecure
  scope: storage.bucket.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce Private and Secure GCP Storage Bucket Access
  rationale: Exposing storage buckets to the public or allowing weak security configurations can lead to unauthorized access, data breaches, and violation of compliance standards such as PCI-DSS and HIPAA. Protecting sensitive data is critical to maintaining trust and avoiding costly data leakage incidents.
  description: This rule ensures that GCP storage buckets are not publicly accessible and do not use insecure configurations. It checks for bucket policies that allow public access and weak authentication settings. To comply, ensure bucket policies explicitly deny public access, and use IAM roles to restrict access to authenticated and authorized users only. Review and update bucket configurations via the GCP Console or CLI to enforce these constraints.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://cloud.google.com/storage/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.storage.bucket.data_protection_require_tls_in_transit
  service: storage
  resource: bucket
  requirement: Data Protection Require TLS In Transit
  scope: storage.bucket.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Buckets Require TLS for Data in Transit
  rationale: Requiring TLS for data in transit protects sensitive information from interception and tampering. This is crucial to maintain data confidentiality and integrity, especially for enterprises handling sensitive or personal data. Compliance with frameworks such as PCI-DSS, HIPAA, and ISO 27001 often mandates encrypted data transmission, reducing legal and financial liabilities.
  description: This rule checks if Google Cloud Storage buckets enforce TLS for data in transit. Buckets should be configured to only permit encrypted connections, ensuring data is protected from unauthorized access during transmission. To verify, ensure that the bucket's settings enforce secure connections using TLS. Remediation involves updating the bucket's policy to require TLS, which can be done via the GCP Console or by using the 'gsutil' command-line tool.
  references:
  - https://cloud.google.com/storage/docs/security#encryption
  - https://cloud.google.com/storage/docs/best-practices#encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/storage/docs/configuring-default-ssl
- rule_id: gcp.storage.bucket.data_protection_versioning_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Versioning Enabled
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Versioning for GCP Storage Buckets
  rationale: Enabling versioning on storage buckets is crucial for data integrity and recovery, as it allows the restoration of previous versions of objects in case of accidental deletion or overwriting. This capability mitigates risks of data loss due to user errors or malicious activity, and supports compliance with data retention policies and regulatory standards that mandate data recovery options.
  description: 'This rule checks if versioning is enabled on Google Cloud Storage buckets. Versioning maintains a history of all changes made to objects within a bucket, facilitating rollback in case of unintended modifications. To verify, check the bucket''s configuration settings in the GCP Console or via the gcloud CLI. Remediation involves enabling versioning by setting the ''versioning'' configuration to ''enabled'' using the GCP Console or through the gcloud CLI command: ''gcloud storage buckets update BUCKET_NAME --versioning''.'
  references:
  - https://cloud.google.com/storage/docs/object-versioning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - 'NIST SP 800-53 Rev. 5: SI-12'
  - 'PCI-DSS v3.2.1: Requirement 10.5.4'
  - ISO/IEC 27001:2013 A.12.3.1
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.storage.bucket.default_encryption
  service: storage
  resource: bucket
  requirement: Default Encryption
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Default Encryption for GCP Storage Buckets
  rationale: Default encryption for Google Cloud Storage buckets is crucial for protecting sensitive data against unauthorized access and potential data breaches. Without encryption, data stored in buckets is vulnerable to interception or exposure, posing significant risks to data privacy and compliance with standards like GDPR and HIPAA. Implementing default encryption helps in safeguarding data integrity and confidentiality, ensuring compliance with legal and regulatory requirements.
  description: This rule checks that all Google Cloud Storage buckets have default encryption configured. To verify, ensure that a Customer-Managed Encryption Key (CMEK) or a Customer-Supplied Encryption Key (CSEK) is set for the bucket. Remediation involves configuring the bucket to use default encryption by updating the bucket's settings via the GCP Console or the gcloud command-line tool. It is recommended to review and rotate encryption keys regularly to maintain security hygiene.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/customer-managed-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 5.3
  - 'NIST SP 800-57 Part 1: Recommendation for Key Management'
  - 'PCI-DSS Requirement 3: Protect Stored Cardholder Data'
  - ISO/IEC 27001:2013 - Information Security Management
- rule_id: gcp.storage.bucket.dr_documentation_access_rbac_least_privilege
  service: storage
  resource: bucket
  requirement: DR Documentation Access RBAC Least Privilege
  scope: storage.bucket.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict DR Documentation Bucket Access to Least Privilege
  rationale: Ensuring least privilege access to DR documentation in GCP Storage reduces the risk of unauthorized data exposure, which can lead to data breaches and non-compliance with regulations like GDPR and HIPAA. Limiting access to essential personnel minimizes the attack surface and protects sensitive recovery documentation critical for business continuity.
  description: This rule checks that access to GCP Storage buckets containing DR documentation is restricted to the minimum necessary permissions. Verify IAM policies to ensure that only users with a legitimate need have read or write access to these buckets. Remediation involves reviewing and modifying IAM policies to remove excessive permissions, utilizing Google Cloud IAM roles effectively to enforce least privilege.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/storage/docs/access-control/iam
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 7.4
  - NIST SP 800-53 Rev. 5 AC-6 Least Privilege
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.dr_documentation_encrypted_and_private
  service: storage
  resource: bucket
  requirement: DR Documentation Encrypted And Private
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DR Documentation Buckets are Encrypted and Private
  rationale: Unencrypted or publicly accessible disaster recovery (DR) documentation can expose sensitive data, leading to unauthorized access and potential data breaches. Ensuring encryption at rest and restricting access to these buckets is critical for compliance with regulations such as GDPR and HIPAA, and helps mitigate risks associated with data leakage in case of a security incident.
  description: This rule checks if DR documentation stored in GCP Storage Buckets is encrypted using Google-managed encryption keys and is not publicly accessible. Verify bucket settings in the Google Cloud Console under 'Cloud Storage' by checking 'Permissions' to ensure only authorized users have access, and under 'Configuration' to confirm encryption settings are enabled. To remediate, configure the bucket to use Google-managed keys for encryption and update IAM policies to restrict access.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53
  - ISO/IEC 27001:2013
- rule_id: gcp.storage.bucket.dr_runbook_access_rbac_least_privilege
  service: storage
  resource: bucket
  requirement: DR Runbook Access RBAC Least Privilege
  scope: storage.bucket.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for DR Runbook Access in GCS Buckets
  rationale: Implementing least privilege for Disaster Recovery (DR) runbook access minimizes the risk of unauthorized data exposure or modification. This is critical in maintaining data integrity and availability, especially during DR scenarios. Failing to enforce this can lead to compliance violations and potential data breaches, impacting business continuity and trust.
  description: This rule checks that only essential personnel have access to Google Cloud Storage (GCS) buckets containing DR runbooks, ensuring roles are assigned based on least privilege principles. Verify that IAM policies limit permissions to necessary actions only, such as 'storage.objects.get'. Remediation involves auditing current permissions and adjusting IAM policies to restrict access appropriately, using predefined roles or custom roles tailored to specific operational needs.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam-roles
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.dr_runbook_change_audit_logging_enabled
  service: storage
  resource: bucket
  requirement: DR Runbook Change Audit Logging Enabled
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Audit Logging for DR Runbook Changes in Storage Buckets
  rationale: Enabling audit logging for Disaster Recovery (DR) runbook changes in Google Cloud Storage buckets is critical to identify unauthorized access and modifications. This practice helps mitigate risks of data breaches and supports compliance with regulations requiring detailed audit trails, thereby protecting sensitive information and maintaining organizational integrity.
  description: This rule verifies that audit logging is enabled for changes to DR runbooks stored in GCP Storage buckets. Ensure that the bucket's logging configuration is set to capture all read and write access, as well as configuration changes. To verify, check the bucket's logging settings in the Google Cloud Console or use the `gsutil logging set` command. To remediate, enable logging by configuring the bucket to write logs to a dedicated logging bucket, ensuring that the logs are retained according to your organization's policy.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/storage/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.storage.bucket.dr_runbook_encrypted_and_private
  service: storage
  resource: bucket
  requirement: DR Runbook Encrypted And Private
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DR Runbooks Are Encrypted and Access-Controlled
  rationale: Encrypting and restricting access to disaster recovery (DR) runbooks is crucial to prevent unauthorized access to sensitive operational procedures. A breach could lead to exposure of critical recovery steps, potentially aiding attackers in disrupting recovery efforts. Compliance with data protection standards like PCI-DSS and ISO 27001 mandates the protection of such sensitive information.
  description: This rule checks that DR runbooks stored in GCP buckets are encrypted with customer-managed encryption keys (CMEK) and have restricted access controls. Verify that buckets containing DR runbooks have CMEK enabled and IAM policies configured to limit access to authorized personnel only. Remediate by updating bucket settings to use CMEK and reviewing IAM policies regularly to ensure compliance with the principle of least privilege.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/access-control/iam-roles
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.storage.bucket.encryption_enabled
  service: storage
  resource: bucket
  requirement: Encryption Enabled
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption at Rest for Cloud Storage Buckets
  rationale: Enabling encryption at rest for Cloud Storage buckets is crucial to protect sensitive data from unauthorized access and potential data breaches. Unencrypted data can be exposed if compromised, leading to severe financial and reputational damage. Compliance with regulations such as GDPR, HIPAA, and PCI-DSS mandates encryption to safeguard personal and financial information.
  description: This rule checks if server-side encryption is enabled for Google Cloud Storage buckets. Cloud Storage supports encryption by default, but verifying that customer-managed encryption keys (CMEK) or Google-managed keys are active ensures enhanced data security. To verify, navigate to the Cloud Console, go to Storage, select the bucket, and check the encryption settings. Remediate by enabling encryption through the 'Edit' option in the bucket settings, choosing the appropriate key management option.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.bucket.iam_no_public_access
  service: storage
  resource: bucket
  requirement: IAM No Public Access
  scope: storage.bucket.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure GCS Buckets Do Not Allow Public Access
  rationale: Publicly accessible storage buckets can expose sensitive data to unauthorized users, leading to data breaches and compliance violations. Ensuring buckets are not publicly accessible protects against unintended data exposure and supports adherence to compliance frameworks like PCI-DSS and HIPAA.
  description: This rule checks for Google Cloud Storage buckets that are publicly accessible due to 'allUsers' or 'allAuthenticatedUsers' having access in the IAM policy. To verify, review the bucket's IAM policy and remove any public roles. Remediation involves setting the IAM policy to restrict access only to specific users or service accounts with necessity. Implement a policy to regularly audit bucket permissions to prevent future exposures.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/storage/docs/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.storage.bucket.level_public_access_blocks
  service: storage
  resource: bucket
  requirement: Level Public Access Blocks
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Enforce Public Access Blocking on GCP Storage Buckets
  rationale: Blocking public access to storage buckets is crucial for preventing unauthorized data exposure, which can lead to data breaches and compliance violations. Publicly accessible storage buckets can be exploited by attackers to access sensitive information, resulting in significant financial and reputational damage. Ensuring buckets are not publicly accessible aligns with regulatory requirements and best practices for data security.
  description: This rule checks if Google Cloud Storage buckets have public access blocks enabled, which prevents all unauthorized users from accessing bucket data. Verify that public access is disabled by using the GCP Console or gcloud CLI to inspect bucket IAM policies and access controls. To remediate, configure the bucket's permissions to restrict public access through the GCP Console or by executing gcloud storage commands to set the correct IAM policies.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://cloud.google.com/storage/docs/public-access-prevention
  - CIS Google Cloud Computing Foundation Benchmark v1.2.0 - Section 4.2
  - NIST SP 800-53 Rev. 5 - Access Control (AC-3)
  - PCI DSS v3.2.1 - Requirement 1.3.7
  - ISO/IEC 27001:2013 - A.8.2 Information classification
- rule_id: gcp.storage.bucket.lifecycle_enabled
  service: storage
  resource: bucket
  requirement: Lifecycle Enabled
  scope: storage.bucket.lifecycle_management
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Bucket Lifecycle Management is Enabled
  rationale: Enabling lifecycle management for GCP Storage buckets helps automate data retention policies, reducing storage costs by transitioning objects to appropriate storage classes or deleting them when they are no longer needed. This prevents accumulation of unnecessary data and mitigates potential data breaches by ensuring that sensitive data is not retained longer than required.
  description: This rule checks if lifecycle management policies are configured for GCP Storage buckets. These policies define actions such as transitioning objects to cheaper storage classes or deleting them after a certain period. To verify, review the bucket configuration in the GCP Console or use the 'gsutil lifecycle get' command. Remediate by setting up lifecycle policies either via the console or by using JSON configuration files with 'gsutil lifecycle set'.
  references:
  - https://cloud.google.com/storage/docs/lifecycle
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.storage.bucket.lifecycle_enabled_gcp_compute_disk_protection_replication
  service: storage
  resource: bucket
  requirement: Lifecycle Enabled Gcp Compute Disk Protection Replication
  scope: storage.bucket.lifecycle_management
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Bucket Lifecycle Management for Compute Disk Replication
  rationale: Enabling lifecycle management for GCP Storage Buckets ensures that data is automatically managed and retained according to policy, thereby optimizing cost and compliance. Without proper lifecycle management, organizations risk accumulating outdated or unnecessary data, which can lead to increased storage costs and potential non-compliance with data retention policies. Additionally, replication of compute disk snapshots without lifecycle policies can result in excessive and unmanaged data growth.
  description: This rule checks if lifecycle management is enabled on GCP Storage Buckets used for replicating GCP Compute Disk snapshots. Lifecycle policies should be configured to automatically delete or transition data to cheaper storage classes after a specified period. To verify, review the bucket's lifecycle settings in the GCP Console or use the gcloud command-line tool. Remediation involves setting up lifecycle rules to manage the retention of disk snapshot data appropriately, ensuring compliance with organizational data management policies.
  references:
  - https://cloud.google.com/storage/docs/lifecycle
  - https://cloud.google.com/compute/docs/disks/snapshots
  - CIS Google Cloud Computing Foundation Benchmark v1.3.0 - Section 5.8
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.storage.bucket.log_retention_policy_lock
  service: storage
  resource: bucket
  requirement: Log Retention Policy Lock
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Storage Bucket Log Retention Policy is Locked
  rationale: Locking the log retention policy on GCP storage buckets prevents accidental or malicious alterations, ensuring logs are retained for appropriate audit and compliance purposes. This is crucial for incident response, forensic investigations, and meeting regulatory requirements such as GDPR or CCPA, which mandate data retention and auditability.
  description: This rule checks if the log retention policy of a GCP storage bucket is locked, preventing changes to the duration logs are stored. A locked policy aids in maintaining data integrity and compliance with organizational and regulatory log retention requirements. To verify, access the Cloud Console, navigate to the storage bucket, and check the log retention settings. To lock the policy, use the Cloud Console or gsutil to set and enforce a policy with a lock, ensuring it cannot be modified without specific permissions.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/audit-logs
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.storage.bucket.logging_enabled
  service: storage
  resource: bucket
  requirement: Logging Enabled
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Storage Bucket Logging is Enabled
  rationale: Enabling logging on storage buckets provides visibility into access patterns and operations, which is crucial for detecting unauthorized access, anomalous behavior, and potential data breaches. It supports compliance with regulatory requirements by maintaining a record of data access and changes, thereby helping in audits and incident response processes.
  description: This rule checks that logging is enabled for Google Cloud Storage buckets. To verify, ensure that the bucket's logging configuration is set to deliver log records to a specified logging bucket. Remediation involves using the Google Cloud Console or gcloud CLI to enable logging by specifying a target bucket for log storage. This helps in maintaining an audit trail of all activities performed on the bucket.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/storage/docs/audit-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.storage.bucket.multi_region_configured
  service: storage
  resource: bucket
  requirement: Multi Region Configured
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure GCP Buckets are Configured with Multi-Region Locations
  rationale: Configuring buckets with multi-region locations enhances data availability and resilience against regional outages. This is crucial for business continuity and can mitigate risks associated with data loss or access issues in a single region. Compliance with industry standards like ISO 27001 and SOC2 often requires robust data availability measures.
  description: This rule checks if GCP storage buckets are configured with multi-region locations, ensuring data is replicated across multiple geographic areas. Verify the bucket's location settings in the GCP Console under 'Storage' or via GCP CLI. To remediate, update bucket configurations to a multi-region setting, such as 'us', 'eu', or 'asia', to ensure data durability and access across different regions.
  references:
  - https://cloud.google.com/storage/docs/locations#location-mr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001/
  - https://cloud.google.com/security/compliance/soc-2/
  - https://cloud.google.com/storage/docs/replication
- rule_id: gcp.storage.bucket.object_retention_configured
  service: storage
  resource: bucket
  requirement: Object Retention Configured
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Object Retention Policies Are Configured for Buckets
  rationale: Configuring object retention policies in GCP Storage buckets is critical to prevent accidental or malicious data deletion, ensuring data integrity and availability. This is particularly important for compliance with data retention regulations and standards, such as GDPR or HIPAA, which mandate specific data retention periods. Without these policies, organizations risk data breaches, legal penalties, and loss of data critical for business operations.
  description: This check verifies that each GCP Storage bucket has an object retention policy configured, ensuring data cannot be deleted or overwritten until a specified retention period has elapsed. To verify, review the bucket's settings in the GCP Console under the 'Retention' section of the bucket's 'Lifecycle' settings. Remediation involves setting a retention policy by specifying the desired period and enabling the policy, which can be done through the GCP Console or by using the 'gsutil retention set' command.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/gsutil/commands/retention
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.storage.bucket.object_versioning
  service: storage
  resource: bucket
  requirement: Object Versioning
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Object Versioning for GCP Storage Buckets
  rationale: Enabling object versioning in GCP Storage Buckets protects against accidental data loss or overwrites by keeping previous versions of objects. This is crucial for business continuity and data integrity, and can help meet compliance requirements for data retention and audit controls.
  description: This rule checks if object versioning is enabled on GCP Storage Buckets. Object versioning allows you to retrieve older versions of objects, providing a safeguard against accidental deletions or changes. To verify, use the `gsutil versioning get gs://[BUCKET_NAME]` command; 'Enabled' should be returned. Remediate by executing `gsutil versioning set on gs://[BUCKET_NAME]` to enable versioning.
  references:
  - https://cloud.google.com/storage/docs/using-object-versioning
  - CIS GCP Foundations Benchmark v1.3.0 - 5.1.1
  - 'NIST SP 800-53: SI-12'
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.policy_public_write_access
  service: storage
  resource: bucket
  requirement: Policy Public Write Access
  scope: storage.bucket.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Write Access to Storage Buckets
  rationale: Allowing public write access to storage buckets poses significant security risks, including unauthorized data insertion or overwriting, which can lead to data corruption or leakage. This misconfiguration can also lead to compliance violations with regulations like GDPR or HIPAA, which mandate the protection of sensitive data from unauthorized access and modification.
  description: This rule checks for storage buckets in Google Cloud Platform that are configured to allow public write access. Specifically, it identifies buckets with IAM policies granting the 'roles/storage.objectCreator' or 'roles/storage.objectAdmin' permissions to 'allUsers' or 'allAuthenticatedUsers'. To remediate, review and modify the bucket's IAM policy to restrict write permissions to trusted and authorized users only. Use the GCP console or CLI to audit and update bucket policies.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.public_access
  service: storage
  resource: bucket
  requirement: Public Access
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Access to Storage Buckets
  rationale: Publicly accessible storage buckets can expose sensitive data to unauthorized individuals, leading to data breaches. This can result in financial losses, reputational damage, and non-compliance with regulations such as GDPR, PCI-DSS, and HIPAA, which mandate data protection and privacy.
  description: This rule checks for any Google Cloud Storage buckets that are configured to allow public access. Buckets should be configured to restrict access to authenticated users only. To verify, inspect the bucket's ACL and IAM policies to ensure no public permissions are granted. To remediate, remove 'allUsers' or 'allAuthenticatedUsers' from the bucket's permissions and apply the principle of least privilege.
  references:
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://www.pcisecuritystandards.org/document_library
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/storage/docs/best-practices-security
- rule_id: gcp.storage.bucket.public_read_prohibited
  service: storage
  resource: bucket
  requirement: Public Read Prohibited
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure Google Cloud Storage Buckets Are Not Publicly Readable
  rationale: Allowing public read access to storage buckets can lead to unauthorized data exposure, resulting in potential data breaches and reputational damage. It is critical to prevent public access to sensitive information to comply with privacy regulations such as GDPR and CCPA, and to mitigate risks associated with insider threats and external attackers.
  description: This rule checks for Google Cloud Storage buckets with configurations that allow public read access. To ensure compliance, verify that the bucket's IAM policies do not include roles like 'roles/storage.objectViewer' assigned to 'allUsers' or 'allAuthenticatedUsers'. Remediate by removing these roles or setting bucket policies that restrict access to authorized users only using IAM conditions or bucket policies.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://cloud.google.com/security/compliance/cis#cis-google-cloud-computing-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.storage.bucket.public_write_prohibited
  service: storage
  resource: bucket
  requirement: Public Write Prohibited
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Write Access to Cloud Storage Buckets
  rationale: Allowing public write access to storage buckets can lead to unauthorized data modifications, data exfiltration, and potential service abuse, which may result in significant data loss, service disruption, and reputational damage. Ensuring buckets are not publicly writable helps comply with regulatory standards such as PCI-DSS and SOC2, which mandate strict access controls to sensitive data.
  description: This rule checks if any Google Cloud Storage buckets have public write access enabled. Public write access allows any internet user to upload or modify data, which poses a severe security risk. To verify, inspect the bucket's IAM policy and ensure 'allUsers' or 'allAuthenticatedUsers' do not have roles/storage.objectCreator or roles/storage.objectAdmin. Remediation involves modifying the bucket's IAM policy to restrict unauthorized write access.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.nist.gov/publications/nist-special-publication-800-53-revision-4-security-and-privacy-controls-federal
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.storage.bucket.replication_enabled
  service: storage
  resource: bucket
  requirement: Replication Enabled
  scope: storage.bucket.replication
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Cross-Region Replication for GCP Storage Buckets
  rationale: Enabling replication for storage buckets enhances data durability and availability by automatically syncing data across regions, mitigating risks from regional outages, and ensuring compliance with data residency requirements. This is critical for business continuity, protecting against data loss, and adhering to regulations that mandate data redundancy and availability.
  description: This rule checks if cross-region replication is enabled for Google Cloud Storage buckets. To verify, ensure that bucket configurations include replication policies specifying the destination location and data classes. Remediation involves configuring the bucket with a replication policy using the Google Cloud Console or CLI, specifying source and destination buckets and regions appropriately.
  references:
  - https://cloud.google.com/storage/docs/using-replication
  - https://cloud.google.com/storage/docs/bucket-locations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.storage.bucket.secure_transport_policy
  service: storage
  resource: bucket
  requirement: Secure Transport Policy
  scope: storage.bucket.policy_management
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Secure Transport for GCS Buckets
  rationale: Securing data in transit by enforcing secure transport policies on Google Cloud Storage (GCS) buckets is crucial to prevent unauthorized interception and access. This reduces the risk of data breaches and ensures compliance with regulations like PCI-DSS and ISO 27001, which mandate secure data transmission. Business operations can be severely impacted by data leaks, leading to financial loss and reputational damage.
  description: This rule checks that all GCS buckets enforce a policy requiring secure transport (HTTPS) for data transmission. To verify, review the bucket policies to ensure 'secureTransport' is set. If not configured, update the bucket's IAM policies to include a condition that denies requests without secure transport. This ensures data is encrypted in transit, aligning with security and compliance standards.
  references:
  - https://cloud.google.com/storage/docs/using-iam-policies
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/storage/docs/encryption
- rule_id: gcp.storage.bucket.server_access_logging_enabled
  service: storage
  resource: bucket
  requirement: Server Access Logging Enabled
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Server Access Logging for GCP Storage Buckets
  rationale: Enabling server access logging on GCP storage buckets is crucial for monitoring and auditing purposes. It helps organizations track access to their data, identify any unauthorized access attempts, and meet compliance requirements for data access logging. This is essential for detecting potential security breaches and ensuring accountability in data handling.
  description: This rule checks if server access logging is enabled for GCP storage buckets. To verify, ensure that the bucket's logging configuration targets a destination bucket for log delivery. Remediation involves configuring the bucket to send access logs to a designated bucket, ensuring the destination bucket has the appropriate permissions to receive logs. Regular review of these logs can help in identifying suspicious activities.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/index.html
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.storage.bucket.server_access_logging_enabled_gcp_logging_dataevent_logging
  service: storage
  resource: bucket
  requirement: Server Access Logging Enabled Gcp Logging Dataevent Logging
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Server Access Logging for GCP Storage Buckets
  rationale: Enabling server access logging for GCP storage buckets provides visibility into access patterns and operations performed on the bucket. This is crucial for detecting unauthorized access, ensuring data integrity, and maintaining compliance with regulatory requirements such as GDPR, PCI-DSS, and SOC2. Without logging, it is challenging to investigate security incidents and implement effective access controls.
  description: This rule checks whether server access logging is enabled for GCP storage buckets. To verify, ensure that the bucket has a log configuration directing logs to a designated logging bucket with proper permissions. Logs should be reviewed regularly for suspicious activities. Remediation involves configuring the logging settings through the GCP Console or using the `gsutil logging set` command to specify a logging bucket.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/gsutil/commands/logging
  - https://cloud.google.com/storage/docs/audit-logging
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.storage.bucket.server_access_logging_enabled_gcp_logging_gcs_replication
  service: storage
  resource: bucket
  requirement: Server Access Logging Enabled Gcp Logging Gcs Replication
  scope: storage.bucket.logging
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: medium
  title: Enable Server Access Logging for GCP Storage Buckets
  rationale: Enabling server access logging for storage buckets is essential for monitoring and auditing access patterns. This helps in identifying unauthorized access attempts and understanding usage trends, thus mitigating security risks. It also supports compliance with regulations such as GDPR and HIPAA, which require detailed access logs for sensitive data.
  description: This rule checks whether server access logging is enabled for GCP Storage buckets. To verify, ensure that the 'logging' configuration of each bucket includes a destination bucket for log delivery. To remediate, configure the source bucket to log to a designated logging bucket by specifying the logging target in the bucket's settings. This setup captures all access logs, aiding in security audits and incident response.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/gsutil/commands/logging
- rule_id: gcp.storage.bucket.versioning_enabled
  service: storage
  resource: bucket
  requirement: Versioning Enabled
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure GCP Storage Bucket Versioning is Enabled
  rationale: Enabling versioning on GCP storage buckets provides a critical layer of protection against accidental deletions or overwrites, ensuring that previous versions of objects can be restored if needed. This is important for maintaining data integrity and availability, especially in scenarios involving human error or malicious activities. Additionally, versioning can aid in compliance with regulations that require data retention and recoverability.
  description: This rule checks whether versioning is enabled on GCP storage buckets. To verify, navigate to the Google Cloud Console, select 'Cloud Storage', and check the 'Versioning' configuration for each bucket. If versioning is not enabled, it can be activated by editing the bucket's settings and turning on 'Object Versioning'. This ensures that older versions of objects are preserved, allowing recovery if current data is lost or corrupted.
  references:
  - https://cloud.google.com/storage/docs/using-object-versioning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.website_https_only_configured
  service: storage
  resource: bucket
  requirement: Website HTTPS Only Configured
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enforce HTTPS for GCP Storage Bucket Website Config
  rationale: Configuring your GCP Storage Bucket to serve content only via HTTPS ensures data in transit is encrypted, protecting against man-in-the-middle attacks and maintaining data integrity. This is crucial for compliance with standards such as PCI-DSS and HIPAA, which mandate secure transmission of sensitive information. Failing to enforce HTTPS could expose sensitive data to attackers, leading to data breaches and reputational damage.
  description: This rule checks if GCP Storage Buckets configured as static websites enforce HTTPS connections. To verify, ensure that the bucket's website configuration has the 'https://www.example.com' URL format and does not allow 'http://'. Remediation involves updating the bucket's configuration to redirect HTTP requests to HTTPS, ensuring all data transfers are encrypted. This can be configured via the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/storage/docs/hosting-static-website
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.storage.notification.data_protection_bucket_cross_account_destinations_restricted
  service: storage
  resource: notification
  requirement: Data Protection Bucket Cross Account Destinations Restricted
  scope: storage.notification.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Restrict Cross-Account Storage Notification Destinations
  rationale: Allowing storage bucket notifications to be sent to destinations in other accounts can expose sensitive data to unauthorized entities, increasing the risk of data breaches. This risk is particularly relevant in scenarios involving misconfigured access controls or compromised credentials. Limiting destinations to within the same account helps maintain control over data flows and supports compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks for storage bucket notifications configured to send data to destinations outside the originating GCP account. It is crucial to ensure that notifications are restricted to trusted endpoints within the same account to prevent unauthorized data exposure. To verify, review the notification configurations in each bucket and ensure they are limited to internal destinations. To remediate, modify the notification settings to remove any external endpoints and configure appropriate IAM policies to enforce this restriction.
  references:
  - https://cloud.google.com/storage/docs/gsutil/commands/notification
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.storage.notification.data_protection_bucket_destination_encrypted
  service: storage
  resource: notification
  requirement: Data Protection Bucket Destination Encrypted
  scope: storage.notification.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Storage Notification Buckets Are Encrypted at Rest
  rationale: Encrypting bucket destinations for storage notifications is critical to protect sensitive data from unauthorized access and breaches. Without encryption, data stored in these buckets is vulnerable to exposure, violating compliance with regulations such as GDPR and ISO 27001, and could lead to significant financial and reputational damage.
  description: This rule checks that all bucket destinations for storage notifications have encryption enabled at rest. It verifies the use of Google-managed encryption keys or customer-supplied encryption keys to ensure data security. To remediate, configure the bucket to use encryption by setting the appropriate key management settings in the Google Cloud Console or via the command line interface (CLI).
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.storage.notification.data_protection_bucket_destination_least_privilege
  service: storage
  resource: notification
  requirement: Data Protection Bucket Destination Least Privilege
  scope: storage.notification.least_privilege
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: high
  title: Enforce Least Privilege on Storage Notification Destinations
  rationale: Implementing least privilege for storage notification destinations minimizes the risk of unauthorized data exposure and potential data breaches. This is crucial for maintaining data integrity and supporting compliance with regulations such as GDPR and HIPAA, which mandate strict access controls to sensitive data. By limiting access, organizations reduce their attack surface and protect against insider threats.
  description: This rule checks that IAM roles assigned to GCS bucket notification destinations do not exceed the permissions necessary for their function. Ensure that roles are tailored to specific actions required, such as publishing messages to a Pub/Sub topic, without granting additional permissions. Verify current permissions through the Google Cloud Console or via the `gcloud` command-line tool. Remediate by reviewing and updating IAM policies to remove superfluous permissions, adhering to the principle of least privilege.
  references:
  - https://cloud.google.com/storage/docs/pubsub-notifications
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.4
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.trace.trace.monitoring_access_rbac_least_privilege
  service: trace
  resource: trace
  requirement: Monitoring Access RBAC Least Privilege
  scope: trace.trace.monitoring
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Trace Monitoring Access
  rationale: Implementing least privilege for Trace monitoring access reduces the risk of unauthorized data exposure and insider threats, ensuring that only necessary personnel can view or manipulate trace data. This aligns with compliance requirements such as PCI-DSS and ISO 27001, which mandate access controls to safeguard sensitive information.
  description: This rule checks whether Trace monitoring access follows the principle of least privilege by reviewing IAM roles assigned to users. It verifies that only essential personnel have roles like 'roles/cloudtrace.agent'. To remediate, audit your IAM policies, remove excessive permissions, and restrict access to only those roles necessary for performing specific tasks. Regularly review and update permissions to adapt to evolving organizational needs.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/trace/docs/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.trace.trace.monitoring_retention_days_minimum
  service: trace
  resource: trace
  requirement: Monitoring Retention Days Minimum
  scope: trace.trace.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Trace Monitoring Retention Meets Minimum Days
  rationale: Retaining trace data for a sufficient period is critical for auditing application behavior, detecting anomalies, and meeting compliance requirements. Insufficient retention can hinder incident investigation and compromise the ability to identify long-term trends or threats, potentially leading to non-compliance with regulatory frameworks like PCI-DSS or SOC 2.
  description: This rule checks that Google Cloud Trace data retention is configured to meet a minimum threshold of days, ensuring adequate historical data is available for analysis. Verify the retention period by reviewing the Stackdriver Trace settings in the GCP Console under 'Monitoring' and 'Trace'. To remediate, adjust the retention settings to meet the minimum required days through the console or via the CLI using `gcloud beta trace` commands.
  references:
  - https://cloud.google.com/trace/docs/setup
  - https://cloud.google.com/trace/docs/overview
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.trace.trace.monitoring_store_encrypted
  service: trace
  resource: trace
  requirement: Monitoring Store Encrypted
  scope: trace.trace.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud Trace Monitoring Data is Encrypted at Rest
  rationale: Encrypting monitoring data at rest in Cloud Trace protects sensitive information from unauthorized access. This is vital for maintaining the confidentiality and integrity of trace data, which may contain insights into application performance and potential vulnerabilities. Furthermore, data encryption helps meet compliance requirements such as GDPR and HIPAA, reducing the risk of data breaches and associated legal penalties.
  description: This rule checks that all monitoring data stored by Google Cloud Trace is encrypted at rest using Google-managed keys. To verify, ensure that the default encryption settings are enabled for Cloud Trace in the Google Cloud Console. Remediation involves enabling encryption settings in the Cloud Trace configuration if they are not already active. Use Google Cloud's built-in encryption capabilities to secure your trace data effectively.
  references:
  - https://cloud.google.com/trace/docs/security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Computing Foundations Benchmark v1.1.0 - 3.2
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r4.pdf
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.workflows.workflow.incident_approval_steps_required_for_destructive_actions
  service: workflows
  resource: workflow
  requirement: Incident Approval Steps Required For Destructive Actions
  scope: workflows.workflow.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Require Approval Steps for Destructive Workflow Actions
  rationale: Implementing incident approval steps for destructive actions in workflows reduces the risk of unauthorized or accidental resource modifications that can lead to data loss or service disruption. This control helps to ensure that critical operations are vetted by multiple stakeholders, enhancing accountability and compliance with governance policies.
  description: This rule checks if workflows involving destructive actions, such as data deletion or modification, include mandatory incident approval steps. Organizations should configure workflows to incorporate human intervention for approval before execution of potentially harmful operations. Remediation involves modifying the workflow to include conditional approval steps or using Google Cloud IAM to enforce stringent access controls.
  references:
  - https://cloud.google.com/workflows/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.workflows.workflow.incident_integrations_least_privilege
  service: workflows
  resource: workflow
  requirement: Incident Integrations Least Privilege
  scope: workflows.workflow.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for GCP Workflow Incident Integrations
  rationale: Implementing least privilege for workflows in GCP ensures that only necessary permissions are granted, reducing the risk of unauthorized access which can lead to data breaches, service disruptions, and non-compliance with regulatory standards such as PCI-DSS and SOC2. By controlling access, organizations can protect sensitive data and maintain trust with customers and partners.
  description: This rule checks that GCP workflows interacting with incident management systems have the minimal set of permissions required. Verify that IAM roles are assigned based on the 'principle of least privilege', ensuring roles are specific to job functions. Remediation involves auditing permissions of workflows, removing excess privileges, and assigning roles that align with the exact tasks the workflow needs to perform.
  references:
  - https://cloud.google.com/workflows/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/best-practices-for-roles-and-permissions
- rule_id: gcp.workflows.workflow.incident_kms_encryption_enabled
  service: workflows
  resource: workflow
  requirement: Incident KMS Encryption Enabled
  scope: workflows.workflow.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption is Enabled for GCP Workflows
  rationale: Enabling KMS encryption for GCP workflows is critical to protecting sensitive information processed by workflows. Without encryption, data is vulnerable to unauthorized access, potentially leading to data breaches and non-compliance with regulations such as GDPR and HIPAA. KMS encryption provides an additional layer of security by ensuring that data is encrypted at rest using Google-managed or customer-managed keys, thereby reducing the risk of data exposure.
  description: This rule checks whether workflows in GCP have Key Management Service (KMS) encryption enabled. Specifically, it verifies that the 'kmsKeyName' attribute is set for each workflow, indicating that data processed by the workflow is encrypted using a specified KMS key. To remediate non-compliant workflows, configure the workflow to use a KMS key by setting the 'kmsKeyName' attribute in the workflow's configuration. This ensures that all data processed by the workflow is encrypted at rest, aligning with best practices for data protection.
  references:
  - https://cloud.google.com/workflows/docs/encryption
  - https://cloud.google.com/kms/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.workspace.user.mfa_status_configured
  service: workspace
  resource: user
  requirement: MFA Status Configured
  scope: workspace.user.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure MFA is Configured for Workspace Users
  rationale: Multi-factor authentication (MFA) significantly reduces the risk of unauthorized access to user accounts by requiring a second form of verification beyond just a password. Without MFA, accounts are vulnerable to phishing attacks and credential theft, potentially leading to data breaches and loss of sensitive information. Compliance frameworks such as PCI-DSS and ISO 27001 often require MFA to secure access to critical systems and data.
  description: This check verifies that all users in Google Workspace have MFA configured. It involves reviewing user account settings to ensure that MFA is enabled and enforced. Administrators can enforce MFA through the Google Admin Console by navigating to 'Security > Authentication > 2-step verification' and setting policies that require all users to configure MFA. Remediation involves guiding users to complete the MFA setup process and ensuring policies are in place to mandate it for all accounts.
  references:
  - https://support.google.com/a/answer/2537800?hl=en
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
  - https://cloud.google.com/blog/products/identity-security/enhancing-security-in-g-suite-with-advanced-protection-program
