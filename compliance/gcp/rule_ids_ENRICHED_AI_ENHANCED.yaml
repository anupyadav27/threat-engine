metadata:
  csp: GCP
  description: Enterprise-grade GCP rules - AI Enhanced
  version: 2.0.0
  enhancement_date: '2025-12-02'
  total_rules: 1576
  quality_grade: A+ (GPT-4o Enhanced)
  ai_engine: gpt-4o
statistics:
  total_rules: 1576
  enhanced: 1568
  failed: 8
  connection_errors: 0
  api_calls: 1576
rules:
- rule_id: gcp.accessapproval.approval_request.enrollment_configured
  service: accessapproval
  resource: approval_request
  requirement: Enrollment Configured
  scope: accessapproval.approval_request.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Access Approval Enrollment is Configured
  rationale: Configuring Access Approval enrollment is critical for maintaining control over sensitive operations and preventing unauthorized access. This rule helps mitigate the risk of unauthorized changes by requiring explicit approval for sensitive actions, aligning with compliance standards such as SOC2 and PCI-DSS. Without proper enrollment, organizations risk non-compliance and potential data breaches.
  description: This rule checks whether Access Approval enrollment is configured for the approval_request resource in GCP. Proper configuration ensures that sensitive actions require explicit approvals before execution, enhancing security oversight. To verify, check that the Access Approval API is enabled and policies are set for relevant projects or folders. Remediation involves enabling the Access Approval API and configuring approval requests for critical operations.
  references:
  - https://cloud.google.com/access-approval/docs
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/access-approval/docs/configure-access-approval
- rule_id: gcp.aiplatform.batch_prediction_job.data_governance_ai_transform_job_input_output_encrypted
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Governance Ai Transform Job Input Output Encrypted
  scope: aiplatform.batch_prediction_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Batch Prediction Job Data Encryption
  rationale: Encrypting input and output data for AI Platform batch prediction jobs is critical to protect sensitive information from unauthorized access and potential data breaches. This is particularly important to mitigate risks associated with data leaks and to comply with data protection regulations such as GDPR, HIPAA, and CCPA. Failure to encrypt data at rest can result in significant legal penalties and damage to an organization's reputation.
  description: This rule checks that all input and output data associated with AI Platform batch prediction jobs are encrypted using Customer-Managed Encryption Keys (CMEK). Verify that the encryption configuration for batch prediction jobs specifies CMEK to ensure data is protected at rest. Remediation involves configuring the batch prediction job to use CMEK by specifying the key in the job's 'encryptionSpec' field. This can be done via the GCP Console, gcloud command-line tool, or the AI Platform client libraries.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/predictions/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.batch_prediction_job.data_governance_ai_transform_job_logs_enabled
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Governance Ai Transform Job Logs Enabled
  scope: aiplatform.batch_prediction_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logs for AI Platform Batch Prediction Job
  rationale: Enabling logs for AI Platform batch prediction jobs is crucial for maintaining data governance and ensuring the traceability of data transformations. This is essential for understanding how data is processed, which aids in identifying potential security incidents and supporting compliance with regulations like GDPR and CCPA. Without these logs, organizations risk undetected data breaches and non-compliance with legal requirements.
  description: This rule checks whether logging is enabled for AI Platform batch prediction jobs. Logging should be configured to capture details about data transformations within AI jobs, aiding in auditing and forensic analysis. To verify, ensure that the logging configuration in AI Platform includes all necessary events. Remediation involves setting the 'logConfig' parameter in the batch prediction job configuration to enable logging.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.batch_prediction_job.data_governance_ai_transform_job_private_networking_enforced
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Governance Ai Transform Job Private Networking Enforced
  scope: aiplatform.batch_prediction_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Platform Batch Prediction Jobs
  rationale: Ensuring that AI Platform batch prediction jobs use private networking is crucial to limit exposure to the public internet, reducing the risk of data breaches and unauthorized access. This measure helps protect sensitive data processed during AI transformations, aligning with compliance mandates such as GDPR and HIPAA that require stringent data protection controls.
  description: This rule checks if AI Platform batch prediction jobs are configured to use private networking, which restricts network access to internal IP addresses only. Verify the configuration by ensuring that 'network' attribute specifies a VPC with private IPs. Remediation involves configuring batch jobs to run within a VPC network, using private IP settings, to ensure secure, internal data flow and connectivity.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/using-private-ip
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.batch_prediction_job.data_governance_ai_transform_job_roles_least_privilege
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Governance Ai Transform Job Roles Least Privilege
  scope: aiplatform.batch_prediction_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Transform Job Roles
  rationale: Implementing least privilege for AI Transform Job roles minimizes the risk of unauthorized access and data leaks, which can lead to financial loss, reputational damage, and regulatory non-compliance. Ensuring roles have only necessary permissions helps prevent malicious or accidental misuse of sensitive data and resources.
  description: This rule checks if AI Platform Batch Prediction Jobs are configured with roles that adhere to the principle of least privilege. Specifically, it verifies that roles attached to these jobs have only the permissions essential for their function, reducing the attack surface. To remediate, review and adjust IAM policies to ensure roles do not exceed required permissions, using predefined roles or custom roles tailored to the job's specific needs.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - NIST SP 800-53 - Access Control
  - ISO 27001 Annex A.9 - Access Control
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.aiplatform.batch_prediction_job.data_privacy_ai_transform_job_input_output_encrypted
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Privacy Ai Transform Job Input Output Encrypted
  scope: aiplatform.batch_prediction_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Transform Job Data is Encrypted at Rest
  rationale: Encrypting AI Transform job input and output data mitigates the risk of unauthorized data access and helps maintain confidentiality and integrity. This is crucial in preventing data breaches and aligns with compliance requirements such as GDPR and HIPAA, which mandate data protection measures. It also safeguards against potential insider threats and reduces the risk of exposure in case of storage system compromise.
  description: This check ensures that batch prediction job input and output data in GCP's AI Platform is encrypted at rest using customer-managed keys (CMKs). Verify that Cloud Storage buckets used for storing job data are configured with a CMK for encryption. To remediate, configure your Cloud Storage bucket to use a CMK by updating the bucket's settings through the GCP Console or by using gcloud CLI commands to set a specific KMS key for the bucket.
  references:
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://cloud.google.com/ai-platform/prediction/docs/custom-service-account
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.aiplatform.batch_prediction_job.data_privacy_ai_transform_job_logs_enabled
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Privacy Ai Transform Job Logs Enabled
  scope: aiplatform.batch_prediction_job.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Logs for AI Transform Jobs to Protect Data Privacy
  rationale: Enabling logs for AI Transform jobs in GCP is crucial to monitor and audit data access and transformations, ensuring data privacy is maintained. Without logging, unauthorized data manipulation might go undetected, potentially leading to data breaches and non-compliance with regulations such as GDPR and HIPAA. Effective logging supports incident response and forensic analysis, reducing the risk of data leaks and enhancing trust in AI operations.
  description: This rule checks if logging is enabled for AI Transform jobs within batch prediction jobs on GCP's AI Platform. To verify, ensure that the 'log_ttl_days' configuration is set for the batch prediction job, indicating that logs are retained for auditing. Remediation involves adjusting the AI Platform job settings to enable and configure logging, ensuring logs are stored in a secure GCP Logging bucket with appropriate access controls.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/logging
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/logging/docs/best-practices
- rule_id: gcp.aiplatform.batch_prediction_job.data_privacy_ai_transform_job_private_networking_enforced
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Privacy Ai Transform Job Private Networking Enforced
  scope: aiplatform.batch_prediction_job.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Private Networking for AI Transform Jobs
  rationale: Enforcing private networking for AI Transform Jobs in GCP minimizes exposure to public networks, reducing the risk of data breaches and unauthorized access. This configuration supports compliance with data protection regulations such as GDPR and HIPAA by ensuring data remains within controlled network boundaries. Without private networking, sensitive data processed in AI jobs could be vulnerable to interception or exfiltration.
  description: This rule checks if AI Transform Jobs in GCP AI Platform are configured to use private networking, which routes job traffic through internal IPs instead of the public internet. To verify, ensure that the 'network' field in the BatchPredictionJob resource is set to a valid VPC network. Remediation involves updating existing jobs to specify a VPC network and configuring new jobs with private networking by default. This can typically be done through the GCP Console, gcloud command-line tool, or API.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/batch-predictions
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/architecture/encryption-at-rest
  - https://cloud.google.com/security/privacy/
- rule_id: gcp.aiplatform.batch_prediction_job.data_privacy_ai_transform_job_roles_least_privilege
  service: aiplatform
  resource: batch_prediction_job
  requirement: Data Privacy Ai Transform Job Roles Least Privilege
  scope: aiplatform.batch_prediction_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Batch Prediction Jobs
  rationale: Ensuring least privilege for AI Platform batch prediction jobs minimizes potential exposure of sensitive data by restricting access to only necessary roles. This reduces the risk of data breaches and unauthorized access, which is crucial for maintaining data privacy and meeting compliance with regulations such as GDPR and HIPAA.
  description: This rule checks that roles assigned to AI Platform batch prediction jobs adhere to the principle of least privilege, ensuring no excessive permissions are granted. Verify IAM policies to ensure only necessary roles are assigned, such as roles/aiplatform.user, while avoiding overly permissive roles like roles/owner. To remediate, review and adjust IAM roles using the Google Cloud Console or gcloud CLI to align with least privilege best practices.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/aiplatform/docs/batch-predictions
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, 1.6 Ensure that IAM users are assigned only the roles they need
- rule_id: gcp.aiplatform.batch_prediction_job.machine_learning_transform_job_input_output_encrypted
  service: aiplatform
  resource: batch_prediction_job
  requirement: Machine Learning Transform Job Input Output Encrypted
  scope: aiplatform.batch_prediction_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt Input and Output for AI Platform Batch Jobs
  rationale: Ensuring that input and output data of AI Platform batch prediction jobs are encrypted is crucial to protect sensitive machine learning data from unauthorized access and breaches. Unencrypted data can lead to exposure of proprietary algorithms or customer information, violating compliance requirements such as GDPR or HIPAA, and resulting in financial and reputational damage.
  description: This rule checks if the input and output files for AI Platform batch prediction jobs are encrypted using Customer-Managed Encryption Keys (CMEK). To verify, ensure that the 'encryptionSpec' field is configured with a valid KMS key in the job settings. Remediation involves modifying the job configuration to specify a CMEK, thereby securing data at rest and aligning with best practices for data protection.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.batch_prediction_job.machine_learning_transform_job_logs_enabled
  service: aiplatform
  resource: batch_prediction_job
  requirement: Machine Learning Transform Job Logs Enabled
  scope: aiplatform.batch_prediction_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for AI Platform Batch Prediction Jobs
  rationale: Enabling logs for AI Platform Batch Prediction Jobs is crucial for monitoring, troubleshooting, and auditing activities within the machine learning environment. Without logging, it becomes challenging to identify and respond to security incidents, compliance breaches, or performance issues, potentially leading to financial losses or regulatory penalties.
  description: This rule checks whether logging is enabled for AI Platform Batch Prediction Jobs. To ensure compliance and security, configure your jobs to send logs to Cloud Logging. Verify the setting by inspecting the job configurations in the Google Cloud Console or via the gcloud CLI. To enable logging, set the 'logging' parameter to 'true' in the job configuration. This allows capturing of job activity, facilitating monitoring and troubleshooting.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/logging
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.aiplatform.batch_prediction_job.machine_learning_transform_job_private_networking_enforced
  service: aiplatform
  resource: batch_prediction_job
  requirement: Machine Learning Transform Job Private Networking Enforced
  scope: aiplatform.batch_prediction_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for ML Transform Jobs
  rationale: Ensuring that machine learning transform jobs utilize private networking significantly reduces the attack surface by preventing exposure to the public internet. This mitigates the risk of data breaches and unauthorized access, supporting compliance with standards like HIPAA and ISO 27001 which require robust network security controls.
  description: This rule checks if batch prediction jobs in the AI Platform are configured to use private networking, ensuring they do not have external IP addresses. Verify that the 'network' field in the job configuration is set to a private network. To enforce this, configure a VPC Network to enable private connectivity, and apply it to your batch prediction jobs. This reduces potential attack vectors by restricting access to internal GCP networks only.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/deploying-models
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.aiplatform.batch_prediction_job.machine_learning_transform_job_roles_least_privilege
  service: aiplatform
  resource: batch_prediction_job
  requirement: Machine Learning Transform Job Roles Least Privilege
  scope: aiplatform.batch_prediction_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Batch Prediction Jobs
  rationale: Implementing least privilege for AI Platform batch prediction jobs minimizes the attack surface by restricting access to only necessary roles and permissions, thus reducing the risk of unauthorized data access or manipulation. This is crucial for protecting sensitive data and maintaining compliance with regulations such as GDPR, HIPAA, and PCI-DSS, which mandate strict access controls.
  description: This rule checks that roles assigned to batch prediction jobs in the AI Platform adhere to the principle of least privilege. Ensure that service accounts associated with these jobs have only the permissions necessary to perform their functions. Review and update IAM policies to eliminate excessive permissions, and regularly audit roles to maintain minimal access. Remediation involves adjusting IAM settings to align with GCP's best practices for access control.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/ai-platform/prediction/docs/access-control
  - CIS GCP Benchmark 1.1.0 - 4.1 Identity and Access Management
  - NIST SP 800-53 Rev. 5 - AC-6 Least Privilege
  - 'PCI DSS 3.2.1 - Requirement 7: Restrict access to cardholder data by business need to know'
  - ISO/IEC 27001:2013 - A.9 Access Control
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_inter_container_traffic_enc_enabled
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job Inter Container Traffic Enc Enabled
  scope: aiplatform.custom_job.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure AI Platform Custom Job Inter-Container Traffic Encryption
  rationale: Encrypting inter-container traffic in AI Platform Custom Jobs mitigates the risk of data interception and unauthorized access during data exchanges between containers. This is crucial for protecting sensitive training data and models from potential man-in-the-middle attacks, ensuring compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule verifies that inter-container traffic for AI Platform Custom Jobs is encrypted. Check the configuration settings to ensure that the 'enable_inter_container_traffic_encryption' flag is set to true for custom jobs. If not enabled, update the job configuration to enforce encryption. This can be done via the Google Cloud Console or using gcloud CLI by specifying the appropriate flag in your custom job configuration file.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-containers-training#configure_encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_network_isolation_enabled
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job Network Isolation Enabled
  scope: aiplatform.custom_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Isolation for AI Training Jobs
  rationale: Enabling network isolation for AI training jobs protects sensitive data from unauthorized network access and mitigates risks such as data leakage and unauthorized data manipulation. This control is crucial for maintaining the confidentiality and integrity of AI models, especially in industries with stringent data protection regulations like healthcare and finance. It supports compliance with standards such as NIST SP 800-53, which emphasizes secure network configurations.
  description: This rule verifies that network isolation is enabled for AI Platform custom training jobs by checking if a valid VPC is configured. Network isolation prevents jobs from accessing the public internet, reducing exposure to external threats. To enable it, specify a VPC network when creating or updating an AI custom job. Validate configuration through the GCP Console or gcloud CLI by ensuring the 'network' field is set in the job configuration.
  references:
  - https://cloud.google.com/ai-platform/training/docs/using-vpc
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/virtual-private-cloud
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_output_encryption_enabled
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job Output Encryption Enabled
  scope: aiplatform.custom_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure AI Job Output Encryption in Custom Jobs
  rationale: Enabling encryption for AI job outputs in GCP mitigates the risk of unauthorized data access and exposure, which is crucial for maintaining data confidentiality and integrity. It addresses potential threat scenarios where sensitive AI model outputs might be intercepted or stored in an unsecured manner. This practice is vital for compliance with data protection regulations such as GDPR, HIPAA, and ensures alignment with organizational security policies.
  description: This rule checks whether AI Platform custom job outputs have encryption enabled at rest using customer-managed encryption keys (CMEK). To verify, ensure that the custom job configurations specify an encryption key. Remediation involves updating the AI job configuration to include a valid CMEK, which can be done via the GCP Console or CLI by specifying the `kmsKeyName` parameter. This ensures that all data written by the AI job is encrypted with the specified key, enhancing data protection.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-jobs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_volume_encryption_enabled
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job Volume Encryption Enabled
  scope: aiplatform.custom_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure AI Training Job Volumes Are Encrypted at Rest
  rationale: Encrypting AI training job volumes at rest is crucial to protect sensitive data from unauthorized access and potential breaches. This practice helps mitigate risks such as data theft or exposure, especially in scenarios where data is stored in shared or multi-tenant environments. Meeting this requirement aligns with compliance mandates like GDPR, HIPAA, and PCI-DSS, which demand robust data protection and privacy measures.
  description: This rule checks if encryption is enabled for volumes used in Google Cloud AI Platform custom training jobs. Specifically, it verifies that Customer-Managed Encryption Keys (CMEK) are configured for storage resources to ensure data is encrypted with keys managed by the customer. To verify, audit the AI Platform custom job configurations for CMEK settings. Remediation involves configuring AI training jobs to specify CMEK for volume encryption, ensuring data is secured using keys within your control.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-jobs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.aiplatform.custom_job.ai_services_training_job_vpc_configured
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Job VPC Configured
  scope: aiplatform.custom_job.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure VPC Configuration for AI Platform Custom Jobs
  rationale: Configuring a VPC for AI Platform custom jobs is vital to control network access and protect data from unauthorized exposure. Without a VPC, data could traverse the public internet, increasing the risk of interception or unauthorized access. Proper VPC setup also helps meet compliance requirements by ensuring that data processing occurs within a controlled network environment.
  description: This rule verifies that AI Platform custom jobs are configured to use a Virtual Private Cloud (VPC) network, which ensures network isolation and secure data transit. To check compliance, confirm that each custom job specifies a VPC network in its configuration. Remediation involves updating the custom job settings to include a VPC network specification, which can be done via the Google Cloud Console or using the gcloud command-line tool.
  references:
  - https://cloud.google.com/ai-platform/docs/custom-jobs
  - https://cloud.google.com/vpc/docs/using-vpc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.aiplatform.custom_job.ai_services_training_pipeline_private_networking_enforced
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Pipeline Private Networking Enforced
  scope: aiplatform.custom_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Private Networking for AI Training Pipelines
  rationale: Enforcing private networking for AI training pipelines reduces the attack surface by preventing unauthorized or unintended access from the public internet. This configuration mitigates risks of data breaches and ensures that sensitive machine learning data remains within secure network boundaries, aligning with compliance mandates like GDPR and HIPAA.
  description: This rule checks whether AI Services Training Pipelines are configured to use private networking. It ensures that custom jobs in AI Platform are executed within a VPC, preventing exposure to the public internet. To verify, check the network settings of the custom job for a private IP configuration. If not compliant, configure the custom job to utilize a VPC network with appropriate firewall rules to restrict access.
  references:
  - https://cloud.google.com/ai-platform/training/docs/use-vpc
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 7.1
  - 'NIST SP 800-53 Rev. 5: AC-4, SC-7'
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.custom_job.ai_services_training_pipeline_secrets_from_vault_only
  service: aiplatform
  resource: custom_job
  requirement: Ai Services Training Pipeline Secrets From Vault Only
  scope: aiplatform.custom_job.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Use Vault for AI Platform Training Pipeline Secrets
  rationale: Storing secrets in Google Cloud Vault ensures they are encrypted and access-controlled, reducing the risk of unauthorized access or data breaches. This practice aligns with compliance requirements for data protection and supports the integrity of AI models by safeguarding sensitive information. It mitigates the risk of exposure during AI pipeline execution, which could lead to intellectual property theft or data misuse.
  description: This rule checks that AI Platform Training Pipelines only use secrets stored in Google Cloud Vault. By ensuring secrets are managed by Vault, you leverage its robust access control and encryption capabilities. To verify, inspect the AI Platform custom job configurations and confirm secrets are sourced from Vault. Remediation involves updating your pipeline configurations to reference secrets stored in Vault, ensuring consistent security practices.
  references:
  - https://cloud.google.com/vertex-ai/docs/pipelines/configure-gcp-resources
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.custom_job.data_governance_ai_training_job_input_output_encrypted
  service: aiplatform
  resource: custom_job
  requirement: Data Governance Ai Training Job Input Output Encrypted
  scope: aiplatform.custom_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of AI Platform Custom Job Data at Rest
  rationale: Encrypting input and output data for AI training jobs in Google's AI Platform Custom Jobs is critical to protect sensitive information from unauthorized access and potential data breaches. This practice mitigates risks related to data leakage and ensures compliance with regulatory standards such as GDPR and CCPA, which mandate strong data protection measures.
  description: This rule checks whether encryption is enabled for input and output data of AI Platform Custom Jobs. To verify, ensure that the AI Platform Custom Job configuration specifies a valid customer-managed encryption key (CMEK) for data at rest. If encryption is not configured, update the job settings to include a CMEK to enhance data protection. This can be done via the Google Cloud Console or using the gcloud CLI by specifying the --kms-key-name flag when creating or updating jobs.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-service-encryption
  - https://cloud.google.com/security/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.aiplatform.custom_job.data_governance_ai_training_job_logs_enabled
  service: aiplatform
  resource: custom_job
  requirement: Data Governance Ai Training Job Logs Enabled
  scope: aiplatform.custom_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for AI Training Custom Jobs
  rationale: Enabling logging for AI training custom jobs is crucial for maintaining transparency and accountability in data operations. It helps in tracking the flow of data, identifying anomalies, and ensuring that data handling complies with organizational policies and regulatory requirements. Without logging, it becomes challenging to audit data processing activities, increasing the risk of data breaches and compliance violations.
  description: This rule checks whether audit logging is enabled for custom jobs in the AI Platform. Audit logs should be configured to capture all data processing activities, including data input, output, and transformation operations. To verify, ensure that Stackdriver Logging is set up for the AI Platform and that custom job logs are actively monitored. Remediation involves configuring audit logs through the Google Cloud Console or using gcloud commands to enable and manage Stackdriver Logging for the AI Platform.
  references:
  - https://cloud.google.com/ai-platform/docs/training/logging
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.custom_job.data_governance_ai_training_job_private_networking_enforced
  service: aiplatform
  resource: custom_job
  requirement: Data Governance Ai Training Job Private Networking Enforced
  scope: aiplatform.custom_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Training Jobs
  rationale: Private networking for AI training jobs is crucial to prevent unauthorized access and mitigate potential data exfiltration risks. This enhances data governance by ensuring that sensitive data processed during AI training remains within a controlled network environment, aligning with compliance requirements such as GDPR and CCPA.
  description: This rule checks if AI Platform Custom Jobs are configured to use private networking, which restricts traffic to private IP addresses and prevents exposure to the public internet. To verify, ensure that the 'network' field in the custom job configuration specifies a VPC network. Remediation involves updating the custom job configurations to include a VPC network that aligns with organizational security policies.
  references:
  - https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs#CustomJob
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.custom_job.data_governance_ai_training_job_roles_least_privilege
  service: aiplatform
  resource: custom_job
  requirement: Data Governance Ai Training Job Roles Least Privilege
  scope: aiplatform.custom_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Custom Job Roles
  rationale: Implementing least privilege for AI Platform custom jobs is crucial to minimizing the potential attack surface and preventing unauthorized access to sensitive data and resources. Over-privileged roles can lead to data breaches, financial loss, and non-compliance with regulations such as GDPR and HIPAA. Ensuring roles have only the necessary permissions helps mitigate insider threats and external attacks.
  description: This rule checks that IAM roles assigned to AI Platform custom jobs adhere to the principle of least privilege, meaning they only have permissions necessary for their intended functions. Verify role assignments by reviewing IAM policies and removing any excess permissions or roles not required for job execution. Remediate by adjusting policies to align with job requirements, ensuring sensitive resources are accessible only to authorized jobs.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/custom-training-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.aiplatform.custom_job.data_privacy_ai_training_job_input_output_encrypted
  service: aiplatform
  resource: custom_job
  requirement: Data Privacy Ai Training Job Input Output Encrypted
  scope: aiplatform.custom_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of AI Training Job Input/Output in GCP
  rationale: Encrypting the input and output data of AI training jobs is crucial for protecting sensitive information from unauthorized access and ensuring compliance with data privacy regulations such as GDPR and CCPA. This reduces the risk of data breaches and supports the organizationâ€™s commitment to maintaining customer trust and safeguarding intellectual property.
  description: This rule checks whether input and output data for AI Platform Custom Jobs are encrypted using Customer-Managed Encryption Keys (CMEK). It involves verifying that the encryption configuration is set in the AI Platform job settings. To remediate, ensure that the AI Platform job configuration specifies a CMEK for data encryption, thereby enhancing data protection by using keys managed by the organization.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-jobs-overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-5/final
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.aiplatform.custom_job.data_privacy_ai_training_job_logs_enabled
  service: aiplatform
  resource: custom_job
  requirement: Data Privacy Ai Training Job Logs Enabled
  scope: aiplatform.custom_job.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Training Job Logs are Enabled for Data Privacy
  rationale: Enabling AI training job logs is crucial for maintaining data privacy and ensuring compliance with regulatory standards such as GDPR and HIPAA. Logs provide a detailed record of access and processing, which helps in identifying unauthorized access or data misuse. This control mitigates risks associated with data breaches and unauthorized data usage, thus protecting sensitive information and maintaining customer trust.
  description: This rule checks whether logging is enabled for custom AI training jobs on GCP. Logging should be configured to capture detailed information about data access and processing activities. To verify, ensure that the 'enable_logging' parameter is set to true in the AI Platform custom job configuration. Remediation involves updating the job configuration to enable logging, thereby ensuring that all operations are tracked and auditable.
  references:
  - https://cloud.google.com/ai-platform/training/docs/logging
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.custom_job.data_privacy_ai_training_job_private_networking_enforced
  service: aiplatform
  resource: custom_job
  requirement: Data Privacy Ai Training Job Private Networking Enforced
  scope: aiplatform.custom_job.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Private Networking for AI Training Jobs
  rationale: Ensuring AI training jobs use private networking is crucial to prevent unauthorized access to sensitive data. Without this configuration, training data can be exposed to public networks, increasing the risk of data breaches and non-compliance with regulations such as GDPR and CCPA. This measure helps maintain data confidentiality and integrity, crucial for trust and compliance in AI operations.
  description: This rule checks that AI Platform custom jobs are configured to use private networking, restricting access to internal networks only. Verify that the 'network' field in AI Platform job configurations specifies a private VPC. Remediation involves setting up a VPC with appropriate firewall rules and ensuring AI Platform jobs reference this network. Confirm this setting via the GCP Console or gcloud CLI to protect data in transit.
  references:
  - https://cloud.google.com/ai-platform/docs/training-overview
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/ai-platform/docs/security
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.custom_job.data_privacy_ai_training_job_roles_least_privilege
  service: aiplatform
  resource: custom_job
  requirement: Data Privacy Ai Training Job Roles Least Privilege
  scope: aiplatform.custom_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Training Job Roles Have Least Privilege
  rationale: Assigning excessive permissions to AI training job roles can expose sensitive data and increase the risk of data breaches. By adhering to the principle of least privilege, organizations can minimize potential attack vectors and protect sensitive datasets used during AI model training. This practice supports compliance with data protection regulations like GDPR and CCPA, which mandate stringent access control to personal data.
  description: This rule checks that roles assigned to AI Platform Custom Jobs are limited to the minimum necessary permissions. It verifies that the roles are not overly permissive and do not include access to resources unrelated to the job's function. To comply, review the roles assigned to each custom job and ensure they align strictly with the job's operational requirements, removing any unnecessary permissions. Use GCP IAM policy analysis tools to identify and rectify any discrepancies.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/ai-platform/training/docs/custom-service-auth
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.aiplatform.dataset.ai_services_dataset_access_logging_enabled
  service: aiplatform
  resource: dataset
  requirement: Ai Services Dataset Access Logging Enabled
  scope: aiplatform.dataset.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure AI Platform Dataset Access Logs are Enabled
  rationale: Enabling access logging for AI Platform datasets is crucial for detecting unauthorized access and ensuring accountability. Without logging, organizations risk undetected data breaches and non-compliance with regulatory standards such as GDPR and HIPAA, which mandate detailed access records for sensitive data. Accurate logs are also essential for forensic investigations and maintaining a strong security posture.
  description: This rule checks if access logging is enabled for datasets in Google Cloud's AI Platform. To verify, ensure that Cloud Audit Logs are configured to capture 'DATA_READ' and 'DATA_WRITE' for 'aiplatform.googleapis.com'. Remediation involves enabling audit logging in the IAM & admin settings of the Google Cloud Console. This will help in tracking access and modifications to datasets, providing an audit trail for compliance and security purposes.
  references:
  - https://cloud.google.com/aiplatform/docs/audit-logs
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.dataset.ai_services_dataset_cloud_storage_block_public_access
  service: aiplatform
  resource: dataset
  requirement: Ai Services Dataset Cloud Storage Block Public Access
  scope: aiplatform.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Block Public Access to AI Platform Datasets in Cloud Storage
  rationale: Allowing public access to AI Platform datasets can expose sensitive data and intellectual property to unauthorized users, leading to data breaches and compliance violations. This exposure can also result in financial and reputational damage, especially when handling regulated data under standards like GDPR or HIPAA.
  description: This rule verifies that public access is blocked for datasets stored in Google Cloud Storage used by AI Platform services. To ensure compliance, check the bucket's IAM policy to confirm that no public access permissions are granted. Remediation involves removing 'allUsers' and 'allAuthenticatedUsers' roles from the bucket's permissions, and configuring appropriate IAM roles for authorized users only.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/storage/docs/public-datasets
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.dataset.ai_services_dataset_cloud_storage_encrypted_at_rest
  service: aiplatform
  resource: dataset
  requirement: Ai Services Dataset Cloud Storage Encrypted At Rest
  scope: aiplatform.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Datasets are Encrypted at Rest
  rationale: Encrypting AI Platform datasets at rest protects sensitive data from unauthorized access and breaches, mitigating risks such as data leaks and financial loss. It aligns with compliance standards like PCI-DSS and HIPAA, which mandate encryption of sensitive data to safeguard customer information and maintain trust.
  description: This rule checks that AI Platform datasets stored in Cloud Storage are encrypted at rest using Google-managed or customer-managed encryption keys. Verify encryption settings by inspecting the datasetâ€™s storage configuration in the Google Cloud Console or using gcloud CLI. To remediate, enable encryption in the Cloud Storage settings for the dataset, ensuring that default or custom encryption keys are applied.
  references:
  - https://cloud.google.com/aiplatform/docs/storage-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/docs/security/encryption/default-encryption
- rule_id: gcp.aiplatform.dataset.ai_services_dataset_lifecycle_policy_configured
  service: aiplatform
  resource: dataset
  requirement: Ai Services Dataset Lifecycle Policy Configured
  scope: aiplatform.dataset.lifecycle_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure AI Dataset Lifecycle Policies are Configured
  rationale: Configuring lifecycle policies for AI datasets helps reduce the risk of unauthorized access and data leakage by ensuring datasets are properly expired, archived, or deleted according to compliance and business requirements. This is crucial for maintaining data integrity and adhering to regulations such as GDPR and CCPA, which mandate data minimization and timely deletion of personal data.
  description: This rule checks if lifecycle management policies are set for AI datasets in Google Cloud's AI Platform. Lifecycle policies define automatic actions on datasets, like deletion or archival, based on conditions such as age or last access time. To verify compliance, inspect the dataset configurations in the AI Platform and ensure policies are active and align with organizational data retention standards. Remediation involves configuring these policies via the Google Cloud Console or using the AI Platform's API to enforce dataset management best practices.
  references:
  - https://cloud.google.com/ai-platform/docs/datasets/lifecycle-management
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/storage/docs/lifecycle
- rule_id: gcp.aiplatform.dataset.ai_services_encrypted_at_rest_cmek
  service: aiplatform
  resource: dataset
  requirement: Ai Services Encrypted At Rest Cmek
  scope: aiplatform.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Datasets Use CMEK for Encryption at Rest
  rationale: Utilizing Customer-Managed Encryption Keys (CMEK) for encrypting AI Platform datasets at rest significantly enhances data security by giving organizations control over their encryption keys. This mitigates the risk of unauthorized data access and aligns with compliance requirements such as GDPR and HIPAA, which demand strict data protection measures. Failure to implement CMEK can result in data breaches, legal penalties, and reputational damage.
  description: This rule checks if AI Platform datasets are encrypted at rest using Customer-Managed Encryption Keys (CMEK). To verify, ensure that dataset configurations specify a CMEK key from Cloud Key Management Service (KMS). Remediation involves updating dataset settings to include a CMEK key, which requires appropriate permissions to access and manage keys within Cloud KMS. This enhances control over data encryption processes and supports compliance with stringent data protection standards.
  references:
  - https://cloud.google.com/ai-platform/docs/datasets
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.dataset.ai_services_public_access_blocked
  service: aiplatform
  resource: dataset
  requirement: Ai Services Public Access Blocked
  scope: aiplatform.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Block Public Access to AI Platform Datasets
  rationale: Blocking public access to AI Platform datasets is critical to prevent unauthorized access to sensitive data, which could lead to data breaches or intellectual property theft. Exposing datasets publicly increases the risk of data leakage and non-compliance with regulations such as GDPR and CCPA, which mandate strict data protection measures.
  description: This rule checks that AI Platform datasets are not publicly accessible by ensuring that IAM policies deny public access. To verify, inspect the dataset's IAM policy and ensure that 'allUsers' or 'allAuthenticatedUsers' are not granted roles that provide read access. Remediation involves updating IAM policies to restrict access to only authorized users and groups, reducing the risk of unauthorized data exposure.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.aiplatform.endpoint.ai_services_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Authn Required
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce Authn for GCP AI Platform Endpoints
  rationale: Requiring authentication for AI Platform endpoints mitigates unauthorized access risks, ensuring that only legitimate users and services can interact with your AI models. This is crucial for protecting sensitive data processed by AI services and maintaining compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks that all AI Platform endpoints are configured to require authentication, preventing public and anonymous access. Verify that IAM roles are correctly assigned to users and service accounts with appropriate permissions. To remediate, configure endpoints to enforce authentication by updating IAM policies to restrict access only to authenticated entities.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.ai_services_authz_policies_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Authz Policies Enforced
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Services Authz Policies Are Enforced on Endpoints
  rationale: Enforcing authorization policies on AI services endpoints is crucial to prevent unauthorized access and data breaches. Without proper controls, sensitive AI models and data can be exposed to malicious actors, leading to potential loss of intellectual property and compliance violations. This is particularly important for organizations handling sensitive information that must adhere to regulatory standards such as GDPR or HIPAA.
  description: This rule checks that authorization policies are enforced on Google Cloud AI Platform endpoints. By ensuring that AI services have proper IAM policies applied, access can be restricted to only authorized users and applications. To verify, review the IAM policies associated with each AI Platform endpoint to confirm they align with organizational security requirements. Remediation involves configuring the IAM settings to restrict access based on the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/ai-platform/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.endpoint.ai_services_inference_endpoint_data_capture_gcs_encrypted
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Inference Endpoint Data Capture Gcs Encrypted
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure GCS Encryption for AI Endpoint Data Capture
  rationale: Encrypting data captured by AI inference endpoints in Google Cloud Storage (GCS) is critical to protect sensitive information from unauthorized access and comply with data protection regulations such as GDPR and CCPA. Unencrypted data at rest increases the risk of data breaches, potentially leading to financial loss and reputational damage.
  description: This rule checks if the data captured by AI Platform inference endpoints is stored in GCS with encryption enabled. Verify that the GCS bucket used for data capture is configured with either Google-managed encryption keys (default) or customer-managed encryption keys (CMEK) for enhanced security. To remediate, configure the GCS bucket to enforce encryption at rest using the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/ai-platform/prediction/docs/data-capture
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.endpoint.ai_services_inference_endpoint_group_restrictive
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Inference Endpoint Group Restrictive
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Access to AI Platform Inference Endpoints
  rationale: Restricting access to AI Platform inference endpoints minimizes exposure to unauthorized data access and reduces the risk of malicious activities such as data exfiltration or inference tampering. Ensuring endpoints are only accessible from trusted sources can help meet compliance requirements for data protection regulations like GDPR and HIPAA, while also supporting business continuity by safeguarding AI models against potential security threats.
  description: This rule checks if the AI Platform inference endpoints have been configured with restrictive access controls, such as using VPC Service Controls or IAM roles to limit access. Verify that only necessary and authorized entities can reach these endpoints by reviewing IAM policies and network settings. To remediate, implement network security policies that restrict access to trusted IPs and configure IAM roles with the principle of least privilege to manage endpoint access.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/deploying-models
  - https://cloud.google.com/security-command-center/docs/how-to-use
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - 'NIST SP 800-53: AC-3 Access Enforcement'
  - ISO/IEC 27001:2013 - A.9.1.2 Access to networks and network services
- rule_id: gcp.aiplatform.endpoint.ai_services_inference_endpoint_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Inference Endpoint KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption for AI Platform Inference Endpoints
  rationale: Enabling KMS encryption for AI Platform inference endpoints ensures that sensitive data processed by AI models is protected at rest. This minimizes the risk of unauthorized data access and exposure, which is critical for maintaining customer trust, complying with data protection regulations like GDPR and HIPAA, and mitigating potential financial and reputational damage.
  description: This rule checks if AI Platform inference endpoints are configured to use Customer-Managed Encryption Keys (CMEK) with Cloud Key Management Service (KMS) for data encryption at rest. Verify that the `encryptionSpec` field is set with the correct KMS key resource ID in the endpoint configuration. Remediate by specifying a KMS key when creating or updating an endpoint using the AI Platform console, gcloud command-line tool, or API, ensuring that all sensitive data processed by the endpoints is encrypted with customer-managed keys.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/predictions/deploy-model-endpoints#specifying_customer_managed_encryption_keys
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.aiplatform.endpoint.ai_services_inference_endpoint_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Inference Endpoint Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Inference Endpoints
  rationale: Enforcing private networking for AI inference endpoints mitigates the risk of unauthorized access and data exfiltration by restricting network access to internal resources. This is crucial for maintaining the confidentiality and integrity of AI services, ensuring that sensitive data is not exposed to the public internet, which could lead to data breaches or compliance violations with standards like ISO 27001 and GDPR.
  description: This rule checks if AI Platform inference endpoints are configured to use private networking, which means they can only be accessed from within the organization's VPC network. To verify, ensure that the `network` field is specified in the endpoint configuration. Remediation involves updating the endpoint to include a private network setting, ensuring it aligns with internal security policies and access controls.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects.locations.endpoints
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/network-connectivity/docs/vpc
- rule_id: gcp.aiplatform.endpoint.ai_services_model_deployment_endpoint_in_vpc
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Model Deployment Endpoint In VPC
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Model Endpoints are Deployed within VPC Network
  rationale: Deploying AI model endpoints within a VPC enhances security by restricting exposure to the public internet, reducing the risk of unauthorized access and potential data breaches. This configuration helps comply with industry standards and regulations that require strict network access control, thereby protecting sensitive data processed by AI models.
  description: This rule checks whether AI model endpoints on Google Cloud are deployed within a Virtual Private Cloud (VPC) network. To verify, ensure that the endpoint's network configuration specifies a VPC. Remediation involves configuring the AI Platform endpoint to use a VPC network, which can be done via the Cloud Console or gcloud CLI, ensuring all communications occur within a controlled network environment.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/predictions/using-private-endpoints
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.aiplatform.endpoint.ai_services_model_deployment_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Model Deployment KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Model Deployments in Endpoints
  rationale: Encrypting AI model deployments using Customer-Managed Encryption Keys (CMEK) mitigates the risk of unauthorized data access and aligns with regulatory requirements for data protection. It ensures sensitive model data is protected against potential breaches, especially in industries with strict data privacy laws like healthcare and finance.
  description: This rule checks if AI model deployments in AI Platform Endpoints use CMEK for encryption at rest. To verify, ensure that the endpoint configuration specifies a valid KMS key. Remediation involves updating the endpoint settings to include a KMS key, which can be done via the Google Cloud Console or gcloud command-line tool. This ensures that the model data is encrypted with a key managed by your organization, enhancing control over data access.
  references:
  - https://cloud.google.com/ai-platform/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.endpoint.ai_services_model_deployment_logging_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Model Deployment Logging Enabled
  scope: aiplatform.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Model Deployment Logging for AI Platform Endpoints
  rationale: Logging for AI model deployments on GCP AI Platform is crucial for tracking changes, diagnosing issues, and ensuring compliance with internal and external regulations. Without proper logging, organizations may be unable to detect unauthorized access, troubleshoot deployment failures, or meet audit requirements, potentially leading to data breaches or regulatory fines.
  description: This rule checks whether logging is enabled for AI model deployments on AI Platform endpoints. It verifies that all interactions and changes to model deployments are captured in Google Cloud Logging. To enable logging, ensure that the AI Platform's endpoint logging settings are correctly configured in the GCP Console or via the gcloud command line tool. Remediation involves setting up and verifying the logging configuration to align with organizational policies and compliance mandates.
  references:
  - https://cloud.google.com/ai-platform/docs/reference/rest/v1/projects.locations.endpoints
  - https://cloud.google.com/audit-logs/docs/configure-data-access
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.aiplatform.endpoint.ai_services_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce AI Services Private Networking for AI Platform Endpoints
  rationale: Enforcing private networking for AI services minimizes exposure to the public internet, reducing the risk of unauthorized access and data breaches. This is crucial for protecting sensitive machine learning models and data, ensuring compliance with regulations such as GDPR and HIPAA, and maintaining customer trust by safeguarding intellectual property.
  description: This rule checks if AI Platform endpoints are configured to use private networking, ensuring that traffic is restricted to internal networks. To verify compliance, review the AI Platform endpoint configuration to confirm that private IP addresses are used. Remediation involves updating endpoint settings to enable private networking, which can be done through the GCP Console or gcloud CLI by specifying a private network during endpoint creation.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/private-ip
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/nist
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.ai_services_prompt_gateway_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Prompt Gateway Authn Required
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Services Prompt Gateway Authentication is Enabled
  rationale: Requiring authentication for AI services prompt gateway access helps prevent unauthorized usage and data exposure, safeguarding sensitive AI model interactions. Lack of authentication can lead to security breaches, compromise of AI models, and non-compliance with regulations such as GDPR and CCPA, impacting business reputation and financial standing.
  description: This rule checks that all AI Platform endpoints have authentication enabled for their prompt gateways, ensuring only authorized users can access AI services. To verify, check the endpoint configuration for the presence of authentication settings. Remediation involves configuring OAuth 2.0 for API access and ensuring IAM roles are correctly assigned. This reduces the risk of unauthorized access and data breaches.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/architecture/framework/security
- rule_id: gcp.aiplatform.endpoint.ai_services_prompt_gateway_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Prompt Gateway Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Services Prompt Gateway
  rationale: Ensuring AI Services Prompt Gateway endpoints use private networking reduces exposure to public internet vulnerabilities, mitigating risks of unauthorized access or data breaches. This control supports compliance with data protection regulations and enhances overall cloud security posture by limiting network attack surfaces.
  description: This rule checks if AI Services Prompt Gateway endpoints are configured to use private networking by verifying the absence of public IP addresses and the presence of VPC Service Controls. To enforce private networking, configure endpoints within a Virtual Private Cloud (VPC) and restrict access using firewall rules and private service access. Remediation involves updating endpoint settings to disable public IPs and ensuring they are only accessible within the private network.
  references:
  - https://cloud.google.com/ai-platform/docs/networking
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc-service-controls/docs/overview
- rule_id: gcp.aiplatform.endpoint.ai_services_prompt_gateway_token_scopes_least_privilege
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Prompt Gateway Token Scopes Least Privilege
  scope: aiplatform.endpoint.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Prompt Gateway Token Scopes
  rationale: Applying the principle of least privilege to AI Platform Prompt Gateway token scopes minimizes the risk of unauthorized access and potential data exposure. This practice reduces attack surface, ensuring that tokens have only the permissions necessary for their function, thereby mitigating the impact of compromised credentials and aligning with compliance mandates like NIST and ISO 27001.
  description: This rule verifies that AI Platform endpoints are configured with token scopes that adhere to the principle of least privilege, granting the minimal permissions required for operation. To ensure compliance, review the IAM policies associated with the AI Platform endpoints and adjust scopes to eliminate unnecessary permissions. Remediation involves using the GCP Console or CLI to edit IAM roles and scopes for the AI Platform endpoints, ensuring they are appropriately restrictive.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/aiplatform/docs/security-overview
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.aiplatform.endpoint.ai_services_rate_limiting_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Rate Limiting Enabled
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enable Rate Limiting for AI Platform Endpoint Services
  rationale: Enabling rate limiting on AI Platform endpoints helps prevent abuse and denial of service attacks, which can lead to service disruption and increased costs. Proper rate limiting ensures that AI services are available to legitimate users while mitigating risks of resource exhaustion and maintaining compliance with organizational security policies.
  description: This rule verifies that rate limiting is configured for AI Platform endpoint services, ensuring that the number of requests per user or IP is controlled. To enable rate limiting, configure API quotas and request limits in the Google Cloud Console under AI Platform settings. Regularly review and adjust these limits based on usage patterns to ensure optimal performance and security.
  references:
  - https://cloud.google.com/ai-platform/docs/resource-quotas
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.aiplatform.endpoint.ai_services_waf_attached
  service: aiplatform
  resource: endpoint
  requirement: Ai Services Waf Attached
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure WAF is Attached to AI Platform Endpoints
  rationale: Attaching a Web Application Firewall (WAF) to AI Platform endpoints helps protect against common web exploits and vulnerabilities that could compromise sensitive data processed by AI models. It mitigates the risk of unauthorized access and data breaches, essential for compliance with regulations like GDPR or CCPA. Without a WAF, endpoints are vulnerable to attacks that could lead to service disruptions or data loss.
  description: This rule checks if a Web Application Firewall (WAF) is attached to Google Cloud AI Platform endpoints. The WAF should be configured to monitor and filter HTTP traffic to the AI services, providing a layer of security against common threats such as SQL injection and XSS attacks. To verify compliance, ensure that the AI Platform endpoints are integrated with Cloud Armor or another WAF solution. Remediation involves configuring a WAF policy and associating it with the respective AI Platform endpoint.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/architecture/security-privacy-compliance
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.aiplatform.endpoint.data_governance_ai_endpoint_config_content_type_whi_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Ai Endpoint Config Content Type Whi Enforced
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Endpoint Config Content Type Whitelisting Enforced
  rationale: Enforcing content type whitelisting for AI endpoints mitigates the risk of data breaches and unauthorized data processing. This practice is essential for maintaining data integrity and privacy, especially in environments dealing with sensitive or regulated data. Failure to enforce such controls can lead to non-compliance with data protection regulations and potential exposure to legal and financial penalties.
  description: This rule checks if AI Platform endpoints have content type whitelisting enforced, ensuring that only approved data types are processed. To verify, inspect the endpoint configuration settings in the Google Cloud Console or via the gcloud command-line tool to ensure the 'contentTypes' field specifies allowed content types. Remediation involves updating the endpoint configurations to include a whitelist of authorized content types, preventing unauthorized data formats from being processed.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/using-resources#endpoint-configuration
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.data_governance_ai_endpoint_config_data_capture_buck_private
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Ai Endpoint Config Data Capture Buck Private
  scope: aiplatform.endpoint.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Endpoint Data Capture Buckets are Private
  rationale: Private data capture buckets help mitigate unauthorized access risks, protecting sensitive data processed by AI endpoints. An open bucket can expose sensitive information to unintended parties, leading to data breaches and violating compliance with standards such as GDPR and HIPAA.
  description: This rule checks that AI Platform endpoint data capture buckets are configured with private access settings. It verifies that these buckets do not allow public access, ensuring data is only accessible to authorized users. To remediate, review bucket permissions in Google Cloud Storage and ensure that no public grants are configured, restricting access to a minimum set of known identities.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/access-control
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.endpoint.data_governance_ai_endpoint_config_kms_keys_restricted
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Ai Endpoint Config KMS Keys Restricted
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Restrict AI Endpoint Config to Specific KMS Keys
  rationale: Limiting AI endpoint configurations to specific KMS keys reduces the risk of unauthorized data access and breaches by ensuring that only approved encryption keys are used. This enhances data protection against threats such as data exfiltration and unauthorized decryption while maintaining compliance with regulations like GDPR and HIPAA.
  description: This rule checks that only designated Cloud KMS keys are used for encrypting data at rest for AI Platform endpoints. To verify, inspect the AI Platform endpoint configurations for compliance with approved KMS key policies. Remediation involves updating endpoint configurations to ensure only specific KMS key versions are used, which can be done via the Google Cloud Console or CLI by setting the 'kmsKeyName' parameter correctly.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/overview
- rule_id: gcp.aiplatform.endpoint.data_governance_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Authn Required
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Endpoints Require Authentication for Data Access
  rationale: Requiring authentication for accessing AI Platform endpoints ensures that only authorized users can access sensitive data, which mitigates the risk of unauthorized data exposure. This is crucial for maintaining data integrity and confidentiality, especially when handling proprietary or regulated data, and helps organizations meet compliance requirements such as GDPR, HIPAA, and PCI-DSS.
  description: This rule checks whether authentication is enforced on AI Platform endpoints to control access to data. To verify compliance, ensure that all AI Platform endpoint configurations require authentication tokens or credentials before granting data access. Remediation involves configuring your AI Platform endpoints to enforce authentication by setting appropriate IAM policies and using service accounts. Regular audits and updates to access controls are recommended to maintain security posture.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/security
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.endpoint.data_governance_authz_policies_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Authz Policies Enforced
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Governance Policies on AI Platform Endpoints
  rationale: Enforcing data governance authorization policies on AI Platform endpoints is crucial for safeguarding sensitive data and adhering to regulatory requirements. Without proper authorization controls, unauthorized access could lead to data breaches, financial losses, and non-compliance with standards like GDPR and HIPAA. Implementing these controls helps maintain data integrity, confidentiality, and ensures that only authorized personnel have access to critical data assets.
  description: This rule checks whether data governance authorization policies are enforced on AI Platform endpoints, ensuring that appropriate identity and access management (IAM) policies are in place. To verify compliance, review the IAM policies associated with AI Platform endpoints and confirm that access is restricted to authorized users and service accounts. Remediation involves applying least privilege principles, updating IAM policies to enforce strict access controls, and regularly auditing access logs to detect unauthorized access attempts.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.endpoint.data_governance_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Data Governance KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption Enabled for AI Platform Endpoints
  rationale: Enabling KMS encryption for AI Platform endpoints protects sensitive data and models from unauthorized access and potential data breaches. This feature helps organizations comply with regulatory standards such as GDPR and HIPAA, which require robust data protection measures. Failure to encrypt data at rest increases the risk of exposure to cyber threats and can lead to significant financial and reputational damage.
  description: This rule checks if AI Platform endpoints are configured to use Customer-Managed Encryption Keys (CMEK) for data encryption at rest. To verify, ensure the endpoint configuration includes a valid KMS key resource identifier. Remediation involves updating the endpoint settings to specify a KMS key, which can be done through the Google Cloud Console or gcloud CLI. It is crucial to regularly audit and manage encryption keys to maintain data security.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/use-cases/data-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0 - 9.1
  - NIST SP 800-57 Part 1 - General Information on Key Management
  - 'PCI DSS v3.2.1 - Requirement 3: Protect Stored Cardholder Data'
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.endpoint.data_governance_logging_enabled
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Logging Enabled
  scope: aiplatform.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Data Governance Logging for AI Platform Endpoints
  rationale: Enabling data governance logging for AI Platform endpoints is essential to ensure full visibility into data access and usage patterns. This logging helps detect unauthorized access attempts and provides a trail for forensic investigations. Additionally, it supports compliance with regulatory requirements like GDPR and CCPA, which mandate detailed data access records.
  description: This rule checks whether data governance logs are enabled for AI Platform endpoints, ensuring audit trails are maintained. Verify that logging is active by inspecting the endpoint's configuration settings in the GCP Console under 'Logging and Monitoring'. To enable logging, navigate to the AI Platform, select the endpoint, and ensure that logging is turned on in the 'Logging' section. Regularly review logs to monitor for suspicious activity.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/security/compliance/cis#cis-gcp-v1.1.0
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/73906.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.aiplatform.endpoint.data_governance_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Governance Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Private Networking for AI Platform Endpoints
  rationale: Enforcing private networking for AI Platform endpoints mitigates the risk of unauthorized access and data breaches by restricting network exposure to only trusted internal networks. This is crucial for protecting sensitive data processed by AI models and complying with data protection regulations such as GDPR and CCPA, which mandate stringent data handling and privacy controls.
  description: This rule checks if AI Platform endpoints are configured to use private networking, ensuring they are not exposed to the public internet. To verify, ensure that endpoints are accessible only via internal IPs within a Google Cloud VPC. Remediation involves configuring endpoints to use the 'privateEndpoints' setting, allowing only VPC-based access and ensuring the appropriate VPC service controls are applied.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/using-private-endpoints
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/security/privacy/
- rule_id: gcp.aiplatform.endpoint.data_privacy_ai_endpoint_config_capture_store_encryp_private
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Ai Endpoint Config Capture Store Encryp Private
  scope: aiplatform.endpoint.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure AI Endpoint Config Capture Data is Encrypted Privately
  rationale: Encrypting data captured by AI endpoints ensures the confidentiality and integrity of sensitive information, mitigating risks such as unauthorized access and data breaches. This is crucial for maintaining trust with stakeholders and complying with data protection regulations like GDPR and CCPA, which mandate strict controls over personal data handling and storage.
  description: This rule checks if the AI Platform endpoint is configured to encrypt captured data using private keys. Ensuring that data is encrypted with private keys enhances security by preventing exposure to unauthorized entities. Verify that the endpoint's data capture configuration specifies encryption settings using customer-managed keys (CMKs) within Cloud Key Management Service (KMS). Remediate by updating the endpoint configuration to use CMKs, thereby applying robust encryption standards.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/privacy/data-encryption
- rule_id: gcp.aiplatform.endpoint.data_privacy_ai_endpoint_config_content_type_whitel_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Ai Endpoint Config Content Type Whitel Enforced
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Content Type Whitelisting on AI Endpoint Configurations
  rationale: Enforcing content type whitelisting on AI endpoint configurations is crucial to prevent data leakage and unauthorized data processing, which can lead to compliance violations and reputational damage. This control helps mitigate risks of improper data handling and exposure to potential security vulnerabilities by ensuring only approved data formats are processed.
  description: This rule checks that all AI Platform endpoint configurations have content type whitelisting enforced, which restricts data input to predefined formats. To verify, review the endpoint configuration settings in the Google Cloud Console for content type specifications. Remediation involves updating the endpoint configurations to include valid and approved content types only, ensuring alignment with organizational data policies.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.aiplatform.endpoint.data_privacy_ai_endpoint_config_kms_keys_restricted
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Ai Endpoint Config KMS Keys Restricted
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Restrict KMS Keys for AI Platform Endpoint Configurations
  rationale: Restricting KMS keys for AI Platform endpoints is crucial to prevent unauthorized data access and ensure that sensitive data is encrypted at rest. Limiting key usage only to specific services and environments mitigates risk of data breaches and supports compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks that AI Platform endpoint configurations use customer-managed encryption keys (CMEK) from Cloud Key Management Service (KMS). Ensure that only authorized personnel and services have access to these keys. Verify key restrictions by reviewing IAM policies associated with each key and limit the access scope to necessary roles. Remediation involves updating IAM policies to enforce principle of least privilege and rotating keys regularly.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/using-cmek
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/key-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.aiplatform.endpoint.data_privacy_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Authn Required
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Endpoint Authentication for Data Privacy
  rationale: Enforcing authentication for AI Platform endpoints is crucial to prevent unauthorized access to sensitive data, which could lead to data breaches and non-compliance with regulations such as GDPR and HIPAA. Inadequate authentication can expose endpoints to malicious actors, increasing the risk of data theft and reputational damage.
  description: This rule checks that AI Platform endpoints require authentication to access data, ensuring that only authorized users can interact with the endpoint. To verify, ensure that IAM roles are correctly assigned and that authentication mechanisms, such as OAuth 2.0, are in place for accessing endpoints. Remediation involves setting up IAM policies that enforce authentication and regularly reviewing access logs to detect unauthorized attempts.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.aiplatform.endpoint.data_privacy_authz_policies_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Authz Policies Enforced
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Data Privacy Policies on AI Platform Endpoints
  rationale: Enforcing data privacy authorization policies on AI Platform endpoints is crucial to protect sensitive data from unauthorized access, reducing the risk of data breaches and maintaining customer trust. It also ensures compliance with regulatory standards such as GDPR and HIPAA, which mandate strict access controls and privacy measures for data handling.
  description: This rule checks that all AI Platform endpoints have data privacy authorization policies enforced, ensuring that only authorized users and services can access sensitive data. Verification involves reviewing IAM roles and policies associated with each endpoint to confirm that they align with organizational data privacy requirements. Remediation includes defining and applying appropriate IAM roles that restrict access to authorized identities, and leveraging VPC Service Controls for additional security boundaries.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - 'CIS GCP Benchmark: 4.1 Ensure that Cloud IAM policies do not allow broad access'
  - 'NIST SP 800-53 Rev. 5: AC-3 Access Enforcement'
- rule_id: gcp.aiplatform.endpoint.data_privacy_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption is Enabled for AI Platform Endpoints
  rationale: Enabling Key Management Service (KMS) encryption for AI Platform endpoints protects sensitive data by encrypting it with customer-managed keys, thereby mitigating the risk of unauthorized data access. This practice is crucial for maintaining data confidentiality, especially in environments handling personal or sensitive information, and it supports compliance with regulatory frameworks such as GDPR and HIPAA.
  description: This rule checks if AI Platform endpoints are configured to use customer-managed KMS keys for data encryption at rest. To verify, ensure that each endpoint within your AI Platform is associated with a KMS key. Remediation involves configuring AI Platform endpoints to use an existing KMS key by updating the endpoint configuration to include the 'encryptionSpec.kmsKeyName' property with the appropriate key identifier.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/encrypt-resources
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/how-tos
  - https://cloud.google.com/architecture/devops/using-kms-to-secure-cloud-resources
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.aiplatform.endpoint.data_privacy_logging_enabled
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Logging Enabled
  scope: aiplatform.endpoint.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Privacy Logging is Enabled for AI Platform Endpoints
  rationale: Enabling data privacy logging for AI Platform endpoints helps ensure that access to sensitive data is monitored and recorded, reducing the risk of unauthorized access and data breaches. This capability supports compliance with data protection regulations such as GDPR and CCPA by providing an audit trail of data access and usage, which is essential for maintaining trust and transparency with clients and stakeholders.
  description: This rule checks that data privacy logging is enabled for AI Platform endpoints, which involves configuring logging settings to capture detailed access logs. To verify, ensure that Cloud Audit Logs are set up to include DATA_READ and DATA_WRITE activities for AI Platform endpoints. If not configured, enable logging via the Google Cloud Console or gcloud CLI by setting the appropriate IAM roles and permissions. This action helps maintain a comprehensive audit log of data access for security and compliance purposes.
  references:
  - https://cloud.google.com/aiplatform/docs/logging
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.aiplatform.endpoint.data_privacy_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Data Privacy Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Platform Endpoints
  rationale: Enforcing private networking for AI Platform endpoints ensures that data is transmitted over secure, controlled networks, reducing exposure to public internet threats such as man-in-the-middle attacks and unauthorized access. This is crucial for safeguarding sensitive data, maintaining customer trust, and meeting regulatory compliance requirements such as GDPR and HIPAA.
  description: This rule checks whether AI Platform endpoints are configured to use private networking, which restricts access to the internal network and avoids exposure to the public internet. To verify, ensure that the endpoint's network configuration specifies a VPC network and that 'enablePrivateServiceConnect' is set to true. Remediation involves updating the endpoint settings to utilize a VPC network with private service access enabled, following GCPâ€™s best practices for private connectivity.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects.locations.endpoints
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/privacy
- rule_id: gcp.aiplatform.endpoint.encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption is Enabled for AI Platform Endpoints
  rationale: Enabling encryption for AI Platform endpoints protects sensitive data from unauthorized access and data breaches, which can lead to significant financial losses and reputational damage. Encryption at rest is critical for meeting compliance with regulations such as GDPR, HIPAA, and CCPA that mandate the protection of personal data stored in cloud environments.
  description: This rule checks if encryption is enabled for AI Platform endpoints to safeguard data at rest. Verify that each AI Platform endpoint has Customer-Managed Encryption Keys (CMEK) configured, which allows for greater control over cryptographic keys and data protection. Remediation involves setting up CMEK in Google Cloud Key Management Service (KMS) and applying it to your AI Platform endpoints through the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/encryption
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.aiplatform.endpoint.machine_learning_authn_required
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Authn Required
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce Authentication for AI Platform Endpoints
  rationale: Requiring authentication for AI Platform endpoints ensures that only authorized users and applications can access machine learning models and services. This prevents unauthorized access, which could lead to data breaches or misuse of machine learning resources. Ensuring proper authentication aligns with compliance requirements such as GDPR and helps safeguard sensitive data processed by AI services.
  description: This rule checks that all AI Platform endpoints have authentication mechanisms enabled to restrict access to authorized entities. To verify, ensure that endpoint configurations require authentication tokens or service account credentials for access. Remediation involves updating endpoint settings to enforce authentication protocols such as OAuth 2.0. This enhances security by validating the identity of users and applications before granting access to AI resources.
  references:
  - https://cloud.google.com/ai-platform/docs/authentication
  - https://cloud.google.com/security/compliance/gdpr
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.aiplatform.endpoint.machine_learning_authz_policies_enforced
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Authz Policies Enforced
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce ML Endpoint Authz Policies in AI Platform
  rationale: Ensuring that machine learning endpoints are protected with robust authorization policies prevents unauthorized access, which could lead to data breaches, model theft, or service disruptions. Proper enforcement of these policies is crucial for maintaining the integrity of machine learning operations and complying with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks if authorization policies are enforced on AI Platform endpoints to control access. It verifies that Identity and Access Management (IAM) roles and policies are correctly configured to only allow permitted entities to interact with endpoints. Remediation involves reviewing IAM configurations, ensuring that least privilege principles are applied, and updating policies to restrict access appropriately.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - 'CIS GCP Benchmark: https://www.cisecurity.org/benchmark/google_cloud_computing_platform'
  - 'NIST SP 800-53: Access Control'
  - 'PCI-DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - ISO 27001:2013 A.9 Access Control
- rule_id: gcp.aiplatform.endpoint.machine_learning_config_content_type_whitelist_enforced
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Config Content Type Whitelist Enforced
  scope: aiplatform.endpoint.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce AI Platform Endpoint Content Type Whitelisting
  rationale: Enforcing a content type whitelist on AI Platform endpoints minimizes the risk of malicious data being processed, which could lead to unauthorized access or data breaches. This control helps protect sensitive data and ensures compliance with regulatory standards that mandate data integrity and confidentiality.
  description: This rule checks if AI Platform endpoints have a content type whitelist configured, restricting data formats to only those explicitly allowed. To verify, review the endpoint's configuration in the Google Cloud Console or via the gcloud CLI to ensure a whitelist is defined. If not configured, update the endpoint settings to specify allowed content types, enhancing security by preventing the processing of potentially harmful data formats.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/architecture/framework/security
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.machine_learning_config_data_capture_bucket_encrypte_private
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Config Data Capture Bucket Encrypte Private
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Data Capture Buckets Use Customer-Managed Encryption Keys
  rationale: Encrypting machine learning data capture buckets using customer-managed encryption keys (CMEK) ensures that sensitive data is protected from unauthorized access and potential breaches. This practice supports regulatory compliance with standards like GDPR and HIPAA, which mandate robust data protection measures. It also mitigates risks associated with data leaks or unauthorized data manipulation, ultimately safeguarding intellectual property and building customer trust.
  description: This rule checks if the machine learning data capture bucket associated with an AI Platform endpoint is encrypted using customer-managed encryption keys. To verify, ensure that the bucket's encryption configuration specifies a valid CMEK. Remediation involves creating a CMEK in Cloud KMS and configuring the bucket to use this key for encryption. This ensures that you maintain control over the encryption keys and comply with organizational policies on data security.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/data-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.endpoint.machine_learning_config_kms_keys_restricted
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Config KMS Keys Restricted
  scope: aiplatform.endpoint.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Restrict KMS Keys for AI Platform Endpoint Config
  rationale: Restricting KMS keys for AI Platform endpoints ensures that only authorized keys are used for encrypting machine learning configurations, protecting sensitive data from unauthorized access. This minimizes the risk of data breaches and supports compliance with regulatory standards such as GDPR and CCPA, which mandate strong data protection measures.
  description: This rule checks that only whitelisted KMS keys are used for encrypting AI Platform endpoint configurations. Ensure that endpoints are configured with specific KMS keys that adhere to your organization's encryption policies. To verify, review the endpoint's configuration in the GCP console or use the gcloud CLI to list the KMS keys in use and compare them with the approved list. Remediate by updating the endpoint configuration to use the correct KMS key and restrict access to unauthorized keys.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/security/encryption
  - https://cloud.google.com/kms/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-security-and-privacy-controls
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.endpoint.machine_learning_kms_encryption_enabled
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning KMS Encryption Enabled
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Platform Endpoints
  rationale: Enabling KMS encryption for AI Platform endpoints is crucial to protect sensitive machine learning models and data from unauthorized access and potential breaches. This practice mitigates the risk of data exposure and aligns with regulatory requirements for data protection, such as GDPR and CCPA, by ensuring that data at rest is encrypted using Cloud KMS keys.
  description: This rule checks whether AI Platform endpoints are configured to use customer-managed encryption keys (CMEK) provided by Google Cloud Key Management Service (KMS). To verify compliance, ensure that each endpoint's encryption settings explicitly reference a valid KMS key. Remediate non-compliant endpoints by configuring them to use a KMS key via the AI Platform's endpoint configuration settings in the GCP Console or by using the appropriate gcloud CLI command.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/encryption
  - https://cloud.google.com/security/compliance/cis#cis_google_cloud_platform_foundations_benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/architecture/security-foundations/encryption
- rule_id: gcp.aiplatform.endpoint.machine_learning_logging_enabled
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Logging Enabled
  scope: aiplatform.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for AI Platform Endpoints
  rationale: Enabling logging for AI Platform endpoints provides visibility into operations and potential misuse by capturing audit trails of requests and responses. This is essential for detecting anomalies, mitigating risks, and ensuring compliance with regulations such as GDPR and HIPAA, which mandate data protection and the ability to audit data access and processing.
  description: This rule checks if logging is enabled for endpoints within the AI Platform, ensuring that all interactions with machine learning models are recorded. To verify and enable logging, ensure that audit logging is configured in the Google Cloud Console under the AI Platform section. Remediation involves enabling 'Admin Activity' and 'Data Access' logs to capture detailed operational insights.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.endpoint.machine_learning_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Machine Learning Private Networking Enforced
  scope: aiplatform.endpoint.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Platform Endpoints
  rationale: Enforcing private networking for AI Platform endpoints reduces exposure to the public internet, minimizing the risk of unauthorized access and data breaches. This improves data confidentiality and integrity, supports compliance with regulatory frameworks such as GDPR and HIPAA, and aligns with best practices for secure machine learning operations.
  description: This rule checks if AI Platform endpoints are configured to use private IPs, ensuring they are accessible only through private networking within a Virtual Private Cloud (VPC). To verify, inspect the endpoint's network configuration for private IP addresses. Remediation involves configuring the endpoint to use private networking through the GCP Console or gcloud CLI, ensuring it is part of a peered VPC network.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/networking-overview
  - https://cloud.google.com/vpc/docs/private-access-options
  - CIS Google Cloud Platform Foundation Benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.endpoint.ml_ops_monitoring_logs_enabled
  service: aiplatform
  resource: endpoint
  requirement: Ml Ops Monitoring Logs Enabled
  scope: aiplatform.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable ML Ops Monitoring Logs for AI Platform Endpoints
  rationale: Enabling ML Ops monitoring logs for AI Platform endpoints is crucial for maintaining visibility into model performance and operational metrics. This logging helps detect anomalies, troubleshoot issues, and ensure compliance with data handling policies. It mitigates risks associated with undetected model drift and performance degradation, which could lead to inaccurate predictions and business losses.
  description: This rule checks whether ML Ops monitoring logs are enabled for AI Platform endpoints, ensuring that model operations are being logged for analysis. To verify, navigate to the Google Cloud Console under AI Platform > Endpoints, and confirm that logging is enabled in the settings. If not enabled, configure logging by setting up a logging sink that routes logs to Cloud Logging. This allows for continuous monitoring and early detection of issues in ML models.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/logging
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/setup
  - 'NIST SP 800-53: AU-2 Audit Events'
  - ISO 27001:2013 A12.4 Logging and Monitoring
- rule_id: gcp.aiplatform.endpoint.ml_ops_monitoring_outputs_encrypted_and_private
  service: aiplatform
  resource: endpoint
  requirement: Ml Ops Monitoring Outputs Encrypted And Private
  scope: aiplatform.endpoint.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure MLOps Monitoring Outputs Are Encrypted and Private
  rationale: Encryption of MLOps monitoring outputs is crucial to protect sensitive data from unauthorized access, which can lead to data breaches and non-compliance with regulations such as GDPR and HIPAA. Ensuring privacy and encryption helps mitigate risks associated with data leaks and enhances trust in GCP's AI Platform services.
  description: This rule checks that MLOps monitoring outputs in AI Platform endpoints are encrypted at rest using customer-managed encryption keys (CMEK). Verify that the outputs are stored in a Google Cloud Storage bucket with encryption enabled and access restricted to authorized users only. To remediate, configure the endpoint to use CMEK for encryption and set appropriate IAM policies to limit access.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.endpoint.ml_ops_monitoring_private_networking_enforced
  service: aiplatform
  resource: endpoint
  requirement: Ml Ops Monitoring Private Networking Enforced
  scope: aiplatform.endpoint.monitoring
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for MLOps Monitoring on AI Platform
  rationale: Enforcing private networking for AI Platform MLOps monitoring reduces exposure to public internet threats, minimizing risks of unauthorized access or data leaks. This ensures sensitive machine learning operations and data are protected, aligning with security best practices and compliance standards like HIPAA and ISO 27001.
  description: This rule checks if AI Platform endpoints for MLOps monitoring are configured to use private networking, preventing exposure to public networks. Verify that endpoints are accessible only through VPCs, enhancing security by ensuring data and operations remain within a controlled network environment. To remediate, configure AI Platform with private IPs in the VPC settings, ensuring all monitoring endpoints are accessible only through secure, internal networks.
  references:
  - https://cloud.google.com/ai-platform/docs/security#private_network_access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/solutions/security-overview
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/architecture/security-best-practices
- rule_id: gcp.aiplatform.endpoint.ml_ops_monitoring_roles_least_privilege
  service: aiplatform
  resource: endpoint
  requirement: Ml Ops Monitoring Roles Least Privilege
  scope: aiplatform.endpoint.monitoring
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Endpoint Monitoring Roles
  rationale: Implementing least privilege for AI Platform endpoint monitoring roles minimizes the risk of unauthorized access and potential data breaches. It ensures that users have only the permissions necessary to perform their job functions, which is crucial for maintaining compliance with security standards like GDPR and HIPAA, as well as reducing the attack surface.
  description: This rule checks that roles associated with AI Platform endpoint monitoring are configured to follow the principle of least privilege. It involves reviewing IAM policies to ensure that only essential permissions are granted. Remediation involves auditing and modifying IAM roles and policies to remove excessive permissions. Verification can be done by examining IAM role bindings and comparing them with operational requirements.
  references:
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.aiplatform.experiment.data_governance_logs_enabled
  service: aiplatform
  resource: experiment
  requirement: Data Governance Logs Enabled
  scope: aiplatform.experiment.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Governance Logs for AI Platform Experiments
  rationale: Enabling data governance logs for AI Platform experiments is crucial for tracking data access and changes, which helps in identifying unauthorized access and ensuring data integrity. This capability allows organizations to meet compliance requirements such as GDPR and CCPA by providing a clear audit trail of data usage. Additionally, it aids in forensic investigations by offering detailed logs that help mitigate the risk of data breaches and insider threats.
  description: This rule checks if data governance logging is enabled for AI Platform experiments. To verify, ensure that audit logs are configured to capture 'DATA_READ', 'DATA_WRITE', and 'ADMIN_READ' activities for the relevant AI Platform resources. Remediation involves enabling audit logging in the Google Cloud Console under the 'Logging' section for AI Platform resources. This will ensure that any access or modifications to experiment data are logged appropriately, enhancing the security posture of the AI Platform.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/ai-platform/docs/security
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 4
  - ISO/IEC 27001:2013
- rule_id: gcp.aiplatform.experiment.data_governance_metadata_store_encrypted
  service: aiplatform
  resource: experiment
  requirement: Data Governance Metadata Store Encrypted
  scope: aiplatform.experiment.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Metadata Store Encryption at Rest
  rationale: Encrypting the Metadata Store for AI Platform Experiments protects sensitive data from unauthorized access and potential breaches. It safeguards intellectual property and research data, ensuring compliance with data protection regulations such as GDPR and CCPA. Failure to encrypt can lead to data leaks that harm business reputation and result in financial penalties.
  description: This rule checks if the Metadata Store in GCP AI Platform Experiments is encrypted using customer-managed encryption keys (CMEK). Verify that the metadata store is configured with CMEK by reviewing the AI Platform settings in the Google Cloud Console. To remediate, enable encryption at rest using CMEK, which provides greater control over encryption keys and meets compliance requirements.
  references:
  - https://cloud.google.com/ai-platform/docs/experiments/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.experiment.data_governance_rbac_least_privilege
  service: aiplatform
  resource: experiment
  requirement: Data Governance RBAC Least Privilege
  scope: aiplatform.experiment.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Experiment Access
  rationale: Applying the principle of least privilege in AI Platform experiments minimizes risk by limiting data access to only those who need it for their roles. This reduces the potential for data breaches and unauthorized access, which can lead to data leaks and non-compliance with regulations like GDPR and HIPAA. Failing to implement this can result in significant financial and reputational damage.
  description: This rule checks that roles assigned to users for accessing AI Platform experiments adhere to the principle of least privilege. It requires that permissions granted are strictly necessary for job functions, reducing excess access. To verify compliance, review IAM policies on AI Platform resources, ensuring that users have the minimum required roles. Remediation involves adjusting IAM roles and permissions to remove unnecessary access rights, potentially using predefined roles instead of overly permissive custom roles.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.aiplatform.experiment.data_privacy_logs_enabled
  service: aiplatform
  resource: experiment
  requirement: Data Privacy Logs Enabled
  scope: aiplatform.experiment.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Data Privacy Logging for AI Platform Experiments
  rationale: Enabling data privacy logs for AI Platform experiments is crucial to monitor and audit data interactions, ensuring data integrity and compliance with privacy regulations such as GDPR and CCPA. Without logging, organizations face increased risk of unauthorized data access, potential data breaches, and non-compliance penalties.
  description: This rule checks that data privacy logs are enabled for AI Platform experiments. To verify, ensure that logging is configured in the AI Platform settings to capture data access and usage details. Remediation involves updating the experiment configurations to enable comprehensive logging, which can be done via the Google Cloud Console or CLI by setting the appropriate logging parameters.
  references:
  - https://cloud.google.com/ai-platform/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/privacy
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/auditing
- rule_id: gcp.aiplatform.experiment.data_privacy_metadata_store_encrypted
  service: aiplatform
  resource: experiment
  requirement: Data Privacy Metadata Store Encrypted
  scope: aiplatform.experiment.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Experiment Metadata Store Encryption
  rationale: Encrypting the metadata store in AI Platform experiments is crucial for protecting sensitive data against unauthorized access and potential data breaches. This practice helps mitigate risks of data exposure in case of security incidents and ensures compliance with regulatory standards such as GDPR and HIPAA, which mandate encryption of sensitive information at rest.
  description: This rule checks whether the metadata store associated with AI Platform experiments is encrypted using customer-managed encryption keys (CMEK). To verify, ensure that the encryption configuration specifies a valid CMEK. Remediation involves configuring AI Platform to use CMEK for the metadata store by updating the experimentâ€™s encryption settings in the GCP Console or via gcloud CLI commands. This enhances control over encryption keys and aligns with best practices for data protection.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.experiment.data_privacy_rbac_least_privilege
  service: aiplatform
  resource: experiment
  requirement: Data Privacy RBAC Least Privilege
  scope: aiplatform.experiment.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Platform Experiments Use Least Privilege Access
  rationale: Implementing least privilege for AI Platform experiments is critical to minimize the risk of unauthorized data access, which could lead to data breaches, financial loss, and non-compliance with regulations like GDPR and HIPAA. Proper RBAC configurations help prevent privilege escalation attacks and ensure that users have only the necessary permissions to perform their tasks, reducing the attack surface.
  description: This rule checks for the correct configuration of Role-Based Access Control (RBAC) on AI Platform experiment resources to enforce the principle of least privilege. It ensures that users have the minimal set of permissions required to manage experiments without unnecessary access to sensitive data. Verify by auditing IAM roles assigned to users and service accounts associated with AI Platform experiments, ensuring they align with their job functions. Remediate by adjusting roles to more restrictive ones, removing excessive permissions, and regularly reviewing permissions for ongoing appropriateness.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.experiment.machine_learning_logs_enabled
  service: aiplatform
  resource: experiment
  requirement: Machine Learning Logs Enabled
  scope: aiplatform.experiment.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Machine Learning Logs are Enabled for AI Platform Experiments
  rationale: Enabling machine learning logs is crucial for monitoring and auditing the activity within AI Platform experiments. This provides visibility into model training processes and data handling, which is essential for detecting anomalies, ensuring data integrity, and meeting compliance requirements such as GDPR and SOC2. Without logging, organizations risk undetected data breaches and non-compliance with regulatory standards, potentially leading to financial and reputational damage.
  description: This rule checks if logging is enabled for machine learning experiments in Google Cloud's AI Platform. To verify, ensure that the AI Platform's logging configurations are correctly set to capture detailed logs of model training and data processing activities. Remediation involves configuring the logging settings within the AI Platform to ensure comprehensive log collection, which can be done via the Google Cloud Console or by using the gcloud command-line tool. This facilitates effective monitoring and auditing of AI workloads.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.aiplatform.experiment.machine_learning_metadata_store_encrypted
  service: aiplatform
  resource: experiment
  requirement: Machine Learning Metadata Store Encrypted
  scope: aiplatform.experiment.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Metadata Store is Encrypted at Rest
  rationale: Encrypting the Machine Learning Metadata Store ensures that sensitive data, such as model parameters and training data lineage, is protected from unauthorized access and potential data breaches. Unencrypted data can lead to compliance violations and exposure of intellectual property, increasing the risk of data theft and business disruption.
  description: This rule checks whether the Machine Learning Metadata Store in GCP's AI Platform is encrypted at rest using customer-managed encryption keys (CMEK). To verify, review the metadata store's configuration in the GCP Cloud Console or use the gcloud CLI to ensure CMEK is enabled. If not configured, enable CMEK by updating the metadata store settings to use a Cloud Key Management Service (KMS) key. This enhances the security and compliance posture by ensuring data encryption aligns with organizational policies.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/architecture/encryption-and-key-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.aiplatform.experiment.machine_learning_rbac_least_privilege
  service: aiplatform
  resource: experiment
  requirement: Machine Learning RBAC Least Privilege
  scope: aiplatform.experiment.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Platform Experiment RBAC
  rationale: Limiting permissions to the least privilege necessary for AI Platform experiments reduces the risk of unauthorized access and potential data breaches. This approach mitigates insider threats and ensures compliance with data protection regulations by minimizing the exposure of sensitive machine learning models and datasets.
  description: This rule verifies that role-based access control (RBAC) for AI Platform experiments is configured to follow the principle of least privilege. Ensure that users and service accounts have only the permissions necessary to perform their specific tasks. Regularly audit and adjust permissions, removing unnecessary or excessive roles. Remediation involves using IAM policies to assign roles at the appropriate resource level and reviewing permissions periodically.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_offline_store_block_public_access
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store Offline Store Block Public Access
  scope: aiplatform.featurestore.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to AI Feature Store Offline Data
  rationale: Public access to AI Feature Store Offline Store can expose sensitive ML data, leading to data breaches, intellectual property theft, and non-compliance with regulations such as GDPR and CCPA. Ensuring data is only accessible to authorized users mitigates risks of unauthorized data manipulation and leakage.
  description: This rule checks that the AI Feature Store Offline Store is configured to block public access. Verify that access policies are set to deny access from 'allUsers' and 'allAuthenticatedUsers'. To remediate, update IAM policies to restrict access to specific service accounts or user groups. Use Google Cloud Console or gcloud CLI to modify IAM settings, ensuring that only authorized entities can interact with the offline data.
  references:
  - https://cloud.google.com/ai-platform/docs/feature-store/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_offline_store_gcs_encrypted
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store Offline Store Gcs Encrypted
  scope: aiplatform.featurestore.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Offline Store GCS Buckets are Encrypted in Feature Store
  rationale: Encrypting data at rest in GCS ensures that sensitive information is protected against unauthorized access and data breaches. This is critical for meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strong data protection measures. Failure to encrypt can lead to financial penalties and loss of customer trust.
  description: This rule checks if the Google Cloud Storage (GCS) buckets used for the Offline Store in AI Platform's Feature Store are encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). To verify, ensure that the `encryption_spec` field is configured for the Feature Store with an appropriate key name. If not configured, update the Feature Store settings to include encryption specifications using the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/featurestore/feature-stores
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis#section1.0
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_online_store_kms_encryptio_enabled
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store Online Store KMS Encryptio Enabled
  scope: aiplatform.featurestore.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure KMS Encryption Enabled for Feature Store Online Store
  rationale: Enabling KMS encryption for AI Feature Store Online Store protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, especially when handling personally identifiable information (PII) or proprietary machine learning features. Compliance with standards like GDPR or CCPA may require encryption to safeguard user data and prevent legal penalties.
  description: This rule checks if Key Management Service (KMS) encryption is enabled for the AI Platform Feature Store Online Store. Without KMS encryption, data at rest may be vulnerable to unauthorized access. To verify, ensure that the Feature Store's online store is configured to use a KMS key. Remediation involves setting up a KMS key in Google Cloud and configuring the Feature Store to utilize this key for encryption, providing an additional layer of security over default encryption.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/featurestore/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_private_networking_enforced
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store Private Networking Enforced
  scope: aiplatform.featurestore.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Feature Store
  rationale: Enforcing private networking for AI Feature Store minimizes exposure to public internet, thereby reducing the risk of unauthorized access and data breaches. This is critical for maintaining data privacy and meeting regulatory requirements such as GDPR and CCPA, which mandate stringent data protection measures. Private networking ensures that sensitive AI data is accessed only through secure, internal networks, thereby mitigating potential security threats.
  description: This rule checks whether AI Feature Store is configured to use private networking, preventing public IP access. Verify that the feature store is set to use Virtual Private Cloud (VPC) networks by checking the network configurations in the Google Cloud Console under AI Platform. To enforce private networking, configure VPC Service Controls and use private endpoints for communication. This ensures that all data traffic remains within Google's secure infrastructure.
  references:
  - https://cloud.google.com/ai-platform/documentation/feature-store/networking
  - https://cloud.google.com/vpc/docs/private-service-connect
  - 'CIS GCP Benchmark: Ensure that VPC Service Controls are used to restrict access'
  - 'NIST SP 800-53: AC-17 Remote Access'
  - 'PCI-DSS Requirement 1: Install and maintain a firewall configuration to protect cardholder data'
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.featurestore.ai_services_feature_store_rbac_least_privilege
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Feature Store RBAC Least Privilege
  scope: aiplatform.featurestore.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Platform Feature Store Roles
  rationale: Implementing least privilege for AI Platform Feature Store ensures that users have only the necessary permissions to perform their job functions, reducing the risk of unauthorized access and potential data breaches. This control is crucial for maintaining data integrity and confidentiality, especially in environments handling sensitive or proprietary data. Compliance with least privilege principles is often a mandate under frameworks like PCI-DSS and ISO 27001, which require strict access controls.
  description: This rule checks if the AI Platform Feature Store roles are configured to adhere to the principle of least privilege. It involves auditing permissions granted to users and service accounts, ensuring they align strictly with required operational tasks. Remediation includes reviewing IAM policies on Feature Store resources and removing or adjusting permissions that exceed the necessary scope of access. Verification can be done through the IAM section of the GCP Console or using gcloud CLI commands to list and review roles.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://cloud.google.com/ai-platform/docs/feature-store
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.aiplatform.featurestore.ai_services_vector_index_authn_required
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Vector Index Authn Required
  scope: aiplatform.featurestore.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Authn for AI Services Vector Index in Featurestore
  rationale: Requiring authentication for accessing AI Services Vector Index prevents unauthorized access to sensitive data and models, mitigating risks such as data leakage and unauthorized data manipulation. This is crucial for maintaining data integrity and compliance with regulations like GDPR, which mandates strict access controls on personal data. Ensuring authentication helps organizations protect intellectual property and maintain trust with stakeholders.
  description: This rule checks that authentication is required when accessing the AI Services Vector Index within a Featurestore in GCP's AI Platform. It verifies whether proper authentication mechanisms, such as OAuth 2.0 or service account tokens, are enforced. To remediate, configure the Featurestore to require authentication by setting appropriate IAM policies and ensuring all accessing services and users have the necessary credentials. Regularly audit access logs to ensure compliance and adjust access controls as necessary.
  references:
  - https://cloud.google.com/ai-platform
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.aiplatform.featurestore.ai_services_vector_index_encrypted_at_rest_cmek
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Vector Index Encrypted At Rest Cmek
  scope: aiplatform.featurestore.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Vector Index Encryption at Rest with CMEK
  rationale: Encrypting the AI Services Vector Index at rest using Customer-Managed Encryption Keys (CMEK) provides enhanced control over the encryption process. This practice mitigates the risk of unauthorized data access, aligns with regulatory compliance requirements, and ensures that sensitive data remains secure even if the underlying storage is compromised.
  description: This rule checks whether AI Platform Featurestore's Vector Index is encrypted at rest using CMEK. To verify, ensure that the Featurestore resource is configured with a customer-managed key rather than the default Google-managed keys. If not configured, update the Featurestore settings to specify a CMEK for encryption. This can be done through the GCP Console or CLI by setting the 'encryptionSpec.kmsKeyName' field to the desired Cloud KMS key.
  references:
  - https://cloud.google.com/vertex-ai/docs/featurestore/concepts
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/customer-managed-encryption-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library
- rule_id: gcp.aiplatform.featurestore.ai_services_vector_index_private_networking_enforced
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Vector Index Private Networking Enforced
  scope: aiplatform.featurestore.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Services Vector Index
  rationale: Enforcing private networking for AI Services Vector Index in GCP's AI Platform Feature Store minimizes exposure to the public internet, reducing the risk of unauthorized access and potential data breaches. This is critical for protecting sensitive data and maintaining compliance with regulations such as GDPR and HIPAA, which mandate stringent data protection measures.
  description: This rule checks if AI Services Vector Index in the AI Platform Feature Store is configured to use private networking, which restricts access to internal IPs only. To verify, ensure that the AI Platform Feature Store's network settings are configured to use VPC Service Controls or private Google access. Remediation involves updating the network configuration to disable public access and enable access only through a private network, ensuring compliance with best practices for network security.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/networking
  - https://cloud.google.com/vpc-service-controls/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.aiplatform.featurestore.ai_services_vector_index_rbac_tenant_isolation_enforced
  service: aiplatform
  resource: featurestore
  requirement: Ai Services Vector Index RBAC Tenant Isolation Enforced
  scope: aiplatform.featurestore.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Tenant Isolation in Feature Store RBAC for AI Services
  rationale: Enforcing tenant isolation in AI services vector index RBAC is crucial to prevent unauthorized access and data leakage between tenants. Without proper isolation, a compromised account or misconfiguration could expose sensitive data to unauthorized parties, increasing the risk of data breaches and violating data protection regulations such as GDPR and CCPA.
  description: This rule checks if Role-Based Access Control (RBAC) is configured to enforce tenant isolation in Google's AI Platform Feature Store. It ensures that each tenant has access only to their vector indexes, preventing unauthorized access to other tenants' data. Verify that roles and permissions are correctly assigned to maintain isolation. Remediation involves reviewing and updating IAM policies to enforce strict tenant boundaries.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/featurestore/overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.aiplatform.model.ai_services_model_artifact_encrypted_at_rest
  service: aiplatform
  resource: model
  requirement: Ai Services Model Artifact Encrypted At Rest
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Service Model Artifacts Are Encrypted at Rest
  rationale: Encrypting AI Service model artifacts at rest protects sensitive data from unauthorized access and breaches, crucial for maintaining data confidentiality and integrity. This is especially important in scenarios where models contain proprietary algorithms or sensitive training data. Compliance with regulations such as GDPR, HIPAA, and PCI-DSS often mandates encryption of sensitive data at rest, reducing the risk of financial and reputational damage.
  description: This rule checks that all AI Platform model artifacts are encrypted at rest using customer-managed encryption keys (CMEK) or Google-managed encryption keys. To verify, ensure that the encryption configuration is set in the AI Platform settings. Remediation involves configuring the AI Platform to use CMEK by specifying the Cloud Key Management Service (KMS) key during model creation or updating existing models to include encryption settings.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-111.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.ai_services_model_artifact_encrypted_at_rest_cmek
  service: aiplatform
  resource: model
  requirement: Ai Services Model Artifact Encrypted At Rest Cmek
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt AI Model Artifacts with CMEK in GCP AI Platform
  rationale: Encrypting AI model artifacts using Customer-Managed Encryption Keys (CMEK) ensures that organizations maintain control over cryptographic keys, reducing the risk of unauthorized access. This measure is crucial for protecting intellectual property and sensitive data, aligning with regulatory mandates like GDPR and HIPAA that require robust data protection practices.
  description: This rule checks if AI model artifacts in Google Cloud AI Platform are encrypted using Customer-Managed Encryption Keys (CMEK). To verify, ensure that model resources are configured with a user-specified KMS key. Remediation involves setting the AI Platform model to use a CMEK by updating the model's encryption settings via the Google Cloud Console or gcloud command-line tool. This enhances data protection by giving you full control over encryption keys.
  references:
  - https://cloud.google.com/ai-platform/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.model.ai_services_model_execution_role_least_privilege
  service: aiplatform
  resource: model
  requirement: Ai Services Model Execution Role Least Privilege
  scope: aiplatform.model.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Model Execution Roles
  rationale: Implementing least privilege for AI model execution roles minimizes the attack surface and reduces the risk of unauthorized access to sensitive resources. Over-privileged roles can lead to data breaches, service disruptions, and non-compliance with regulations such as GDPR and PCI-DSS, which mandate strict access controls to protect sensitive data.
  description: This rule checks whether AI model execution roles in Google Cloud's AI Platform are configured with the minimum necessary permissions. It verifies the role assignments to ensure they do not exceed the required permissions for executing AI models. Remediation involves auditing current role permissions, removing unnecessary access, and adhering strictly to the least privilege principle. To verify, review the IAM policy bindings for AI models and adjust roles using the GCP Console or gcloud CLI to ensure they are limited to execution-related permissions only.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/best-practices-for-using-iam
- rule_id: gcp.aiplatform.model.ai_services_model_image_scan_passed
  service: aiplatform
  resource: model
  requirement: Ai Services Model Image Scan Passed
  scope: aiplatform.model.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure AI Model Images Pass Security Scans
  rationale: Unscanned AI model images can harbor vulnerabilities that may be exploited by attackers, leading to unauthorized access or data breaches. Regular scanning of model images mitigates these risks by identifying and addressing vulnerabilities before deployment, ensuring compliance with security standards and protecting organizational assets.
  description: This rule verifies that AI model images in the Google Cloud AI Platform have passed security scans for vulnerabilities. It involves configuring automatic image scanning using Google Cloud's Container Analysis API and monitoring scan results. If vulnerabilities are detected, remediate them by updating the image or applying necessary patches before deploying the model. Regularly review and maintain scanning configurations to ensure new images are also scanned.
  references:
  - https://cloud.google.com/container-analysis/docs/overview
  - https://cloud.google.com/security-command-center/docs/posture-management-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/ai-platform/training/docs/using-containers
- rule_id: gcp.aiplatform.model.ai_services_model_kms_cmk_used
  service: aiplatform
  resource: model
  requirement: Ai Services Model KMS CMK Used
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Models Use Customer-Managed Encryption Keys
  rationale: Utilizing Customer-Managed Encryption Keys (CMKs) for AI models ensures that sensitive data used in AI processing is protected and under the control of your organization. This reduces the risk of unauthorized access or data breaches by providing granular control over encryption keys and meeting regulatory requirements for data protection, such as GDPR and HIPAA.
  description: This rule verifies that AI models in Google Cloud AI Platform are encrypted using Customer-Managed Encryption Keys (CMKs) instead of Google-managed keys. To ensure compliance, check that all AI models are configured to use a KMS CMK by specifying the 'kmsKeyName' during model deployment. Remediate by updating model configurations to use a CMK, which can be done via the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/encryption-at-rest
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/itl/nist-special-publication-800-57
- rule_id: gcp.aiplatform.model.ai_services_model_registry_access_rbac_least_privilege
  service: aiplatform
  resource: model
  requirement: Ai Services Model Registry Access RBAC Least Privilege
  scope: aiplatform.model.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Model Registry Access
  rationale: Implementing least privilege for AI Model Registry access minimizes the risk of unauthorized access, which can lead to data breaches, intellectual property theft, and compliance violations. Limiting access strictly to necessary roles reduces the attack surface and aligns with regulatory requirements for data protection and privacy, such as GDPR and PCI-DSS.
  description: This rule verifies that AI Model Registry access in Google Cloud Platform is configured according to the principle of least privilege. It checks that only specific roles with necessary permissions are granted access to the model registry, preventing over-privileged identities from making unauthorized modifications. Remediation involves auditing current access roles, revoking unnecessary permissions, and ensuring that roles like 'Viewer' or 'Editor' are only assigned to users with a clear business need.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/ai-platform/training/docs/using-gpus
- rule_id: gcp.aiplatform.model.ai_services_model_version_image_scan_passed
  service: aiplatform
  resource: model
  requirement: Ai Services Model Version Image Scan Passed
  scope: aiplatform.model.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure AI Model Image Scans Pass Security Checks
  rationale: Unscanned or vulnerable images in AI models can introduce security risks, such as unauthorized code execution or data breaches, potentially leading to financial loss or reputational damage. Ensuring images pass security scans helps mitigate these risks by identifying vulnerabilities before deployment, aligning with compliance standards like PCI-DSS and ISO 27001, and supporting secure AI service operations.
  description: This rule checks if AI Platform model versions have their container images scanned for vulnerabilities, ensuring any detected issues are addressed before deployment. To verify, ensure that the image scanning feature is enabled and configured to block deployments of images with high-severity vulnerabilities. Remediation involves configuring automatic scans and reviewing scan results regularly, addressing any critical or high vulnerabilities immediately.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/container-analysis/docs/container-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-207/final
- rule_id: gcp.aiplatform.model.ai_services_model_version_kms_encryption_enabled
  service: aiplatform
  resource: model
  requirement: Ai Services Model Version KMS Encryption Enabled
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Model Versions
  rationale: Encrypting AI model versions with Customer-Managed Encryption Keys (CMEK) protects sensitive AI data from unauthorized access and ensures compliance with regulatory frameworks like GDPR and HIPAA. Without proper encryption, AI models may be vulnerable to data breaches, risking intellectual property theft and regulatory fines.
  description: This rule checks if AI Platform model versions are encrypted using Google Cloud Key Management Service (KMS). To verify, ensure that the 'kmsKeyName' attribute is specified in the model's configuration. If not set, update the model configuration to include a KMS key. This enforces encryption at rest using customer-managed keys, enhancing data security and control.
  references:
  - https://cloud.google.com/ai-platform/prediction/docs/custom-service-account
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.model.ai_services_model_version_package_approved
  service: aiplatform
  resource: model
  requirement: Ai Services Model Version Package Approved
  scope: aiplatform.model.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Approve AI Model Versions in GCP AI Platform
  rationale: Approving AI model versions ensures that only vetted and secure model packages are deployed, reducing the risk of introducing vulnerabilities or unauthorized changes. This supports compliance with regulatory standards and protects sensitive data processed by AI models from potential exploitation and breaches.
  description: This check verifies that all AI model versions deployed in the Google Cloud AI Platform have been approved through a defined security review process. The configuration should include a mechanism for model version approval before deployment, ensuring that each model version package meets security and compliance standards. Administrators can remediate by implementing an approval workflow and ensuring that only approved versions are deployed. Verification involves reviewing audit logs for model deployments to confirm compliance with approval processes.
  references:
  - https://cloud.google.com/ai-platform/docs/model-management
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/frameworks/cis#cis-benchmarks
  - https://cloud.google.com/security/compliance/frameworks/iso-27001
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/security/overview
- rule_id: gcp.aiplatform.model.ai_services_supply_chain_model_artifact_image_scann_critical
  service: aiplatform
  resource: model
  requirement: Ai Services Supply Chain Model Artifact Image Scann Critical
  scope: aiplatform.model.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Critical Vulnerability Scanning for AI Model Artifacts
  rationale: Scanning AI model artifacts for critical vulnerabilities is essential to prevent the deployment of compromised software which could lead to data breaches or unauthorized access. Neglecting this can result in significant business risks, including loss of customer trust and potential non-compliance with regulatory standards that mandate secure software supply chains.
  description: This rule checks whether AI model artifacts in Google Cloud AI Platform undergo critical vulnerability scanning to identify and mitigate risks before deployment. Ensure that Artifact Registry or Container Analysis is configured to automatically scan for vulnerabilities in model artifacts. If critical vulnerabilities are found, remediation involves updating the artifact with a secure version or applying security patches. Regular monitoring and scanning are crucial to maintain a robust security posture.
  references:
  - https://cloud.google.com/container-analysis/docs
  - https://cloud.google.com/artifact-registry/docs/enable-vulnerability-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/architecture/implementing-devops/security
- rule_id: gcp.aiplatform.model.ai_services_supply_chain_model_artifact_sbom_present
  service: aiplatform
  resource: model
  requirement: Ai Services Supply Chain Model Artifact Sbom Present
  scope: aiplatform.model.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure SBOM Present for AI Services Supply Chain Model Artifacts
  rationale: Having a Software Bill of Materials (SBOM) for AI models is critical for transparency and security in the AI supply chain. It helps identify components and dependencies, mitigating risks of vulnerabilities and compliance failures due to unverified third-party software. This practice supports regulatory requirements and enhances trust in AI deployments by providing a clear understanding of software components used.
  description: This rule checks that every AI model artifact in the AI Platform has an associated SBOM. An SBOM provides a detailed inventory of all software components, including versioning and licensing information. To verify, ensure the SBOM is generated and stored alongside the model artifacts in your repository. Regularly update the SBOM as part of your CI/CD pipeline to reflect any changes. Remediation involves generating an SBOM using tools like Syft or CycloneDX and ensuring it's linked to the model artifact.
  references:
  - https://cloud.google.com/vertex-ai/docs/training-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/news-events/news/2021/05/nist-releases-software-supply-chain-security-guidance
  - https://www.iso.org/iso-27001-information-security.html
  - https://owasp.org/www-project-cyclonedx/
  - https://slsa.dev/spec/v0.1/
- rule_id: gcp.aiplatform.model.ai_services_supply_chain_model_artifact_signed_and_verified
  service: aiplatform
  resource: model
  requirement: Ai Services Supply Chain Model Artifact Signed And Verified
  scope: aiplatform.model.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Verify AI Model Artifacts with Signed and Trusted Certificates
  rationale: Ensuring that AI model artifacts are signed and verified protects against the introduction of malicious or tampered code, which can lead to unauthorized data access or manipulation. This practice is essential for maintaining the integrity of AI systems, meeting compliance requirements, and safeguarding business operations from potential threats.
  description: This rule checks that AI model artifacts in Google Cloud's AI Platform are signed with a trusted certificate and verified before deployment. To verify, ensure that all model artifacts have been signed using a certificate from a trusted Certificate Authority (CA) and that the verification process is in place to check the signatures before model execution. Remediation involves configuring the AI Platform to enforce signature verification before loading models, using tools like Binary Authorization for verification workflows.
  references:
  - https://cloud.google.com/ai-platform/deep-learning-vm/docs/security-best-practices
  - https://cloud.google.com/binary-authorization/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.data_governance_package_artifact_signed_and_verified
  service: aiplatform
  resource: model
  requirement: Data Governance Package Artifact Signed And Verified
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Model Artifacts Are Signed and Verified
  rationale: Signing and verifying AI model artifacts is crucial to ensure the integrity and authenticity of the models being deployed. Unsigned or improperly verified artifacts pose risks such as model tampering, which can lead to incorrect predictions, data leaks, or unauthorized access. Compliance with data protection regulations like GDPR and industry standards necessitates proper data governance and artifact management.
  description: This rule checks that AI Platform model artifacts are signed and verified before deployment. To achieve this, configure your models to include a digital signature using a secure key management system. Verify these signatures upon deployment to ensure they have not been altered. Remediation involves implementing artifact signing processes and using Cloud KMS for secure key management. Regular audits should be conducted to ensure compliance with this policy.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.model.data_governance_package_encrypted
  service: aiplatform
  resource: model
  requirement: Data Governance Package Encrypted
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Models Use Encrypted Data Governance Packages
  rationale: Encrypting data governance packages for AI Platform models is crucial to protect sensitive information from unauthorized access and data breaches. This practice mitigates risks associated with data leakage and ensures compliance with data protection regulations like GDPR and CCPA, reducing potential legal and financial penalties.
  description: This rule checks if the data governance packages for AI Platform models are encrypted using Google Cloud's Customer-Managed Encryption Keys (CMEK). To verify compliance, ensure that the AI Platform model resources specify an encryption key from Cloud Key Management Service (KMS). Remediation involves setting up a CMEK in Cloud KMS and configuring the model to use this key during deployment.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/data-governance
  - https://cloud.google.com/security/compliance/cis#cis-google-cloud-computing-foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.model.data_governance_package_not_publicly_shared
  service: aiplatform
  resource: model
  requirement: Data Governance Package Not Publicly Shared
  scope: aiplatform.model.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure AI Platform Model Data Governance Package is Not Publicly Shared
  rationale: Publicly sharing AI model data governance packages can lead to unauthorized access, data breaches, and exploitation of sensitive information. This exposure can result in financial loss, reputational damage, and non-compliance with privacy regulations such as GDPR and CCPA, undermining trust and operational integrity.
  description: This rule checks the access permissions of AI Platform model data governance packages to ensure they are not publicly accessible. It verifies that IAM policies restrict access only to authorized users and groups. If public sharing is detected, it is critical to immediately revise IAM settings to limit access. Remediation involves removing 'allUsers' or 'allAuthenticatedUsers' from the IAM policy bindings of the model resource.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/permissions-ai-platform
  - https://cloud.google.com/security-command-center/docs/how-to-remediate-security-findings
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.model.data_governance_registry_artifact_encryption_enabled
  service: aiplatform
  resource: model
  requirement: Data Governance Registry Artifact Encryption Enabled
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption of AI Platform Model Artifacts
  rationale: Enabling encryption for AI Platform model artifacts protects sensitive data from unauthorized access and potential data breaches. This is critical for maintaining data integrity and confidentiality, especially when dealing with proprietary algorithms and datasets. Compliance with regulations such as GDPR, HIPAA, and CCPA mandates robust data protection measures to safeguard user data and intellectual property.
  description: This rule checks whether encryption is enabled for artifacts stored in the AI Platform's Data Governance Registry. It verifies that the 'encryptionSpec' field is configured with a customer-managed encryption key (CMEK). To ensure compliance, navigate to the AI Platform model settings in the Google Cloud Console and configure the 'encryptionSpec' using a CMEK. This enhances data security by allowing users to control the encryption keys used for their data.
  references:
  - https://cloud.google.com/ai-platform/docs/general/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57pt1r4.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.aiplatform.model.data_governance_registry_cross_account_sharing_restricted
  service: aiplatform
  resource: model
  requirement: Data Governance Registry Cross Account Sharing Restricted
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Sharing in AI Platform Models
  rationale: Restricting cross-account sharing of AI models in GCP mitigates the risk of unauthorized access and data breaches, thereby protecting sensitive data and intellectual property. This practice is crucial to maintaining data privacy and meeting compliance requirements such as GDPR and CCPA, which mandate stringent data protection measures.
  description: This rule checks if AI Platform models are configured to prevent cross-account sharing, ensuring that access is limited to authorized accounts only. To verify, review the IAM policies associated with the AI model and ensure no permissions allow cross-account access. Remediation involves updating IAM policies to restrict sharing to within the organization or specific trusted accounts only.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.aiplatform.model.data_governance_registry_rbac_least_privilege
  service: aiplatform
  resource: model
  requirement: Data Governance Registry RBAC Least Privilege
  scope: aiplatform.model.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Platform Model Uses Least Privilege RBAC
  rationale: Implementing least privilege RBAC in the AI Platform models minimizes the risk of unauthorized access, which can lead to data breaches or malicious manipulation of machine learning models. This helps in maintaining data integrity, complying with regulatory standards such as GDPR and CCPA, and protecting sensitive intellectual property.
  description: This rule checks if the AI Platform model employs least privilege principles by verifying the role assignments in the Data Governance Registry. Ensure that only necessary roles with minimal permissions are granted to users and service accounts. To remediate, review and adjust IAM policies to remove excessive permissions while ensuring operational needs are met.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.data_privacy_package_artifact_signed_and_verified
  service: aiplatform
  resource: model
  requirement: Data Privacy Package Artifact Signed And Verified
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Model Artifacts are Signed and Verified
  rationale: Ensuring that AI Platform model artifacts are signed and verified is crucial for maintaining data integrity and preventing unauthorized modifications. Unsigned or unverified artifacts pose a risk of data breaches, intellectual property theft, and non-compliance with data protection regulations such as GDPR and HIPAA. This practice helps protect sensitive data and intellectual property while supporting trust in AI models.
  description: This rule checks that all data privacy package artifacts for AI Platform models are signed and verified to ensure authenticity and integrity. Organizations should leverage GCP's Artifact Registry to sign artifacts and use Binary Authorization to verify signatures before deployment. Regularly audit artifact signing processes and implement automated alerts for unsigned or tampered artifacts to mitigate risks. Remediation involves signing all unsigned artifacts and configuring verification policies.
  references:
  - https://cloud.google.com/ai-platform/docs
  - https://cloud.google.com/artifact-registry/docs/signing
  - https://cloud.google.com/binary-authorization/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.model.data_privacy_package_encrypted
  service: aiplatform
  resource: model
  requirement: Data Privacy Package Encrypted
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Model Data Privacy Package is Encrypted
  rationale: Encrypting data privacy packages for AI models mitigates the risk of unauthorized access to sensitive data, which can result in data breaches and compliance violations. This measure is crucial for protecting intellectual property and maintaining trust with customers and stakeholders. Non-compliance with encryption standards can lead to severe regulatory penalties and reputational damage.
  description: This rule checks whether the data privacy packages associated with AI Platform models are encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that the 'encryption_spec' field is specified in the model's configuration settings, which should point to a valid Cloud KMS key. Remediation involves updating the model configuration to include an appropriate encryption key for securing the data privacy package.
  references:
  - https://cloud.google.com/ai-platform/training/docs/model-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.aiplatform.model.data_privacy_package_not_publicly_shared
  service: aiplatform
  resource: model
  requirement: Data Privacy Package Not Publicly Shared
  scope: aiplatform.model.public_access
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: critical
  title: Ensure AI Platform Model Data Privacy Packages Not Publicly Shared
  rationale: Publicly sharing AI model data privacy packages can lead to unauthorized access to sensitive data, increasing the risk of data breaches and non-compliance with data protection regulations such as GDPR and CCPA. This could expose the organization to significant legal liabilities and reputational damage.
  description: This rule checks whether the data privacy packages associated with AI Platform models are publicly shared. Ensuring these packages are not publicly accessible is crucial for safeguarding sensitive data. Administrators should verify that IAM policies are correctly configured to restrict public access and apply encryption at rest to protect data integrity. Remediation involves reviewing and updating access controls and encryption settings in the GCP Console under the AI Platform's model permissions.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.model.data_privacy_registry_artifact_encryption_enabled
  service: aiplatform
  resource: model
  requirement: Data Privacy Registry Artifact Encryption Enabled
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure AI Platform Model Registry Artifacts are Encrypted
  rationale: Encrypting registry artifacts in AI Platform is crucial to protect sensitive data from unauthorized access and potential breaches, especially given the valuable nature of ML models and training datasets. It helps in mitigating risks of data leakage and loss, thus preserving intellectual property and maintaining regulatory compliance with standards like GDPR and HIPAA.
  description: This rule checks if encryption is enabled for data artifacts stored within the AI Platform's model registry. To verify, ensure that the AI Platform models utilize Customer-Managed Encryption Keys (CMEK) for data protection at rest. Remediation involves configuring the model registry to use CMEK by specifying a valid key from Cloud Key Management Service (KMS) during model creation or update.
  references:
  - https://cloud.google.com/ai-platform/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.data_privacy_registry_cross_account_sharing_restricted
  service: aiplatform
  resource: model
  requirement: Data Privacy Registry Cross Account Sharing Restricted
  scope: aiplatform.model.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict AI Model Cross-Account Sharing for Data Privacy
  rationale: Restricting cross-account sharing of AI models is crucial for maintaining data privacy and preventing unauthorized access to sensitive information. This control mitigates risks such as data breaches and intellectual property theft, which can lead to significant financial and reputational damage. Ensuring compliance with privacy regulations like GDPR and CCPA is essential to avoid legal penalties and maintain trust with stakeholders.
  description: This rule checks whether AI model resources in Google Cloud's AI Platform are configured to prevent cross-account sharing. It ensures that models are not shared with accounts outside the designated organization without explicit authorization. To verify, review the sharing settings of AI models and ensure they are not accessible to unauthorized external accounts. Remediation involves adjusting IAM policies to restrict model access to trusted accounts only.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.aiplatform.model.data_privacy_registry_rbac_least_privilege
  service: aiplatform
  resource: model
  requirement: Data Privacy Registry RBAC Least Privilege
  scope: aiplatform.model.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Model RBAC Follows Least Privilege Principle
  rationale: Applying the least privilege principle to AI model access controls minimizes the risk of unauthorized data access, which could lead to data breaches and non-compliance with data protection regulations. Improper access management can result in sensitive data exposure and potential financial and reputational damage.
  description: This rule checks that roles and permissions assigned to users accessing AI models are limited to the minimum necessary for their job functions, adhering to the least privilege principle. Verification involves reviewing IAM policies and ensuring that roles such as 'Viewer', 'Editor', and 'Owner' are not excessively granted. Remediation includes auditing IAM roles, removing unnecessary permissions, and employing custom roles for specific needs.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.aiplatform.model.machine_learning_package_artifact_signed_and_verified
  service: aiplatform
  resource: model
  requirement: Machine Learning Package Artifact Signed And Verified
  scope: aiplatform.model.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure ML Artifacts are Signed and Verified
  rationale: Ensuring that machine learning package artifacts are signed and verified mitigates the risk of deploying malicious code, which can lead to data breaches, model corruption, and business disruption. This practice is crucial for maintaining integrity and trust in AI models, aligning with compliance standards such as SOC2 and ISO 27001.
  description: This rule checks whether all machine learning package artifacts in the AI Platform are signed and verified before deployment. Verification ensures that only authorized and untampered artifacts are deployed, preventing potential security breaches. To remediate, ensure the signing process is integrated into the CI/CD pipeline and that verification occurs before deployment. Use GCP's Binary Authorization to enforce this policy.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/general/security-overview
  - https://cloud.google.com/binary-authorization/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.model.machine_learning_package_encrypted
  service: aiplatform
  resource: model
  requirement: Machine Learning Package Encrypted
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Packages in AI Platform are Encrypted at Rest
  rationale: Encrypting machine learning packages at rest in AI Platform protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining the confidentiality and integrity of data, especially in regulated industries where compliance with standards such as HIPAA and PCI-DSS is mandatory. Encryption helps mitigate risks related to data exposure in case of unauthorized access to storage systems.
  description: This rule checks whether machine learning packages stored in AI Platform are encrypted using Google-managed or customer-managed encryption keys. To verify, ensure that the AI Platform models have encryption configurations enabled in their settings. Remediation involves configuring the AI Platform to use Google Cloud Key Management Service (KMS) for encrypting data at rest by specifying a customer-managed key during model creation or update.
  references:
  - https://cloud.google.com/ai-platform/docs/model-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption/
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.aiplatform.model.machine_learning_package_not_publicly_shared
  service: aiplatform
  resource: model
  requirement: Machine Learning Package Not Publicly Shared
  scope: aiplatform.model.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Sharing of AI Platform Models
  rationale: Publicly shared machine learning models can expose sensitive algorithms and data, leading to intellectual property theft or misuse. Unauthorized access can result in model tampering or data leaks, violating compliance with frameworks like GDPR and HIPAA, and potentially causing reputational and financial damage.
  description: This rule checks if AI Platform models are publicly accessible, which could lead to unauthorized use or data breaches. Ensure that model IAM policies do not grant 'allUsers' or 'allAuthenticatedUsers' roles. Remediation involves reviewing model access policies and restricting permissions to specified users or service accounts only. Verify settings using the Google Cloud Console or CLI, and update policies to comply with the principle of least privilege.
  references:
  - https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/iam/docs/overview
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.aiplatform.model.machine_learning_registry_artifact_encryption_enabled
  service: aiplatform
  resource: model
  requirement: Machine Learning Registry Artifact Encryption Enabled
  scope: aiplatform.model.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption for AI Platform Model Artifacts
  rationale: Encrypting machine learning registry artifacts helps protect sensitive model data from unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality, meeting legal and regulatory requirements such as GDPR or HIPAA, and preserving the integrity of AI models used in critical business processes.
  description: This rule verifies that all machine learning model artifacts stored in Google Cloud AI Platform are encrypted at rest using customer-managed encryption keys (CMEK). To comply, ensure that each model in the AI Platform has encryption enabled with a specified CMEK in the model's settings. This can be validated by checking the encryption configuration in the Google Cloud Console under AI Platform settings or via the gcloud command-line tool. To remediate, configure the AI Platform model to use a CMEK through the Google Cloud Console or update the model configuration via the AI Platform API.
  references:
  - https://cloud.google.com/ai-platform/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.aiplatform.model.machine_learning_registry_cross_account_sharing_restricted
  service: aiplatform
  resource: model
  requirement: Machine Learning Registry Cross Account Sharing Restricted
  scope: aiplatform.model.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Restrict Cross-Account Sharing in AI Platform Model Registry
  rationale: Restricting cross-account sharing in the AI Platform Model Registry is essential to prevent unauthorized access to machine learning models, which may contain sensitive data or proprietary algorithms. Unrestricted sharing can lead to data breaches, intellectual property theft, and compliance violations, such as failing to adhere to GDPR or CCPA privacy regulations.
  description: This rule checks that machine learning models in Google Cloud's AI Platform Model Registry are not shared across accounts unless explicitly authorized. To verify, review the sharing settings of your AI models to ensure they are not publicly accessible or shared with unauthorized accounts. Remediation involves setting appropriate IAM policies to limit access to trusted users within the same organization and auditing access logs regularly.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.model.machine_learning_registry_rbac_least_privilege
  service: aiplatform
  resource: model
  requirement: Machine Learning Registry RBAC Least Privilege
  scope: aiplatform.model.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce RBAC Least Privilege for AI Platform Models
  rationale: Implementing least privilege in ML registry access minimizes the risk of unauthorized data exposure and model manipulation, which could lead to operational disruptions or regulatory non-compliance. Misconfigured permissions can result in data breaches, model theft, or biased model training, impacting business reputation and financial stability.
  description: This rule checks if permissions assigned to users or service accounts accessing AI Platform models adhere to the principle of least privilege. Users should only have the minimum roles necessary to perform their tasks, such as Viewer or Editor, rather than Owner. Verification involves auditing IAM policies associated with AI Platform models for overly permissive roles. Remediation includes adjusting IAM roles to limit access based on role-specific duties and regularly reviewing access logs for anomalies.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.notebook.ai_services_secrets_vault_access_rbac_least_privilege
  service: aiplatform
  resource: notebook
  requirement: Ai Services Secrets Vault Access RBAC Least Privilege
  scope: aiplatform.notebook.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Notebook Vault Access in AI Platform
  rationale: Implementing least privilege for access to AI Services Secrets Vault ensures that only authorized users and services can access sensitive secrets, minimizing the risk of data breaches. This control is critical for protecting intellectual property and maintaining compliance with data protection regulations like GDPR, HIPAA, and others, as unauthorized access could lead to significant financial and reputational harm.
  description: This rule verifies that access to the AI Services Secrets Vault from AI Platform notebooks is restricted to only those roles and identities that absolutely require it. Users should configure Identity and Access Management (IAM) roles to grant the minimum permissions necessary to perform their intended functions. To comply, audit current access policies using the GCP Console or CLI, ensuring that only essential service accounts and users have roles such as 'Secret Manager Secret Accessor'. Revoke any unnecessary permissions immediately.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.aiplatform.notebook.ai_services_secrets_vault_kms_encryption_and_rotatio_enabled
  service: aiplatform
  resource: notebook
  requirement: Ai Services Secrets Vault KMS Encryption And Rotatio Enabled
  scope: aiplatform.notebook.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption & Rotation for AI Notebook Secrets
  rationale: Using Cloud KMS for encrypting secrets in AI Platform notebooks enhances data security by protecting sensitive information against unauthorized access and potential data breaches. Regular key rotation further mitigates risks by ensuring that encryption keys are periodically changed, aligning with compliance standards like NIST and PCI-DSS, and thereby reducing the risk of key compromise.
  description: This rule checks if AI Platform notebook instances use Cloud KMS for encrypting secrets stored in the AI Services Secrets Vault and if key rotation is enabled. Verify that the notebooks have KMS keys configured for encryption, and key rotation periods are set in accordance with security policies. To remediate, configure AI Platform notebooks to use KMS keys by navigating to the notebook instance settings and specifying a KMS key, ensuring that automatic key rotation is enabled in the Cloud KMS settings.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/security
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-risk-management-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.notebook.ai_services_secrets_vault_no_plaintext_secrets_in_config
  service: aiplatform
  resource: notebook
  requirement: Ai Services Secrets Vault No Plaintext Secrets In Config
  scope: aiplatform.notebook.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure No Plaintext Secrets in AI Platform Notebook Configurations
  rationale: Storing plaintext secrets in AI Platform notebooks poses a significant security risk by potentially exposing sensitive data to unauthorized access. This can lead to data breaches, intellectual property theft, and non-compliance with regulatory frameworks such as GDPR, HIPAA, or PCI-DSS, which mandate the protection of sensitive information. Ensuring secrets are stored securely mitigates these risks and upholds data integrity and confidentiality.
  description: This rule checks for the presence of plaintext secrets in the configuration of AI Platform notebooks, ensuring that all secrets are stored securely using a dedicated secret management service such as Google Cloud Secret Manager. To verify compliance, inspect notebook configurations for hardcoded secrets and replace them with references to the secret management solution. Remediation involves migrating any plaintext secrets found to the secret manager and updating the notebook configuration to access secrets programmatically.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/best-practices-security
  - https://cloud.google.com/secret-manager/docs
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 6.4
  - NIST SP 800-53 Rev. 5 - System and Communications Protection
  - 'PCI DSS v3.2.1 - Requirement 3: Protect Stored Cardholder Data'
  - 'ISO/IEC 27001:2013 - Annex A.10: Cryptography'
- rule_id: gcp.aiplatform.notebook.ai_services_workspace_encryption_at_rest_enabled
  service: aiplatform
  resource: notebook
  requirement: Ai Services Workspace Encryption At Rest Enabled
  scope: aiplatform.notebook.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure AI Notebook Workspace Encryption at Rest is Enabled
  rationale: Enabling encryption at rest for AI Notebook workspaces protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining confidentiality and integrity, especially in environments where sensitive or proprietary data is processed, supporting compliance with regulations like GDPR and HIPAA.
  description: This rule checks whether encryption at rest is enabled for AI Notebook workspaces within Google Cloud's AI Platform. The configuration ensures that all data stored within the workspace is encrypted using Google-managed encryption keys. To verify, navigate to the AI Platform Notebooks in the GCP Console and ensure encryption settings are configured. If not enabled, update the notebook instance settings to use encryption at rest, leveraging Google-managed keys or customer-managed keys for enhanced security.
  references:
  - https://cloud.google.com/ai-platform-notebooks/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.aiplatform.notebook.ai_services_workspace_iam_role_least_privileged
  service: aiplatform
  resource: notebook
  requirement: Ai Services Workspace IAM Role Least Privileged
  scope: aiplatform.notebook.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privileged IAM Roles for AI Notebook Workspaces
  rationale: Applying the principle of least privilege to IAM roles for AI Notebook workspaces minimizes the risk of unauthorized access, data leakage, and potential misuse of AI resources. This is crucial for maintaining a secure environment, protecting sensitive data, and adhering to compliance standards such as PCI-DSS, HIPAA, and ISO 27001.
  description: This rule verifies that IAM roles assigned to AI Notebook workspaces are granted only the permissions necessary for their function. It checks for overly permissive roles and suggests adjustments to align with the least privilege principle. Remediation involves reviewing assigned roles, identifying excessive permissions, and modifying or creating custom roles with restricted privileges. Ensure only essential permissions are granted by using Google Cloud IAM's predefined or custom roles appropriately.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
  - https://cloud.google.com/iam/docs/roles-best-practices
- rule_id: gcp.aiplatform.notebook.ai_services_workspace_idle_shutdown_configured
  service: aiplatform
  resource: notebook
  requirement: Ai Services Workspace Idle Shutdown Configured
  scope: aiplatform.notebook.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure AI Services Workspace Idle Shutdown is Configured
  rationale: Configuring idle shutdown for AI Services Workspaces helps optimize resource usage and reduce costs by automatically powering down idle resources. This minimizes the attack surface by ensuring resources are not left running unnecessarily, which could be exploited by attackers. It also supports compliance with cost management and operational efficiency standards.
  description: This check verifies that AI Services Workspaces in GCP have idle shutdown configured. Idle shutdown automatically shuts down workspaces after a specified period of inactivity, which can be configured in the GCP Console under the AI Platform Notebook settings. To verify, navigate to the AI Platform Notebooks section and ensure that the 'Idle Shutdown' option is enabled with an appropriate time limit set. Remediation involves enabling this setting and selecting a suitable time interval to balance resource availability with security and cost considerations.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/monitoring-and-logging#idle_shutdown
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-using-google-cloud-identity-and-access-management
  - https://cloud.google.com/blog/products/data-analytics/reducing-ai-ml-costs-with-gcp
- rule_id: gcp.aiplatform.notebook.ai_services_workspace_private_networking_enforced
  service: aiplatform
  resource: notebook
  requirement: Ai Services Workspace Private Networking Enforced
  scope: aiplatform.notebook.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Services Notebooks
  rationale: Enforcing private networking for AI Services notebooks reduces the attack surface by preventing exposure to the public internet. This minimizes the risk of unauthorized access, data breaches, and ensures compliance with regulations that mandate secure data handling, such as HIPAA and GDPR. It is crucial for protecting sensitive machine learning models and data.
  description: This rule checks if AI Services notebooks are configured to use private networking, ensuring that they are only accessible within a VPC. To verify, ensure that the notebooks have the 'network' attribute set to a valid VPC network. Remediation involves updating the notebook configuration to specify a private VPC network, which can be done through the Google Cloud Console or gcloud CLI. This configuration enhances security by limiting access to internal IPs only.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/configure-private-ip
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.8
  - https://cloud.google.com/architecture/best-practices-vpc-design
  - NIST SP 800-53 Rev. 5, AC-17 Remote Access
  - ISO/IEC 27001:2013 A.13.1 Network Security Management
- rule_id: gcp.aiplatform.pipeline_job.ai_services_agent_orchestrator_data_exfiltration_po_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Agent Orchestrator Data Exfiltration Po Enforced
  scope: aiplatform.pipeline_job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Services Agent Orchestrator Prevents Data Exfiltration
  rationale: Preventing data exfiltration in AI services is crucial to protect sensitive data and comply with data protection regulations. Uncontrolled data flow can lead to unauthorized data access, violating privacy laws such as GDPR and impacting organizational reputation and trust.
  description: This rule checks if the AI Services Agent Orchestrator within AI Platform Pipeline Jobs is configured to prevent data exfiltration. Ensure the orchestration policies enforce data encryption and restrict network access to authorized endpoints only. Verify compliance by reviewing the pipeline job's configuration in the GCP console and apply necessary IAM policies and VPC Service Controls to mitigate exfiltration risks.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security-overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.aiplatform.pipeline_job.ai_services_agent_orchestrator_secrets_isolation_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Agent Orchestrator Secrets Isolation Enabled
  scope: aiplatform.pipeline_job.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure AI Services Agent Orchestrator Secrets Isolation
  rationale: Isolating orchestrator secrets in AI Platform ensures that sensitive credentials and keys are not inadvertently exposed or misused, mitigating the risk of unauthorized access and potential data breaches. This practice is crucial for maintaining the confidentiality and integrity of operations, especially in environments where multiple AI models and pipelines are managed. It also aligns with compliance requirements such as PCI-DSS and HIPAA, which mandate stringent controls on sensitive data handling.
  description: This rule checks if AI Services Agent Orchestrator secrets are isolated in AI Platform Pipeline Jobs by verifying specific configuration settings that separate and protect sensitive information. To enable secrets isolation, users must configure their pipeline jobs to use dedicated secret management tools, such as Secret Manager, and ensure that secrets are not hardcoded or stored in plaintext. Remediation involves auditing existing pipeline configurations, updating them to utilize secure secret storage, and enforcing access controls. Verification can be performed by reviewing pipeline job configurations for compliance with these practices.
  references:
  - https://cloud.google.com/ai-platform/docs/pipelines/secrets
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/iam/docs
- rule_id: gcp.aiplatform.pipeline_job.ai_services_agent_orchestrator_tool_use_allowlist_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Agent Orchestrator Tool Use Allowlist Enforced
  scope: aiplatform.pipeline_job.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce AI Services Agent Orchestrator Tool Allowlist
  rationale: Enforcing an allowlist for AI Services Agent Orchestrator tools helps mitigate security risks by ensuring only authorized tools can orchestrate AI workloads, reducing the attack surface. This approach is crucial for maintaining data integrity and preventing unauthorized access or manipulation of AI pipelines, aligning with compliance requirements like SOC2 and HIPAA.
  description: This rule checks whether an allowlist is enforced for tools used by the AI Services Agent Orchestrator in GCP AI Platform pipeline jobs. To verify, ensure that the allowlist is configured in the IAM policies, restricting tool access to only those explicitly approved. Remediation involves updating IAM policies to include a specific list of permitted tools, thereby preventing unauthorized orchestration activities.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.pipeline_job.ai_services_data_pipeline_access_controls_least_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Data Pipeline Access Controls Least Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Pipeline Job Access
  rationale: Implementing least privilege access for AI Platform pipeline jobs minimizes the risk of unauthorized data access and potential data breaches. Over-permissioned roles can lead to exploitation by malicious actors, impacting business operations and violating compliance standards such as GDPR and CCPA.
  description: This rule verifies that AI Platform pipeline jobs are configured with the least privilege necessary, ensuring that service accounts and users have only the permissions required to perform their tasks. To verify compliance, review the IAM roles assigned to the pipeline jobs and ensure they align with the principle of least privilege. Remediation involves adjusting IAM policies to limit permissions, removing unnecessary roles, and applying custom roles where appropriate.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/security-best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/enterprise/best-practices-for-identity-and-access-management
- rule_id: gcp.aiplatform.pipeline_job.ai_services_data_pipeline_private_networking_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Data Pipeline Private Networking Enforced
  scope: aiplatform.pipeline_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Data Pipelines
  rationale: Enforcing private networking for AI services data pipelines helps protect sensitive data from exposure to the public internet, reducing the risk of unauthorized access and data breaches. This is particularly important for organizations handling proprietary or regulated data, as it supports compliance with industry standards and regulations while ensuring the integrity and confidentiality of data during processing.
  description: This rule checks that AI Platform pipeline jobs are configured to use private networking, which restricts network access to authorized internal IPs only. To verify compliance, inspect the pipeline job configuration to ensure that private networking is enabled, which typically involves setting up a VPC with appropriate firewall rules. Remediation involves modifying pipeline configurations to route traffic through a Virtual Private Cloud (VPC) and applying necessary firewall rules to allow only trusted sources.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/configure-private-networking
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations-vpc-design
- rule_id: gcp.aiplatform.pipeline_job.ai_services_pipeline_iam_role_least_privileged
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Pipeline IAM Role Least Privileged
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Platform Pipeline IAM Roles
  rationale: Implementing least privilege for AI Platform Pipeline IAM roles minimizes the risk of unauthorized access to sensitive data and resources, reducing potential security breaches. This aligns with compliance requirements, such as NIST and ISO 27001, which mandate strict access controls to protect data integrity and confidentiality. Inadequate privilege management can lead to data leaks, financial loss, and reputational damage.
  description: This rule checks if AI Platform Pipeline jobs are assigned IAM roles that adhere to the principle of least privilege. It ensures that roles only have permissions necessary to perform their tasks, without excess access. To verify, review IAM role assignments for pipeline jobs and adjust permissions to the minimum required. Remediation involves auditing current roles and applying more restrictive policies using GCP's IAM policy management tools.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.aiplatform.pipeline_job.ai_services_pipeline_kms_encryption_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Pipeline KMS Encryption Enabled
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption for AI Platform Pipeline Jobs
  rationale: Enabling KMS encryption for AI Platform pipeline jobs is crucial to protect sensitive data processed and stored by AI models. This reduces the risk of data breaches by ensuring that data is encrypted at rest using customer-managed keys. It helps organizations meet compliance requirements such as GDPR and HIPAA, by ensuring that sensitive information remains secure and under the control of the organization.
  description: This rule checks whether AI Platform pipeline jobs are configured to use Customer-Managed Encryption Keys (CMEK) with Google Cloud Key Management Service (KMS). To verify, ensure that the `kmsKeyName` attribute is set in the pipeline job configuration. To remediate, update the pipeline job definition to include a valid KMS key name, ensuring that the key is properly granted permissions to the AI Platform service account for encryption and decryption operations.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security#encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.pipeline_job.ai_services_pipeline_logging_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Ai Services Pipeline Logging Enabled
  scope: aiplatform.pipeline_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure AI Services Pipeline Logging is Enabled
  rationale: Enabling logging for AI Services pipeline jobs is crucial for monitoring and auditing purposes. It helps in detecting anomalies, understanding user behavior, and maintaining compliance with data protection regulations such as GDPR and CCPA. Inadequate logging can result in undetected security incidents and non-compliance with legal and regulatory mandates.
  description: This check verifies that logging is enabled for AI Platform pipeline jobs, ensuring that all operations are recorded and auditable. It involves configuring the AI Platform to capture logs related to pipeline execution, which can be reviewed for troubleshooting and compliance audits. To enable logging, configure the AI Platform to integrate with Cloud Logging and ensure that the appropriate log sinks are set up to capture pipeline job activities. This can be done through the Google Cloud Console or using gcloud commands.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.pipeline_job.data_governance_ai_human_review_artifacts_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Ai Human Review Artifacts Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Human Review Artifacts are Encrypted at Rest
  rationale: Encrypting AI human review artifacts at rest protects sensitive data from unauthorized access, reducing the risk of data breaches and ensuring compliance with data protection regulations such as GDPR and HIPAA. This is crucial for maintaining trust with stakeholders and preventing potential financial and reputational damage from data leaks.
  description: This rule checks whether AI human review artifacts within AI Platform Pipeline Jobs are encrypted at rest using Customer-Managed Encryption Keys (CMEK). To verify, ensure that encryption settings in the pipeline job configurations are set to use CMEK. Remediation involves updating the pipeline job configuration to specify a CMEK for encrypting artifacts, which can be done via the GCP Console or gcloud command-line tool by setting the appropriate encryption key during job creation.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.aiplatform.pipeline_job.data_governance_ai_human_review_ui_private_networki_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Ai Human Review Ui Private Networki Enforced
  scope: aiplatform.pipeline_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Human Review UI Uses Private Network in AI Platform Pipelines
  rationale: Enforcing private network connections for the AI Human Review UI in AI Platform pipelines reduces exposure to unauthorized access, minimizing data breaches and protecting sensitive information. This control is crucial for maintaining confidentiality and integrity, particularly when handling regulated or proprietary data, and supports compliance with data protection regulations.
  description: This rule verifies that AI Platform pipeline jobs utilize a private network for the AI Human Review UI, enhancing data governance through network isolation. Check that the 'network' field in pipeline job configuration specifies a private VPC. Remediation involves updating the pipeline job configuration to include a private VPC network, ensuring all data interactions are confined within a controlled environment.
  references:
  - https://cloud.google.com/ai-platform/docs/pipelines/configure-networking
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/security-foundations/networking
  - https://cloud.google.com/vpc/docs/vpc
- rule_id: gcp.aiplatform.pipeline_job.data_governance_ai_human_review_workteam_access_rb_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Ai Human Review Workteam Access Rb Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict Human Review Workteam Access in AI Platform Pipelines
  rationale: Ensuring least privilege access for AI human review workteams is critical to prevent unauthorized data exposure and maintain compliance with data protection regulations. Improper access controls can lead to data breaches, impacting both business reputation and customer trust. Adhering to access control best practices mitigates the risk of insider threats and aligns with regulatory standards.
  description: This rule checks if AI Platform's pipeline job configurations grant only necessary privileges to human review workteams. It verifies that access roles are restricted to the minimum required for task execution, reducing the risk of data leakage. To remediate, review and adjust IAM policies to ensure users have the least privilege necessary for their roles. Regular access reviews and audits should also be conducted to maintain compliance.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.pipeline_job.data_governance_artifacts_private_and_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Artifacts Private And Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Pipeline Job Artifacts are Encrypted and Private
  rationale: Encrypting and restricting access to AI Platform Pipeline Job artifacts helps protect sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining data integrity and confidentiality, especially in industries with strict data protection regulations like healthcare and finance. Non-compliance can lead to reputational damage and financial penalties.
  description: This rule checks if AI Platform Pipeline Job artifacts are stored in private and encrypted storage buckets. Verify that the Google Cloud Storage buckets used for storing pipeline job outputs have Bucket Policy Only or Uniform Bucket-Level Access enabled, and that they use Customer-Managed Encryption Keys (CMEK) for encryption. Remediation involves configuring the storage bucket settings to ensure private access and enabling CMEK for encryption.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://cloud.google.com/storage/docs/access-control/uniform-bucket-level-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.pipeline_job.data_governance_kms_encryption_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance KMS Encryption Enabled
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Platform Pipeline Jobs
  rationale: Enabling KMS encryption for AI Platform pipeline jobs protects sensitive data by encrypting it at rest using a customer-managed key. This reduces the risk of unauthorized data access and helps meet compliance requirements for data protection standards such as HIPAA, PCI-DSS, and GDPR. It ensures that only authorized users with access to the encryption key can decrypt the data, mitigating potential data breaches.
  description: This rule checks whether AI Platform pipeline jobs are configured to use Cloud KMS customer-managed keys for data encryption at rest. To verify, ensure the pipeline job configuration includes an 'encryptionSpec' with a specified 'kmsKeyName'. If not configured, update the pipeline job settings to include a valid KMS key from your Cloud KMS service. This enhances data security by leveraging encryption best practices.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/kms/docs
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.pipeline_job.data_governance_logging_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Logging Enabled
  scope: aiplatform.pipeline_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Data Governance Logging for AI Platform Pipeline Jobs
  rationale: Data governance logging is crucial for monitoring and auditing AI workflows, providing visibility into data access and manipulation within pipeline jobs. Without proper logging, organizations risk unauthorized data access, potential data breaches, and non-compliance with regulations such as GDPR or HIPAA, which can lead to severe legal and financial repercussions.
  description: This rule verifies that data governance logging is enabled for AI Platform pipeline jobs to ensure all data activities are captured in audit logs. To implement, configure AI Platform pipeline jobs in the Google Cloud Console to ensure Cloud Audit Logs are activated for data access. Remediation involves enabling logging through IAM permissions and configuring logging sinks to capture and store logs in Cloud Logging for review and analysis.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/audit-logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.aiplatform.pipeline_job.data_governance_roles_least_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Data Governance Roles Least Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Pipeline Jobs
  rationale: Limiting roles to the least privilege necessary for AI Platform pipeline jobs reduces the risk of unauthorized access and data breaches. Over-privileged access can lead to accidental or malicious data manipulation, violating compliance requirements such as GDPR and HIPAA, and potentially causing reputational damage and financial loss.
  description: This rule checks that users and service accounts assigned to AI Platform pipeline jobs have the minimal necessary permissions. Ensure roles like 'Viewer' or 'Custom Role' with specific permissions are used instead of broad roles like 'Editor'. Verify by reviewing IAM policies and adjust using the GCP Console or gcloud CLI to enforce strict access controls, ensuring compliance with organizational policies.
  references:
  - https://cloud.google.com/ai-platform/docs/pipelines/security
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_ai_human_review_artifacts_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Ai Human Review Artifacts Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Human Review Artifacts Are Encrypted
  rationale: Encrypting human review artifacts in AI Platform ensures that sensitive data used in machine learning processes is protected from unauthorized access. This mitigates the risk of data breaches and aligns with compliance requirements such as GDPR and HIPAA, which mandate the protection of personally identifiable information (PII) and sensitive data. Insufficient encryption can lead to significant financial and reputational damage due to data exposure.
  description: This rule verifies that all human review artifacts generated by AI Platform Pipeline Jobs are encrypted at rest using customer-managed encryption keys (CMEK). To ensure compliance, configure the AI Platform to use CMEK for encryption by specifying the key in the pipeline job configuration. Regularly audit your encryption settings to ensure they align with your organization's data protection policies. Remediation involves updating the pipeline job configurations to specify a valid CMEK.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - 'CIS GCP Benchmark: 4.7 Ensure that Cloud Storage bucket is encrypted with CMK'
  - 'NIST SP 800-57 Part 1: Recommendation for Key Management'
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_ai_human_review_ui_private_networking_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Ai Human Review Ui Private Networking Enforced
  scope: aiplatform.pipeline_job.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Private Networking for AI Human Review UI
  rationale: Enforcing private networking for AI Human Review UI mitigates the risk of data exposure over the internet, reducing vulnerability to unauthorized access and data breaches. This is crucial for maintaining data privacy and meeting compliance requirements such as GDPR and HIPAA, especially when handling sensitive data through AI platforms.
  description: This rule checks that AI Human Review UI is configured to operate within a private network, ensuring data is not exposed to public networks. Verify that the 'network' field in pipeline job configurations references a valid VPC network. Remediation involves configuring the AI Platform to use private networking by setting up a private Google Access and ensuring the pipeline job is deployed within this network. This setup helps in securing data in transit and aligns with organizational data protection policies.
  references:
  - https://cloud.google.com/ai-platform/docs/pipelines/networking
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_ai_human_review_workteam_access_rbac__privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Ai Human Review Workteam Access RBAC Privilege
  scope: aiplatform.pipeline_job.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure RBAC for AI Human Review Workteam in Pipeline Jobs
  rationale: Proper Role-Based Access Control (RBAC) for AI Human Review Workteams is critical to prevent unauthorized access to sensitive data processed in AI pipelines. Unauthorized access can lead to data breaches, compliance violations, and loss of trust. Ensuring appropriate privileges helps mitigate risks associated with data privacy regulations such as GDPR and CCPA.
  description: This rule checks if AI Human Review Workteams involved in AI Platform pipeline jobs have the necessary and appropriate roles assigned. Configurations should ensure that only authorized personnel have access to sensitive data, following the principle of least privilege. To verify, review IAM permissions for workteams associated with pipeline jobs and adjust roles to match their responsibilities. Remediation involves updating IAM policies to restrict access to only those who require it for their job functions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/docs/pipelines
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_artifacts_private_and_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Artifacts Private And Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform Pipeline Data is Private and Encrypted
  rationale: Data privacy is critical for protecting sensitive information processed by AI Platform pipelines. Unencrypted data artifacts can be exposed to unauthorized access, leading to data breaches and non-compliance with regulations such as GDPR and HIPAA. Ensuring data privacy and encryption mitigates risks of data leakage and supports maintaining trust and regulatory adherence.
  description: This rule verifies that artifacts created by AI Platform pipeline jobs are stored privately and encrypted at rest. Ensure that all output artifacts are stored in Google Cloud Storage buckets with bucket policies enforcing encryption, such as Customer-Managed Encryption Keys (CMEK). Verify encryption settings through the Google Cloud Console or CLI, and apply necessary policies to restrict access and enforce encryption standards.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/architecture/guidelines-for-securing-ai-platform-pipelines
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_kms_encryption_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy KMS Encryption Enabled
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for AI Platform Pipeline Jobs
  rationale: Using KMS encryption for AI Platform pipeline jobs protects sensitive data at rest, reducing the risk of unauthorized access or data breaches. This practice helps organizations comply with regulatory requirements such as GDPR and HIPAA, which mandate robust data protection measures. Encrypting data also mitigates potential financial and reputational damage from data leaks.
  description: This rule checks that all AI Platform pipeline jobs are configured with Cloud KMS keys for encryption. The pipeline job must specify a customer-managed encryption key (CMEK) to ensure that data is securely encrypted at rest. To verify, review pipeline job configurations for the `encryptionSpec` field and confirm it references a valid KMS key. To remediate, update the pipeline job to include a KMS key in its encryption settings.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_logging_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Logging Enabled
  scope: aiplatform.pipeline_job.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Privacy Logging is Enabled for AI Platform Pipeline Jobs
  rationale: Enabling data privacy logging for AI Platform pipeline jobs is crucial for monitoring access and usage of sensitive data, which helps in identifying unauthorized access and mitigating data breaches. It supports compliance with data protection regulations such as GDPR and CCPA by providing audit trails that demonstrate adherence to privacy policies. This practice also aids in detecting anomalies and ensuring accountability in data processing activities.
  description: This rule checks whether data privacy logging is enabled for AI Platform pipeline jobs, ensuring that all data access and processing activities are logged and monitored. To verify, ensure that logging is configured in the AI Platform settings by enabling 'logConfig' for pipeline jobs. Remediation involves updating the pipeline job specifications to include the necessary logging configurations, allowing for comprehensive data access tracking.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/reference
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.pipeline_job.data_privacy_roles_least_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Data Privacy Roles Least Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Pipeline Job Roles
  rationale: Properly assigning roles with the least privilege principle is crucial to minimize the risk of unauthorized access or data leakage. Misconfigured roles can lead to excessive permissions, increasing the attack surface and potential for data breaches. Adhering to least privilege is often a requirement under compliance frameworks such as GDPR and HIPAA, which emphasize data protection and access control.
  description: This rule checks that roles assigned to users and service accounts for AI Platform pipeline jobs are limited to the minimum necessary permissions. It verifies that custom roles or predefined roles do not grant broader access than required for their function. To remediate, review and adjust IAM policies to ensure roles are aligned with job responsibilities and audit access logs regularly to identify any deviations from expected usage.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_human_review_artifacts_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Human Review Artifacts Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt Human Review Artifacts in AI Platform Pipelines
  rationale: Encrypting human review artifacts in AI Platform pipelines is crucial to protect sensitive data from unauthorized access and potential breaches. This ensures compliance with data protection regulations such as GDPR and HIPAA, which mandate encryption of data at rest. Failure to encrypt can lead to data exposure, resulting in legal penalties and reputational damage.
  description: This rule verifies that machine learning human review artifacts generated by AI Platform pipeline jobs are encrypted using Customer Managed Encryption Keys (CMEK). To ensure compliance, configure the pipeline job to use a CMEK by specifying the `encryptionSpec` field in the job's configuration. Validate the setup by checking the job's metadata in the Cloud Console or using gcloud CLI. Remediate by updating pipeline configurations to include a valid CMEK.
  references:
  - https://cloud.google.com/ai-platform-unified/docs/pipelines/create-pipeline#encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_human_review_ui_private_networking_enforced
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Human Review Ui Private Networking Enforced
  scope: aiplatform.pipeline_job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure ML Human Review UI Uses Private Networking
  rationale: Enforcing private networking for the Machine Learning Human Review UI mitigates potential data breaches by restricting network access to authorized and internal networks only. This reduces exposure to the public internet, thereby protecting sensitive machine learning data and models from unauthorized access and tampering. It also helps meet compliance requirements for data protection and privacy standards, such as GDPR and HIPAA, by ensuring secure data transmission within a controlled environment.
  description: This rule checks if the Machine Learning Human Review UI for AI Platform pipeline jobs is configured to use private networking. To verify, ensure that the UI is accessed through a VPC Service Control perimeter, blocking access from public IP addresses. Remediation involves configuring private Google Access and setting up appropriate firewall rules within the VPC to allow only necessary internal traffic. This enhances security by ensuring that sensitive machine learning operations are conducted over a secure and private network.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/reference/rest/v1beta1/projects.locations.pipelineJobs
  - https://cloud.google.com/vpc/docs/private-access-options
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 6.6 Ensure VPC Flow Logs is enabled for every subnet in VPC Network
  - NIST SP 800-53 Rev. 5 - SC-7 Boundary Protection
  - PCI-DSS v3.2.1 - 1.3 Prohibit direct public access between the Internet and any system component in the cardholder data environment
  - ISO 27001:2013 - A.13.1 Network Security Management
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_human_review_workteam_access_rbac_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Human Review Workteam Access RBAC Privilege
  scope: aiplatform.pipeline_job.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure ML Human Review Workteam RBAC Privilege is Minimally Scoped
  rationale: Properly scoping access for machine learning human review workteams is crucial to prevent unauthorized data exposure and manipulation, which can lead to data breaches and compromise model integrity. Inadequate access management can result in non-compliance with regulatory frameworks such as GDPR and CCPA, posing legal and financial risks.
  description: This rule checks if the role-based access control (RBAC) privileges assigned to machine learning human review workteams are aligned with the principle of least privilege. Verify that only necessary permissions are granted to perform specific review tasks by auditing IAM policies associated with pipeline jobs. Remediation involves adjusting access levels to ensure compliance with organizational policy and relevant regulations, removing unnecessary roles, and periodically reviewing access permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/ai-platform/docs/pipelines/iam
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_pipeline_artifacts_private_and_encrypted
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Pipeline Artifacts Private And Encrypted
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Pipeline Artifacts Are Private and Encrypted
  rationale: Encrypting machine learning pipeline artifacts at rest ensures data confidentiality and integrity, reducing the risk of unauthorized access and data breaches. This is crucial for maintaining trust with stakeholders, meeting compliance requirements (such as GDPR and HIPAA), and protecting sensitive intellectual property or personal data involved in machine learning processes.
  description: This rule checks if AI Platform pipeline artifacts are stored with encryption at rest and access is restricted to authorized users only. Ensure that encryption keys are managed securely using Google Cloud Key Management Service (KMS) and access is controlled via IAM roles with least privilege. Verify encryption settings in AI Platform's pipeline job configurations and update policies to enforce encryption and privacy controls across all artifacts.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/kms/docs
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_pipeline_kms_encryption_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Pipeline KMS Encryption Enabled
  scope: aiplatform.pipeline_job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption for AI Platform Pipeline Jobs
  rationale: Enabling KMS encryption for machine learning pipeline jobs on GCP secures sensitive data by ensuring that it is encrypted at rest. This reduces the risk of unauthorized access and data breaches, safeguarding intellectual property and personal data. Compliance with industry standards such as PCI-DSS and HIPAA often requires encryption of sensitive data, making this a critical control for organizations handling regulated data.
  description: This rule checks whether AI Platform pipeline jobs have Cloud Key Management Service (KMS) encryption enabled. To verify, ensure the 'encryptionSpec.kmsKeyName' field is specified in the pipeline job configuration. Remediation involves updating the pipeline job settings to include a valid KMS key, providing enhanced security by encrypting the data processed by the pipeline. This configuration helps maintain data confidentiality and integrity.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_pipeline_logging_enabled
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Pipeline Logging Enabled
  scope: aiplatform.pipeline_job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for Machine Learning Pipelines in GCP AI Platform
  rationale: Enabling logging for machine learning pipelines ensures comprehensive audit trails, aiding in the detection of anomalies, troubleshooting, and ensuring compliance with data governance policies. Insufficient logging can lead to undetected unauthorized activities or misconfigurations, posing a risk to data integrity and confidentiality. Compliance with frameworks like SOC2 and ISO 27001 often requires detailed logging for accountability and auditability.
  description: This rule checks if logging is enabled for machine learning pipelines in GCP AI Platform. To verify, ensure that the pipeline job configurations include logging options that capture necessary events and activities. Remediation involves configuring the pipeline jobs to utilize Google's Cloud Logging service, ensuring all relevant logs are captured and retained according to your organization's policy. This can be done through the AI Platform Console or via API configurations.
  references:
  - https://cloud.google.com/ai-platform/pipelines/docs/logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.aiplatform.pipeline_job.machine_learning_pipeline_roles_least_privilege
  service: aiplatform
  resource: pipeline_job
  requirement: Machine Learning Pipeline Roles Least Privilege
  scope: aiplatform.pipeline_job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for ML Pipeline Roles
  rationale: Implementing least privilege for machine learning pipeline roles minimizes the attack surface by ensuring that users and service accounts have only the necessary permissions to perform their tasks. This reduces the risk of unauthorized data access, accidental data loss, and potential exploitation by malicious actors. Adhering to this principle helps in meeting compliance requirements such as GDPR and ISO 27001.
  description: This rule audits the permissions associated with roles used in AI Platform pipeline jobs, ensuring they adhere to the principle of least privilege. It checks for excessive permissions that are not required for the pipeline's functionality, such as permissions allowing access to unrelated resources. Remediation involves reviewing the roles assigned to users and service accounts and tailoring them to include only the necessary permissions. This can be done using the Google Cloud Console or gcloud CLI to update IAM policies.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/pipelines/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/architecture/identity-access-management
  - https://www.nist.gov/privacy-framework
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_automl_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Automl Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable AI Platform AutoML Training Pipeline Audit Logs
  rationale: Enabling audit logs for AI Platform AutoML training pipelines is crucial for tracking access and changes to machine learning models, which helps in detecting unauthorized or anomalous activities. This logging is vital for forensic analysis in case of data breaches, ensuring accountability and transparency, and supporting compliance with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks whether audit logs are enabled for the AI Platform AutoML training pipelines to ensure that all significant actions are recorded. To verify, ensure that logging is configured in the Google Cloud Console under the AI Platform settings, specifically under 'Audit Logs'. If not enabled, configure audit logs by navigating to the IAM & Admin > Audit Logs section and enabling 'Admin Read', 'Data Read', and 'Data Write' for the AI Platform service. This ensures comprehensive logging of administrative and data access activities.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_automl_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Automl Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI Platform AutoML
  rationale: Enforcing private networking for AI Platform AutoML training pipelines helps mitigate the risk of unauthorized access and data exfiltration. By ensuring that models are trained in a secure, private network, organizations can maintain data confidentiality and integrity, which is crucial for compliance with data protection regulations such as GDPR and CCPA. This also reduces exposure to potential cyber threats and enhances the overall security posture of the AI infrastructure.
  description: This rule checks if AI Platform AutoML training pipelines are configured to use private networking, ensuring that all data and model training processes remain within a secure internal network. To verify, inspect the network configuration settings of the training pipeline to confirm that it is set to use a private IP range. Remediation involves updating the network settings of the pipeline to utilize a VPC that does not allow public IP addresses, ensuring all traffic remains private and internal.
  references:
  - https://cloud.google.com/ai-platform/docs/general/deployment-overview#private-networking
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.nist.gov/cyberframework
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_automl_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Automl Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Platform Training Pipelines Use Least Privilege Roles
  rationale: Assigning excessive permissions to AI Platform training pipelines can lead to unauthorized data access, accidental data modification, and increased risk of insider threats. Ensuring least privilege helps protect sensitive data, supports compliance with data protection regulations, and minimizes potential attack vectors.
  description: This rule checks that AI Platform training pipelines are assigned only the minimal roles necessary for their operation, specifically focusing on Automl roles. Verify roles by reviewing IAM policies attached to training pipelines and ensure they are limited to essential permissions. If over-privileged roles are identified, modify IAM policies to restrict access to only what is necessary for the pipeline's functionality.
  references:
  - https://cloud.google.com/ai-platform/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_hpo_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Hpo Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure AI Platform HPO Logs Are Enabled for Training Pipelines
  rationale: Enabling HPO logs for AI Platform training pipelines is critical for tracking hyperparameter optimization processes, which can help in auditing AI model training activities. This logging aids in identifying potential anomalies or misconfigurations, and ensures transparency and accountability, which are essential for complying with data governance frameworks and regulatory standards.
  description: This rule checks if hyperparameter optimization logs are enabled for AI Platform training pipelines on GCP. Proper logging settings should be configured to capture all relevant events, which can be verified through the AI Platform console or via the gcloud CLI. To remediate, ensure that logging is activated under the 'Training Pipelines' section in AI Platform, confirming that the 'Hyperparameter Optimization Logs' option is set to active.
  references:
  - https://cloud.google.com/ai-platform/training/docs/monitor-debug
  - https://cloud.google.com/solutions/best-practices-for-ai-ml
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_hpo_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Hpo Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for AI HPO Training Pipelines
  rationale: Private networking in AI HPO (Hyperparameter Optimization) ensures that data and model training processes are protected from unauthorized access and potential data breaches. By enforcing private networking, organizations can mitigate risks associated with exposure to public networks, ensuring compliance with data protection regulations such as GDPR and CCPA. This practice also aligns with security best practices, reducing the attack surface and enhancing data confidentiality.
  description: This rule checks that AI Platform Training Pipelines with hyperparameter optimization (HPO) are configured to use private networking, preventing exposure to public IP addresses. To verify, ensure that the 'network' parameter is set to a VPC network in the training pipeline configuration. Remediation involves updating pipeline configurations to specify a private VPC network, providing an isolated and secure environment for data processing and model training.
  references:
  - https://cloud.google.com/ai-platform/training/docs/custom-containers-training#using_private_ips
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#private_networking
- rule_id: gcp.aiplatform.training_pipeline.data_governance_ai_hpo_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Data Governance Ai Hpo Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for AI Platform Training Pipelines
  rationale: Ensuring least privilege access for AI Platform training pipelines minimizes the risk of unauthorized data access and manipulation. Misconfigured access can lead to data breaches, intellectual property theft, and non-compliance with regulatory standards such as GDPR and CCPA, which mandate strict data protection and privacy. By restricting roles to only necessary permissions, you reduce the attack surface and potential for insider threats.
  description: This rule checks that roles assigned to AI Platform training pipelines adhere to the principle of least privilege. Specifically, it ensures that only the necessary permissions are granted for data governance and hyperparameter optimization. To verify, review IAM policies associated with AI Platform resources and confirm that roles are appropriately scoped. Remediation involves auditing current roles, removing excessive permissions, and only assigning predefined roles like 'AI Platform User', ensuring they are limited to required operations.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/aiplatform/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_automl_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Automl Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Platform AutoML Logging is Enabled for Training Pipelines
  rationale: Enabling Ai Automl logs for training pipelines is crucial for maintaining data privacy and security. Without logging, it is difficult to track data access and modifications, which increases the risk of unauthorized data exposure and potential compliance violations. Logging also aids in forensic investigations and monitoring for suspicious activities, which are vital for achieving compliance with standards such as GDPR and HIPAA.
  description: This rule checks if logging is enabled for AutoML training pipelines in Google Cloud's AI Platform. To ensure data privacy, logs should be configured to capture access and modification events for all training data processed by AutoML. Administrators can verify this by checking the logging configuration in the AI Platform settings. If logging is not enabled, it should be configured through the Google Cloud Console or gcloud CLI by setting the appropriate log sinks to capture the required events.
  references:
  - https://cloud.google.com/ai-platform/docs/getting-started-automl#logging
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/logging/docs/setup
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_automl_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Automl Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Private Networking for AI Platform Training Pipelines
  rationale: Enforcing private networking for AI Platform training pipelines mitigates the risk of data exposure by restricting access to sensitive data over the public internet. This ensures that training data remains protected from unauthorized access and potential interception, aligning with data privacy regulations and reducing the risk of data breaches. It supports compliance with frameworks like GDPR and HIPAA that mandate stringent data protection measures.
  description: This rule checks if AI Platform training pipelines are configured to use private networking, which involves setting up VPC Peering or Private Google Access. To verify, ensure the training pipeline's execution environment is within a VPC network and that no public IP addresses are assigned. Remediation involves configuring private services access and updating the pipeline to operate within the secure network settings. This enhances security by isolating traffic within a controlled network perimeter.
  references:
  - https://cloud.google.com/ai-platform/training/docs/networking
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/itl/applied-cybersecurity/nist-special-publication-800-53
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_automl_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Automl Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Training Pipeline Roles
  rationale: Implementing least privilege in AI Platform training pipelines minimizes the risk of unauthorized access to sensitive data and operations, reducing the potential for data breaches and service disruptions. This approach aligns with compliance requirements such as GDPR and CCPA, which mandate strict data access controls to protect personal data. Additionally, by limiting permissions, organizations can prevent privilege escalation attacks and maintain the integrity of their AI models.
  description: This rule checks that roles assigned to AI Platform training pipelines follow the principle of least privilege. It verifies that service account permissions are restricted to only those necessary for executing specific tasks within the training pipeline. To remediate, review and adjust IAM roles, removing unnecessary permissions and ensuring only essential access is granted. This can be verified through the IAM policy analysis tools available in the GCP Console, focusing on roles such as 'roles/aiplatform.user' and 'roles/aiplatform.viewer' as opposed to broader roles like 'owner' or 'editor'.
  references:
  - https://cloud.google.com/ai-platform/docs/security
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_hpo_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Hpo Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable AI HPO Logs for Enhanced Data Privacy in Training Pipelines
  rationale: Enabling AI Hyperparameter Optimization (HPO) logs is crucial for maintaining data privacy, as it helps track and audit model training processes. This reduces the risk of unauthorized data access and supports compliance with data protection regulations such as GDPR and CCPA. Without these logs, organizations may face data breaches or non-compliance fines, potentially harming their reputation and financial standing.
  description: This rule checks whether AI HPO logs are enabled for training pipelines on Google Cloud AI Platform. To verify, ensure that logging is configured to capture all relevant HPO activities within the AI Platform's training pipelines. Remediation involves enabling logging through the AI Platform settings and ensuring that logs are stored securely with appropriate access controls. This helps in monitoring and auditing AI training activities, providing an additional layer of security for sensitive data.
  references:
  - https://cloud.google.com/ai-platform/training/docs/logging
  - https://cloud.google.com/architecture/framework/security/encryption
  - CIS Google Cloud Platform Foundation Benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_hpo_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Hpo Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Private Networking for AI Platform Training Pipelines
  rationale: Enforcing private networking for AI Platform training pipelines mitigates the risk of unauthorized access and data exposure by restricting network traffic to a secure VPC. This reduces the threat surface for potential attackers and complies with data privacy regulations such as GDPR and HIPAA, which mandate stringent data protection measures.
  description: This rule checks whether AI Platform training pipelines are configured to use private networking, which ensures data is transmitted over a secure, internal network rather than the public internet. To verify, ensure the training pipeline's networking configuration specifies a VPC network. Remediation involves updating the training pipeline configuration to include a VPC network, ensuring all data traffic remains within Google Cloud's secure infrastructure.
  references:
  - https://cloud.google.com/ai-platform/training/docs/private-ip
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.aiplatform.training_pipeline.data_privacy_ai_hpo_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Data Privacy Ai Hpo Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Platform Training Pipeline Roles
  rationale: Applying the principle of least privilege to AI Platform roles minimizes the risk of unauthorized access to sensitive data and operations, reducing potential data breaches and violations of compliance standards. This approach helps maintain data privacy, especially when handling sensitive datasets in training pipelines, aligning with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks that roles assigned to users and service accounts for managing AI Platform training pipelines are configured with the least privilege principle. Verify that users only have permissions necessary for their tasks, avoiding roles with excessive access like 'Owner'. Remediate by auditing current role assignments and adjusting permissions to adhere strictly to required operations, using predefined roles such as 'AI Platform Viewer' or custom roles with minimum necessary permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/training/docs/using-iam
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/standard/54534.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_automl_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Automl Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Audit Logs for AutoML Training Pipelines
  rationale: Enabling audit logs for AutoML training pipelines is crucial for tracking access and modifications to machine learning resources, which helps prevent unauthorized actions and ensures data integrity. This logging capability supports forensic investigations by providing a detailed record of operations, which is essential for compliance with data protection regulations and maintaining accountability in AI operations.
  description: This rule checks if audit logs are enabled for AutoML training pipelines in Google Cloud AI Platform. Specifically, it verifies that all read, write, and administrative access logs are captured and stored in Cloud Logging. To enable logging, ensure that the AI Platform service account has the appropriate roles to access Logging, and configure the logging level in the AI Platform settings. Compliance can be verified by reviewing the 'Audit Logs' section under IAM & Admin in the GCP Console and ensuring logs are being generated for all relevant pipeline activities.
  references:
  - https://cloud.google.com/ai-platform/docs/audit-logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/automl/docs
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_automl_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Automl Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for Automl Training Pipelines
  rationale: Enforcing private networking for Machine Learning AutoML training pipelines minimizes exposure to public internet threats, reducing the risk of data breaches and unauthorized access. This measure supports compliance with regulatory standards such as GDPR and HIPAA by ensuring sensitive data remains within the controlled environment of the Google Cloud Platform. It also enhances the overall security posture by limiting attack vectors and potential vulnerabilities.
  description: This rule checks if the AutoML training pipelines in GCP's AI Platform are configured to use private networking, which means they communicate over a Virtual Private Cloud (VPC) instead of the public internet. Ensure that the 'enablePrivateServiceConnect' option is set to true in the training pipeline configuration. Verify this setting via the Google Cloud Console or by using the gcloud CLI. To remediate, update existing pipelines to enforce private networking by modifying the network configuration to include a private connection.
  references:
  - https://cloud.google.com/ai-platform/training/docs/using-vpc
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/network-connectivity/docs/service-directory
  - https://cloud.google.com/vpc
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_automl_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Automl Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AutoML Roles in Training Pipelines
  rationale: Implementing least privilege for AutoML roles in GCP's AI Platform is crucial to minimize potential unauthorized access and data breaches. Over-privileged accounts can lead to accidental or malicious misuse of resources, causing significant business disruption and financial loss. Furthermore, adhering to least privilege principles helps in meeting compliance requirements such as GDPR and HIPAA, which mandate strict access controls.
  description: This rule checks that roles assigned to users and service accounts in AI Platform training pipelines are limited to only those permissions necessary for their tasks. It involves reviewing IAM policies to ensure no excessive permissions are granted beyond what is needed for AutoML operations. Remediation involves adjusting IAM roles to align with the principle of least privilege by using predefined roles or creating custom roles tailored to specific needs.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/ai-platform/training/docs/custom-service-account
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/creating-custom-roles
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_hpo_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Hpo Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for ML Hyperparameter Optimization in AI Platform
  rationale: Enabling logs for Machine Learning hyperparameter optimization (HPO) pipelines in Google Cloud AI Platform helps organizations monitor, troubleshoot, and optimize their ML models effectively. This practice reduces security risks by enabling visibility into data processing activities and supports compliance with regulatory requirements that mandate logging of critical operations. It also aids in detecting potential misconfigurations and unauthorized access.
  description: This rule checks whether logging is enabled for hyperparameter optimization (HPO) in Google Cloud AI Platform training pipelines. Ensuring logs are active allows teams to capture detailed information about the HPO process, which is essential for debugging and improving model performance. To enable logging, navigate to the AI Platform's Training Pipelines section, and configure logging settings to capture detailed operational data. Regularly review these logs to maintain optimal security and compliance posture.
  references:
  - https://cloud.google.com/ai-platform/training/docs/auditing
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_hpo_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Hpo Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for ML HPO in AI Platform
  rationale: Enforcing private networking for Machine Learning Hyperparameter Optimization (HPO) ensures that data and model training communications are restricted to secure, internal networks. This reduces the risk of data breaches and unauthorized access, which can lead to intellectual property theft and compliance violations with standards like GDPR and HIPAA.
  description: This rule checks whether Machine Learning HPO processes in Google AI Platform use private networking configurations. Specifically, it verifies that training pipelines are set to use a VPC network, preventing exposure to the public internet. To remediate, configure the AI Platform training pipeline with a VPC network by setting the 'network' field in the pipeline's trainingInput to the appropriate VPC name.
  references:
  - https://cloud.google.com/ai-platform/training/docs/using-vpc
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_hpo_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Hpo Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for ML HPO Roles in AI Platform
  rationale: Granting excessive permissions to roles associated with machine learning hyperparameter optimization (HPO) can expose sensitive data and increase the attack surface. Least privilege access helps mitigate risks such as unauthorized data access, accidental deletion, or misuse of resources, which are critical for maintaining the integrity and confidentiality of your ML workloads. Additionally, adhering to least privilege principles aligns with compliance requirements such as PCI-DSS and ISO 27001, ensuring an organization's data protection posture is robust.
  description: This rule checks for adherence to the principle of least privilege in AI Platform training pipelines, specifically for roles involved in machine learning hyperparameter optimization. It ensures that roles assigned to users, service accounts, or applications have only the necessary permissions needed to perform their functions. To verify, audit IAM policies associated with AI Platform and modify roles to restrict permissions to the minimum necessary. Remediation involves updating IAM policies and roles to remove excess permissions and regularly reviewing access logs for compliance.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_training_job_input_output_encrypted
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Training Job Input Output Encrypted
  scope: aiplatform.training_pipeline.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Training Job Data is Encrypted at Rest
  rationale: Encrypting machine learning training job input and output data at rest mitigates the risk of unauthorized data access, which is crucial for maintaining data confidentiality and integrity. This is particularly important in industries with strict compliance requirements such as healthcare and finance, where sensitive data protection is mandated by regulations like HIPAA and PCI-DSS. Failing to encrypt data can lead to data breaches, financial loss, and reputational damage.
  description: This rule checks whether the data used in and produced by AI Platform training pipelines is encrypted at rest using customer-managed encryption keys (CMEK). To verify, ensure that the training pipeline is configured with CMEK through the Google Cloud Console or using the AI Platform API. Remediation involves specifying a customer-managed key during the creation of the training pipeline, which provides greater control over data security and compliance with organizational policies.
  references:
  - https://cloud.google.com/ai-platform/training/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_training_job_logs_enabled
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Training Job Logs Enabled
  scope: aiplatform.training_pipeline.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for AI Platform Training Jobs
  rationale: Enabling logs for AI Platform training jobs is crucial for monitoring and troubleshooting machine learning workflows. This practice helps in detecting anomalies, understanding model performance, and ensuring accountability by maintaining an audit trail of all activities. It also supports compliance with regulations that mandate logging of data processing activities, such as GDPR and HIPAA.
  description: This rule checks if logging is enabled for AI Platform training jobs, ensuring that the logs are captured and stored for review and analysis. To verify, ensure that the 'enableLogging' flag is set to true in the training pipeline configuration. Remediation involves updating the training pipeline's configuration to include logging settings, which can be done via the GCP Console, gcloud command-line tool, or by updating the pipeline configuration script.
  references:
  - https://cloud.google.com/ai-platform/training/docs/audit-logging
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/architecture/framework/security
  - https://cloud.google.com/security/compliance
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_training_job_private_networking_enforced
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Training Job Private Networking Enforced
  scope: aiplatform.training_pipeline.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for ML Training Jobs
  rationale: Enforcing private networking for Machine Learning training jobs on GCP ensures that communication between AI Platform resources and the training environment occurs over a secure, private network. This reduces exposure to external threats such as unauthorized access or data exfiltration. It also aids in compliance with regulatory standards that mandate secure data handling, such as PCI-DSS and HIPAA, by minimizing network attack surfaces.
  description: This rule checks that all Machine Learning training jobs within AI Platform Training Pipelines are configured to use private networking. Specifically, it verifies that the 'enablePrivateServiceConnect' option is set to true, ensuring that traffic stays within Googleâ€™s private network. To remediate non-compliant resources, configure the training job to use a VPC network by setting the 'network' field in the TrainingPipeline configuration. This helps protect sensitive data and aligns with best practices for network access control.
  references:
  - https://cloud.google.com/ai-platform/training/docs/networking
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/vpc/docs/private-service-connect
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.aiplatform.training_pipeline.machine_learning_training_job_roles_least_privilege
  service: aiplatform
  resource: training_pipeline
  requirement: Machine Learning Training Job Roles Least Privilege
  scope: aiplatform.training_pipeline.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for ML Training Job Roles
  rationale: Utilizing least privilege access for machine learning training jobs minimizes the risk of unauthorized access and potential data breaches. It is crucial for maintaining data confidentiality and integrity, especially when handling sensitive data within AI models. Adhering to this principle supports compliance with regulations such as GDPR and HIPAA, and helps prevent privilege escalation attacks.
  description: This rule verifies that roles assigned to AI Platform training jobs have the minimum necessary permissions. Review IAM policies associated with training pipelines to ensure roles are limited to only essential actions. Remediation involves auditing current permissions, using predefined roles where possible, and removing unnecessary access rights. Implementing these changes can be verified through IAM policy reviews in the Google Cloud Console or using gcloud CLI commands.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/ai-platform/training/docs/using-iam-permissions
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#access
- rule_id: gcp.apigateway.api.api_access_logging_enabled
  service: apigateway
  resource: api
  requirement: API Access Logging Enabled
  scope: apigateway.api.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Gateway Access Logging is Enabled
  rationale: Enabling access logging for API Gateway is crucial for monitoring API usage, detecting unauthorized access attempts, and conducting forensic investigations in case of security incidents. It aids in compliance with regulatory requirements like GDPR, PCI-DSS, and HIPAA, which mandate detailed access logs to ensure accountability and data protection.
  description: This rule checks if access logging is enabled for API Gateway resources in GCP. Without access logging, you may miss critical insights into API usage patterns and potential security threats. To verify, navigate to the API Gateway settings in the GCP Console and ensure that logging is activated. Remediation involves enabling logging by configuring the API Gateway to send logs to Cloud Logging, ensuring all access requests are logged and monitored.
  references:
  - https://cloud.google.com/api-gateway/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api.api_key_restrictions_verified
  service: apigateway
  resource: api
  requirement: API Key Restrictions Verified
  scope: apigateway.api.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure API Key Restrictions are Properly Configured
  rationale: Unrestricted API keys pose significant risks as they can be easily exploited by malicious actors, leading to unauthorized access to sensitive data and services. Implementing API key restrictions minimizes the attack surface and aligns with regulatory requirements such as GDPR for protecting user data.
  description: This rule checks whether API keys used in Google Cloud's API Gateway are configured with appropriate restrictions, such as IP address restrictions, application restrictions, and service restrictions. To verify, navigate to the API key settings in the Google Cloud Console and ensure that restrictions are applied. Remediate by editing the API key to add necessary restrictions, which helps prevent misuse and unauthorized access.
  references:
  - https://cloud.google.com/docs/authentication/api-keys#api_key_restrictions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/security-best-practices
- rule_id: gcp.apigateway.api.certificate_enabled
  service: apigateway
  resource: api
  requirement: Certificate Enabled
  scope: apigateway.api.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Certificates are Enabled for API Gateway
  rationale: Enabling certificates for API Gateway ensures secure communication between clients and the API, protecting sensitive data from interception and man-in-the-middle attacks. This is crucial for maintaining data privacy and integrity, especially for APIs handling personal or sensitive data, and is often required for compliance with regulations such as PCI-DSS and HIPAA.
  description: This rule checks if SSL/TLS certificates are enabled for API Gateway endpoints, ensuring encrypted data transmission. To verify, confirm that API Gateway configurations include SSL/TLS certificates for all endpoints. Remediation involves configuring API Gateway with a valid SSL/TLS certificate, which can be obtained from a trusted Certificate Authority or managed through Google Cloud's Certificate Manager.
  references:
  - https://cloud.google.com/api-gateway/docs/get-started
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/load-balancing/docs/ssl-certificates
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.apigateway.api.gateway_api_gateway_https_required
  service: apigateway
  resource: api
  requirement: Gateway API Gateway HTTPS Required
  scope: apigateway.api.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure HTTPS is Enabled for API Gateway Endpoints
  rationale: Using HTTPS for API Gateway endpoints is crucial to protect the data in transit from eavesdropping and man-in-the-middle attacks, ensuring the confidentiality and integrity of information exchanged. This is especially important for APIs handling sensitive data or personal information, as it helps organizations comply with regulatory standards like GDPR, PCI-DSS, and HIPAA, which mandate secure data transmission.
  description: This rule checks if API Gateway endpoints are configured to use HTTPS protocols. Configuring HTTPS involves setting up SSL/TLS certificates for secure communication. To verify, review the API Gateway settings to ensure 'https' is specified in the gateway URL. Remediation involves updating the gateway configuration to include SSL certificates and enforcing HTTPS-only connections, which can be done via the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/api-gateway/docs/secure-traffic
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api.gateway_authorization_enabled
  service: apigateway
  resource: api
  requirement: Gateway Authorization Enabled
  scope: apigateway.api.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure API Gateway Authorization is Enabled
  rationale: Enabling authorization in GCP API Gateway is critical to ensure that only authenticated and authorized users can access API resources. This mitigates the risk of unauthorized data exposure and potential data breaches, aligning with regulatory requirements like PCI-DSS and HIPAA, which mandate secure access controls for sensitive data.
  description: This rule checks if authorization is enabled for API Gateways in GCP. It verifies that an authorization configuration, such as a JWT token validation, is in place for APIs to prevent unauthorized access. To remediate, ensure that API Gateways are configured with appropriate authentication mechanisms like Google ID tokens or OAuth2. This can be done through the GCP Console by navigating to the API Gateway service, selecting the relevant API, and configuring the authorization settings under the 'Gateway' section.
  references:
  - https://cloud.google.com/api-gateway/docs/auth-overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.apigateway.api.gateway_restapi_logging_enabled
  service: apigateway
  resource: api
  requirement: Gateway Restapi Logging Enabled
  scope: apigateway.api.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for API Gateway REST APIs
  rationale: Enabling logging for API Gateway REST APIs is crucial for monitoring and forensic analysis. Without logging, detecting and investigating security incidents such as unauthorized access or data breaches becomes challenging, potentially leading to compliance issues with frameworks like PCI-DSS and HIPAA.
  description: This rule checks if logging is enabled for API Gateway REST APIs in Google Cloud Platform. Logging should be configured to capture all relevant request and response metadata to facilitate auditing and troubleshooting. To enable logging, navigate to the API Gateway console, select the API, and configure the logging settings to send logs to Cloud Logging. Regularly review and analyze these logs for unusual activities.
  references:
  - https://cloud.google.com/api-gateway/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.apigateway.api.gateway_restapi_waf_acl_attached_configured
  service: apigateway
  resource: api
  requirement: Gateway Restapi Waf ACL Attached Configured
  scope: apigateway.api.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure WAF Configured for API Gateway REST APIs
  rationale: Configuring a Web Application Firewall (WAF) for API Gateway REST APIs is crucial for protecting against common web exploits and vulnerabilities such as SQL injection, cross-site scripting (XSS), and others. Without a WAF, APIs are vulnerable to attacks that can lead to data breaches, service interruptions, and non-compliance with security standards like PCI-DSS and NIST. Proper WAF configuration helps mitigate these risks and ensures that sensitive data is adequately protected from external threats.
  description: This rule checks that a Web Application Firewall (WAF) is attached and properly configured for each API Gateway REST API. It involves verifying that the WAF policies are applied and that they align with security requirements to filter malicious traffic. To ensure compliance, review the WAF configuration in the GCP Console under API Gateway settings and confirm that the appropriate firewall rules are active. Remediation involves attaching a suitable WAF policy to the API Gateway and regularly updating the rules to respond to new threats.
  references:
  - https://cloud.google.com/api-gateway/docs/configure-waf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api.platform_endpoint_authn_required
  service: apigateway
  resource: api
  requirement: Platform Endpoint Authn Required
  scope: apigateway.api.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure API Gateway Uses Endpoint Authentication
  rationale: Requiring authentication for API Gateway endpoints is crucial to prevent unauthorized access to APIs, which can lead to data breaches and unauthorized data manipulation. It helps in complying with regulatory requirements such as GDPR and HIPAA, which mandate secure access controls for sensitive data. Without authentication, APIs are vulnerable to abuse, potentially impacting business operations and customer trust.
  description: This rule checks if API Gateway endpoints require authentication to ensure that only authorized users can access the API resources. To verify, inspect the API Gateway configurations to ensure that authentication mechanisms such as OAuth 2.0 or API keys are enabled. Remediation involves configuring the API Gateway to require authentication by setting up a proper authentication provider and associating it with the API endpoint.
  references:
  - https://cloud.google.com/api-gateway/docs/authenticating-users
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigateway.api.platform_endpoint_authz_policies_enforced
  service: apigateway
  resource: api
  requirement: Platform Endpoint Authz Policies Enforced
  scope: apigateway.api.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce Platform Endpoint Authz Policies for API Gateway
  rationale: Enforcing authorization policies on API Gateway endpoints mitigates risks of unauthorized access and data breaches, which could lead to financial loss, reputational damage, and compliance violations. It is crucial for maintaining secure communication channels and protecting sensitive data flowing through APIs.
  description: This rule checks whether authorization policies are enforced on all API Gateway platform endpoints. Specifically, it verifies that IAM roles and conditions are correctly configured to restrict access based on the principle of least privilege. To verify, review API Gateway configurations and ensure that appropriate authorization mechanisms, such as OAuth 2.0 or API keys, are implemented. Remediate by updating IAM policies to include necessary roles and conditions to enforce access control.
  references:
  - https://cloud.google.com/api-gateway/docs/authz
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.apigateway.api.platform_endpoint_private_networking_enforced
  service: apigateway
  resource: api
  requirement: Platform Endpoint Private Networking Enforced
  scope: apigateway.api.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for API Gateway Endpoints
  rationale: Enforcing private networking for API Gateway endpoints protects sensitive data from exposure to the public internet, mitigating risks such as unauthorized access and data breaches. This control is crucial for maintaining data confidentiality and integrity, especially for organizations handling sensitive information subject to compliance requirements like PCI-DSS and HIPAA.
  description: This rule checks that all API Gateway endpoints are configured to use private networking, ensuring they are only accessible within a VPC. To verify, inspect the API Gateway configuration to ensure the 'network' attribute is set to a valid VPC network. Remediation involves updating the API configuration to enforce private networking, which can be achieved by using the 'gcloud apigateway apis update' command with the appropriate network settings.
  references:
  - https://cloud.google.com/api-gateway/docs/network-security
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/
  - https://cloud.google.com/blog/products/api-management/using-google-cloud-api-gateway-to-securely-manage-and-monitor-apis
- rule_id: gcp.apigateway.api.platform_endpoint_waf_attached
  service: apigateway
  resource: api
  requirement: Platform Endpoint Waf Attached
  scope: apigateway.api.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure WAF is Attached to API Gateway Endpoints
  rationale: Attaching a Web Application Firewall (WAF) to API Gateway endpoints helps protect against common web exploits that could compromise application security, such as SQL injection and cross-site scripting (XSS). This is crucial for maintaining the integrity and availability of APIs, which are often critical components of business operations. Additionally, this can aid in compliance with regulatory standards that mandate the protection of sensitive data and the implementation of security controls.
  description: This rule checks if a Web Application Firewall is attached to API Gateway endpoints to filter and monitor HTTP traffic. To verify, inspect the API Gateway configuration in the GCP Console to ensure a WAF policy is associated with the API. Remediation involves configuring a WAF policy using Google Cloud Armor and attaching it to the API Gateway. This setup helps mitigate risks from malicious traffic and enhances the security posture of your APIs.
  references:
  - https://cloud.google.com/api-gateway/docs
  - https://cloud.google.com/armor/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.apigateway.api_config.platform_authorizer_cache_ttl_reasonable
  service: apigateway
  resource: api_config
  requirement: Platform Authorizer Cache Ttl Reasonable
  scope: apigateway.api_config.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Reasonable TTL for API Gateway Authorizer Cache
  rationale: Setting a reasonable time-to-live (TTL) for the API Gateway authorizer cache helps balance security with performance. Short TTLs can lead to increased latency and higher costs due to frequent authorizer invocations, while overly long TTLs may allow outdated authorization data to persist, potentially exposing the API to unauthorized access. Ensuring an appropriate TTL supports compliance with security policies and reduces the risk of exposing sensitive data.
  description: This rule checks if the TTL for the API Gateway authorizer cache is set to a reasonable duration. Verify the TTL setting in the api_config resource of the API Gateway service to ensure it aligns with security best practices, typically not exceeding 300 seconds. To remediate, adjust the TTL value in the API Gateway configuration to ensure it is neither too short, causing performance issues, nor too long, risking stale authorization data.
  references:
  - https://cloud.google.com/api-gateway/docs/using-api-gateway
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigateway.api_config.platform_authorizer_tls_min_1_2_enforced
  service: apigateway
  resource: api_config
  requirement: Platform Authorizer TLS Min 1 2 Enforced
  scope: apigateway.api_config.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS 1.2+ for API Gateway Platform Authorizer
  rationale: Enforcing TLS 1.2 or higher for API Gateway Platform Authorizer is crucial for protecting data in transit. Using outdated TLS versions exposes APIs to vulnerabilities such as man-in-the-middle attacks, potentially leading to unauthorized data access and breaches. Ensuring compliance with industry standards like PCI-DSS and NIST enhances trust and security posture.
  description: This rule verifies that the API Gateway Platform Authorizer is configured to enforce a minimum of TLS 1.2 for securing communications. Check the API Gateway configuration settings to ensure TLS 1.2 or higher is specified. Remediation involves updating the API Gateway settings to disable lower TLS versions, ensuring that only secure connections are permitted.
  references:
  - https://cloud.google.com/api-gateway/docs/security-overview
  - https://cloud.google.com/security/compliance/cis-2-0-0
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.apigateway.api_config.platform_authorizer_token_audience_restricted
  service: apigateway
  resource: api_config
  requirement: Platform Authorizer Token Audience Restricted
  scope: apigateway.api_config.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Restrict API Gateway Token Audience for Security
  rationale: Restricting the token audience for API Gateway's platform authorizer is crucial to prevent unauthorized access to APIs. An unrestricted audience can lead to token misuse, potentially allowing attackers to impersonate clients and access sensitive data. This practice supports compliance with security standards like PCI-DSS and SOC2, ensuring that only designated services can interact with your APIs.
  description: This rule checks that the API Gateway configuration specifies a restricted token audience in the platform authorizer settings. This ensures that only authorized services can use the token to access the API. To verify, examine the API Gateway's configuration settings for 'audiences' under the 'authorizer' section. Remediation involves updating the API configuration to specify a limited set of audience values that match only intended client identifiers.
  references:
  - https://cloud.google.com/api-gateway/docs/authenticating-users
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://trust.salesforce.com/en/compliance/soc-2/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigateway.api_config.platform_stage_logging_enabled
  service: apigateway
  resource: api_config
  requirement: Platform Stage Logging Enabled
  scope: apigateway.api_config.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Gateway Platform Stage Logging is Enabled
  rationale: Enabling logging for API Gateway's platform stages is critical for tracking access patterns, identifying potential misuse, and ensuring compliance with data protection regulations. Without logging, organizations may be unaware of unauthorized access, leading to data breaches and non-compliance with frameworks like PCI-DSS and SOC2.
  description: This rule checks that logging is enabled for all API Gateway platform stages in GCP. Logging provides visibility into request and response activities, which is essential for auditing and forensic investigations. To verify, ensure the 'logConfig' setting is populated in the API Gateway configurations. Remediation involves configuring the API Gateway to send logs to a designated Cloud Logging sink, ensuring all platform stages are properly monitored.
  references:
  - https://cloud.google.com/api-gateway/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs/setup
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api_config.platform_stage_require_usage_plan_for_api_keys
  service: apigateway
  resource: api_config
  requirement: Platform Stage Require Usage Plan For API Keys
  scope: apigateway.api_config.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Usage Plans for API Keys in API Gateway
  rationale: Requiring usage plans for API keys is critical in managing API access, preventing abuse, and ensuring fair usage. Without enforcing usage plans, there's a higher risk of API misuse, leading to potential data breaches or service disruptions. This measure also aligns with compliance requirements for data protection and access control.
  description: This rule checks that all API configurations in the API Gateway have usage plans associated with API keys. Usage plans define request quotas and rate limits, which are essential for controlling access to your APIs. To verify, review the API Gateway settings to ensure that all API keys are linked to a defined usage plan. Remediation involves creating or updating usage plans and associating them with the relevant API keys.
  references:
  - https://cloud.google.com/api-gateway/docs/quotas-overview
  - https://cloud.google.com/api-gateway/docs/using-api-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigateway.api_config.platform_stage_throttling_enabled
  service: apigateway
  resource: api_config
  requirement: Platform Stage Throttling Enabled
  scope: apigateway.api_config.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Throttling for API Gateway Configurations
  rationale: Enabling throttling on API Gateway configurations helps prevent abuse and overuse of APIs, which can lead to service degradation or denial of service. It ensures that APIs are only used within the limits intended by the organization, protecting backend services from excessive load and potential security threats. Compliance with regulations like NIST and PCI-DSS often requires such controls to manage access and protect data integrity.
  description: This rule checks if throttling is enabled on the API Gateway platform for each API configuration. Throttling controls the rate of requests to ensure API stability and security. To verify, check the API Gateway settings for configured limits on requests per second. If throttling is not enabled, configure it by setting appropriate limits in the GCP Console under the API Gateway configuration section. This helps mitigate risks of API misuse and supports compliance with security standards.
  references:
  - https://cloud.google.com/api-gateway/docs/quotas-overview
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/api-gateway/docs/quotas
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigateway.api_config.platform_usage_plan_quota_limits_configured
  service: apigateway
  resource: api_config
  requirement: Platform Usage Plan Quota Limits Configured
  scope: apigateway.api_config.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Gateway Usage Plan Quotas Are Configured
  rationale: Configuring usage plan quotas for API Gateway is critical to control and limit the usage of your APIs, preventing misuse and potential denial of service attacks. It helps in managing costs by avoiding unexpected high usage and ensuring fair usage across different clients, aligning with compliance requirements for resource management and availability.
  description: This rule checks whether usage plan quotas are configured for API Gateway on GCP. Quotas should be set to restrict the number of requests or data that can be sent through your APIs. To verify, review the API configuration settings in the GCP Console under API Gateway and ensure quotas are defined and active. Remediation involves setting appropriate quotas via the GCP Console or API to enforce limits according to your operational and security needs.
  references:
  - https://cloud.google.com/api-gateway/docs/quotas-overview
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/blog/products/api-management/api-gateway-is-now-generally-available
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.apigateway.api_config.platform_usage_plan_rate_limits_configured
  service: apigateway
  resource: api_config
  requirement: Platform Usage Plan Rate Limits Configured
  scope: apigateway.api_config.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: API Gateway Rate Limits for Usage Plans Configured
  rationale: Configuring platform usage plan rate limits for API Gateway ensures that APIs are protected against denial-of-service attacks and prevents overconsumption of resources, which could lead to increased costs and potential service disruptions. By setting appropriate rate limits, organizations can improve the reliability and availability of their services while adhering to compliance requirements for resource usage management.
  description: This rule checks if usage plans in API Gateway have rate limits configured to control the number of requests that can be made to an API. Proper configuration involves setting both request and burst limits to safeguard against traffic spikes. To verify, inspect the API Gateway configurations for set usage plans and ensure they include defined request quotas. Remediation involves using the GCP Console or gcloud CLI to define rate limits in the API configurations, thus ensuring robust traffic management.
  references:
  - https://cloud.google.com/api-gateway/docs/quotas-overview
  - https://cloud.google.com/security/compliance/cis#section-6.7
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/73906.html
  - https://cloud.google.com/architecture/api-design-best-practices
- rule_id: gcp.apigee.api_proxy.api_api_content_type_whitelist_enforced
  service: apigee
  resource: api_proxy
  requirement: API API Content Type Whitelist Enforced
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce API Proxy Content Type Whitelist in Apigee
  rationale: Enforcing a content type whitelist for API proxies in Apigee helps mitigate security risks such as injection attacks, data breaches, and service misuse by ensuring only allowed content types are processed. This is crucial for maintaining data integrity, meeting compliance requirements, and protecting sensitive information from unauthorized access.
  description: This rule checks if API proxies in Apigee have a content type whitelist enforced, ensuring that only predefined content types are accepted. To verify, inspect the API proxy configurations to ensure the 'Content-Type' header is validated against an approved list. To remediate, configure Apigee API proxies to include a 'Content-Type' validation policy that specifies allowed content types, thereby preventing the processing of potentially harmful or unexpected data.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/security/secure-api-proxies
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/json-threat-protection-policy
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/xml-threat-protection-policy
- rule_id: gcp.apigee.api_proxy.api_api_parameters_validation_enabled
  service: apigee
  resource: api_proxy
  requirement: API API Parameters Validation Enabled
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Apigee API Proxy Parameters Validation is Enabled
  rationale: Enabling API parameters validation in Apigee API Proxies is crucial for preventing malicious inputs that could lead to data breaches or unauthorized access. It helps mitigate security risks such as injection attacks and ensures compliance with data integrity and protection standards. This practice supports adherence to regulatory frameworks like PCI-DSS and NIST, thereby safeguarding business operations and client trust.
  description: This check verifies that API parameters validation is enabled for Apigee API Proxies. Parameter validation is an essential security measure that ensures only expected and safe inputs are processed by the API, reducing the risk of attacks. To implement this, configure policies in Apigee to validate parameters, ensuring they conform to expected formats and values. Regularly audit and update these policies to adapt to evolving security threats.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/security/parameter-validation
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apigee.api_proxy.api_api_request_body_size_limit_configured
  service: apigee
  resource: api_proxy
  requirement: API API Request Body Size Limit Configured
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Request Body Size Limit is Configured for Apigee Proxies
  rationale: Configuring an API request body size limit helps prevent denial-of-service (DoS) attacks and resource exhaustion by limiting the size of incoming requests. This is crucial for maintaining system performance and reliability, especially in environments with high transaction volumes. Additionally, it supports compliance with security standards that require protections against resource abuse.
  description: This rule checks if an API request body size limit is configured for Apigee API proxies. To verify, navigate to the Apigee console, select the relevant API proxy, and ensure that a 'Request Body Size Limit' policy is applied. This configuration prevents excessively large requests from impacting system performance. Remediation involves setting a maximum allowable request size, thereby mitigating potential DoS risks.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/request-body-size-limit-policy
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/best-practices-for-content-filtering
- rule_id: gcp.apigee.api_proxy.api_api_request_schema_validation_enabled
  service: apigee
  resource: api_proxy
  requirement: API API Request Schema Validation Enabled
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable API Request Schema Validation in Apigee API Proxies
  rationale: Enabling schema validation for API requests in Apigee helps prevent unauthorized data input, reduces the risk of malicious payloads, and ensures data integrity. This is crucial for maintaining the security posture of applications that rely on Apigee for API management, especially in environments where regulatory compliance such as PCI-DSS or HIPAA is required.
  description: This rule checks if API request schema validation is enabled for Apigee API proxies. Schema validation ensures incoming API requests adhere to a defined structure, preventing unexpected or harmful data from being processed. To verify and enable this setting, navigate to the Apigee Management Console, access the specific API proxy, and ensure that the request schema validation is configured in the proxy's policies. Remediation involves updating the API proxy configuration to include proper JSON or XML schema validation policies.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/json-schema-validator-policy
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/xml-schema-validator-policy
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.hipaajournal.com/hipaa-compliance-guide
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.apigee.api_proxy.api_api_required_security_headers_enforced
  service: apigee
  resource: api_proxy
  requirement: API API Required Security Headers Enforced
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Proxies Enforce Required Security Headers
  rationale: Enforcing security headers in API proxies is critical to protect against common web vulnerabilities such as cross-site scripting (XSS) and clickjacking. Failure to enforce these headers can lead to unauthorized access or data breaches, impacting business operations and customer trust. Compliance with security standards often mandates the use of such protective measures to mitigate risks.
  description: This rule checks whether API proxies in Apigee enforce necessary security headers such as Content-Security-Policy, X-Frame-Options, and X-Content-Type-Options. Verifying this involves reviewing API proxy configurations to ensure these headers are included in HTTP responses. Remediation involves updating the API proxy policies to set these headers correctly, thereby enhancing the security posture against web-based attacks.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/security/security-best-practices
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://owasp.org/www-project-secure-headers/
- rule_id: gcp.apigee.api_proxy.api_api_response_schema_validation_enabled
  service: apigee
  resource: api_proxy
  requirement: API API Response Schema Validation Enabled
  scope: apigee.api_proxy.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Response Schema Validation is Enabled in Apigee
  rationale: Enabling API response schema validation in Apigee ensures that the data returned by APIs conforms to the defined structure, reducing the risk of unexpected data exposure and ensuring data integrity. This helps maintain API reliability and trustworthiness while meeting compliance requirements related to data protection and privacy such as GDPR and CCPA.
  description: This rule checks if response schema validation is enabled for API proxies in Apigee. API response schema validation helps in verifying that the response payloads match the expected schema definitions, preventing schema violations that could lead to data leakage or security vulnerabilities. To enable this, configure the `Response` policy with schema validation in the Apigee API proxy settings. Regular audits and updates to the schema definitions are recommended to adapt to new compliance standards.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/fundamentals/security
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/response-schema-validation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/apigee/docs/api-platform/fundamentals/proxy-endpoints
- rule_id: gcp.apigee.environment.api_rate_limiting_api_quota_limit_configured
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Quota Limit Configured
  scope: apigee.environment.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Rate Limiting and Quota Limits in Apigee Environments
  rationale: Configuring API rate limiting and quota limits in Apigee environments is crucial for preventing service abuse, ensuring fair usage among clients, and protecting backend services from overuse. Lack of such configurations can lead to resource exhaustion, degraded service performance, and increased operational costs, while also potentially violating regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule checks for the configuration of API rate limiting and quota limits within Apigee environments. Properly setting these limits helps manage traffic, control usage, and enforce policies that safeguard the API infrastructure. To verify, review the Apigee environment configurations for defined rate limits and quotas. Remediation involves using the Apigee management interface to set appropriate rate and quota policies aligned with business needs and compliance standards.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/config-overview
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/apigee/docs/api-platform/antipatterns/rate-limiting
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
- rule_id: gcp.apigee.environment.api_rate_limiting_api_stage_burst_limit_configured
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Stage Burst Limit Configured
  scope: apigee.environment.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Stage Burst Limit Configured for Apigee Environments
  rationale: Configuring burst limits in Apigee environments helps prevent API abuse, ensuring service stability and protecting against denial-of-service attacks. It mitigates the risk of unexpected spikes in request traffic that can deplete resources, leading to service disruption and potential financial losses. Proper rate limiting also aids in compliance with industry regulations that mandate robust security controls for API management.
  description: This rule checks that burst limits are configured for APIs in Apigee environments to control the maximum number of requests allowed over a short time. To verify, review the API proxy configurations in the Apigee console or via Apigee API Management APIs, ensuring that burst limit policies are applied. To remediate, define and implement burst limit policies according to your organization's traffic management policies, using the Apigee Edge management interface or API.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
  - https://cloud.google.com/security/compliance/cis/gcp/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/apigee/docs/api-platform/antipatterns/rate-limiting
- rule_id: gcp.apigee.environment.api_rate_limiting_api_stage_configured
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Stage Configured
  scope: apigee.environment.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Rate Limiting is Configured for Apigee Environments
  rationale: Configuring API rate limiting in Apigee environments is crucial to prevent abuse and ensure the availability of APIs. Without proper rate limits, an API can be overwhelmed by excessive requests, leading to service degradation or downtime. This configuration helps in meeting compliance with data protection regulations and preventing potential denial-of-service attacks.
  description: This rule checks whether API rate limiting is configured for each API stage within an Apigee environment. It verifies that appropriate thresholds are set to control the number of requests an API can handle over a specified period. To verify, review the API proxy settings in the Apigee management console and ensure rate-limiting policies are applied. Remediation involves configuring rate limit policies in the Apigee Edge UI by specifying the allowed request rate and burst capacity.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apigee.environment.api_rate_limiting_api_stage_throttle_overrides_not_unbounded
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Stage Throttle Overrides Not Unbounded
  scope: apigee.environment.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure API Rate Limits Are Set for Apigee Environments
  rationale: Unbounded API rate limits can lead to resource exhaustion, service degradation, and increased operational costs. By setting proper rate limits, organizations can mitigate the risk of denial-of-service attacks and ensure fair usage of APIs. This aligns with compliance frameworks that require managing system capacity and maintaining service availability.
  description: This rule checks that API rate limiting and stage throttle overrides in Apigee environments are not set to unbounded values. Administrators should configure specific limit values to control the number of requests per second allowed for each API. Verify this by reviewing the Apigee environment's API proxy configurations. To remediate, set appropriate throttle limits in the API proxy settings to prevent excessive use and potential abuse.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/architecture/best-practices-for-building-api-products
- rule_id: gcp.apigee.environment.api_rate_limiting_api_usage_plan_required_for_api_keys
  service: apigee
  resource: environment
  requirement: API Rate Limiting API Usage Plan Required For API Keys
  scope: apigee.environment.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enforce API Rate Limiting with Usage Plans for API Keys in Apigee
  rationale: Implementing API rate limiting with usage plans is crucial to protect APIs from abuse and unexpected traffic spikes that could lead to degraded performance or denial of service. It also helps in managing costs and maintaining service quality by controlling the consumption of API resources. Regulatory frameworks like PCI-DSS require mechanisms to prevent excessive user activity that could compromise system integrity.
  description: This rule checks if API rate limiting is enforced for API keys through usage plans in Apigee environments. Without usage plans, API keys might allow unregulated access, leading to potential service disruptions and security incidents. To verify, ensure that all APIs associated with API keys have a corresponding usage plan that defines limits. Remediation involves creating or updating usage plans to include specific rate limits for APIs accessed via API keys.
  references:
  - https://cloud.google.com/apigee/docs/api-platform/reference/policies/quota-policy
  - https://cloud.google.com/apigee/docs/api-platform/security/api-security-best-practices
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/apigee/docs/api-platform/antipatterns/rate-limiting
- rule_id: gcp.apikeys.key.api_restrictions_configured
  service: apikeys
  resource: key
  requirement: API Restrictions Configured
  scope: apikeys.key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure API Keys Have Configured API Restrictions
  rationale: Configuring API restrictions for API keys limits their use to specific APIs, minimizing the risk of unauthorized access and misuse. Without restrictions, compromised keys can be exploited to access multiple services, leading to data breaches and potential financial and reputational damage. Compliance with standards such as GDPR and PCI-DSS often requires strict access controls on sensitive APIs.
  description: This rule checks if API keys in GCP have API restrictions configured to limit their use to authorized APIs only. To verify, review the API key settings in the Google Cloud Console under 'API & Services > Credentials' and ensure that each key has specific APIs selected in the 'API restrictions' section. Remediation involves adding the necessary API restrictions to each key, ensuring they are only usable with the intended services.
  references:
  - https://cloud.google.com/docs/authentication/api-keys#restricting_api_keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.apikeys.key.platform_api_expiration_required
  service: apikeys
  resource: key
  requirement: Platform API Expiration Required
  scope: apikeys.key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Require API Key Expiration for Platform APIs
  rationale: Requiring an expiration date for API keys mitigates the risk of unauthorized access by ensuring that unused or compromised keys are invalidated after a certain period. This practice reduces the attack surface and aligns with best practices for key management, supporting compliance with standards like PCI-DSS and SOC2. Expiring API keys also helps prevent data breaches and unauthorized data exposure, protecting sensitive business information.
  description: This rule checks if API keys used for accessing platform APIs have an expiration date set. To verify, examine the API key configuration in the GCP Console or using the gcloud command-line tool. If an expiration date is not set, update the key settings to include one, ensuring it aligns with your organization's security policy on key rotation and lifecycle management. Regularly review and update expiration dates to adapt to changing security requirements.
  references:
  - https://cloud.google.com/docs/authentication/api-keys#enforcing_expiration_dates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/iam/docs/using-manage-api-keys
  - https://cloud.google.com/security/best-practices/encryption
- rule_id: gcp.apikeys.key.platform_api_rotation_policy_defined
  service: apikeys
  resource: key
  requirement: Platform API Rotation Policy Defined
  scope: apikeys.key.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure API Key Rotation Policy is Defined for Platform APIs
  rationale: Defining a key rotation policy for API keys is crucial to mitigate the risk of key compromise. Regular rotation reduces the window of opportunity for an attacker who obtains an API key, enhancing the overall security posture. It also aligns with compliance requirements such as PCI-DSS and ISO 27001, which mandate regular review and rotation of keys to protect sensitive data.
  description: This rule checks whether a key rotation policy is defined for API keys used to access Google Cloud Platform services. A rotation policy should specify the frequency of key changes and ensure that keys are not used beyond their intended lifespan. To verify, ensure that API keys have an associated rotation schedule in the GCP Console under the 'API & Services' section. Remediation involves setting up a key rotation policy using automation tools like Cloud Functions or external scripts to regularly rotate keys.
  references:
  - https://cloud.google.com/docs/authentication/api-keys
  - https://cloud.google.com/iam/docs/managing-api-keys
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0, Control 1.9
  - PCI-DSS Requirement 3.6.4
  - ISO 27001:2013 Section A.9.2.3
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.apikeys.key.platform_api_scopes_least_privilege
  service: apikeys
  resource: key
  requirement: Platform API Scopes Least Privilege
  scope: apikeys.key.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure API Keys Use Least Privilege Platform API Scopes
  rationale: Limiting API key permissions to the minimum required reduces the risk of unauthorized access and data breaches. By adhering to the principle of least privilege, organizations can prevent privilege escalation and minimize the attack surface, meeting compliance requirements for frameworks like NIST and ISO 27001.
  description: This rule checks that API keys are configured with the minimum necessary platform API scopes to perform their intended functions. Ensure that each API key is scoped to access only the required APIs, avoiding overly broad permissions. To verify, review API key configurations in the Google Cloud Console or via the gcloud command-line tool. Remediation involves updating the API key settings to restrict scopes to the least privilege necessary for application functionality.
  references:
  - https://cloud.google.com/docs/authentication/api-keys
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundation
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.apikeys.key.rotated_in_90_days
  service: apikeys
  resource: key
  requirement: Rotated In 90 Days
  scope: apikeys.key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: API Key Rotation Every 90 Days
  rationale: Regular rotation of API keys reduces the risk of unauthorized access by minimizing the window of opportunity for credential compromise. This practice is crucial for protecting sensitive data and maintaining compliance with security standards, such as PCI-DSS, which mandate strict access control measures. By rotating API keys, organizations can mitigate risks associated with leaked or exposed credentials, thereby enhancing overall data security.
  description: This rule checks whether API keys in Google Cloud Platform are rotated at least every 90 days. Regular key rotation is a best practice to ensure that any compromised keys are quickly rendered useless. To verify compliance, review the creation and last rotation dates of API keys in the GCP console or through the API. If a key is older than 90 days, it should be regenerated and updated in all applications that use it. This process involves updating configurations and ensuring that old keys are properly revoked.
  references:
  - https://cloud.google.com/docs/authentication/api-keys
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/cyberframework
- rule_id: gcp.appengine.application.enforce_https_in_transit
  service: appengine
  resource: application
  requirement: Enforce HTTPS In Transit
  scope: appengine.application.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce HTTPS for App Engine Application Traffic
  rationale: Enforcing HTTPS ensures that data in transit is encrypted, protecting it from interception and tampering. This is crucial for maintaining user trust and meeting compliance requirements such as PCI-DSS and HIPAA, which mandate secure transmission of sensitive information. Unsecured data transmission can lead to data breaches, resulting in financial losses and reputational damage.
  description: This rule checks that your App Engine applications are configured to enforce HTTPS for all incoming traffic. To verify, ensure that the 'secure' option for URL handlers in the app.yaml file is set to 'always'. Remediation involves updating the app.yaml configuration and redeploying the application to require HTTPS. This practice prevents unencrypted data from being transmitted over the network, mitigating the risk of interception.
  references:
  - https://cloud.google.com/appengine/docs/standard/python/config/appref
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-52r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.appengine.application.engine_managed_updates_enabled
  service: appengine
  resource: application
  requirement: Engine Managed Updates Enabled
  scope: appengine.application.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure App Engine Managed Updates Are Enabled
  rationale: Enabling managed updates for App Engine applications helps maintain the security and stability of applications by automatically applying security patches and updates. This reduces the risk of vulnerabilities being exploited and ensures compliance with security best practices, which is critical for protecting sensitive data and maintaining customer trust.
  description: This rule checks if managed updates are enabled for App Engine applications. Managed updates automatically apply the latest security patches and updates, minimizing the need for manual intervention. To verify, check the App Engine settings in the Google Cloud Console or use the gcloud command-line tool. To enable, navigate to the App Engine settings and select the option to enable managed updates, ensuring your application remains secure and compliant.
  references:
  - https://cloud.google.com/appengine/docs/standard#automatic_updates
  - https://cloud.google.com/security/compliance/fedramp
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.appengine.application.paas_app_env_secrets_from_vault_only
  service: appengine
  resource: application
  requirement: Paas App Env Secrets From Vault Only
  scope: appengine.application.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure App Engine Uses Vault for Environment Secrets
  rationale: Storing application secrets directly within code or environment variables poses a significant security risk, including unauthorized access and data breaches. Using a centralized secret management solution like Google Cloud Secret Manager ensures that secrets are securely stored, accessed, and audited, aligning with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks that all environment secrets used in Google App Engine applications are sourced exclusively from Google Cloud Secret Manager. Verify that applications access secrets via the Secret Manager API and do not hardcode them in configuration files or environment variables. To remediate, refactor applications to retrieve secrets using the Secret Manager client library and update deployment procedures to remove sensitive information from environment settings.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/appengine/docs/security
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/kms/docs/secrets
- rule_id: gcp.appengine.application.paas_app_logging_enabled
  service: appengine
  resource: application
  requirement: Paas App Logging Enabled
  scope: appengine.application.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure App Engine Logging is Enabled
  rationale: Enabling logging for App Engine applications is crucial for monitoring application behavior, detecting anomalies, and responding to security incidents. Without logging, it becomes difficult to trace actions and events within the application, increasing the risk of undetected breaches and non-compliance with regulations like PCI-DSS and ISO 27001.
  description: This rule checks that logging is enabled for App Engine applications, ensuring that application requests and system events are recorded in Stackdriver. Verify by checking the App Engine settings in the Google Cloud Console under the 'Logging' section. Enable logging by setting the appropriate logging level and ensuring logs are exported to a centralized logging solution. This facilitates audit trails and enhances incident response capabilities.
  references:
  - https://cloud.google.com/appengine/docs/standard/python/logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.appengine.application.paas_app_private_networking_enforced
  service: appengine
  resource: application
  requirement: Paas App Private Networking Enforced
  scope: appengine.application.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for App Engine Applications
  rationale: Enforcing private networking for App Engine applications ensures that the services are only accessible within your VPC, reducing exposure to the public internet. This minimizes the risk of unauthorized access and potential data breaches, aligning with compliance requirements such as PCI-DSS and HIPAA. Private networking also supports better control and monitoring of traffic, enhancing overall security posture.
  description: This rule checks whether App Engine applications are configured to use private networking, restricting access to the application to internal VPC resources. To verify, ensure that the 'app.yaml' file includes 'vpc_access_connector' settings or that the App Engine is using a serverless VPC access connector. Remediation involves configuring a serverless VPC access connector and updating the App Engine app configuration to reference this connector, thereby routing traffic through the VPC.
  references:
  - https://cloud.google.com/appengine/docs/standard/networking-and-firewalls
  - https://cloud.google.com/vpc/docs/configure-serverless-vpc-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.appengine.application.paas_app_tls_min_1_2_enforced
  service: appengine
  resource: application
  requirement: Paas App TLS Min 1 2 Enforced
  scope: appengine.application.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce TLS 1.2+ for App Engine Applications
  rationale: Ensuring that App Engine applications enforce a minimum of TLS 1.2 is critical for safeguarding data in transit against interception and tampering. This reduces vulnerability to known cryptographic attacks targeting older versions of TLS, thereby complying with industry standards such as PCI-DSS and enhancing customer trust by protecting sensitive data.
  description: This rule checks if Google App Engine applications are configured to enforce a minimum TLS version of 1.2. Applications not meeting this requirement should be updated to specify TLS 1.2 or higher to ensure secure communication. This can be verified and configured through the Google Cloud Console under App Engine settings, or via the gcloud CLI tool by setting the 'min_tls_version' parameter in the 'app.yaml' file.
  references:
  - https://cloud.google.com/appengine/docs/standard#languages
  - https://cloud.google.com/security/encryption-in-transit/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/publications/security-and-privacy-controls-information-systems-and-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.appengine.version.paas_app_artifact_encrypted
  service: appengine
  resource: version
  requirement: Paas App Artifact Encrypted
  scope: appengine.version.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt App Engine Version Artifacts at Rest
  rationale: Encrypting App Engine artifacts at rest is crucial for protecting sensitive data from unauthorized access and potential breaches. It mitigates the risks of data exposure in case of storage compromise and ensures compliance with data protection regulations such as GDPR and HIPAA, which mandate encryption of sensitive information. Implementing encryption also aligns with industry best practices to safeguard intellectual property and maintain customer trust.
  description: This rule checks whether the artifacts for App Engine versions are encrypted at rest using Google-managed keys. To verify, ensure that the App Engine service is configured to utilize Google Cloud's default encryption mechanisms. Remediation involves reviewing and confirming the encryption settings in the App Engine configuration and enabling encryption if not already set. Consider using Customer-Managed Encryption Keys (CMEK) for additional control over encryption processes.
  references:
  - https://cloud.google.com/appengine/docs/standard#encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
- rule_id: gcp.appengine.version.paas_app_immutable
  service: appengine
  resource: version
  requirement: Paas App Immutable
  scope: appengine.version.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure App Engine Versions Are Immutable
  rationale: Immutable application versions in GCP's App Engine prevent unauthorized changes to application code and configurations post-deployment, reducing the risk of introducing vulnerabilities or errors. This practice supports compliance with standards requiring controlled software deployment processes, such as ISO 27001 and SOC 2, and mitigates the risk of inadvertent or malicious alterations that could lead to service disruptions or data breaches.
  description: This rule checks that deployed versions of applications in GCP App Engine are immutable, meaning no changes can be made once a version is deployed. To verify, ensure that versioning and deployment practices enforce immutability, such as using CI/CD pipelines that lock versions post-deployment. Remediation involves adjusting deployment strategies and scripts to disallow updates to existing versions, instead deploying new versions for changes. This ensures a consistent and auditable deployment process.
  references:
  - https://cloud.google.com/appengine/docs/standard
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/cis-benchmarks/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/solutions/devops/devops-process
- rule_id: gcp.artifactregistry.repository.container_analysis_enabled
  service: artifactregistry
  resource: repository
  requirement: Container Analysis Enabled
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Container Analysis is Enabled for Artifact Registry Repositories
  rationale: Enabling Container Analysis for repositories is crucial as it provides vulnerability scanning and metadata storage for container images, enhancing the security posture by identifying and mitigating potential vulnerabilities early. This is particularly important for maintaining the integrity and security of containerized applications, reducing the risk of deploying vulnerable containers into production environments, and aiding in compliance with security standards and regulations.
  description: This rule checks if Container Analysis is enabled for repositories in Google Artifact Registry. Container Analysis automatically scans images for vulnerabilities and stores metadata, ensuring that security issues are identified and addressed before deployment. To verify, navigate to the Artifact Registry in the GCP Console and check the settings for each repository to ensure Container Analysis is activated. If not enabled, configure the repository settings to include vulnerability scanning. This enhances security monitoring and helps maintain compliance with industry standards.
  references:
  - https://cloud.google.com/container-analysis/docs/container-analysis-overview
  - https://cloud.google.com/artifact-registry/docs/overview
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/container-analysis/docs/setting-up
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
- rule_id: gcp.artifactregistry.repository.container_scanning_enabled
  service: artifactregistry
  resource: repository
  requirement: Container Scanning Enabled
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Container Scanning in Artifact Registry
  rationale: Enabling container scanning in GCP Artifact Registry is crucial for identifying vulnerabilities and security misconfigurations in container images before they are deployed. This proactive measure helps prevent potential breaches and data leaks, minimizing the risk of exposure to malware and ensuring compliance with industry standards such as PCI-DSS and HIPAA.
  description: This rule checks whether container scanning is enabled for repositories in Google Cloud Artifact Registry. To verify, navigate to the Artifact Registry in the GCP Console, select the desired repository, and ensure that the 'Enable Vulnerability Scanning' option is activated. Remediate by configuring the repository settings to automatically scan images upon upload, providing an additional security layer in your CI/CD pipeline.
  references:
  - https://cloud.google.com/artifact-registry/docs/enable-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-security-rule-requirements/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.artifactregistry.repository.registry_image_scan_on_push_enabled
  service: artifactregistry
  resource: repository
  requirement: Registry Image Scan On Push Enabled
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Image Scan on Push for Artifact Registry Repositories
  rationale: Enabling image scan on push in Artifact Registry helps identify vulnerabilities in container images early in the development process, reducing the risk of deploying insecure applications. This proactive approach aligns with security best practices and regulatory requirements such as PCI-DSS and ISO 27001, which emphasize the importance of vulnerability management and secure software development lifecycle.
  description: This rule checks if the image scan on push feature is enabled for repositories in Google's Artifact Registry. Enabling this setting ensures that each image pushed to the registry is automatically scanned for vulnerabilities, providing immediate feedback to developers and preventing vulnerable images from progressing through the deployment pipeline. To verify and enable this setting, navigate to the GCP Console, select Artifact Registry, and configure the repository settings to enable automatic scanning. Alternatively, use the `gcloud` command-line tool to set the `scan-on-push` flag to true.
  references:
  - https://cloud.google.com/artifact-registry/docs/scan-images
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.artifactregistry.repository.registry_repository_encryption_enabled
  service: artifactregistry
  resource: repository
  requirement: Registry Repository Encryption Enabled
  scope: artifactregistry.repository.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Artifact Registry Repositories Use Encryption at Rest
  rationale: Enabling encryption at rest for Artifact Registry repositories ensures that sensitive data stored within the repositories is protected against unauthorized access and potential data breaches. This is crucial for maintaining the confidentiality and integrity of artifacts, especially in environments subject to regulatory requirements like GDPR, HIPAA, and PCI-DSS, which mandate strong data protection measures.
  description: This rule checks whether encryption at rest is enabled for Google Cloud Artifact Registry repositories. Encryption at rest is a security feature that encrypts your data while stored on disk, preventing exposure from unauthorized access. To verify, ensure that each repository is configured with Google's default encryption or a customer-managed encryption key (CMEK). Remediation involves configuring the repository settings to include encryption options, either through the console or CLI.
  references:
  - https://cloud.google.com/artifact-registry/docs/manage-repos#encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.0.0 - 6.6
  - https://cloud.google.com/security/compliance
  - 'NIST Special Publication 800-57: Recommendation for Key Management'
- rule_id: gcp.artifactregistry.repository.repository_minimum_user_access
  service: artifactregistry
  resource: repository
  requirement: Repository Minimum User Access
  scope: artifactregistry.repository.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Minimal User Access to Artifact Registry Repositories
  rationale: Restricting user access to GCP Artifact Registry repositories minimizes the risk of unauthorized data exposure, accidental deletions, or malicious modifications. Inadequate access controls can lead to compliance issues with standards like PCI-DSS and HIPAA, impacting both business reputation and financial standing.
  description: This rule checks that user access to Artifact Registry repositories is restricted to the minimum necessary permissions. It involves auditing the IAM policies associated with these repositories to ensure that users only have roles that are essential for their tasks. To verify, review IAM policies and adjust roles to comply with the principle of least privilege. Remediation includes removing unnecessary user roles and periodically auditing access controls.
  references:
  - https://cloud.google.com/artifact-registry/docs/access-control
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.artifactregistry.repository.supply_chain_policy_storage_encrypted
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Policy Storage Encrypted
  scope: artifactregistry.repository.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Artifact Registry Repositories Use Encrypted Storage
  rationale: Encrypting storage containing supply chain artifacts protects against unauthorized data access and potential data breaches, which can compromise the integrity of software supply chains. This is crucial for compliance with data protection regulations like PCI-DSS and HIPAA, and mitigates risks of intellectual property theft and reputational damage.
  description: This rule checks if Artifact Registry repositories have encryption enabled at rest. Ensure that Google-managed keys or Customer-Managed Encryption Keys (CMEK) are used to encrypt repository storage. Verification can be done through the GCP Console or gcloud CLI by inspecting repository configurations. To remediate, enable encryption by configuring CMEK or using default encryption settings provided by Google.
  references:
  - https://cloud.google.com/artifact-registry/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-at-rest/
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_image_scanning_enabled
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Image Scanning Enabled
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Image Scanning in Artifact Registry Repositories
  rationale: Enabling image scanning in Artifact Registry repositories helps identify vulnerabilities in container images before they are deployed. This proactive measure mitigates the risk of deploying compromised or vulnerable software, thereby enhancing the security posture of your containerized applications. It also supports compliance with regulations that mandate vulnerability management and secure software development practices.
  description: This rule checks whether image scanning is enabled for repositories in Google Cloud's Artifact Registry. Image scanning should be configured to automatically scan container images for known vulnerabilities, ensuring that only secure images are used within your environment. Verification can be done by checking the repository settings in the Google Cloud Console to ensure that the 'Enable Vulnerability Scanning' option is active. If not enabled, navigate to the Artifact Registry section, select the repository, and activate the image scanning feature under 'Vulnerability Scanning'.
  references:
  - https://cloud.google.com/artifact-registry/docs/monitoring-and-securing
  - https://cloud.google.com/artifact-registry/docs/enable-vulnerability-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_immutable_tags_or_signing_enforced
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Immutable Tags Or Signing Enforced
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Immutable Tags or Signing in Artifact Registry
  rationale: Ensuring that tags in the Artifact Registry are immutable or that all artifacts are signed enhances the integrity and authenticity of software components. This practice mitigates risks such as unauthorized changes to artifacts, which could introduce vulnerabilities or malicious code, impacting business operations and compliance with standards like PCI-DSS and SOC2.
  description: This rule checks that all repositories in the Artifact Registry enforce either immutable tags or artifact signing. Immutable tags prevent changes to existing tags, ensuring that once a version is published, it cannot be altered. Alternatively, enforcing artifact signing requires that all artifacts are cryptographically signed, verifying their origin and integrity. To remediate, configure the repository settings in the Google Cloud Console to enable either immutable tags or set up a signing policy, ensuring all users are aware of the new requirements.
  references:
  - https://cloud.google.com/artifact-registry/docs/creating-repositories
  - https://cloud.google.com/artifact-registry/docs/configuring-repo-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/soc2.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_no_public_pull_push
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry No Public Pull Push
  scope: artifactregistry.repository.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Pull/Push on Artifact Registry Repositories
  rationale: Allowing public pull and push access to artifact repositories poses a significant security risk, as unauthorized users could introduce malicious artifacts or exfiltrate sensitive data. This exposure could lead to supply chain attacks, compromising the integrity of software deployments and violating compliance mandates such as SOC 2, which emphasize access control. Limiting repository access helps protect against these threats, ensuring that only trusted entities can interact with critical software components.
  description: This rule checks for repositories in Google Cloud's Artifact Registry that have public pull or push access enabled. Ensuring that repositories are not publicly accessible prevents unauthorized access and modifications. To verify and remediate, review the IAM policies of each repository to ensure that roles like 'roles/artifactregistry.reader' and 'roles/artifactregistry.writer' are not granted to 'allUsers' or 'allAuthenticatedUsers'. Update these policies to restrict access to only necessary and trusted Google Cloud identities.
  references:
  - https://cloud.google.com/artifact-registry/docs/access-control
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_no_wildcard_admin
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry No Wildcard Admin
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Restrict Admin Rights in Artifact Registry Repositories
  rationale: Using wildcard entries for admin access in Artifact Registry repositories poses a significant security risk by allowing unrestricted access, which can lead to unauthorized actions and potential data breaches. This can compromise the integrity of the software supply chain, impact business operations, and violate compliance standards such as PCI-DSS and SOC2.
  description: This rule ensures that no wildcard entries are used for granting admin roles within Artifact Registry repositories. It checks for IAM policies that assign roles/artifactregistry.admin with wildcard identities, such as 'allUsers' or 'allAuthenticatedUsers.' To verify, review and modify IAM policies to replace wildcard entries with specific user or service account identities. Remediation involves updating IAM policies in the Google Cloud Console or using gcloud CLI to remove wildcard permissions.
  references:
  - https://cloud.google.com/artifact-registry/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_private_or_access_restricted
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Private Or Access Restricted
  scope: artifactregistry.repository.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Artifact Registry Repositories Are Private or Access Restricted
  rationale: Restricting access to Artifact Registry repositories is essential to prevent unauthorized access and potential supply chain attacks. Publicly accessible repositories can expose sensitive artifacts to malicious actors, increasing the risk of data breaches and non-compliance with regulations such as GDPR and PCI-DSS. Limiting access helps maintain the integrity and confidentiality of your software supply chain.
  description: This rule checks if your Artifact Registry repositories are configured to be private or have restricted access to trusted identities only. To verify, ensure that IAM policies are applied to limit access to specific service accounts, groups, or users. Remediation involves updating IAM settings and network policies to restrict repository access to approved identities and networks. Utilize VPC Service Controls to further enhance security by defining a service perimeter.
  references:
  - https://cloud.google.com/artifact-registry/docs/repositories
  - 'CIS GCP Benchmark: Ensure Artifact Registry repositories are not publicly accessible'
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc-service-controls/docs/concepts
  - 'NIST SP 800-53: AC-3 Access Enforcement'
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_replication_cross_region_encrypted
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Replication Cross Region Encrypted
  scope: artifactregistry.repository.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region Encrypted Replication for Artifact Repositories
  rationale: Encrypting cross-region replicated artifacts mitigates the risk of unauthorized data access during transit and storage, protecting sensitive data from exposure and ensuring compliance with data protection standards such as GDPR and HIPAA. It also reduces the impact of potential data breaches and aligns with best practices for securing supply chain integrity in cloud environments.
  description: This rule checks that all cross-region replications of Artifact Registry repositories are encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). Verify configuration by ensuring that encryption settings are enabled for replicated repositories. To remediate, configure the repository settings to use CMEK or GMEK for cross-region replication, ensuring that the encryption keys are properly managed and rotated according to security policies.
  references:
  - https://cloud.google.com/artifact-registry/docs/repositories
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/artifact-registry/docs/cmek
- rule_id: gcp.artifactregistry.repository.supply_chain_registry_replication_destinations_lea_privilege
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Registry Replication Destinations Lea Privilege
  scope: artifactregistry.repository.replication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Limit Artifact Registry Replication Destination Privileges
  rationale: Improperly configured replication destinations in the Artifact Registry can introduce vulnerabilities by allowing unauthorized access or replication of sensitive artifacts. This can lead to data exfiltration, unauthorized access to proprietary software, and potential breaches of compliance with standards such as PCI-DSS and ISO 27001. Limiting privileges helps mitigate these risks by ensuring that only authorized destinations can replicate sensitive data.
  description: This rule checks that replication destinations for Artifact Registry repositories are configured with the least privilege necessary to perform their functions. Specifically, it ensures that only designated and trusted projects or repositories are permitted as replication destinations. Verification involves reviewing IAM policies and permissions associated with the Artifact Registry repositories and ensuring they adhere to the principle of least privilege. Remediation includes updating IAM policies to restrict replication permissions to trusted destinations only.
  references:
  - https://cloud.google.com/artifact-registry/docs/access-control
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundation
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/blog/products/identity-security/understanding-the-shared-responsibility-model-in-google-cloud
- rule_id: gcp.artifactregistry.repository.supply_chain_unused_or_old_tags_expire
  service: artifactregistry
  resource: repository
  requirement: Supply Chain Unused Or Old Tags Expire
  scope: artifactregistry.repository.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Expiration of Unused or Old Tags in Artifact Repositories
  rationale: Allowing unused or old tags to persist in artifact repositories can result in increased storage costs, clutter, and potential security vulnerabilities. By expiring these tags, organizations reduce the risk of deploying outdated or vulnerable artifacts and maintain a cleaner, more efficient repository. This practice supports compliance with best practices in supply chain security by ensuring only current and necessary artifacts are available for deployment.
  description: This rule checks that unused or old tags within Artifact Registry repositories are configured to expire after a defined period. This involves setting lifecycle management policies to automatically remove tags that are not accessed or modified within a specified timeframe. Verification can be done by reviewing and configuring lifecycle policies in the Artifact Registry settings. Remediation includes implementing expiration policies that align with organizational security and operational requirements.
  references:
  - https://cloud.google.com/artifact-registry/docs/tagging
  - https://cloud.google.com/artifact-registry/docs/manage-repos#lifecycle-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.artifactregistry.repository.vulnerability_scanning_enabled
  service: artifactregistry
  resource: repository
  requirement: Vulnerability Scanning Enabled
  scope: artifactregistry.repository.vulnerability_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Enable Vulnerability Scanning for Artifact Registry Repositories
  rationale: Enabling vulnerability scanning on Artifact Registry repositories is crucial for identifying and mitigating security flaws in container images and other artifacts. This proactive measure helps prevent the deployment of vulnerable dependencies, reduces the attack surface, and supports compliance with security standards such as NIST and PCI-DSS. Failing to scan for vulnerabilities can expose your workloads to potential exploits and data breaches.
  description: This rule checks whether vulnerability scanning is enabled for repositories in Google Cloud Artifact Registry. It ensures that images and artifacts stored in the registry are automatically scanned for known vulnerabilities, providing detailed reports on potential security issues. To verify, navigate to the repository settings in the GCP Console and confirm that vulnerability scanning is enabled. Enable it by setting up a scanning configuration in the Artifact Registry settings, which will automatically scan all new and existing artifacts.
  references:
  - https://cloud.google.com/artifact-registry/docs/monitoring-and-scanning
  - https://cloud.google.com/security/compliance/cis#section-5
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
- rule_id: gcp.asset.asset.governance_aggregation_authorization_not_expired_or_revoked
  service: asset
  resource: asset
  requirement: Governance Aggregation Authorization Not Expired Or Revoked
  scope: asset.asset.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Governance Authorization is Valid and Not Revoked
  rationale: Expired or revoked governance aggregation authorizations pose a risk by allowing unauthorized access to sensitive asset data, potentially leading to data breaches and non-compliance with regulatory frameworks like GDPR or SOC2. Maintaining valid authorizations ensures that only designated entities can view and aggregate asset data, mitigating unauthorized data access and maintaining organizational security posture.
  description: This rule checks if governance aggregation authorizations for Google Cloud assets are neither expired nor revoked. It verifies that each authorization token used for asset aggregation is active and has not been revoked, ensuring secure and compliant aggregation processes. To remediate, review the asset governance authorization settings, renew expired authorizations, and revoke any that are no longer needed or have been compromised. Regularly auditing these settings helps maintain a secure and compliant environment.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.asset.asset.governance_aggregation_authorization_target_accounts_correct
  service: asset
  resource: asset
  requirement: Governance Aggregation Authorization Target Accounts Correct
  scope: asset.asset.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Proper Authorization for Governance Aggregation
  rationale: Proper authorization for governance aggregation in GCP is crucial to prevent unauthorized access and potential data breaches. Incorrect configurations may lead to unauthorized users gaining access to sensitive data, posing compliance risks with regulations like GDPR and CCPA. This rule helps maintain the integrity and confidentiality of aggregated governance data across accounts.
  description: This rule checks that target accounts for governance aggregation in GCP have the correct authorization settings. It verifies that only authorized accounts are allowed to aggregate governance data, preventing unauthorized access. To remediate, review IAM policies and ensure that only necessary accounts have permissions for governance aggregation. Confirm with the GCP Console or gcloud CLI that the IAM roles and permissions are correctly set for each target account.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.asset.asset.governance_aggregation_authorization_trusts_least_privilege
  service: asset
  resource: asset
  requirement: Governance Aggregation Authorization Trusts Least Privilege
  scope: asset.asset.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege in Governance Aggregation Authorization
  rationale: Ensuring that governance aggregation authorizations adhere to the principle of least privilege is crucial to minimizing the risk of unauthorized access and potential data breaches. This approach reduces the attack surface by ensuring that users or services have only the permissions necessary to perform their functions. Compliance with least privilege principles helps meet regulatory requirements such as PCI-DSS and ISO 27001, which mandate strict access controls.
  description: This rule checks that permissions granted for governance aggregation in Google Cloud are limited to the minimum necessary. Specifically, it verifies that roles assigned to service accounts or users for asset aggregation tasks do not exceed the required permissions. To verify, review IAM policies for roles associated with asset aggregation and adjust any overly permissive roles. Use predefined roles like 'roles/viewer' instead of 'roles/editor' or 'roles/owner'.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.asset.asset.governance_config_recorder_enabled
  service: asset
  resource: asset
  requirement: Governance Config Recorder Enabled
  scope: asset.asset.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Governance Config Recorder is Enabled for Asset Monitoring
  rationale: Enabling the Governance Config Recorder is crucial for maintaining visibility into asset configurations and changes within your GCP environment. This helps in identifying unauthorized alterations, ensuring compliance with regulatory frameworks, and mitigating risks associated with misconfigurations or malicious activities. Without it, organizations may face compliance violations and increased security vulnerabilities.
  description: This rule checks if the Governance Config Recorder is enabled on your GCP assets. The Governance Config Recorder is a critical component for tracking configuration changes and maintaining a historical record of asset states. To verify, ensure that the Asset Inventory API is enabled and properly configured in your GCP project. Remediation involves enabling the API via the GCP Console under APIs & Services, and setting up the necessary permissions for continuous asset monitoring.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.asset.asset.governance_config_recorder_global_resource_types_tracked
  service: asset
  resource: asset
  requirement: Governance Config Recorder Global Resource Types Tracked
  scope: asset.asset.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Track Global Resource Types with Governance Config Recorder
  rationale: Ensuring that all global resource types in Google Cloud are tracked by the Governance Config Recorder is crucial for maintaining visibility and compliance across cloud assets. Failing to track these resources could lead to undetected policy violations, configuration drift, and security vulnerabilities, which could have significant business and regulatory impacts.
  description: This check verifies that the Governance Config Recorder has been configured to track all global resource types across your Google Cloud environment. Administrators should ensure that the Asset Inventory service is properly set up to record changes in global resources, such as organization policies and IAM settings. To remediate, update the recorder settings to include all necessary global resource types and regularly review the asset inventory reports for anomalies.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.asset.asset.governance_delivery_channel_secure_destination_configured
  service: asset
  resource: asset
  requirement: Governance Delivery Channel Secure Destination Configured
  scope: asset.asset.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Secure Configuration for Governance Delivery Channel
  rationale: Misconfigured delivery channels can expose sensitive data to unauthorized entities, leading to potential data breaches or non-compliance with regulatory standards such as GDPR and CCPA. Ensuring secure destinations for governance data helps protect against unauthorized access and eavesdropping, reducing the risk of data leaks and enhancing the overall security posture.
  description: This rule checks for the secure configuration of destination settings in the Governance Delivery Channel for GCP assets. It verifies that delivery channels are configured to send data to secure, authorized destinations using encrypted protocols (e.g., HTTPS, TLS). To verify, ensure that all delivery channel destinations are correctly configured with encryption and access controls. Remediation involves updating the delivery channel settings to use secure endpoints and validating access permissions.
  references:
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/framework/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/docs/security/security-controls
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.asset.feed.governance_aggregation_authorization_present_and_valid
  service: asset
  resource: feed
  requirement: Governance Aggregation Authorization Present And Valid
  scope: asset.feed.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Valid Authorization for Asset Feed Governance Aggregation
  rationale: Proper authorization for governance aggregation is crucial as it ensures that only legitimate and verified entities can access and aggregate sensitive asset metadata. A lack of this oversight can lead to unauthorized access, data breaches, or inadvertent exposure of confidential information, potentially resulting in non-compliance with data protection regulations such as GDPR or CCPA.
  description: This rule checks whether the authorization for asset feed governance aggregation is present and valid within GCP. It verifies that correct IAM policies are applied to ensure only authorized users or service accounts have access to create and manage asset feeds. To remediate any issues, review and update IAM policies to include only necessary permissions, ensuring that the principle of least privilege is adhered to. Regularly audit and validate these configurations to maintain compliance and security integrity.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/impersonating-service-accounts
- rule_id: gcp.asset.feed.governance_aggregator_auto_enroll_new_accounts
  service: asset
  resource: feed
  requirement: Governance Aggregator Auto Enroll New Accounts
  scope: asset.feed.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Auto-Enroll New Accounts in Governance Aggregator
  rationale: Automatically enrolling new accounts in an asset governance aggregator ensures that all resources are monitored from the outset, reducing the risk of unmanaged assets which can lead to compliance failures and security vulnerabilities. This proactive approach supports regulatory compliance by maintaining visibility and control over the entire asset inventory as per standards such as ISO 27001 and NIST.
  description: This rule verifies that new accounts are automatically enrolled in the Governance Aggregator within the GCP Asset service. It ensures that asset feeds are configured to include new projects, allowing for continuous security monitoring and compliance checks. To verify, check the asset feed configuration for automatic enrollment settings. Remediation involves setting up the asset feed with the necessary permissions and configuration to include new accounts by default.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.asset.feed.governance_aggregator_org_aggregator_enabled
  service: asset
  resource: feed
  requirement: Governance Aggregator Org Aggregator Enabled
  scope: asset.feed.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Governance Aggregator for Organization Asset Monitoring
  rationale: Enabling Governance Aggregator Org Aggregator allows centralized visibility into asset changes across the organization, which is crucial for detecting unauthorized modifications, ensuring policy adherence, and maintaining compliance with regulatory frameworks such as NIST and ISO 27001. Without this feature, organizations risk missing critical asset changes that could lead to data breaches or non-compliance penalties.
  description: This rule checks whether the Governance Aggregator Org Aggregator is enabled in GCP Asset Inventory. This feature aggregates asset data across all projects within an organization, providing a comprehensive view for security and compliance monitoring. To verify, ensure that asset feeds are configured with an organizational scope and that necessary permissions are granted. Remediation involves configuring the asset feed using Cloud Asset Inventory's API or Console, ensuring it's set to monitor organization-wide asset changes.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/asset-inventory/docs/monitoring-asset-changes
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.asset.feed.governance_delivery_channel_destination_access_lea_privilege
  service: asset
  resource: feed
  requirement: Governance Delivery Channel Destination Access Lea Privilege
  scope: asset.feed.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Asset Feed Delivery Channel
  rationale: Limiting access privileges for asset feed delivery channels minimizes the risk of unauthorized access to sensitive data, ensuring compliance with security standards such as NIST and ISO 27001. By enforcing least privilege, organizations can reduce the attack surface and protect against potential data breaches and insider threats.
  description: This rule verifies that the IAM policies associated with the destination of asset feed delivery channels are configured with the principle of least privilege. It checks for overly permissive roles that may grant unnecessary access to sensitive data. To remediate, review and adjust IAM policies to only include roles essential for operational needs, prioritizing roles with least privilege access. Regular audits should be performed to maintain compliance.
  references:
  - https://cloud.google.com/asset-inventory/docs/manage-feeds
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/best-practices/identity
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.asset.feed.governance_delivery_channel_kms_encryption_enabled
  service: asset
  resource: feed
  requirement: Governance Delivery Channel KMS Encryption Enabled
  scope: asset.feed.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Asset Feed Governance Channels
  rationale: Enabling KMS encryption for asset feed governance channels is crucial to protect sensitive data from unauthorized access and potential breaches. Without encryption, data at rest could be exposed to malicious actors, leading to legal penalties and damage to the organization's reputation. This practice aligns with compliance requirements such as GDPR and HIPAA, ensuring that encryption is enforced as a standard security measure.
  description: This rule checks whether Cloud Asset Governance Delivery Channels have Google Cloud Key Management Service (KMS) encryption enabled. To verify, ensure that each asset feed within your GCP environment is configured to use a customer-managed encryption key (CMEK) from KMS. To remediate, navigate to the Cloud Console, select the appropriate asset feed, and configure it to use a CMEK. This ensures that all data transmitted through governance channels is encrypted at rest, using strong encryption standards.
  references:
  - https://cloud.google.com/asset-inventory/docs/feeds-overview
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.backupdr.backup_plan.backup_access_backup_admins_mfa_required
  service: backupdr
  resource: backup_plan
  requirement: Backup Access Backup Admins MFA Required
  scope: backupdr.backup_plan.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure MFA for Backup Admins Accessing Backup Plans
  rationale: Requiring multi-factor authentication (MFA) for backup administrators helps mitigate the risk of unauthorized access to backup plans, reducing potential data breaches and ensuring data integrity. This practice is crucial for maintaining compliance with regulations such as NIST SP 800-63B, PCI-DSS, and ISO 27001, which emphasize strong authentication mechanisms.
  description: This rule verifies that all accounts with administrative access to GCP Backup and DR backup plans have MFA enabled. To ensure compliance, verify that IAM policies enforce MFA for roles with backup admin privileges. Remediation involves configuring IAM policy bindings to require MFA and ensuring that all backup admin accounts are enrolled in an MFA program. Regularly audit IAM roles and policies to confirm ongoing compliance.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/backup-and-dr/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/digital-identity-guidelines
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_plan.backup_access_backup_keys_access_least_privilege
  service: backupdr
  resource: backup_plan
  requirement: Backup Access Backup Keys Access Least Privilege
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: high
  title: Ensure Least Privilege Access to Backup Keys in Backup Plans
  rationale: Limiting access to backup keys is crucial to prevent unauthorized data retrieval, which could lead to data breaches or loss. Ensuring least privilege access aligns with best practices for data protection and is essential for meeting compliance standards such as ISO 27001 and PCI-DSS, which mandate stringent access controls to sensitive information.
  description: This rule checks that access to backup keys within GCP Backup and Disaster Recovery (backupdr) backup plans is restricted to only those identities that absolutely need it. Verify that IAM roles associated with backup keys are set to the minimum scope necessary by reviewing and adjusting IAM policies. Remediation involves auditing current access and revoking unnecessary permissions, ensuring only authorized personnel have access to these critical resources.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/backup-disaster-recovery/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.backupdr.backup_plan.backup_access_backup_vault_rbac_least_privilege
  service: backupdr
  resource: backup_plan
  requirement: Backup Access Backup Vault RBAC Least Privilege
  scope: backupdr.backup_plan.backup_recovery
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Backup Vault Access Follows Least Privilege Principle
  rationale: Implementing least privilege for Backup Vault access helps prevent unauthorized data access and potential data breaches. It minimizes the risk of insider threats and limits the impact of compromised accounts. Adhering to least privilege is crucial for compliance with standards such as ISO 27001 and NIST SP 800-53, which mandate strict access controls.
  description: This rule checks if backup plans in GCP Backup and DR service implement RBAC policies that adhere to the least privilege principle. It verifies that users and service accounts have only the necessary permissions to perform their roles. Remediation involves auditing current IAM roles associated with backup plans and adjusting permissions to ensure minimal necessary access. Verify configurations in the IAM section of the Google Cloud Console and adjust roles as needed to comply with best practices.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/backup-disaster-recovery/docs/access-control
  - https://www.nist.gov/publications/security-and-privacy-controls-information-systems-and-organizations
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.backupdr.backup_plan.backup_access_no_public_access_to_backup_vault
  service: backupdr
  resource: backup_plan
  requirement: Backup Access No Public Access To Backup Vault
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Restrict Public Access to Backup Vaults in GCP
  rationale: Public access to backup vaults can lead to unauthorized data exposure and potential data breaches. This poses significant risks to business continuity and compliance with data protection regulations such as GDPR or HIPAA. Threat actors could exploit publicly accessible backups to extract sensitive information or disrupt recovery processes.
  description: This rule checks that backup vaults in GCP's Backup and DR service are not publicly accessible. Verify that IAM policies do not grant 'allUsers' or 'allAuthenticatedUsers' roles on backup vault resources. Remediation involves adjusting IAM policies to restrict access to only trusted identities and roles, ensuring that backup data remains secure and accessible only to authorized personnel.
  references:
  - https://cloud.google.com/backup-and-dr/docs/security-iam
  - CIS Google Cloud Platform Foundation Benchmark, v1.2.0, Section 7.2
  - NIST SP 800-53 Rev. 5, CP-9 Information System Backup
  - 'PCI-DSS Requirement 3: Protect stored cardholder data'
  - ISO/IEC 27001:2013 - A.12.3 Backup
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.backupdr.backup_plan.backup_monitoring_alert_destinations_configured
  service: backupdr
  resource: backup_plan
  requirement: Backup Monitoring Alert Destinations Configured
  scope: backupdr.backup_plan.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Backup Plan Alert Destinations Are Configured
  rationale: Configuring alert destinations for backup plans is crucial for timely notifications about backup failures or issues. Without proper alerting, organizations risk data loss due to unmonitored backup failures, which can lead to business disruptions, financial loss, and non-compliance with data protection regulations.
  description: This rule checks whether alert destinations are configured for GCP backup plans. Proper configuration ensures that alerts are sent to designated personnel or systems whenever a backup operation fails or encounters issues. To verify, navigate to the Backup and DR service in the GCP Console, select the backup plan, and review the alert configurations under monitoring settings. Remediation includes setting up alert policies in Cloud Monitoring to notify stakeholders via email, SMS, or webhook.
  references:
  - https://cloud.google.com/backup-dr/docs
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.backupdr.backup_plan.backup_monitoring_alert_rules_for_job_failures_configured
  service: backupdr
  resource: backup_plan
  requirement: Backup Monitoring Alert Rules For Job Failures Configured
  scope: backupdr.backup_plan.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Configure Alert Rules for Backup Job Failures
  rationale: Configuring backup monitoring alert rules is crucial for ensuring data resilience and integrity. Without timely alerts on backup job failures, organizations risk data loss, impacting business continuity and compliance with data protection regulations like GDPR and HIPAA. Early detection of backup failures helps mitigate risks associated with data breaches and operational disruptions.
  description: This rule checks if alerting rules are configured for monitoring backup job failures in Google Cloud Backup and DR service. It ensures that alerts are set up to notify administrators of any failed backup operations, enabling prompt investigation and remediation. To verify, access the Google Cloud Console, navigate to Monitoring, and ensure that alerting policies are defined for backup job statuses. Remediation involves creating alerting policies that target backup job failure logs and configuring notification channels.
  references:
  - https://cloud.google.com/backup-dr/docs/monitoring
  - https://cloud.google.com/monitoring/alerts
  - 'CIS GCP Benchmark: 4.5 Ensure Cloud Monitoring is Configured for All Backup Plans'
  - 'NIST SP 800-53: CP-9 - Information System Backup'
  - PCI DSS Requirement 10.6 - Review Logs and Security Events
  - https://cloud.google.com/security
- rule_id: gcp.backupdr.backup_plan.backup_monitoring_alert_rules_for_sla_breach_configured
  service: backupdr
  resource: backup_plan
  requirement: Backup Monitoring Alert Rules For Sla Breach Configured
  scope: backupdr.backup_plan.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Configure SLA Breach Alerts for Backup Plans
  rationale: Configuring SLA breach alerts for backup plans is crucial to ensure timely responses to potential data loss incidents, minimizing downtime and protecting business continuity. Failure to respond promptly to SLA breaches can result in data integrity issues, financial losses, and potential non-compliance with data protection regulations.
  description: This rule checks that Google Cloud Backup and DR (backupdr) service has monitoring and alerting rules configured to notify stakeholders when a Service Level Agreement (SLA) breach occurs. To verify, ensure that alert policies are set up in Cloud Monitoring to track backup operations status and trigger notifications when SLAs are not met. Remediation involves setting up specific alert policies via Google Cloud Console or using Terraform scripts to automate alert configurations.
  references:
  - https://cloud.google.com/backup-and-dr/docs/monitoring-and-alerting
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/terraform/resource-docs/monitoring/alert_policy
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.backupdr.backup_plan.backup_monitoring_metrics_export_enabled
  service: backupdr
  resource: backup_plan
  requirement: Backup Monitoring Metrics Export Enabled
  scope: backupdr.backup_plan.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Backup Monitoring Metrics Export for Backup Plans
  rationale: Exporting backup monitoring metrics is crucial for maintaining visibility into the health and performance of backup operations. This enables organizations to quickly identify and respond to potential issues, ensuring data integrity and compliance with regulations like GDPR and HIPAA that mandate reliable data protection. Without these metrics, organizations face increased risk of undetected backup failures, leading to data loss and potential non-compliance fines.
  description: This rule checks if backup monitoring metrics export is enabled for backup plans on GCP. Ensure that the `monitoring_metrics_enabled` flag is set to true in your backup configuration. Verify this setting in the Cloud Console under Backup and DR service, or use the CLI command `gcloud backup plans describe [PLAN_NAME]` to confirm. If not enabled, update the backup plan settings to include metric exports or configure it using Infrastructure as Code (IaC) tools like Terraform.
  references:
  - https://cloud.google.com/backup-and-dr/docs/monitoring
  - https://cloud.google.com/backup-and-dr/docs/creating-backup-plans
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.backupdr.backup_plan.dr_configured
  service: backupdr
  resource: backup_plan
  requirement: DR Configured
  scope: backupdr.backup_plan.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Backup Plans Have Disaster Recovery Configured
  rationale: Configuring disaster recovery (DR) for backup plans is crucial to minimize data loss and ensure business continuity in the event of an incident. Without a proper DR strategy, organizations are at risk of prolonged downtime and potential non-compliance with industry regulations such as ISO 27001 or SOC2, which could lead to financial and reputational damage.
  description: This rule checks if disaster recovery configurations are in place for Google Cloud backup plans. Specifically, it verifies that the backup plans include settings for cross-region replication and regular DR testing. To verify, review your backup plans in the Google Cloud Console under 'Backup & DR' to ensure DR configurations are active. To remediate, enable cross-region replication and schedule regular DR drills to ensure data resilience across geographic locations.
  references:
  - https://cloud.google.com/backup-and-dr/docs/configure-disaster-recovery
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/architecture/dr-scenarios
- rule_id: gcp.backupdr.backup_plan.plan_min_retention_configured
  service: backupdr
  resource: backup_plan
  requirement: Plan Min Retention Configured
  scope: backupdr.backup_plan.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Backup Plan Minimum Retention Period is Configured
  rationale: Configuring a minimum retention period for backup plans ensures that data is retained for a sufficient time to recover from data loss incidents, supporting business continuity and compliance with data retention policies. Without a defined retention period, critical data might be prematurely deleted, increasing the risk of data loss and potential non-compliance with regulatory requirements.
  description: This rule checks whether a minimum retention period is set for each backup plan in GCP's Backup for DR service. To verify, ensure that each backup plan has the 'minimum retention period' parameter configured according to your organization's data retention policy. Failure to configure this setting could result in insufficient data availability for recovery. To remediate, access the GCP Console, navigate to Backup for DR, and set the 'minimum retention period' for each backup plan based on organizational or regulatory requirements.
  references:
  - https://cloud.google.com/backup-dr/docs/create-backup-plan#retention
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_plan.recovery_point_retention_configured
  service: backupdr
  resource: backup_plan
  requirement: Recovery Point Retention Configured
  scope: backupdr.backup_plan.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Recovery Point Retention is Configured for Backup Plans
  rationale: Configuring recovery point retention ensures that backup data is available for restoration when needed, minimizing data loss in disaster scenarios. This is crucial for meeting business continuity requirements and compliance obligations, such as data retention policies mandated by regulations like GDPR and HIPAA.
  description: This rule checks if recovery point retention is configured for GCP Backup and DR backup plans. To verify, inspect the backup plan settings in the GCP Console under Backup and DR, ensuring that a retention policy is specified. Remediation involves setting a retention period based on business needs and compliance requirements, using the GCP Console or gcloud CLI to adjust settings as necessary.
  references:
  - https://cloud.google.com/backup-disaster-recovery/docs/backup-plan
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/backup-disaster-recovery/docs/how-to/configure-backup-plan
  - https://cloud.google.com/backup-disaster-recovery/docs/how-to/manage-recovery-points
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_coverage_policy_assignme_complete
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Coverage Policy Assignme Complete
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Complete Backup Coverage in Backup Plan
  rationale: Complete backup coverage is critical for ensuring that all crucial data can be recovered in the event of a disaster. Incomplete backup assignments can lead to significant data loss, disrupt business operations, and potentially violate regulatory requirements such as GDPR or HIPAA. Ensuring thorough backup coverage mitigates risks related to data breaches and operational downtime.
  description: This rule checks if all necessary resources are included in the backup plan to ensure full recovery capability. It verifies that the resilience recovery backup coverage policy is assigned to all relevant backup plans. To remediate, review your backup plans and confirm that all critical resources are included. Utilize the GCP Console or gcloud commands to update and assign policies where necessary.
  references:
  - https://cloud.google.com/backup-and-dr/docs
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-34/rev-1/final
  - https://cloud.google.com/backup-and-dr/docs/configuring-backup-plans
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_coverage_unprotected_asse_enabled
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Coverage Unprotected Asse Enabled
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup Coverage for All Critical Assets in GCP Backup Plan
  rationale: Ensuring all critical assets are covered in backup plans enhances resilience by minimizing data loss and reducing downtime during disasters. Unprotected assets can lead to significant business impacts, including financial losses and operational disruptions. Compliance with standards like ISO 27001 and NIST requires comprehensive backup strategies, thus protecting the organization's reputation and customer trust.
  description: This rule checks that all critical assets are included in a GCP backup plan to ensure resilience and recovery capabilities. Verify that backup plans are configured to include all necessary resources by reviewing backup plan policies and settings in the GCP Console. Remediation involves identifying any unprotected assets and updating the backup plan to incorporate these resources, ensuring alignment with organizational recovery objectives.
  references:
  - https://cloud.google.com/backup-and-dr/docs/overview
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/backup-and-dr/docs/create-backup-plan
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_health_failed_jobs_alerti_enabled
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Health Failed Jobs Alerti Enabled
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Enable Alerting for Failed Backup Jobs in Backup Plans
  rationale: Alerting on failed backup jobs is crucial to ensure that data recovery processes are operational and reliable. Without timely alerts, failed backups could go unnoticed, potentially leading to data loss and impacting business continuity. This is especially critical in industries with stringent data protection regulations, where data availability and integrity are mandated.
  description: This rule checks whether alerting is enabled for failed backup jobs within Google Cloud Backup and DR backup plans. It verifies that notifications are configured to alert administrators in case of backup failures, allowing for prompt investigation and remediation. Administrators should ensure that alerting is enabled by configuring appropriate monitoring and notification settings in the Google Cloud Console. Remediation involves setting up Cloud Monitoring alerts to track backup job statuses and configuring notification channels for alert delivery.
  references:
  - https://cloud.google.com/backup-and-dr/docs/overview
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/backup-and-dr/docs/monitoring-backups
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_health_job_success_rate_minimum
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Health Job Success Rate Minimum
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup Plan Job Success Rate Meets Minimum Threshold
  rationale: Achieving a high success rate in backup jobs is crucial for ensuring data resilience and availability during disasters. A low success rate can lead to incomplete or unsuccessful data recovery, posing business continuity risks. Regulatory frameworks often mandate robust backup and recovery plans to protect sensitive data and maintain compliance.
  description: This rule verifies that the success rate of backup health jobs within a GCP Backup and Disaster Recovery (backupdr) plan meets or exceeds a specified minimum threshold. It checks the backup plan configurations to ensure that backups are consistently successful, identifying frequent failures that require attention. To remediate, review backup configurations, analyze failure logs, and adjust settings or resources to improve reliability. Regular monitoring and testing of backup operations are recommended to maintain compliance with resilience standards.
  references:
  - https://cloud.google.com/backup-and-dr/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/architecture/framework/security
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_health_last_backup_recency_w_days
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Health Last Backup Recency W Days
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup Health by Monitoring Last Backup Recency
  rationale: Timely and regular backups are crucial for maintaining business continuity and minimizing data loss in disaster recovery scenarios. Delayed or irregular backups can lead to significant risks, including data unavailability, potential non-compliance with data retention policies, and increased vulnerability to data loss from unforeseen incidents.
  description: This rule checks the recency of the last successful backup in the GCP Backup and DR service. It ensures that backup plans for critical resources are regularly executed within a specified number of days. To verify, access the Backup and DR dashboard and review the 'Last Backup' timestamp. Remediate by scheduling regular backups and configuring alerts for backup failures or delays to ensure compliance with organizational resilience policies.
  references:
  - https://cloud.google.com/backup-and-dr/docs/concepts
  - https://cloud.google.com/architecture/dr-scenarios
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_plan.resilience_recovery_backup_inventory_resource_discove_synced
  service: backupdr
  resource: backup_plan
  requirement: Resilience Recovery Backup Inventory Resource Discove Synced
  scope: backupdr.backup_plan.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Backup Inventory Resource Discovery is Synced
  rationale: Maintaining an up-to-date and accurate backup inventory is crucial for effective disaster recovery and business continuity planning. Unsynced or outdated resource inventories can lead to incomplete or ineffective recovery processes, potentially resulting in data loss, increased downtime, and non-compliance with industry regulations such as ISO 27001 or SOC2.
  description: This check verifies that the resource discovery for backup inventory within the backup plan is consistently synced. A correctly synchronized backup inventory ensures that all resources are accounted for in the event of a disaster recovery operation. To verify, ensure the backup plan settings include automatic synchronization of resource discovery and regularly audit the sync status. Remediation involves configuring the backup plan to enable regular sync operations and monitoring for any sync failures.
  references:
  - https://cloud.google.com/backup-and-dr/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/backup-and-dr/docs/troubleshooting
- rule_id: gcp.backupdr.backup_vault.backup_encryption_backup_storage_immutability_enabled
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Backup Storage Immutability Enabled
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Storage Immutability and Encryption
  rationale: Enabling encryption and immutability for backup storage ensures that sensitive data is protected from unauthorized access and tampering, reducing the risk of data breaches and ransomware attacks. This measure is critical for maintaining data integrity and meeting regulatory requirements such as GDPR and HIPAA, which mandate protection of personal and sensitive information.
  description: This rule checks that backup storage within the Google Cloud Backup and DR service is configured with encryption and immutability. Encryption at rest protects data by making it unreadable without the appropriate decryption keys, while immutability ensures that backups cannot be modified or deleted within a specified timeframe. Verify these settings in the GCP Console under Backup and DR policies, and enable encryption and immutability in the backup settings to comply with best practices and regulatory standards.
  references:
  - https://cloud.google.com/backup-dr/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.backupdr.backup_vault.backup_encryption_cmk_cmek_key_configured
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption CMK Cmek Key Configured
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vaults Use Customer-Managed Encryption Keys (CMEK)
  rationale: Encrypting backup data with Customer-Managed Encryption Keys (CMEK) strengthens data protection by providing customers with control over encryption keys. This enhances security against unauthorized access and aids in meeting regulatory compliance requirements such as GDPR and HIPAA by ensuring that sensitive data remains encrypted and under customer control.
  description: This rule checks whether backup vaults in GCP Backup and DR Service are configured to use Customer-Managed Encryption Keys (CMEK) for encrypting data at rest. To verify, examine the backup vault settings in the Google Cloud Console or use gcloud commands to ensure a CMEK is specified. Remediation involves configuring the backup vault to use a CMEK by assigning a Google Cloud Key Management Service (KMS) key to the vault settings, thereby enhancing data protection.
  references:
  - https://cloud.google.com/backup-dr/docs/using-cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/publications/sp-800-53r5-security-and-privacy-controls
- rule_id: gcp.backupdr.backup_vault.backup_encryption_enabled
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Enabled
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vault Encryption is Enabled
  rationale: Enabling encryption for backup vaults in GCP ensures that sensitive data is protected against unauthorized access, reducing the risk of data breaches and ensuring compliance with data protection regulations such as GDPR, HIPAA, and PCI-DSS. Without encryption, backups are vulnerable to unauthorized access, which could lead to data leaks and significant financial and reputational damage.
  description: This rule checks whether encryption is enabled for backup vaults in the GCP Backup and Disaster Recovery service. Encryption at rest should be enabled to protect backup data from unauthorized access. To verify, navigate to the Backup and Disaster Recovery settings in the GCP Console and ensure that encryption settings are configured. Remediation involves enabling Google-managed or customer-managed encryption keys (CMEK) for backup vaults.
  references:
  - https://cloud.google.com/backup-disaster-recovery/docs/encrypting-data
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_vault.backup_encryption_encryption_at_rest_enabled
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Encryption At Rest Enabled
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vaults Are Encrypted At Rest
  rationale: Encrypting backups at rest is critical to protect sensitive data from unauthorized access and potential data breaches. It ensures compliance with regulatory requirements such as GDPR and HIPAA, which mandate data protection measures. In the event of physical theft or unauthorized access to storage media, encryption at rest mitigates the risk of data compromise.
  description: This rule verifies that all backup vaults within the Backup and DR service have encryption at rest enabled. This is accomplished by checking the configuration settings of backup vaults to ensure that the 'encryption' property is set to 'true'. To remediate, ensure that encryption is enabled during the creation of backup vaults or by modifying existing vault settings. Regular audits should be conducted to maintain compliance.
  references:
  - https://cloud.google.com/backup-dr/docs/encryption-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
- rule_id: gcp.backupdr.backup_vault.backup_encryption_encryption_in_transit_tls_min_1_2
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Encryption In Transit TLS Min 1 2
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vault Uses TLS 1.2+ for In-Transit Encryption
  rationale: Utilizing TLS version 1.2 or higher for data in transit within Backup Vaults is crucial to protect sensitive data against interception and man-in-the-middle attacks. This practice helps avoid potential data breaches and aligns with compliance frameworks like PCI-DSS and HIPAA, which mandate strong encryption standards to safeguard customer information.
  description: This rule checks that Google Cloud's Backup Vault service is configured to use TLS version 1.2 or higher for encrypting data in transit. To verify compliance, ensure that all backup configurations use the appropriate encryption settings as specified in the GCP console under Backup settings. Remediation involves updating the configurations to enforce the use of TLS 1.2 or later, thereby enhancing the security posture of your backup data.
  references:
  - https://cloud.google.com/backup-and-dr/docs/encryption
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
- rule_id: gcp.backupdr.backup_vault.backup_encryption_key_policy_least_privilege
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Key Policy Least Privilege
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Least Privilege for Backup Vault Encryption Key Policy
  rationale: Applying the principle of least privilege to encryption key policies mitigates the risk of unauthorized access to backup vaults, preventing data breaches and ensuring compliance with data protection regulations. Limiting access reduces the potential attack surface, protecting sensitive data from malicious insiders or external threats.
  description: This rule checks whether the encryption key policy for backup vaults adheres to the principle of least privilege by ensuring only authorized users and services have access. Review IAM roles and permissions associated with the encryption keys to confirm that they are restricted to necessary personnel. Remediation involves auditing current permissions, removing unnecessary access, and aligning with organizational security policies.
  references:
  - https://cloud.google.com/backup-and-dr/docs/security
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.backupdr.backup_vault.backup_encryption_key_rotation_enabled
  service: backupdr
  resource: backup_vault
  requirement: Backup Encryption Key Rotation Enabled
  scope: backupdr.backup_vault.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Backup Vault Encryption Key Rotation is Enabled
  rationale: Regular rotation of encryption keys for backup vaults is crucial to mitigate the risk of unauthorized data access and to adhere to data protection regulations. Frequent key rotation reduces the potential impact of key compromise, ensuring that encrypted data remains secure over time. It is also a requirement for compliance with several data protection standards, providing a safeguard against evolving security threats.
  description: This rule checks whether the encryption keys for Google Cloud Backup Vaults are configured to rotate automatically. To verify, ensure that the key rotation schedule is set in the Cloud Key Management Service (KMS) for the keys associated with your backup vaults. Remediation involves configuring automatic key rotation in KMS and setting an appropriate rotation period, such as every 90 days, to enhance data security and compliance.
  references:
  - https://cloud.google.com/kms/docs/rotation
  - https://cloud.google.com/backup-and-dr/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.backupdr.backup_vault.enabled_for_vms
  service: backupdr
  resource: backup_vault
  requirement: Enabled For Vms
  scope: backupdr.backup_vault.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Backup Vaults are Enabled for VMs in GCP
  rationale: Enabling backup vaults for virtual machines (VMs) in GCP is critical for ensuring data resilience and disaster recovery. In the event of data loss or corruption, having backups allows for quick restoration, minimizing downtime and potential revenue loss. This practice also supports compliance with regulatory frameworks that mandate data integrity and availability, such as ISO 27001 and PCI-DSS.
  description: This rule checks if backup vaults are configured and enabled for virtual machines within the GCP environment. Ensuring that backup vaults are operational involves verifying that backup policies are correctly applied to VMs and that the backups are regularly scheduled. Remediation involves configuring the backup vault through the GCP Console or command-line interface to include all necessary VMs and setting appropriate backup schedules to meet organizational recovery objectives.
  references:
  - https://cloud.google.com/backup-and-disaster-recovery/docs/backup-overview
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_access_allowed_cidrs_minimized
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Access Allowed Cidrs Minimized
  scope: bigquery.connection.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit Allowed CIDRs for BigQuery Connection Endpoints
  rationale: Restricting CIDR ranges for BigQuery connection endpoints minimizes exposure to unauthorized access, thereby reducing the risk of data breaches and unauthorized data manipulation. This is crucial for protecting sensitive data and ensuring compliance with regulatory frameworks such as GDPR and CCPA, which mandate strict control over data access.
  description: This rule checks whether the CIDR ranges allowed for BigQuery connection endpoints are minimized to only those necessary for legitimate access. It ensures that only specific, trusted IP ranges can access the BigQuery data warehouse, lowering the risk of malicious actors exploiting overly permissive access controls. Verification involves reviewing the CIDR configurations in the BigQuery connection settings and adjusting them to the least permissive ranges necessary for operational requirements.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_access_private_only
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Access Private Only
  scope: bigquery.connection.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict BigQuery Connections to Private Networks Only
  rationale: Limiting BigQuery connections to private networks reduces exposure to unauthorized access and potential data breaches. This approach aligns with best practices for network security by minimizing attack surfaces and ensuring data is only accessible through secure, controlled environments. It is critical for compliance with regulations that require safeguarding data through restricted network access.
  description: This rule checks if all BigQuery connections are configured to use private networking, ensuring that data access is restricted to approved private endpoints. To verify, review the network settings in the BigQuery console to confirm the 'Private Service Connect' option is enabled. Remediation involves updating connections to use private endpoints, minimizing external network exposure.
  references:
  - https://cloud.google.com/bigquery/docs/reference/private-ip
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_access_tls_required
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Access TLS Required
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS for BigQuery Data Warehouse Endpoint Access
  rationale: Enforcing TLS for BigQuery connections ensures data integrity and confidentiality during transit. Without TLS, data transferred between clients and the BigQuery service can be intercepted or tampered with, posing a risk of data breaches. Ensuring TLS compliance helps meet regulatory standards such as GDPR and HIPAA, which mandate secure data transmission practices.
  description: This rule checks if TLS is enforced for data warehouse endpoints in BigQuery connections. It ensures that any connection to the BigQuery service uses TLS to encrypt data in transit. To verify, inspect the configuration settings under BigQuery connections and ensure the 'requireTLS' property is set to true. Remediation involves updating the settings to enforce TLS for all connections, which can typically be configured via the GCP Console or using the bq command-line tool.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-overview
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_authz_no_anonymous_access
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Authz No Anonymous Access
  scope: bigquery.connection.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Anonymous Access to BigQuery Connection Endpoints
  rationale: Allowing anonymous access to BigQuery connection endpoints can lead to unauthorized data retrieval or manipulation, posing significant risks to data confidentiality and integrity. This can result in non-compliance with regulations like GDPR and HIPAA, and expose organizations to potential data breaches, financial penalties, and reputational damage.
  description: This rule checks if BigQuery connection endpoints are configured to prevent anonymous access by ensuring authorization is required for all connections. To verify, ensure that all connection endpoints have IAM policies granting access only to authenticated users with appropriate roles. Remediation involves reviewing and updating IAM policies to restrict access to authorized identities only, removing any wildcard ('allUsers' or 'allAuthenticatedUsers') permissions.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/projects.connections
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/docs/enterprise/best-practices-for-identity-and-access-management#restrict-usage
- rule_id: gcp.bigquery.connection.data_warehouse_endpoint_authz_rbac_least_privilege
  service: bigquery
  resource: connection
  requirement: Data Warehouse Endpoint Authz RBAC Least Privilege
  scope: bigquery.connection.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure BigQuery Endpoint Uses RBAC with Least Privilege
  rationale: Implementing RBAC with the principle of least privilege on BigQuery connections minimizes the risk of unauthorized data access and potential data breaches. This approach helps organizations comply with regulatory standards and reduces the attack surface by limiting permissions to only those necessary for specific roles. Ensuring least privilege is critical in environments where sensitive data is processed or stored, aligning with security best practices and compliance mandates.
  description: This rule checks that all BigQuery connection endpoints enforce Role-Based Access Control (RBAC) with the principle of least privilege. Verify that roles assigned to users or service accounts have only the necessary permissions required for their specific tasks. Remediation involves reviewing current roles and permissions, removing any excessive access rights, and utilizing predefined roles where applicable to enforce strict access controls.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_client_cert_key_length_minimum
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Client Cert Key Length Minimum
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure HSM Client Cert Key Length Meets Minimum Requirements
  rationale: Using a minimum key length for HSM client certificates in BigQuery connections ensures robust encryption, protecting sensitive data against brute-force attacks. Shorter key lengths can expose data to unauthorized access, which may lead to data breaches and violate compliance with standards such as PCI-DSS and HIPAA.
  description: This rule checks that the HSM client certificate key length for BigQuery connections is at least 2048 bits. To verify, review the key length configuration in the BigQuery connection settings. If the key length is shorter, regenerate the certificate with a compliant key length and update the connection configuration accordingly.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/Connection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_client_cert_not_expired
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Client Cert Not Expired
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery HSM Client Certificates Are Valid
  rationale: Expired HSM client certificates in BigQuery connections can lead to unauthorized data access, data breaches, and non-compliance with data protection regulations. Ensuring that these certificates are valid helps maintain secure data transfer and aligns with regulatory requirements such as PCI-DSS and HIPAA, reducing the risk of data exposure and financial penalties.
  description: This rule checks whether the HSM client certificates used in BigQuery connections have expired. Expired certificates can compromise encrypted data integrity and confidentiality. Administrators should regularly monitor certificate expiration dates and renew certificates before expiration to maintain secure connections. Verification can be done through the GCP Console or CLI by reviewing the certificate's validity period associated with the BigQuery connection.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/Connection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_client_cert_trusted_issuer
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Client Cert Trusted Issuer
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery HSM Client Cert Has Trusted Issuer
  rationale: Trusted issuers for HSM client certificates in BigQuery ensure that only authorized entities can establish secure connections, mitigating risks of data breaches and unauthorized access. This is crucial for maintaining data confidentiality and integrity, especially in environments handling sensitive information. Compliance with industry standards and regulatory requirements often mandates the use of trusted certificate authorities.
  description: This rule checks that BigQuery connections using HSM client certificates originate from trusted certificate issuers. To validate, ensure that the issuer's certificate is listed in the trusted certificate authorities of your GCP environment. Remediation involves updating the list of trusted issuers or contacting your certificate authority to address any discrepancies. Proper configuration helps prevent unauthorized data access and supports encryption at rest policies.
  references:
  - https://cloud.google.com/bigquery/docs/hardware-security-modules
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_configuration_present
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Configuration Present
  scope: bigquery.connection.configuration_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery HSM Configuration for Data Warehouse Connections
  rationale: Configuring a Hardware Security Module (HSM) for BigQuery connections enhances data protection by ensuring that encryption keys are securely stored and managed. This reduces the risk of unauthorized data access and helps meet compliance requirements for data protection standards such as PCI-DSS and HIPAA, which mandate strong encryption measures for sensitive data.
  description: This rule checks for the presence of an HSM configuration in BigQuery connection settings, ensuring that data is encrypted at rest using keys managed by a secure external source. To verify, inspect the BigQuery connection configurations to ensure they reference a Cloud HSM key. Remediation involves configuring your BigQuery connections to use Cloud HSM for managing encryption keys, which can be done via the GCP Console or the gcloud command-line tool.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/hsm/docs
  - https://cloud.google.com/bigquery/docs/reference/rest
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_keys_in_hsm_only
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Keys In Hsm Only
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure HSM-Only Storage for BigQuery HSM Keys
  rationale: Storing HSM keys securely in HSMs is critical to prevent unauthorized access and ensure that cryptographic operations are performed in a tamper-resistant environment. This reduces the risk of data breaches and supports compliance with data protection regulations like PCI-DSS and HIPAA, which mandate strong encryption management practices.
  description: This rule checks whether BigQuery connections are configured to use HSM-stored keys exclusively for encryption at rest. Verify that all relevant BigQuery configurations utilize Cloud HSM for key management by inspecting the key ring and key location settings in your GCP project. Remediation involves updating BigQuery connections to point to HSM-stored keys in Cloud KMS, ensuring that encryption keys never leave the secure boundaries of the HSM.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs/hsm
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt3r1.pdf
- rule_id: gcp.bigquery.connection.data_warehouse_hsm_only_approved_hsm_endpoints
  service: bigquery
  resource: connection
  requirement: Data Warehouse Hsm Only Approved Hsm Endpoints
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict BigQuery HSM Connections to Approved Endpoints
  rationale: Restricting BigQuery connections to approved HSM endpoints ensures data encryption keys are managed securely, preventing unauthorized access and maintaining data integrity. This is critical for protecting sensitive data against breaches and meeting compliance with standards like PCI-DSS and HIPAA, which mandate stringent encryption practices.
  description: This rule checks that connections to BigQuery data warehouses are only established with approved HSM endpoints, ensuring that encryption keys are securely managed and stored. Verification involves reviewing the list of configured HSM endpoints against approved lists in your organization's security policies. Remediation requires updating connection settings to exclude unapproved endpoints and ensuring all connections route through approved HSMs.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.connection.datalake_credentials_in_secrets_manager
  service: bigquery
  resource: connection
  requirement: Datalake Credentials In Secrets Manager
  scope: bigquery.connection.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Store Datalake Credentials Using GCP Secrets Manager
  rationale: Storing Datalake credentials directly in code or unprotected environments poses significant security risks, including unauthorized data access and potential data breaches. By using GCP Secrets Manager, organizations can centrally manage, audit, and control access to these sensitive credentials, aligning with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that Datalake credentials used in BigQuery connections are securely stored in GCP Secrets Manager instead of being hardcoded or stored in less secure environments. To verify, review the BigQuery connection settings and ensure that any credentials are referenced via Secrets Manager. Remediation involves migrating existing credentials to Secrets Manager and updating connection configurations to retrieve credentials from there.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.connection.datalake_private_networking_enforced
  service: bigquery
  resource: connection
  requirement: Datalake Private Networking Enforced
  scope: bigquery.connection.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Datalake Private Networking for BigQuery Connections
  rationale: Enforcing private networking for BigQuery connections mitigates the risk of data exposure by restricting access to internal networks, reducing the attack surface for unauthorized access. This is crucial for maintaining data integrity and confidentiality, especially in environments handling sensitive or regulated data. Compliance with standards such as PCI-DSS and HIPAA often requires stringent network access controls to prevent unauthorized data access.
  description: This rule checks if BigQuery connections are configured to use private networking, ensuring that data interactions occur within the confines of a secure, internal network. To verify compliance, inspect the BigQuery connection settings to confirm that private IP is enforced. Remediation involves updating the connection settings to enforce private networking through the use of VPC Service Controls and private Google access. This configuration helps maintain a secure data environment by limiting exposure to the public internet.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/Connection
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.connection.datalake_role_least_privilege
  service: bigquery
  resource: connection
  requirement: Datalake Role Least Privilege
  scope: bigquery.connection.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure BigQuery Connections Use Least Privilege Roles
  rationale: Restricting roles to the least privilege necessary reduces the risk of unauthorized data access and potential data breaches. Over-privileged roles can lead to inadvertent data exposure, impacting business confidentiality and compliance with regulations such as GDPR and HIPAA.
  description: This rule checks that BigQuery connections are assigned the minimal roles necessary to perform their functions. Review and adjust IAM policies to ensure roles do not exceed the required permissions. Use predefined roles where possible and customize only when necessary to align with least privilege principles. Regular audits and updates of permissions should be conducted to adapt to changing requirements and threats.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0
  - NIST SP 800-53 Access Control (AC)
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.bigquery.connection.datalake_tls_required
  service: bigquery
  resource: connection
  requirement: Datalake TLS Required
  scope: bigquery.connection.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS is Enabled for BigQuery Datalake Connections
  rationale: Requiring TLS for BigQuery Datalake connections helps protect sensitive data from interception and unauthorized access during transmission. Failing to enforce TLS can lead to data breaches, non-compliance with data protection regulations like GDPR and HIPAA, and potentially severe financial and reputational damage to the organization.
  description: This rule checks if TLS is enforced for connections to BigQuery Datalake. TLS encryption ensures that data in transit is secure and protected against eavesdropping and man-in-the-middle attacks. To verify, review the connection settings in the Google Cloud Console or use gcloud commands to ensure `--require-tls` is set to true. If TLS is not enforced, update the connection settings to enable TLS by following the Google Cloud documentation for configuring secure connections.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/ConnectionService
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.bigquery.dataset.backup_enabled
  service: bigquery
  resource: dataset
  requirement: Backup Enabled
  scope: bigquery.dataset.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure BigQuery Datasets Have Backup and Recovery Configured
  rationale: Enabling backups for BigQuery datasets is crucial for data resilience and disaster recovery. In the event of accidental data deletion or corruption, maintaining up-to-date backups enables quick restoration of data, minimizing business disruption. Non-compliance with data protection regulations like GDPR or HIPAA can result in significant legal and financial penalties.
  description: This rule verifies that BigQuery datasets have an appropriate backup and recovery configuration. It checks for the presence of mechanisms such as scheduled exports to Google Cloud Storage or Data Transfer Service for automated backups. To remediate, configure regular data exports to a secure storage location, ensuring backup integrity and accessibility. Verify the backup settings through the GCP Console or by using command-line tools like 'bq' to ensure that backup procedures are active and properly configured.
  references:
  - https://cloud.google.com/bigquery/docs/managing-tables#copying_a_table
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#resource
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 5.2
  - 'NIST SP 800-53 Rev. 5 CP-9: Information System Backup'
  - 'PCI-DSS Requirement 9.5.1: Protect stored cardholder data'
  - https://cloud.google.com/solutions/bigquery-data-transfer-service
- rule_id: gcp.bigquery.dataset.classification_configured
  service: bigquery
  resource: dataset
  requirement: Classification Configured
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Dataset Classification is Configured
  rationale: Configuring classification for BigQuery datasets is crucial for data protection and privacy, as it helps categorize and apply appropriate security controls to sensitive data. Failure to configure classification can result in inadequate data protection, potentially leading to unauthorized access, data breaches, and non-compliance with data privacy regulations such as GDPR, HIPAA, and PCI-DSS.
  description: This rule checks if classification is configured for BigQuery datasets, ensuring that data is properly categorized and protected. Verify that datasets have appropriate classification tags and labels configured in the BigQuery console or via API. Remediation involves reviewing your datasets, identifying the sensitivity of the data they contain, and applying suitable classification labels to enhance security measures and meet compliance requirements.
  references:
  - https://cloud.google.com/bigquery/docs/labeling-datasets
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/security
  - https://cloud.google.com/security/compliance/frameworks
- rule_id: gcp.bigquery.dataset.data_analytics_access_policies_least_privilege
  service: bigquery
  resource: dataset
  requirement: Data Analytics Access Policies Least Privilege
  scope: bigquery.dataset.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure BigQuery Datasets Use Least Privilege Access
  rationale: Implementing least privilege access for BigQuery datasets minimizes the risk of data breaches by ensuring users and services have only the permissions they need to perform their tasks. This approach reduces the potential attack surface and limits damage in case of compromised credentials, thus supporting compliance with privacy regulations such as GDPR and CCPA.
  description: This rule checks BigQuery dataset access policies to ensure they adhere to the principle of least privilege. It verifies that permissions are granted based on necessity rather than convenience, avoiding overly broad roles like 'Editor' or 'Owner' for routine data analytics tasks. To remediate, audit all IAM policies associated with BigQuery datasets and restrict roles to the minimum required for functionality, using predefined roles or custom roles where necessary.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 7.1
  - NIST SP 800-53 Rev. 5 - AC-6 Least Privilege
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.bigquery.dataset.data_analytics_catalog_cross_account_sharing_restricted
  service: bigquery
  resource: dataset
  requirement: Data Analytics Catalog Cross Account Sharing Restricted
  scope: bigquery.dataset.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict BigQuery Dataset Cross-Account Sharing
  rationale: Limiting cross-account sharing of BigQuery datasets mitigates security risks associated with unauthorized access and data leakage. This is crucial for maintaining data confidentiality, integrity, and availability, as well as ensuring compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks for any instances of BigQuery datasets being shared with external accounts or domains outside of your organization. It is important to verify that datasets are only shared with trusted entities and to audit sharing configurations regularly. Remediation involves reviewing dataset IAM policies, removing unauthorized accounts, and implementing strict sharing controls to prevent inadvertent data exposure.
  references:
  - https://cloud.google.com/bigquery/docs/shared-datasets#restricting_access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset.data_analytics_catalog_metadata_encryption_enabled
  service: bigquery
  resource: dataset
  requirement: Data Analytics Catalog Metadata Encryption Enabled
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Dataset Metadata Encryption is Enabled
  rationale: Enabling encryption for BigQuery dataset metadata protects sensitive data from unauthorized access and exposure. This is crucial for maintaining data confidentiality, preventing data breaches, and complying with regulatory standards such as HIPAA and PCI-DSS which mandate encryption of sensitive information. By encrypting metadata, organizations reduce the risk of data theft and enhance trust with stakeholders.
  description: This rule checks if BigQuery datasets have encryption enabled for their metadata using Customer-Managed Encryption Keys (CMEK). By default, Google Cloud encrypts data at rest using Google-managed keys. For enhanced security, configure datasets to use CMEK, which offers greater control over encryption keys. Verification can be done via the Google Cloud Console or gcloud CLI by inspecting the encryption configuration of each dataset. To remediate, update the dataset's encryption settings to use a Cloud Key Management Service (KMS) key.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.hhs.gov/hipaa
- rule_id: gcp.bigquery.dataset.data_analytics_catalog_rbac_least_privilege
  service: bigquery
  resource: dataset
  requirement: Data Analytics Catalog RBAC Least Privilege
  scope: bigquery.dataset.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege in BigQuery Dataset Access
  rationale: Adhering to the principle of least privilege in BigQuery ensures that users have only the permissions necessary to perform their job functions, reducing the risk of data breaches and unauthorized data access. This is crucial for protecting sensitive data and maintaining compliance with regulatory requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strict access controls to safeguard personal and financial information.
  description: This rule checks that IAM roles assigned to users and service accounts on BigQuery datasets are limited to the minimum necessary permissions. Roles such as 'bigquery.dataViewer' should be used instead of broader roles like 'bigquery.admin'. Regularly audit and review IAM policies associated with datasets to ensure compliance with the least privilege principle. Implementing resource-level access controls can effectively minimize the risk of unauthorized access and data leaks.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.bigquery.dataset.data_analytics_encrypted_at_rest_cmek
  service: bigquery
  resource: dataset
  requirement: Data Analytics Encrypted At Rest Cmek
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Datasets Use CMEK for Encryption at Rest
  rationale: Encrypting BigQuery datasets with Customer-Managed Encryption Keys (CMEK) at rest enhances data security by providing greater control over encryption keys. This reduces the risk of unauthorized access and meets compliance requirements for data protection, such as those in GDPR, HIPAA, and other frameworks. It helps safeguard sensitive data from potential breaches and insider threats.
  description: This rule checks if BigQuery datasets are encrypted using CMEK, offering stronger security by allowing customers to manage and rotate their own encryption keys. Verify that CMEK is configured by checking the dataset's encryption configuration settings in the GCP Console or via the `bq` command-line tool. To remediate, update the dataset to use a CMEK by applying a key from Cloud Key Management Service (KMS) and ensure that the necessary IAM permissions are granted to BigQuery to use the key.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.dataset.data_analytics_group_external_group_sharing_restricted
  service: bigquery
  resource: dataset
  requirement: Data Analytics Group External Group Sharing Restricted
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict External Sharing for BigQuery Data Analytics Groups
  rationale: Restricting external sharing of datasets in BigQuery minimizes the risk of unauthorized data access, which could lead to data breaches or loss of sensitive information. This control helps organizations comply with data protection regulations such as GDPR and CCPA by ensuring that data is only shared with trusted entities.
  description: This rule checks that datasets in BigQuery are not shared with external groups outside of the organization. To verify compliance, inspect the dataset's IAM policies to ensure that no external identities have been granted access. Remediation involves reviewing and modifying IAM policies to remove any permissions granted to external groups, thus limiting data sharing to internal users or trusted partners only.
  references:
  - https://cloud.google.com/bigquery/docs/security-iam
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.bigquery.dataset.data_analytics_group_roles_minimal
  service: bigquery
  resource: dataset
  requirement: Data Analytics Group Roles Minimal
  scope: bigquery.dataset.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict BigQuery Dataset Roles for Data Analytics Group
  rationale: Limiting roles to the Data Analytics Group ensures that users have only the necessary permissions, reducing the risk of data exposure and unauthorized access. This adherence to the principle of least privilege helps prevent potential data breaches and supports compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks that BigQuery datasets do not grant excessive permissions to data analytics groups, specifically ensuring roles are limited to only what is necessary for analytics tasks. Verify that roles such as 'roles/bigquery.dataViewer' or 'roles/bigquery.dataEditor' are assigned instead of broader roles like 'roles/bigquery.admin'. Remediation involves reviewing IAM policies and adjusting the roles to align with the principle of least privilege, ensuring compliance with security best practices.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/bigquery/docs/security
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset.data_analytics_public_sharing_disabled
  service: bigquery
  resource: dataset
  requirement: Data Analytics Public Sharing Disabled
  scope: bigquery.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Sharing for BigQuery Datasets
  rationale: Allowing public sharing of BigQuery datasets can expose sensitive data to unauthorized users, leading to data breaches and compliance violations. Public access increases the risk of data exfiltration and non-compliance with regulations such as GDPR, HIPAA, and PCI-DSS. Ensuring datasets are not publicly shared helps mitigate these threats and protect organizational assets.
  description: This rule checks for public access permissions on BigQuery datasets and ensures that sharing with 'allUsers' and 'allAuthenticatedUsers' is disabled. To verify, review the Access Control settings of BigQuery datasets and remove any entries that allow public access. Remediation involves using the GCP Console or CLI to modify IAM policies, ensuring datasets are shared only with specific, authorized users or groups.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/itl/applied-cybersecurity/nist-special-publication-800-53
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.dataset.data_analytics_row_level_security_enabled
  service: bigquery
  resource: dataset
  requirement: Data Analytics Row Level Security Enabled
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Row-Level Security for BigQuery Datasets
  rationale: Enabling row-level security in BigQuery datasets is crucial for protecting sensitive data by restricting access to specific rows based on user roles. This minimizes the risk of unauthorized data exposure and helps meet compliance requirements like HIPAA and GDPR, which mandate strict data access controls.
  description: This rule checks if row-level security is enabled for BigQuery datasets, ensuring that access to data is controlled at a granular level. To verify, examine the dataset's access policies to see if row-level access controls are configured. Remediation involves setting up appropriate row-level security policies using GCP's BigQuery interface or API, ensuring that only authorized users can view specific data rows.
  references:
  - https://cloud.google.com/bigquery/docs/row-level-security-intro
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://hipaa.jotform.com/what-is-hipaa-compliance/
  - https://gdpr.eu/what-is-gdpr/
- rule_id: gcp.bigquery.dataset.data_analytics_workgroup_allowed_data_sources_allowlist
  service: bigquery
  resource: dataset
  requirement: Data Analytics Workgroup Allowed Data Sources Allowlist
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Data Analytics Workgroup to Approved BigQuery Data Sources
  rationale: Allowing unrestricted data source access can lead to unauthorized data exposure and breaches. Limiting access to predefined data sources mitigates the risk of data leakage and ensures compliance with regulations such as GDPR and HIPAA by enforcing strict data governance and control.
  description: This rule checks if BigQuery datasets accessed by the Data Analytics Workgroup are limited to a specific allowlist of data sources. Verify that the IAM policies and access configurations for the datasets are set to restrict access only to datasets approved by your organizationâ€™s data governance policies. To remediate, configure IAM roles and permissions to enforce access only to the allowed data sources, thereby ensuring data integrity and compliance with internal policies.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/security-and-compliance-overview
  - https://cloud.google.com/architecture/bigquery-data-governance-guide
- rule_id: gcp.bigquery.dataset.data_analytics_workgroup_network_access_private
  service: bigquery
  resource: dataset
  requirement: Data Analytics Workgroup Network Access Private
  scope: bigquery.dataset.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure BigQuery Dataset Access via Private Networks
  rationale: Restricting BigQuery dataset access to private networks reduces exposure to unauthorized access and potential data breaches. This practice aligns with security best practices by limiting the attack surface and ensures compliance with regulations like HIPAA and PCI-DSS that emphasize data security and privacy.
  description: This rule verifies that BigQuery datasets are only accessible through private IPs by configuring VPC Service Controls and enabling Private Google Access. To confirm, ensure that datasets are part of a service perimeter and that network access is restricted to private subnets. Remediation involves setting up appropriate network policies and VPC configurations to enforce private access.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/security-controls
- rule_id: gcp.bigquery.dataset.data_analytics_workgroup_query_result_encryption_enabled
  service: bigquery
  resource: dataset
  requirement: Data Analytics Workgroup Query Result Encryption Enabled
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Query Result Encryption for Data Analytics
  rationale: Encrypting query results at rest protects sensitive data from unauthorized access, minimizing the risk of data breaches and ensuring compliance with data protection regulations such as GDPR and HIPAA. Without encryption, query results may be exposed to internal and external threats, potentially leading to significant financial and reputational damage.
  description: This rule checks that BigQuery datasets used by data analytics workgroups have query result encryption enabled, using either Google-managed or customer-managed encryption keys (CMEK). To verify, ensure that datasets are configured with the 'encryptionConfiguration.kmsKeyName' property set. Remediation involves updating the dataset's encryption settings through the GCP Console or gcloud command-line tool by specifying a valid KMS key.
  references:
  - https://cloud.google.com/bigquery/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_admin_access_least_privilege
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster Admin Access Least Privilege
  scope: bigquery.dataset.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Least Privilege for BigQuery Dataset Admin Access
  rationale: Over-provisioned access to BigQuery datasets can lead to unauthorized data exposure and manipulation, potentially resulting in data breaches and regulatory compliance failures (e.g., GDPR, HIPAA). Minimizing access rights to only what is necessary reduces the attack surface and helps prevent both insider and external threats.
  description: This rule verifies that the IAM roles associated with BigQuery datasets are configured to adhere to the principle of least privilege. Specifically, it checks for and flags overly permissive roles like 'roles/bigquery.admin' assigned at the dataset level. Remediation involves auditing existing permissions, removing unnecessary roles, and using predefined roles such as 'roles/bigquery.dataEditor' or custom roles tailored to specific user needs to limit access appropriately.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_audit_logging_enabled
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster Audit Logging Enabled
  scope: bigquery.dataset.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure BigQuery Audit Logging is Enabled for Datasets
  rationale: Enabling audit logging for BigQuery datasets is crucial for monitoring access and changes to data, which helps in detecting unauthorized access and potential data breaches. Audit logs play a key role in fulfilling compliance requirements like GDPR and HIPAA, which mandate tracking data access and modifications to protect sensitive information.
  description: This rule verifies that audit logging is enabled for BigQuery datasets, ensuring all access and changes are logged for review. To check, navigate to the Google Cloud Console, select the relevant dataset, and ensure that audit logging is configured. Remediation involves enabling audit logging by setting up the appropriate logging configurations in the GCP Console under the 'Logging' section for BigQuery datasets.
  references:
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_encryption_at_rest_cmek
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster Encryption At Rest Cmek
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Dataset Uses CMEK for Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encrypting BigQuery datasets at rest enhances data security by allowing organizations to have full control over their encryption keys. This reduces the risk of unauthorized data access and can help meet regulatory requirements such as GDPR and HIPAA, which mandate stringent data protection measures.
  description: This rule checks if BigQuery datasets are encrypted with Customer-Managed Encryption Keys (CMEK) instead of Google-managed keys. To verify, ensure that the 'encryptionConfiguration' field is set with a valid 'kmsKeyName' in the dataset's configuration. If not configured, update the dataset settings to use a key from Google Cloud KMS to enable CMEK. This ensures that datasets are protected with keys managed by your organization, offering an additional layer of security.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_private_networking_enforced
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster Private Networking Enforced
  scope: bigquery.dataset.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for BigQuery Data Warehouse Clusters
  rationale: Ensuring that BigQuery data warehouse clusters are configured with private networking mitigates the risk of unauthorized access over the public internet. This configuration helps protect sensitive data from exposure and is crucial for compliance with regulations such as GDPR and HIPAA, which mandate stringent data protection measures. By restricting access to private IP addresses, organizations can better manage network traffic and reduce the attack surface.
  description: This rule verifies that BigQuery datasets designated as data warehouses are configured to enforce private networking. Specifically, this involves ensuring that the datasets are only accessible via internal IP addresses and not exposed to the public internet. To verify compliance, check the dataset's network configuration in the GCP Console or via the gcloud CLI, ensuring that 'privateServiceConnect' is enabled. Remediation involves updating the dataset's networking settings to restrict access only to the private network.
  references:
  - https://cloud.google.com/bigquery/docs/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/architecture/using-vpc-service-controls-to-protect-data
- rule_id: gcp.bigquery.dataset.data_warehouse_cluster_tls_min_1_2_enforced
  service: bigquery
  resource: dataset
  requirement: Data Warehouse Cluster TLS Min 1 2 Enforced
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery TLS Version 1.2+ for Data Warehouse Cluster
  rationale: Enforcing a minimum TLS version of 1.2 for BigQuery datasets helps protect sensitive data from interception and tampering during transit. This is crucial for maintaining data privacy and integrity, reducing the risk of data breaches and ensuring compliance with data protection regulations like GDPR and HIPAA. Organizations can avoid potential financial penalties and reputational damage by adhering to these security standards.
  description: This rule checks that all BigQuery datasets in a data warehouse cluster enforce a minimum TLS version of 1.2. To verify, ensure that your BigQuery configurations specify TLS 1.2 or higher for data in transit. If a dataset does not meet this requirement, update the dataset configuration via the Google Cloud Console or use the bq command-line tool to enforce the required TLS version. Regular audits should be conducted to ensure compliance.
  references:
  - https://cloud.google.com/bigquery/docs/encryption#encryption_in_transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/nist-special-publication-800-52-revision-2-guidelines-selection-configuration-and-use
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.bigquery.dataset.datalake_database_cross_account_sharing_restricted
  service: bigquery
  resource: dataset
  requirement: Datalake Database Cross Account Sharing Restricted
  scope: bigquery.dataset.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account BigQuery Dataset Sharing
  rationale: Restricting cross-account sharing of BigQuery datasets helps prevent unauthorized access to sensitive data, mitigating risks of data breaches and ensuring compliance with privacy laws such as GDPR and CCPA. It also protects intellectual property by limiting data access to trusted entities within the organization, reducing the attack surface.
  description: This rule checks if BigQuery datasets are being shared with accounts outside the organization's domain. By ensuring datasets are only shared internally, organizations can protect data from unauthorized access. Verify by reviewing dataset IAM policies and ensure external accounts are not granted roles. Remediate by removing any permissions granted to external accounts and regularly auditing access controls.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.bigquery.dataset.datalake_database_encrypted
  service: bigquery
  resource: dataset
  requirement: Datalake Database Encrypted
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Datasets Are Encrypted At Rest
  rationale: Encrypting BigQuery datasets at rest protects sensitive data from unauthorized access and breaches, which can lead to data theft and financial losses. It is crucial for meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS, as well as maintaining trust with stakeholders by ensuring that data is stored securely.
  description: This rule checks if BigQuery datasets are encrypted using Customer-Managed Encryption Keys (CMEK) to enhance data protection over the default Google-managed keys. Verify that datasets have CMEK configured by reviewing the dataset encryption settings in the Google Cloud Console or via the gcloud command-line tool. To remediate, configure CMEK for BigQuery datasets by creating a Cloud Key Management Service (KMS) key and associating it with the dataset.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/patch
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.bigquery.dataset.datalake_database_policy_least_privilege
  service: bigquery
  resource: dataset
  requirement: Datalake Database Policy Least Privilege
  scope: bigquery.dataset.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure BigQuery Dataset Policies Enforce Least Privilege
  rationale: Implementing the principle of least privilege in BigQuery dataset policies minimizes the risk of unauthorized access and data breaches. Over-permissioned roles can lead to data exfiltration, unauthorized data manipulation, and compliance violations, impacting business continuity and integrity. Compliance with frameworks like NIST and HIPAA requires stringent access controls to protect sensitive data.
  description: This rule checks that all BigQuery datasets have IAM policies granting only the necessary permissions required for specific roles, avoiding broad permissions like 'Owner' or overly permissive custom roles. Verify the IAM policies using the Google Cloud Console or gcloud CLI to ensure roles are appropriately assigned. Remediation involves reviewing and modifying IAM policies to align with least privilege principles, creating custom roles if necessary, and regularly auditing permissions.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.bigquery.dataset.dataset_cmk_encryption_configured
  service: bigquery
  resource: dataset
  requirement: Dataset CMK Encryption Configured
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Datasets Use Customer-Managed Encryption Keys
  rationale: Using Customer-Managed Encryption Keys (CMKs) for BigQuery datasets enhances data protection by allowing organizations to control and rotate encryption keys, mitigating the risk of unauthorized access. This is especially crucial for sensitive data, helping to meet regulatory requirements such as GDPR, HIPAA, and PCI-DSS, and protecting against data breaches and insider threats.
  description: This rule verifies that BigQuery datasets are configured to use Customer-Managed Encryption Keys (CMKs) rather than the default Google-managed keys. To ensure compliance, check the dataset settings in the Google Cloud Console or use the bq command-line tool to confirm that a CMK is specified. To remediate, create a Cloud KMS key in the desired location and update the dataset's encryption settings to use this key. This process involves setting the kmsKeyName property in the dataset's encryptionConfiguration.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 6.8
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource
  - https://cloud.google.com/kms/docs/rotating-keys
- rule_id: gcp.bigquery.dataset.dataset_public_access_configured
  service: bigquery
  resource: dataset
  requirement: Dataset Public Access Configured
  scope: bigquery.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure BigQuery Datasets Are Not Publicly Accessible
  rationale: Public access to BigQuery datasets poses significant security risks, including unauthorized data exposure and potential data breaches. This can lead to non-compliance with data protection regulations such as GDPR and CCPA, resulting in financial penalties and damage to organizational reputation. Protecting datasets by restricting public access helps mitigate these risks and safeguard sensitive information.
  description: This rule checks for datasets in BigQuery that are configured with public access, meaning any user on the internet can access the data without authentication. Verify that the dataset's permissions do not include 'allUsers' or 'allAuthenticatedUsers' roles, which grant public access. To remediate, update the IAM policy of the dataset to remove these roles and ensure access is limited to authorized users only. This can be done via the Google Cloud Console, gcloud command-line tool, or the BigQuery API.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/setIamPolicy
- rule_id: gcp.bigquery.dataset.default_cmek_encryption_configured
  service: bigquery
  resource: dataset
  requirement: Default Cmek Encryption Configured
  scope: bigquery.dataset.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Datasets Use Default CMEK Encryption
  rationale: Configuring Customer-Managed Encryption Keys (CMEK) for BigQuery datasets protects sensitive data by providing control over encryption keys, reducing the risk of unauthorized data access. This is essential for meeting compliance requirements like GDPR and HIPAA, which mandate strict data protection measures. Additionally, using CMEK helps mitigate the risk of data breaches by ensuring that access to encryption keys is tightly controlled and auditable.
  description: This rule checks that all BigQuery datasets have a default Customer-Managed Encryption Key (CMEK) configured. To verify, ensure that each dataset in your Google Cloud project specifies a CMEK in its encryption configuration. If not configured, update the dataset settings in the Google Cloud Console or use the `bq` command-line tool to specify a key from Cloud Key Management Service (Cloud KMS). This will ensure that data is encrypted at rest using keys that you control.
  references:
  - https://cloud.google.com/bigquery/docs/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.bigquery.dataset.public_access_configured
  service: bigquery
  resource: dataset
  requirement: Public Access Configured
  scope: bigquery.dataset.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Access to BigQuery Datasets
  rationale: Public access to BigQuery datasets can expose sensitive data to unauthorized users, leading to data breaches and compliance violations. Organizations risk reputational damage and financial penalties if confidential information is leaked. Securing datasets aligns with regulatory requirements like GDPR, HIPAA, and PCI-DSS, which mandate strict access controls to protect personal and financial data.
  description: This rule checks if any BigQuery datasets are configured with public access, which allows any user with an internet connection to read or modify data. To verify, inspect the dataset's permissions in the Google Cloud Console or via gcloud CLI for entries granting roles to 'allUsers' or 'allAuthenticatedUsers'. To remediate, remove such entries and set permissions to only include authorized users or groups with specific roles like 'READER' or 'WRITER'.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis#section_5.8
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset_access_entry.data_analytics_external_user_access_reviewed
  service: bigquery
  resource: dataset_access_entry
  requirement: Data Analytics External User Access Reviewed
  scope: bigquery.dataset_access_entry.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Review External User Access to BigQuery Datasets
  rationale: Regularly reviewing external user access to BigQuery datasets is crucial because unauthorized or outdated access can lead to data breaches, compromising sensitive information and violating compliance regulations such as GDPR, HIPAA, and SOC2. Ensuring that only necessary external users have access minimizes the attack surface and helps maintain data integrity and confidentiality.
  description: This rule checks whether access entries for external users on BigQuery datasets have been reviewed and updated regularly. It involves verifying access configurations through the IAM policy bindings for datasets, ensuring that permissions are granted based on the principle of least privilege. Remediation includes conducting a periodic audit of dataset access entries, removing unnecessary external user permissions, and documenting the review process for accountability.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.dataset_access_entry.data_analytics_roles_minimal
  service: bigquery
  resource: dataset_access_entry
  requirement: Data Analytics Roles Minimal
  scope: bigquery.dataset_access_entry.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit BigQuery Dataset Access to Essential Data Analytics Roles
  rationale: Restricting dataset access to minimal required roles reduces the risk of unauthorized data exposure, which is crucial for maintaining data confidentiality and integrity. Over-permissioned roles can lead to accidental or malicious data leakage, impacting business reputation and violating data protection regulations like GDPR and HIPAA.
  description: This rule checks that BigQuery dataset access entries are assigned only to necessary data analytics roles, such as 'roles/bigquery.dataViewer' or 'roles/bigquery.metadataViewer', to minimize privilege levels. Verify by reviewing IAM policies attached to datasets and adjust roles to ensure least privilege. Remediate by removing excessive roles and assigning only essential roles needed for specific data analysis tasks.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles#bigquery-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.bigquery.dataset_access_entry.data_analytics_sso_required
  service: bigquery
  resource: dataset_access_entry
  requirement: Data Analytics Sso Required
  scope: bigquery.dataset_access_entry.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Require SSO for BigQuery Dataset Access
  rationale: Requiring Single Sign-On (SSO) for BigQuery dataset access enhances security by ensuring that user identities are verified through a centralized authentication service. This mitigates risks such as unauthorized access due to compromised credentials and reduces the attack surface by managing access through a single point of enforcement. It also helps organizations comply with identity and access management requirements in compliance frameworks like SOC 2 and ISO 27001.
  description: This rule checks whether Single Sign-On (SSO) is enforced for accessing BigQuery datasets. To verify, ensure that all dataset access entries are configured to authenticate through an enterprise SSO provider. Remediate by integrating GCP IAM policies with your organization's Identity Provider (IdP) to enforce SSO. Update IAM roles and permissions to align with the centralized authentication strategy, minimizing direct assignments of roles to users and leveraging group-based access.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/best-practices-for-enterprise-organizations
  - https://cloud.google.com/architecture/identity-federation-best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.bigquery.reservation.capacity_monitoring_configured
  service: bigquery
  resource: reservation
  requirement: Capacity Monitoring Configured
  scope: bigquery.reservation.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure BigQuery Reservation Capacity Monitoring is Configured
  rationale: Monitoring BigQuery reservation capacity is crucial for optimizing resource allocation and cost management. Without proper monitoring, organizations risk over-provisioning, leading to unnecessary expenses, or under-provisioning, which can cause performance issues and service disruptions. Ensuring capacity monitoring helps in maintaining efficient resource utilization and meeting compliance requirements for data usage transparency.
  description: This rule checks if capacity monitoring is configured for BigQuery reservations. To verify, ensure that the BigQuery reservation settings include monitoring metrics for slot utilization and reservation usage. Remediation involves configuring Stackdriver Monitoring to track these metrics and setting up alerts for threshold breaches. Such configurations enable proactive management of resource capacity and prevent unexpected billing spikes due to inefficient resource use.
  references:
  - https://cloud.google.com/bigquery/docs/reservations-intro
  - https://cloud.google.com/monitoring
  - 'CIS GCP Benchmark: Section 4.6 - Ensure logging and monitoring is configured for BigQuery'
  - 'NIST SP 800-53: SI-4 - Monitoring tools and techniques'
  - ISO/IEC 27001:2013 - A.12.4 Logging and monitoring
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf (PCI DSS 10.6)
- rule_id: gcp.bigquery.routine.data_analytics_group_logging_enabled_where_supported
  service: bigquery
  resource: routine
  requirement: Data Analytics Group Logging Enabled Where Supported
  scope: bigquery.routine.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for BigQuery Routines in Data Analytics Group
  rationale: Enabling logging for BigQuery routines helps organizations track and monitor access and modifications, providing a comprehensive audit trail essential for data integrity and compliance. It mitigates security risks by identifying unauthorized access and allows for timely incident response. Logging is crucial for compliance with regulations such as GDPR, HIPAA, and SOC2 which require detailed activity records.
  description: This rule checks whether logging is enabled for BigQuery routines within the Data Analytics group. To ensure proper logging, configure the appropriate logging sinks in Google Cloud's Logging service to capture routine execution. Verify that logs are being generated and stored securely by checking the integration with Cloud Logging. Remediation involves enabling logging settings in the Cloud Console or using gcloud CLI commands to capture detailed audit logs for routine operations.
  references:
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.bigquery.routine.data_analytics_group_tls_required
  service: bigquery
  resource: routine
  requirement: Data Analytics Group TLS Required
  scope: bigquery.routine.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS is Enforced for BigQuery Routine Connections
  rationale: Enforcing TLS for BigQuery routines is critical to protect data in transit from unauthorized access and potential interception. Failure to secure these connections can lead to data breaches, exposing sensitive analytics and violating compliance standards such as GDPR and HIPAA.
  description: This rule checks if TLS is required for all connections to BigQuery routines, ensuring that data transmitted between clients and BigQuery services is encrypted. To verify, review the IAM policies and network settings associated with BigQuery routines for TLS enforcement. Remediation involves configuring network security settings to mandate TLS for all external and internal communications, ensuring alignment with organizational security policies.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/routines
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.routine.data_warehouse_group_logging_enabled_where_supported
  service: bigquery
  resource: routine
  requirement: Data Warehouse Group Logging Enabled Where Supported
  scope: bigquery.routine.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for BigQuery Routines in Data Warehouse Group
  rationale: Enabling logging for BigQuery routines is crucial for monitoring and auditing data access and manipulation activities. This helps identify unauthorized access or malicious activities, ensuring data integrity and supporting compliance with regulations like GDPR and CCPA. Without logging, detecting anomalies or tracing data breaches becomes challenging, increasing the risk of data loss and non-compliance fines.
  description: This rule checks whether logging is enabled for BigQuery routines within the data warehouse group. To verify, ensure audit logging is configured for all relevant BigQuery datasets and projects. If not enabled, configure audit logs by setting up Google Cloud's Audit Logs for BigQuery to capture Data Access logs. This involves enabling the BigQuery Data Access logs in Google Cloud Logging under the project's IAM & Admin section.
  references:
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/security/compliance/cis/gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.bigquery.routine.data_warehouse_group_require_ssl
  service: bigquery
  resource: routine
  requirement: Data Warehouse Group Require SSL
  scope: bigquery.routine.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Routines Use SSL for Data Warehouse Access
  rationale: Requiring SSL for BigQuery routines accessing data warehouses protects data in transit from eavesdropping and man-in-the-middle attacks. This practice is essential for maintaining data confidentiality and integrity, especially when handling sensitive or regulated data. It aligns with compliance requirements like GDPR, HIPAA, and PCI-DSS that mandate secure data transmission.
  description: This rule checks if BigQuery routines are configured to use SSL when accessing data warehouses. SSL ensures encrypted connections, protecting data from interception during transmission. To verify, inspect routine settings in BigQuery to ensure SSL is enforced. Remediation involves modifying routine configurations to mandate SSL usage, which can be done via the GCP Console or by updating routine definitions using SQL scripts.
  references:
  - https://cloud.google.com/bigquery/docs/reference/standard-sql/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
- rule_id: gcp.bigquery.table.audit_logging
  service: bigquery
  resource: table
  requirement: Audit Logging
  scope: bigquery.table.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for BigQuery Tables
  rationale: Audit logging is crucial in detecting unauthorized access and changes to BigQuery tables, which can lead to data breaches or non-compliance with regulations like GDPR and HIPAA. It provides a comprehensive record of data access and modifications, enabling organizations to monitor and respond to security incidents effectively.
  description: This rule checks if audit logging is enabled for all BigQuery tables within the project. Audit logs should capture 'Admin Read', 'Admin Write', and 'Data Read' events to ensure visibility into all access and modification actions. To verify, check the Logging section under IAM & Admin in the GCP Console and ensure that the appropriate log types are enabled. Remediate by configuring the necessary log types in the IAM policies for BigQuery resources.
  references:
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.bigquery.table.automated_snapshot
  service: bigquery
  resource: table
  requirement: Automated Snapshot
  scope: bigquery.table.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Enable BigQuery Table Automated Snapshots
  rationale: Automated snapshots of BigQuery tables enhance data resilience by allowing recovery from data loss events, such as accidental deletions or corruptions. This practice ensures business continuity, minimizes downtime, and supports compliance with data protection regulations by providing a recovery point objective (RPO).
  description: This rule checks if automated snapshots are enabled for BigQuery tables, ensuring regular backups. Automated snapshots should be configured through the BigQuery service to create point-in-time copies of data, which can be retained for a defined period. To enable, use the Google Cloud Console or gcloud command-line tool to set up snapshot schedules. Remediation involves reviewing table configurations and enabling automated snapshots where missing.
  references:
  - https://cloud.google.com/bigquery/docs/managing-snapshots
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.bigquery.table.automatic_upgrades
  service: bigquery
  resource: table
  requirement: Automatic Upgrades
  scope: bigquery.table.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure BigQuery Tables Use Automatic Security Updates
  rationale: Enabling automatic upgrades for BigQuery tables ensures that the latest security patches and enhancements are applied without delay. This reduces the risk of data breaches and compliance violations by protecting against newly discovered vulnerabilities. It also aligns with regulatory requirements for maintaining up-to-date security measures, thereby safeguarding sensitive data and maintaining trust with stakeholders.
  description: This rule checks whether automatic security updates are enabled for BigQuery tables. To verify, navigate to the Google Cloud Console, select 'BigQuery', and review the settings under 'Automatic Updates' for each table. If not enabled, configure the table settings to allow automatic updates, ensuring that security patches are applied regularly. This ensures that security vulnerabilities are mitigated promptly, minimizing exposure to potential threats.
  references:
  - https://cloud.google.com/bigquery/docs/security
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/document-1000
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.bigquery.table.cmek_at_rest_encryption_configured
  service: bigquery
  resource: table
  requirement: Cmek At Rest Encryption Configured
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Tables Use CMEK for Data Encryption
  rationale: Configuring Customer-Managed Encryption Keys (CMEK) for BigQuery tables ensures that sensitive data is protected using encryption keys managed by the organization. This approach mitigates risks of unauthorized data access and enhances control over data encryption policies. It also supports compliance with regulations such as GDPR, HIPAA, and PCI-DSS that require stringent data protection measures.
  description: This rule verifies that all BigQuery tables are configured to use CMEK for encryption at rest. Specifically, it checks if a customer-managed key is specified in the table's encryption configuration. To comply, update your table settings to reference a Cloud Key Management Service (KMS) key. This can be done via the GCP Console, gcloud CLI, or REST API by specifying the appropriate KMS key resource ID.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/kms/docs
  - 'CIS Google Cloud Platform Foundation Benchmark v1.3.0, ID: 8.1'
  - 'NIST SP 800-53: SC-13, SC-28'
  - 'PCI-DSS v3.2.1: Requirement 3.5'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.table.data_analytics_encrypted_at_rest_cmek
  service: bigquery
  resource: table
  requirement: Data Analytics Encrypted At Rest Cmek
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Tables Use CMEK for Data Encryption at Rest
  rationale: Encrypting data at rest using Customer-Managed Encryption Keys (CMEK) in BigQuery enhances data security by allowing organizations to control the encryption keys, reducing the risk of unauthorized data access. This is crucial for protecting sensitive analytics data against potential threats and complying with strict regulatory requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strong data protection measures.
  description: This rule checks if BigQuery tables are encrypted at rest using CMEK instead of Google-managed keys. To verify, ensure that the 'encryptionConfiguration' property of a BigQuery table includes a valid KMS key reference. Remediation involves configuring BigQuery table settings to specify a CMEK by setting the 'kmsKeyName' parameter. This process not only enhances data security but also provides compliance with regulatory standards requiring encryption at rest.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 7.2
  - NIST SP 800-53 Rev. 5 - SC-13 Cryptographic Protection
  - 'PCI-DSS v3.2.1 - Requirement 3: Protect Stored Cardholder Data'
- rule_id: gcp.bigquery.table.data_analytics_kms_key_policy_least_privilege
  service: bigquery
  resource: table
  requirement: Data Analytics KMS Key Policy Least Privilege
  scope: bigquery.table.policy_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Table KMS Key Policy Uses Least Privilege
  rationale: Implementing the least privilege principle for KMS key policies in BigQuery ensures that only authorized users have access to decrypt data, reducing the risk of data breaches. This is crucial for complying with data protection regulations like GDPR and CCPA, and for safeguarding sensitive business information from unauthorized access or misuse.
  description: This rule checks that BigQuery tables encrypted with Customer-Managed Encryption Keys (CMEK) have KMS key policies granting minimal access only to necessary identities. Verify that the IAM policy on the KMS key does not include overly broad roles, such as roles/cloudkms.admin. Remediation involves reviewing the KMS key IAM policy, ensuring only specific roles like roles/cloudkms.cryptoKeyEncrypterDecrypter are granted to trusted identities, and removing unnecessary permissions.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.table.data_analytics_not_publicly_shared
  service: bigquery
  resource: table
  requirement: Data Analytics Not Publicly Shared
  scope: bigquery.table.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Sharing of BigQuery Tables
  rationale: Publicly sharing BigQuery tables can lead to unauthorized data access, exposing sensitive information to external threats and increasing the risk of data breaches. This practice can result in compliance violations with regulations such as GDPR and HIPAA, potentially resulting in financial penalties and reputational damage.
  description: This rule checks for BigQuery tables that have been configured with public access, which allows anyone on the Internet to view or download the data. To verify, review the table's IAM policy bindings and ensure that 'allUsers' or 'allAuthenticatedUsers' are not granted roles such as 'roles/bigquery.dataViewer'. Remediation involves removing any public bindings and applying the principle of least privilege by granting access only to specific users or groups who require it.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.table.data_warehouse_cross_region_copy_encrypted
  service: bigquery
  resource: table
  requirement: Data Warehouse Cross Region Copy Encrypted
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region BigQuery Table Copies Use Encryption
  rationale: Encrypting cross-region BigQuery table copies protects sensitive data from unauthorized access during transit and storage. Without encryption, data is vulnerable to interception and breaches, posing risks to data integrity and compliance with regulations such as GDPR and CCPA. Encryption at rest helps mitigate potential data exposure in the event of unauthorized access to the storage infrastructure.
  description: This rule checks that BigQuery tables copied across regions are encrypted. Ensure that Customer-Managed Encryption Keys (CMEK) are applied when creating cross-region copies to secure data against unauthorized access. Verify table settings in the BigQuery console or via CLI to ensure encryption is enabled and configured with CMEK. If encryption is not enabled, update the table settings to apply CMEK or transfer data with encryption options enabled.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS GCP Benchmark 1.2.0, Section 7.3
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
- rule_id: gcp.bigquery.table.data_warehouse_encrypted_at_rest_cmek
  service: bigquery
  resource: table
  requirement: Data Warehouse Encrypted At Rest Cmek
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Tables Use CMEK for Data Encryption at Rest
  rationale: Encrypting BigQuery data at rest with Customer-Managed Encryption Keys (CMEK) provides enhanced security by allowing control over cryptographic keys, thus protecting sensitive data against unauthorized access. This approach mitigates risks associated with data breaches and non-compliance with data protection regulations like GDPR and HIPAA, which mandate stringent encryption standards.
  description: This rule checks if BigQuery tables are encrypted using Customer-Managed Encryption Keys (CMEK) rather than Google-managed keys. To verify, inspect the table metadata for the encryption configuration. If not using CMEK, update the table settings to specify a CMEK, ensuring the key is stored in Cloud Key Management Service (KMS). This action enhances data security by giving organizations full control over encryption keys, including rotation and access permissions.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
- rule_id: gcp.bigquery.table.data_warehouse_not_publicly_shared
  service: bigquery
  resource: table
  requirement: Data Warehouse Not Publicly Shared
  scope: bigquery.table.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Sharing of BigQuery Data Warehouse Tables
  rationale: Publicly shared BigQuery tables can expose sensitive data to unauthorized users, increasing the risk of data breaches and non-compliance with privacy regulations such as GDPR and CCPA. Limiting access to only authorized personnel helps protect sensitive information and maintain data integrity.
  description: This rule checks if any BigQuery tables, particularly those used as data warehouses, are publicly accessible. Public access can be verified through the IAM policy of the table, ensuring that no 'allUsers' or 'allAuthenticatedUsers' roles are granted. To remediate, review and update the IAM policy to restrict access to necessary users only, removing any public roles.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.bigquery.table.datalake_access_policies_least_privilege
  service: bigquery
  resource: table
  requirement: Datalake Access Policies Least Privilege
  scope: bigquery.table.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege Access to BigQuery Datalake Tables
  rationale: Implementing least privilege access controls for BigQuery tables mitigates the risk of unauthorized data access, reducing the attack surface and potential data breaches. It ensures compliance with data protection regulations such as GDPR and CCPA by restricting access to sensitive data. Inadequate access management could lead to data leaks, insider threats, and non-compliance with industry standards.
  description: This rule checks whether access to BigQuery tables is granted based on the principle of least privilege. It verifies that IAM roles assigned to users and service accounts have only the necessary permissions for their tasks. To remediate, audit table-level IAM policies and remove any users or roles with excessive permissions. Use predefined roles where appropriate and regularly review access logs to ensure continued compliance.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.bigquery.table.datalake_column_level_access_controls_enabled
  service: bigquery
  resource: table
  requirement: Datalake Column Level Access Controls Enabled
  scope: bigquery.table.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Column-Level Access Controls on BigQuery Tables
  rationale: Implementing column-level access controls in BigQuery is crucial to protecting sensitive data by ensuring that users only access data necessary for their roles. This reduces the risk of data breaches and unauthorized access, which can lead to significant financial and reputational damage, and ensures compliance with regulations like GDPR that mandate data minimization and access restrictions.
  description: This rule checks if column-level access controls are enabled on BigQuery tables, which restrict access to certain columns based on user roles. To verify, inspect the table's access policy configuration in the GCP Console or via the bq command-line tool, and ensure that appropriate IAM roles are defined for sensitive columns. To remediate, define and apply IAM policies that specify who can access sensitive columns, using the GCP Console or the bq tool.
  references:
  - https://cloud.google.com/bigquery/docs/column-level-security-intro
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/Table
  - https://cloud.google.com/bigquery/docs/access-control
- rule_id: gcp.bigquery.table.datalake_encrypted
  service: bigquery
  resource: table
  requirement: Datalake Encrypted
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Tables are Encrypted at Rest
  rationale: Encrypting BigQuery tables at rest protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, especially for organizations handling regulated data. Compliance with encryption requirements is necessary for adhering to standards like GDPR, HIPAA, and PCI-DSS, reducing legal and financial risks.
  description: This rule verifies that all BigQuery tables in the datalake are encrypted at rest using Google-managed or customer-managed keys. Ensure that the `encryptionConfiguration` property is set appropriately for each table. Remediation involves configuring BigQuery to use either Google Cloud's default encryption or specifying a customer-managed encryption key (CMEK) through Cloud KMS. Regular audits and updates to key management policies are recommended to maintain security posture.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#resource
- rule_id: gcp.bigquery.table.datalake_partition_access_policies_least_privilege
  service: bigquery
  resource: table
  requirement: Datalake Partition Access Policies Least Privilege
  scope: bigquery.table.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for BigQuery Table Partition Access
  rationale: Implementing least privilege access policies for BigQuery table partitions reduces the risk of unauthorized data access and potential data leaks. By minimizing permissions, organizations can mitigate insider threats and comply with regulatory requirements such as GDPR and HIPAA that mandate strict access controls on sensitive data.
  description: This rule checks that access to BigQuery table partitions is restricted based on the principle of least privilege. Ensure that only necessary roles are granted to users or service accounts accessing table partitions. Regularly audit and review IAM policies to remove excessive permissions. Utilize GCP's IAM policy analysis tools to identify and remediate over-privileged access.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/bigquery/docs/best-practices-security
- rule_id: gcp.bigquery.table.datalake_partition_catalog_encrypted
  service: bigquery
  resource: table
  requirement: Datalake Partition Catalog Encrypted
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure BigQuery Table Partitions Have Encryption at Rest
  rationale: Encrypting data in BigQuery tables, particularly in datalake partitions, safeguards against unauthorized access and data breaches, which can lead to significant financial and reputational damage. It is critical for meeting compliance requirements such as GDPR and HIPAA that mandate data protection measures. Without encryption, sensitive information is vulnerable to exposure from both internal and external threats.
  description: This rule verifies that all BigQuery table partitions in the datalake catalog are encrypted using Google-managed or customer-managed encryption keys. To check compliance, review the encryption configuration of BigQuery tables via the GCP Console or CLI. If tables are not encrypted, apply Google-managed encryption keys by default, or configure Customer-Managed Encryption Keys (CMEK) for enhanced control. Ensure that all partitions of each table are consistently encrypted at rest.
  references:
  - https://cloud.google.com/bigquery/docs/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
- rule_id: gcp.bigquery.table.datalake_public_sharing_disabled
  service: bigquery
  resource: table
  requirement: Datalake Public Sharing Disabled
  scope: bigquery.table.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Sharing for BigQuery Datalake Tables
  rationale: Allowing public sharing of BigQuery tables can lead to unauthorized access to sensitive data, resulting in data breaches and regulatory non-compliance. Publicly accessible data lakes may expose personally identifiable information (PII) or intellectual property, leading to financial and reputational damage. Ensuring data is only accessible to authorized users mitigates the risk of data leaks and supports compliance with regulations such as GDPR and HIPAA.
  description: This rule checks if any BigQuery tables within the datalake are publicly accessible, which is a critical security concern. Public access should be explicitly disabled by setting the appropriate IAM policies, ensuring that only authorized users or service accounts can access the data. To remediate, review table permissions and remove 'allUsers' or 'allAuthenticatedUsers' from the IAM policy bindings. Regular audits and monitoring should be conducted to prevent unauthorized access.
  references:
  - https://cloud.google.com/bigquery/docs/security
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.bigquery.table.datalake_rbac_least_privilege
  service: bigquery
  resource: table
  requirement: Datalake RBAC Least Privilege
  scope: bigquery.table.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for BigQuery Table Access
  rationale: Implementing least privilege access for BigQuery tables reduces the risk of unauthorized data exposure and potential data breaches. It helps in maintaining compliance with data protection regulations like GDPR and HIPAA, where access to sensitive data must be strictly controlled and auditable. Failing to enforce this can lead to significant financial penalties and reputational damage.
  description: This rule checks for overly permissive roles assigned to BigQuery tables within your GCP projects. Specifically, it ensures that only necessary roles are granted at the table level, thereby adhering to the principle of least privilege. To verify, review the IAM policies and ensure that roles like 'Owner', 'Editor', or custom roles with excessive permissions are not broadly granted. Remediate by assigning specific roles that limit access to only what is required for users to perform their job functions.
  references:
  - https://cloud.google.com/bigquery/docs/access-control
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Computing Foundations Benchmark v1.1.0, 4.2 Ensure that BigQuery datasets are not anonymously or publicly accessible
  - NIST SP 800-53 AC-6 Least Privilege
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigquery.table.datalake_version_immutability_enforced
  service: bigquery
  resource: table
  requirement: Datalake Version Immutability Enforced
  scope: bigquery.table.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Immutability for BigQuery Table Versions
  rationale: Enforcing immutability on BigQuery table versions ensures data integrity and prevents unauthorized or accidental modifications. This is crucial for maintaining data trustworthiness, supporting forensic investigations, and complying with regulations such as GDPR and HIPAA, which mandate stringent data protection and privacy measures.
  description: This rule checks if BigQuery tables have immutability enforced on their versions to prevent changes post-creation. Immutability can be achieved by configuring appropriate access controls and using table snapshots. To verify, ensure that table access policies restrict editing permissions and that table snapshots are used for historical data. Remediation involves reviewing IAM roles and permissions and setting up a regular snapshot schedule to preserve data states.
  references:
  - https://cloud.google.com/bigquery/docs/managing-table-snapshots
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://documentation.compliance.io/gdpr-overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigquery.table.encryption_enabled
  service: bigquery
  resource: table
  requirement: Encryption Enabled
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Table Encryption at Rest
  rationale: Encrypting BigQuery tables at rest mitigates the risk of unauthorized data access and data breaches, which can have significant business and reputational impacts. It is crucial for meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate encryption of sensitive data to protect user privacy and integrity.
  description: This rule checks whether encryption is enabled for BigQuery tables, ensuring that all data stored is encrypted using Google-managed or customer-managed encryption keys. To verify, ensure that BigQuery tables are configured with the appropriate encryption settings in the GCP Console or via the bq command-line tool. If encryption is not enabled, configure the tables using default Google-managed keys or specify customer-managed keys for enhanced control.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
- rule_id: gcp.bigquery.table.maintenance_settings
  service: bigquery
  resource: table
  requirement: Maintenance Settings
  scope: bigquery.table.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure BigQuery Table Maintenance Settings are Configured
  rationale: Proper maintenance settings in BigQuery tables ensure that data access and changes are monitored and controlled, reducing the risk of unauthorized modifications or data loss. This is crucial for maintaining data integrity and compliance with regulatory standards such as GDPR and HIPAA, which require strict data handling and auditing practices.
  description: This check verifies that BigQuery tables have appropriate maintenance settings configured, including setting up update and delete protections and ensuring that audit logging is enabled. To verify, check the table configurations for the presence of IAM policies that restrict access to data modification operations and ensure that logging is enabled via the GCP Console or gcloud CLI. Remediation involves setting up IAM roles with the principle of least privilege and configuring audit logs in the Cloud Logging service.
  references:
  - https://cloud.google.com/bigquery/docs/table-deletion
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.bigquery.table.public_access
  service: bigquery
  resource: table
  requirement: Public Access
  scope: bigquery.table.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to BigQuery Tables
  rationale: Allowing public access to BigQuery tables can lead to unauthorized data exposure, resulting in potential data breaches and compliance violations. This is critical for organizations handling sensitive or regulated data, as it can impact customer trust and lead to significant financial penalties under frameworks like GDPR, HIPAA, or PCI-DSS.
  description: This rule checks if any BigQuery tables are accessible by the 'allUsers' or 'allAuthenticatedUsers' groups, which indicates public access. To verify, inspect the IAM policy bindings of each table for these entries. Remediation involves removing these public entries and ensuring that only specific, authenticated users or groups have access, achieved via the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/getIamPolicy
  - https://cloud.google.com/security-command-center/docs/how-to-remediate-biqquery-findings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/bigquery/docs/table-access-controls-intro
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.bigquery.table.table_cmk_encryption_configured
  service: bigquery
  resource: table
  requirement: Table CMK Encryption Configured
  scope: bigquery.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure BigQuery Tables Use Customer-Managed Keys (CMK) for Encryption
  rationale: Using Customer-Managed Keys (CMK) for BigQuery table encryption ensures that organizations maintain control over encryption keys, thereby reducing the risk of unauthorized data access. This approach aligns with data protection regulations such as GDPR and CCPA, which necessitate strict control over data security and privacy. Additionally, CMK provides an additional layer of security against potential insider threats and supports forensic investigations.
  description: This rule verifies that BigQuery tables are encrypted using Customer-Managed Keys (CMK) instead of the default Google-managed keys. To ensure compliance, configure the BigQuery table to use a Cloud Key Management Service (KMS) key by specifying the 'encryptionConfiguration.kmsKeyName' property. Remediation involves updating table settings to include a specific KMS key for encryption, which can be done via the GCP Console or gcloud command-line tool. Regular audits should be conducted to ensure ongoing compliance with this encryption policy.
  references:
  - https://cloud.google.com/bigquery/docs/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://cloud.google.com/kms/docs/key-rotation
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.bigtable.instance.backup_enabled
  service: bigtable
  resource: instance
  requirement: Backup Enabled
  scope: bigtable.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Enable Backups for Bigtable Instances
  rationale: Enabling backups for Bigtable instances ensures data resilience and disaster recovery capabilities, mitigating risks of data loss due to accidental deletions, corruption, or regional outages. This practice supports business continuity and aligns with compliance requirements by preserving critical data integrity in regulated environments.
  description: This rule checks if backups are enabled for Bigtable instances, a vital step in ensuring data recovery and protection against data loss. To verify, navigate to the Bigtable console, select your instance, and ensure that automated backups are configured. Remediation involves setting up a backup schedule and retention policy appropriate to your organizational needs, leveraging GCP's Bigtable backup capabilities.
  references:
  - https://cloud.google.com/bigtable/docs/backup-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.bigtable.table.data_protection_storage_global_cross_region_replic_encrypted
  service: bigtable
  resource: table
  requirement: Data Protection Storage Global Cross Region Replic Encrypted
  scope: bigtable.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region Replicated Bigtable Data is Encrypted
  rationale: Encrypting data across regions protects sensitive information from unauthorized access during transit and storage. Unencrypted data replication can expose your organization to data breaches, resulting in financial loss and compliance violations with regulations like GDPR and CCPA.
  description: This rule checks that all Bigtable tables replicated across regions use encryption to safeguard data at rest. Ensure that the service account associated with Bigtable has the necessary permissions to encrypt data with customer-managed encryption keys (CMEK). Verify this by checking Bigtable table settings in the Google Cloud Console or using gcloud commands. To remediate, configure Bigtable to use CMEK for cross-region replication.
  references:
  - https://cloud.google.com/bigtable/docs/encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/security/encryption-in-transit/
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.bigtable.table.data_protection_storage_global_encryption_at_rest_al_regions
  service: bigtable
  resource: table
  requirement: Data Protection Storage Global Encryption At Rest Al Regions
  scope: bigtable.table.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Bigtable Tables Use CMEK for Encryption At Rest in All Regions
  rationale: Using customer-managed encryption keys (CMEK) for Bigtable ensures that your organization retains control over the encryption keys that protect data at rest, which is crucial for meeting stringent compliance requirements and mitigating risks of unauthorized data access. This setup is especially important for businesses with sensitive data in highly regulated industries such as finance and healthcare.
  description: This rule verifies that all Bigtable tables across all regions are configured to use customer-managed encryption keys (CMEK) instead of default Google-managed keys. To check, ensure that each Bigtable instance is associated with a key from Cloud Key Management Service (KMS). Remediation involves creating a Cloud KMS key and applying it to your Bigtable tables via instance settings. This helps maintain data integrity and confidentiality by leveraging the enhanced security controls of CMEK.
  references:
  - https://cloud.google.com/bigtable/docs/using-cmek
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57pt1r4.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/resource-hierarchy
- rule_id: gcp.bigtable.table.data_protection_storage_global_pitr_enabled_where_supported
  service: bigtable
  resource: table
  requirement: Data Protection Storage Global Pitr Enabled Where Supported
  scope: bigtable.table.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Bigtable Global PITR is Enabled Where Supported
  rationale: Enabling Point-in-Time Recovery (PITR) for Bigtable provides a critical safeguard against data loss due to accidental deletion or corruption. This feature is essential in maintaining data integrity and availability, which are key to business continuity and compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks if Point-in-Time Recovery (PITR) is enabled for Google Cloud Bigtable tables where it is supported. To verify, ensure that the 'Enable PITR' option is activated in the Bigtable instance settings. If not enabled, data recovery to a specific point in time may not be possible, increasing the risk of data loss. Remediation involves accessing the Bigtable console, navigating to the table settings, and enabling the PITR feature.
  references:
  - https://cloud.google.com/bigtable/docs/data-protection
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.billing.anomaly.cost_alert_destinations_configured
  service: billing
  resource: anomaly
  requirement: Cost Alert Destinations Configured
  scope: billing.anomaly.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Billing Anomaly Cost Alert Destinations Configured
  rationale: Configuring cost alert destinations for billing anomalies is crucial to timely detect and respond to unexpected expenses, which can indicate security breaches or misconfigurations leading to cost spikes. This reduces financial risk and ensures accountability in billing management, while supporting compliance with financial governance standards.
  description: This rule checks if alert destinations are configured for billing anomalies in GCP. It requires setting up notifications to specific channels (e.g., email, Pub/Sub) to alert stakeholders of abnormal cost activities. To verify, ensure that alert policies are configured in the Cloud Console under Billing > Budgets & alerts, and specify destinations. Remediation involves setting up an alert policy with notification channels to promptly detect and handle billing anomalies.
  references:
  - https://cloud.google.com/billing/docs/how-to/notifications
  - https://cloud.google.com/iam/docs/roles-and-permissions#billing
  - https://cloud.google.com/billing/docs/how-to/budgets
  - CIS GCP Benchmark - Billing Alerts
  - 'NIST SP 800-53: SA-9 External Information System Services'
  - ISO/IEC 27001:2013 - A.15 Supplier Relationships
- rule_id: gcp.billing.anomaly.cost_detectors_enabled
  service: billing
  resource: anomaly
  requirement: Cost Detectors Enabled
  scope: billing.anomaly.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable GCP Billing Cost Anomaly Detectors
  rationale: Enabling cost anomaly detection in GCP billing helps organizations proactively identify unexpected charges, potentially indicating security breaches or misconfigurations. Early detection of billing anomalies can prevent financial loss and ensure compliance with budgetary constraints, aligning with governance and compliance frameworks.
  description: This rule checks whether cost anomaly detectors are enabled in GCP billing. To verify, navigate to the 'Billing' section in the GCP Console, and ensure that anomaly detection is configured under 'Budgets & alerts.' If not enabled, configure cost anomaly detectors by setting thresholds and notification preferences to receive alerts on any unusual billing activities promptly.
  references:
  - https://cloud.google.com/billing/docs/how-to/monitor-anomalies
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.billing.anomaly.cost_severity_thresholds_configured
  service: billing
  resource: anomaly
  requirement: Cost Severity Thresholds Configured
  scope: billing.anomaly.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Cost Severity Thresholds Are Configured for Billing Anomalies
  rationale: Configuring cost severity thresholds is crucial to promptly identify unusual billing activities that could indicate misconfigurations or unauthorized usage, potentially leading to significant financial losses. It helps organizations detect and address anomalies before they escalate, ensuring budget adherence and financial governance. Compliance with financial controls and governance frameworks such as SOC2 and ISO 27001 often requires proactive monitoring of billing anomalies.
  description: This rule checks whether cost severity thresholds have been configured for billing anomalies in GCP. Proper configuration involves setting thresholds that trigger alerts when billing exceeds expected levels, allowing for timely investigation. To verify, access the Google Cloud Console, navigate to the Billing section, and ensure that anomaly detection is enabled with defined severity thresholds. Remediate by configuring these thresholds to align with your organization's budget and risk tolerance.
  references:
  - https://cloud.google.com/billing/docs/how-to/notifications
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/billing/docs/how-to/budgets
- rule_id: gcp.billing.billing_account.cost_access_admins_mfa_required
  service: billing
  resource: billing_account
  requirement: Cost Access Admins MFA Required
  scope: billing.billing_account.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce MFA for Cost Access Admins on Billing Accounts
  rationale: Requiring Multi-Factor Authentication (MFA) for Cost Access Admins reduces the risk of unauthorized access to sensitive billing data, which can lead to financial losses and data breaches. This practice helps to protect against credential theft and account hijacking, ensuring compliance with regulations such as PCI-DSS and ISO 27001 that mandate strong authentication controls.
  description: This rule checks whether MFA is enabled for users with Cost Access Admin privileges on GCP billing accounts. To verify, ensure that all IAM users with `billing.accounts.get` or `billing.accounts.list` permissions have MFA enforced through Identity and Access Management (IAM) policies. Remediation involves configuring IAM to require MFA for these roles, typically by enabling security key enforcement or app-based authentication for user accounts.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/managing-policies
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/identity
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.billing.billing_account.cost_access_console_rbac_least_privilege
  service: billing
  resource: billing_account
  requirement: Cost Access Console RBAC Least Privilege
  scope: billing.billing_account.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Billing Account Access Uses Least Privilege RBAC
  rationale: Implementing least privilege for billing account access reduces the risk of unauthorized financial data exposure and potential mismanagement of billing settings, which could lead to financial loss or non-compliance with regulations requiring data confidentiality and integrity. Proper access control is crucial for maintaining organizational security posture and meeting compliance standards such as PCI-DSS and SOC 2.
  description: This rule checks if billing account access is granted based on the principle of least privilege using Role-Based Access Control (RBAC). Ensure users have only the permissions necessary for their roles by assigning predefined roles such as 'Billing Viewer' or 'Billing Admin' instead of overly permissive custom roles. Verify and adjust permissions through the Google Cloud Console by reviewing IAM policies for each billing account and removing excess permissions. Regular audits should be conducted to ensure ongoing compliance.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.billing.billing_account.cost_access_cost_data_exports_private_and_encrypted
  service: billing
  resource: billing_account
  requirement: Cost Access Cost Data Exports Private And Encrypted
  scope: billing.billing_account.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cost Data Exports are Private and Encrypted
  rationale: Encrypting cost data exports ensures that sensitive financial information is protected from unauthorized access and potential breaches. This practice mitigates risks such as data leakage and supports compliance with financial regulations, safeguarding organizational reputation and maintaining customer trust.
  description: This rule checks that billing account cost data exports are configured to be private and encrypted at rest. To verify, ensure that cost data export settings are configured to use Google Cloud Storage buckets with default encryption enabled and access permissions restricted to necessary personnel. Remediation involves setting up a bucket with encryption and applying IAM policies to limit access.
  references:
  - https://cloud.google.com/billing/docs/how-to/export-data-bigquery
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.billing.billing_account.cost_access_cost_export_destinations_least_privilege
  service: billing
  resource: billing_account
  requirement: Cost Access Cost Export Destinations Least Privilege
  scope: billing.billing_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict Cost Export Destinations to Least Privilege
  rationale: Implementing least privilege for cost export destinations reduces the risk of unauthorized access to sensitive billing data, which can lead to financial exposure and compliance violations. Ensuring that only necessary permissions are granted helps prevent data leaks and unauthorized financial insights, aligning with regulatory standards such as PCI-DSS and SOC2.
  description: This rule checks whether cost export destinations in GCP Billing Accounts are configured with the least privilege principle. Specifically, it verifies that only necessary roles and permissions are assigned to users or service accounts accessing cost export destinations. To remediate, review IAM policies associated with billing accounts and adjust roles to ensure they have only the permissions needed for their function. Use the GCP Console or gcloud command-line tool to audit and modify IAM roles.
  references:
  - https://cloud.google.com/billing/docs/how-to/export-data-bigquery
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/security/access-management
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.billing.billing_account.cost_savings_plan_admins_mfa_required
  service: billing
  resource: billing_account
  requirement: Cost Savings Plan Admins MFA Required
  scope: billing.billing_account.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enable MFA for Cost Savings Plan Admins on Billing Accounts
  rationale: Requiring multi-factor authentication (MFA) for Cost Savings Plan Admins mitigates the risk of unauthorized access to critical financial settings and data. This prevents potential financial losses and exposure of sensitive billing information, which could arise from compromised credentials. Ensuring MFA aligns with compliance requirements such as PCI-DSS and enhances security posture against identity-based threats.
  description: This rule checks that all users with the 'Cost Savings Plan Admin' role on GCP billing accounts have MFA enabled. Without MFA, accounts are vulnerable to unauthorized access from compromised passwords. Verify by reviewing identity settings in the IAM section of the GCP Console and ensure MFA is enforced for these users. Remediate by configuring identity policies to enforce MFA for all billing account administrators.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/mfa
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-63/3/final
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.billing.billing_account.cost_savings_plan_approval_workflow_required
  service: billing
  resource: billing_account
  requirement: Cost Savings Plan Approval Workflow Required
  scope: billing.billing_account.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce Approval Workflow for Cost Savings Plans
  rationale: Implementing an approval workflow for Cost Savings Plans helps prevent unauthorized commitments that could lead to unexpected financial liabilities. This measure mitigates risks of overspending and helps maintain budgetary control, ensuring alignment with strategic financial goals and compliance with organizational policies.
  description: This rule checks if an approval workflow is in place for any Cost Savings Plans created under the billing account. The workflow should require authorization from designated stakeholders prior to plan activation. Verification involves reviewing the billing account's configuration settings to ensure that requests for Cost Savings Plans trigger approval mechanisms. To remediate, configure billing account settings to enforce an approval workflow, ensuring all cost-saving initiatives are vetted and authorized appropriately.
  references:
  - https://cloud.google.com/billing/docs/how-to/manage-billing-account
  - https://cloud.google.com/docs/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.billing.billing_account.cost_savings_plan_purchase_permissions_restricted
  service: billing
  resource: billing_account
  requirement: Cost Savings Plan Purchase Permissions Restricted
  scope: billing.billing_account.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Cost Savings Plan Purchase Permissions
  rationale: Restricting permissions for purchasing Cost Savings Plans mitigates the risk of unauthorized financial commitments that could lead to unexpected expenses and budget overruns. It ensures that only authorized personnel with a clear understanding of the organization's financial strategy can make such purchases, aligning with financial governance policies and reducing the risk of financial mismanagement.
  description: This rule checks that only designated IAM roles have permissions to purchase Cost Savings Plans in GCP. Specifically, it ensures that roles like 'roles/billing.admin' are assigned to users who are responsible for financial decisions. To verify, review IAM policies for the billing account and limit permissions using the principle of least privilege. Remediation involves auditing IAM policies and adjusting permissions as needed to restrict access to authorized personnel only.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/understanding-roles#billing-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.billing.budget.cost_alert_destinations_configured
  service: billing
  resource: budget
  requirement: Cost Alert Destinations Configured
  scope: billing.budget.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Cost Alert Destinations Are Configured
  rationale: Configuring cost alert destinations in GCP budgets helps organizations manage spending effectively, preventing unexpected costs and ensuring financial accountability. Without these alerts, businesses might exceed budgets, leading to financial risk and impacting profitability. Compliance frameworks often require monitoring of spending to manage financial risk, making this practice essential for adhering to standards like ISO 27001 and SOC2.
  description: This rule checks whether cost alert destinations are configured within GCP budgets. To verify, ensure that each budget has designated alert recipients, such as email addresses, configured in the Google Cloud Console under 'Budgets & alerts' settings. Remediation involves accessing the 'Budgets & alerts' section, selecting a budget, and adding notification recipients to receive alerts when spending exceeds specified thresholds.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/billing/docs/how-to/notifications
- rule_id: gcp.billing.budget.cost_alert_thresholds_configured
  service: billing
  resource: budget
  requirement: Cost Alert Thresholds Configured
  scope: billing.budget.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Billing Budget Cost Alert Thresholds Are Configured
  rationale: Configuring cost alert thresholds in GCP billing budgets helps organizations proactively manage and monitor their cloud expenditure, preventing unexpected overspending. This is crucial for maintaining budget discipline and ensuring financial accountability. By setting cost alerts, businesses can mitigate the risk of financial mismanagement and align their cloud spending with strategic financial goals.
  description: This rule checks if cost alert thresholds are configured for GCP billing budgets. Proper configuration involves setting thresholds at specific percentage levels of the budget that trigger notifications when exceeded. To verify, review your GCP Console under Billing > Budgets & alerts and ensure thresholds are set at strategic levels like 50%, 90%, and 100%. Remediation involves accessing the GCP Console and configuring these thresholds to align with your financial policies.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0 - Section 7.7
  - NIST SP 800-53 Rev. 5 - CP-11
  - ISO/IEC 27001:2013 - A.8.3.3
- rule_id: gcp.billing.budget.cost_budgets_defined_for_accounts_or_projects
  service: billing
  resource: budget
  requirement: Cost Budgets Defined For Accounts Or Projects
  scope: billing.budget.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Cost Budgets Are Set for GCP Projects or Accounts
  rationale: Setting cost budgets for GCP projects or accounts is crucial to prevent unexpected overspending, which can lead to financial constraints and impact operational budgets. Without defined budgets, organizations risk exceeding financial limits, potentially violating internal governance policies or external compliance requirements related to cost management.
  description: This rule checks whether cost budgets are established for GCP billing accounts or specific projects. A budget helps monitor spending and sends alerts when costs exceed defined thresholds. To verify, navigate to the Google Cloud Console, select 'Billing', and ensure that budgets are set under the 'Budgets & alerts' tab. To remediate, create a budget by specifying a target amount and configuring alert thresholds to align with financial policies.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-171/rev-2/final
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.billing.budget.cost_cost_tag_policy_enforced
  service: billing
  resource: budget
  requirement: Cost Cost Tag Policy Enforced
  scope: billing.budget.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Cost Tag Policy on GCP Billing Budgets
  rationale: Enforcing cost tag policies on billing budgets ensures that costs are accurately tracked and attributed to the appropriate projects or departments, which aids in financial management and accountability. Without this enforcement, there is a risk of cost misallocation, leading to budget overruns and potential financial loss. This practice also supports compliance with financial regulations and audit requirements by ensuring transparent and traceable spending.
  description: This rule checks whether a cost tag policy is enforced on all GCP billing budgets to ensure that each expenditure is tagged appropriately. To verify, check the billing settings in the GCP Console to confirm that a cost tag policy is associated with each budget. To remediate, navigate to the Billing section of the GCP Console, create or edit a budget, and apply a cost tag policy to ensure all expenditures are categorized correctly. This configuration helps in maintaining strict budgetary controls and financial transparency.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/resource-manager/docs/tags/tags-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.billing.budget.cost_modify_permissions_restricted
  service: billing
  resource: budget
  requirement: Cost Modify Permissions Restricted
  scope: billing.budget.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Cost Modification Permissions for GCP Budgets
  rationale: Restricting cost modification permissions for GCP budgets is crucial to prevent unauthorized changes that could lead to financial mismanagement and unexpected expenditures. This control helps mitigate insider threats, where individuals with excessive permissions might alter budget settings, resulting in budget overruns or misaligned financial reporting. Ensuring only authorized personnel have these permissions supports compliance with financial governance requirements and audit readiness.
  description: This rule verifies that only designated users or groups have permissions to modify cost settings in GCP Budgets. To check this, review IAM roles assigned to users for billing accounts and ensure that only roles with necessary permissions, like 'Billing Account Admin', are assigned. Remediation involves auditing current IAM policies, revoking excessive permissions, and implementing least privilege principles by assigning roles like 'Billing Account Viewer' to users who do not need modification capabilities.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.billing.budget.cost_required_cost_tags_present
  service: billing
  resource: budget
  requirement: Cost Required Cost Tags Present
  scope: billing.budget.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Cost Tags are Present for GCP Billing Budgets
  rationale: Having cost tags in place allows organizations to effectively track and allocate spending within GCP, ensuring financial accountability and enabling cost optimization. Without these tags, it becomes challenging to identify unnecessary expenditures and allocate costs accurately across departments, potentially leading to overspending and budget mismanagement. Regulatory requirements may also necessitate detailed financial reporting, making proper cost tagging essential for compliance.
  description: This rule checks if all required cost tags are applied to GCP billing budgets. Cost tags help categorize and track expenses related to specific projects or departments within an organization. To verify compliance, ensure that each budget within your GCP billing account has the necessary tags applied. If tags are missing, update your budget settings in the GCP Console to include all required tags. This can typically be done under the 'Budgets & Alerts' section by editing the budget settings.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/docs/overview/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53
  - https://cloud.google.com/billing/docs/how-to/export-data
  - ISO/IEC 27001
- rule_id: gcp.billing.budget.cost_untagged_resource_alerts_enabled
  service: billing
  resource: budget
  requirement: Cost Untagged Resource Alerts Enabled
  scope: billing.budget.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Alerts for Untagged Resource Costs
  rationale: Enabling alerts for untagged resource costs helps organizations identify and manage potential cost overruns due to misconfigured or untracked resources. This can mitigate the risk of unexpected expenses, ensure better budget adherence, and support financial compliance. Untagged resources can lead to inefficiencies and lack of accountability, impacting overall cloud cost management.
  description: This rule checks that alerts are configured for resources without tags that incur costs, enabling proactive monitoring. To verify, ensure that budget alerts in the GCP Billing console are set up for untagged resource expenditures. Remediation involves configuring alerts via the GCP Console or using Cloud Monitoring to notify stakeholders when costs for untagged resources exceed predefined thresholds.
  references:
  - https://cloud.google.com/billing/docs/how-to/budgets
  - https://cloud.google.com/monitoring/docs/alerting
  - https://cloud.google.com/billing/docs/how-to/cost-allocation-tagging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.certificatemanager.certificate.certificates_expiration_check_gcp_load_balancing_s_protocols
  service: certificatemanager
  resource: certificate
  requirement: Certificates Expiration Check Gcp Load Balancing S Protocols
  scope: certificatemanager.certificate.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Certificates Are Valid for GCP Load Balancing Protocols
  rationale: Expired SSL/TLS certificates on GCP load balancers can lead to service disruptions and expose data in transit to interception, increasing vulnerability to man-in-the-middle attacks. Regularly checking certificate expiration supports compliance with data protection standards such as PCI-DSS, ensuring secure data transmission.
  description: This rule verifies that SSL/TLS certificates managed by GCP Certificate Manager for load balancing are not expired. It is essential to monitor certificate expiration dates and set up alerts for timely renewals. To remediate, implement automated certificate renewal processes and regularly audit certificate validity through the GCP console or API. Ensure all certificates associated with load balancers are valid and support secure protocols.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-certificates
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/certificate-manager/docs/using
- rule_id: gcp.certificatemanager.certificate.certificates_expiration_check_gcp_storage_bucket_enf_enabled
  service: certificatemanager
  resource: certificate
  requirement: Certificates Expiration Check Gcp Storage Bucket Enf Enabled
  scope: certificatemanager.certificate.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Certificates Expiration Check for GCP Storage Buckets
  rationale: Regularly checking certificate expiration is crucial to prevent service disruptions and maintain secure communications. Expired certificates can lead to failed connections, exposing data to potential interception or unauthorized access. This practice supports compliance with security standards and reduces the risk of data breaches and operational downtime.
  description: This rule checks whether the expiration of SSL/TLS certificates stored in GCP storage buckets is monitored and enforced. It requires configuring alerts for approaching expiration dates to ensure timely renewal. To verify, review certificate manager settings for alert configurations and automate renewal processes where possible. Enabling this check helps mitigate risks associated with expired certificates by maintaining continuous and secure operations.
  references:
  - https://cloud.google.com/certificate-manager/docs
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.certificatemanager.certificate.expiration
  service: certificatemanager
  resource: certificate
  requirement: Expiration
  scope: certificatemanager.certificate.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Monitor and Renew Expiring Certificates
  rationale: Certificates play a critical role in securing data transmission. Expired certificates can lead to disrupted services, loss of customer trust, and exposure to man-in-the-middle attacks. Ensuring timely renewal is essential for maintaining secure and compliant operations, particularly under regulations like PCI-DSS and HIPAA.
  description: This rule checks for certificates managed by GCP's Certificate Manager that are nearing expiration. Regular monitoring and renewal of these certificates are crucial to prevent service interruptions and vulnerabilities. To verify, review the certificate expiration date through the GCP Console or CLI. Set up alerts for certificates expiring within a 30-day window and automate renewals where possible.
  references:
  - https://cloud.google.com/certificate-manager/docs/concepts
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.certificatemanager.certificate.expiration_configured
  service: certificatemanager
  resource: certificate
  requirement: Expiration Configured
  scope: certificatemanager.certificate.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Certificate Expiration is Configured in Certificate Manager
  rationale: Properly configuring certificate expiration is crucial to prevent service disruptions and ensure data integrity. Without expiration settings, certificates may become invalid without notice, leading to potential downtime, security vulnerabilities, and non-compliance with industry regulations such as PCI-DSS and ISO 27001.
  description: This rule checks whether expiration settings are defined for certificates managed in GCP Certificate Manager. To verify, ensure that each certificate has a defined expiration date and that renewal processes are in place. To remediate, configure automatic renewals or set alerts for manual renewals to avoid unexpected certificate expiry, which can lead to service interruptions and security risks.
  references:
  - https://cloud.google.com/certificate-manager/docs/managing-certificates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.certificatemanager.certificate.manager_certificates_expiration_configured
  service: certificatemanager
  resource: certificate
  requirement: Manager Certificates Expiration Configured
  scope: certificatemanager.certificate.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Manager Certificates Have Expiration Configured
  rationale: Properly configuring certificate expiration is critical to preventing the use of outdated cryptographic credentials, which can lead to vulnerabilities such as man-in-the-middle attacks. Regularly expiring certificates ensure that they are rotated and updated, reducing the risk of unauthorized access and helping to maintain trust in secure communications. Compliance with industry standards and regulations like PCI-DSS and ISO 27001 often mandates periodic certificate renewal.
  description: This rule checks that all certificates managed by Certificate Manager have an expiration date configured. Certificates without expiration dates may remain in use long after their intended lifespan, exposing systems to security risks. Verify that each certificate has an expiration policy set by using the Google Cloud Console or gcloud command-line tool. Remediation involves setting an appropriate expiration date for certificates, and configuring alerts for certificate renewal prior to expiration.
  references:
  - https://cloud.google.com/certificate-manager/docs/overview
  - https://cloud.google.com/solutions/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://owasp.org/www-project-top-ten/
- rule_id: gcp.cloudfunctions.function.concurrency_limit_configured
  service: cloudfunctions
  resource: function
  requirement: Concurrency Limit Configured
  scope: cloudfunctions.function.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Concurrency Limit is Configured for Cloud Functions
  rationale: Configuring a concurrency limit for Cloud Functions helps prevent resource exhaustion and ensures predictable performance under load. Without a set limit, functions may consume excessive resources, leading to increased costs, potential denial-of-service conditions, and performance degradation. This practice helps meet compliance requirements by maintaining system stability and availability.
  description: This rule checks whether Cloud Functions have a concurrency limit configured. A specified limit restricts the number of concurrent executions, helping to control resource usage and costs. To verify, review the Cloud Function's settings in the GCP Console or use the gcloud CLI. Remediation involves setting a suitable concurrency limit based on expected load and performance requirements, which can be done via the GCP Console or by updating the function configuration using gcloud commands.
  references:
  - https://cloud.google.com/functions/docs/concepts/exec#concurrency
  - https://cloud.google.com/functions/docs/bestpractices/tips#concurrency
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudfunctions.function.not_publicly_accessible
  service: cloudfunctions
  resource: function
  requirement: Not Publicly Accessible
  scope: cloudfunctions.function.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Cloud Functions are Not Publicly Accessible
  rationale: Publicly accessible Cloud Functions can be exploited by unauthorized parties, leading to potential data breaches and service disruptions. This exposure increases the risk of malicious actors executing harmful code, consuming resources, or accessing sensitive information. Compliance with standards such as PCI-DSS and ISO 27001 requires limiting public access to minimize attack surfaces.
  description: This check verifies that Google Cloud Functions are not set to be publicly accessible by reviewing IAM policies for roles that grant 'allUsers' or 'allAuthenticatedUsers' permissions. To remediate, adjust the function's IAM policies to restrict access to specific users or service accounts that require it. Ensure the 'roles/cloudfunctions.invoker' role is only granted to trusted entities by modifying permissions in the GCP Console or using the gcloud CLI.
  references:
  - https://cloud.google.com/functions/docs/securing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.cloudfunctions.function.serverless_dead_letter_queue_configured
  service: cloudfunctions
  resource: function
  requirement: Serverless Dead Letter Queue Configured
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Dead Letter Queue Configured for Cloud Functions
  rationale: Configuring a Dead Letter Queue (DLQ) for Cloud Functions ensures that unprocessed messages are not lost, helping maintain data integrity and enabling easier troubleshooting in case of failures. Without a DLQ, failed executions can result in data loss and impact business continuity, potentially leading to non-compliance with data handling regulations such as GDPR and HIPAA.
  description: This rule checks if a Dead Letter Queue is configured for Google Cloud Functions. A DLQ captures messages that cannot be processed successfully, providing a mechanism for reprocessing or analyzing failures. To verify, ensure that the 'deadLetterPolicy' is set within the function's 'eventTrigger' configuration. Remediation involves specifying a Pub/Sub topic for unprocessed messages in the Cloud Function's settings, ensuring failed events are retained for further analysis.
  references:
  - https://cloud.google.com/functions/docs/monitoring/logging
  - https://cloud.google.com/functions/docs/bestpractices/reliability
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/pubsub/docs/dead-letter-topics
- rule_id: gcp.cloudfunctions.function.serverless_env_no_plaintext_secrets
  service: cloudfunctions
  resource: function
  requirement: Serverless Env No Plaintext Secrets
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Avoid Plaintext Secrets in Cloud Functions Environment Variables
  rationale: Storing plaintext secrets in environment variables of Cloud Functions can lead to unauthorized access if the function's configuration is exposed. This exposure can result in data breaches or system compromise, particularly if sensitive credentials are leaked. Ensuring secrets are encrypted helps meet compliance requirements like PCI-DSS and protects against potential insider threats and external attacks.
  description: This rule checks whether Google Cloud Functions use plaintext environment variables to store sensitive information, such as API keys or passwords. Functions should leverage Google Secret Manager for storing sensitive data securely. To verify, inspect the function's configuration for any hardcoded secrets in the environment variables. Remediation involves migrating these secrets to Secret Manager and updating the function to access secrets at runtime securely.
  references:
  - https://cloud.google.com/functions/docs/configuring/secrets
  - https://cloud.google.com/secret-manager/docs/best-practices
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.1
  - NIST SP 800-53 Rev. 5, System and Communications Protection
  - 'PCI-DSS v3.2.1, Requirement 3: Protect Stored Cardholder Data'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudfunctions.function.serverless_logging_and_tracing_enabled
  service: cloudfunctions
  resource: function
  requirement: Serverless Logging And Tracing Enabled
  scope: cloudfunctions.function.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging and Tracing for Cloud Functions
  rationale: Enabling logging and tracing for Google Cloud Functions is crucial for monitoring function executions, diagnosing issues, and ensuring compliance with regulatory standards. This capability provides insights into the function's behavior and performance, helping to detect anomalies and unauthorized access, thereby mitigating potential security threats and operational inefficiencies.
  description: This rule checks whether logging and tracing are enabled for Cloud Functions. To ensure compliance, verify that each function has 'logging' and 'tracing' configurations enabled within the Google Cloud Console or via deployment scripts. Remediation involves setting up Stackdriver Logging and enabling Cloud Trace to capture detailed execution logs and performance traces, which can be done by adjusting the function's environment variables and permissions.
  references:
  - https://cloud.google.com/functions/docs/monitoring/logging
  - https://cloud.google.com/functions/docs/monitoring/error-reporting
  - https://cloud.google.com/functions/docs/monitoring
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/stackdriver/docs/solutions/functions
- rule_id: gcp.cloudfunctions.function.serverless_permissions_least_privilege
  service: cloudfunctions
  resource: function
  requirement: Serverless Permissions Least Privilege
  scope: cloudfunctions.function.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for GCP Cloud Functions Permissions
  rationale: Enforcing least privilege on Cloud Functions reduces the risk of unauthorized access and potential data breaches. Excessive permissions can lead to privilege escalation and exploitation, posing significant security risks and compliance violations with frameworks like PCI-DSS and ISO 27001.
  description: This rule checks if Cloud Functions are assigned only the necessary permissions to perform their tasks. Verify that roles granted do not exceed the minimum required and remove any excess permissions. Use IAM roles such as 'roles/cloudfunctions.invoker' judiciously to avoid broad access. Review and adjust IAM policies regularly to maintain compliance with least privilege principles.
  references:
  - https://cloud.google.com/functions/docs/securing
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices-for-managing-iam-policies
- rule_id: gcp.cloudfunctions.function.serverless_prov_conc_execution_role_least_privilege
  service: cloudfunctions
  resource: function
  requirement: Serverless Prov Conc Execution Role Least Privilege
  scope: cloudfunctions.function.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Cloud Functions Use Least Privilege for Execution Roles
  rationale: Using the least privilege principle for Cloud Functions execution roles minimizes the risk of unauthorized access and data breaches. Over-privileged roles can lead to potential exploitation if compromised, impacting data confidentiality and integrity. Adhering to this principle supports compliance with regulatory frameworks such as NIST and PCI-DSS, which mandate minimal access necessary for functionality.
  description: This rule checks whether Cloud Functions in GCP are configured to use execution roles with the least privilege necessary. It ensures that the role assigned to a function has only the necessary permissions to perform its operations, reducing the attack surface. Verify by reviewing the IAM policies attached to execution roles and adjust permissions to align with the function's operational requirements. Remediation involves editing the IAM policy to remove any excessive permissions not required by the function.
  references:
  - https://cloud.google.com/functions/docs/securing/function-identity
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/best-practices-for-using-granting-roles
- rule_id: gcp.cloudfunctions.function.serverless_published_and_immutable
  service: cloudfunctions
  resource: function
  requirement: Serverless Published And Immutable
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud Functions Are Published and Immutable
  rationale: Making serverless functions immutable after publication helps prevent unauthorized modifications that could introduce vulnerabilities or disrupt operations. This practice supports regulatory compliance by ensuring auditability and maintaining a secure baseline, reducing the risk of data breaches or service disruptions due to unapproved changes.
  description: This rule checks whether Google Cloud Functions are configured to be published and immutable, ensuring that once deployed, they cannot be altered without going through proper version control processes. Verification involves checking that the functions are deployed from source control systems or using versioned artifacts. Remediation includes implementing a CI/CD pipeline that enforces immutability by deploying only version-tagged code from secure repositories.
  references:
  - https://cloud.google.com/functions/docs/bestpractices/security
  - https://cloud.google.com/functions/docs/deploying
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudfunctions.function.serverless_role_least_privilege
  service: cloudfunctions
  resource: function
  requirement: Serverless Role Least Privilege
  scope: cloudfunctions.function.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Cloud Functions Service Accounts
  rationale: Assigning excessive permissions to Cloud Functions can lead to unauthorized access, data breaches, and compliance violations. By adhering to the principle of least privilege, organizations minimize the potential impact of compromised credentials or malicious actors exploiting over-privileged roles. This practice aligns with compliance frameworks such as PCI-DSS and ISO 27001, which require strict access controls.
  description: This rule checks that Cloud Functions are not granted excessive permissions beyond what is necessary for their operation. Ensure that the service accounts associated with Cloud Functions have roles that are specific to their function and activity. Remediation involves reviewing the roles and permissions associated with each function's service account and adjusting them to match the minimum necessary permissions. It is recommended to use IAM Policy Analyzer and IAM Recommender to evaluate and refine these permissions.
  references:
  - https://cloud.google.com/functions/docs/securing/function-identity
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudfunctions.function.serverless_rollforward_rollback_controls_present
  service: cloudfunctions
  resource: function
  requirement: Serverless Rollforward Rollback Controls Present
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Rollforward and Rollback Controls for Cloud Functions
  rationale: Having rollforward and rollback controls in place for Google Cloud Functions mitigates the risk of deploying faulty code, which can lead to service disruptions, security vulnerabilities, and non-compliance with industry regulations. Such controls are crucial in maintaining operational resilience and ensuring that any issues can be quickly remediated by reverting to a known good state.
  description: This rule checks that Cloud Functions have appropriate rollforward and rollback mechanisms implemented, allowing for safe deployment practices. Ensure versioning is enabled for functions to track changes and facilitate easy rollbacks if necessary. Remediation involves configuring deployment workflows to include automated rollback capabilities using tools like Google Cloud Build or third-party CI/CD solutions. Verification can be done by reviewing the deployment scripts and ensuring they include these controls.
  references:
  - https://cloud.google.com/functions/docs/bestpractices
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/devops/devops-processes
  - https://cloud.google.com/build/docs/configuring-builds/configure-deployment
  - https://cloud.google.com/solutions/best-practices-for-building-continuous-delivery-pipelines
- rule_id: gcp.cloudfunctions.function.serverless_source_authenticated
  service: cloudfunctions
  resource: function
  requirement: Serverless Source Authenticated
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud Functions Have Authenticated Source Traffic
  rationale: Allowing unauthenticated traffic to Cloud Functions can expose your application to unauthorized access, leading to data breaches or resource misuse. Enforcing authentication ensures that only intended and verified sources can invoke the functions, mitigating risks associated with unauthorized data exposure and potential exploitation. This is crucial for maintaining the integrity and confidentiality of your serverless applications and meeting compliance standards.
  description: This rule checks that Google Cloud Functions are configured to allow traffic only from authenticated sources. Specifically, it verifies the IAM policies and ingress settings to ensure only authorized entities can invoke the functions. To remediate, configure the Cloud Function's ingress settings to 'Allow internal traffic only' or set up identity-based authentication using IAM roles and policies. Verify using the GCP Console or gcloud CLI to ensure the function's ingress settings are correctly configured.
  references:
  - https://cloud.google.com/functions/docs/securing/authenticating
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/architecture/serverless/security
- rule_id: gcp.cloudfunctions.function.serverless_source_trusted
  service: cloudfunctions
  resource: function
  requirement: Serverless Source Trusted
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud Functions Source is from Trusted Locations
  rationale: Allowing Cloud Functions to execute code from untrusted sources poses security risks such as data exfiltration, unauthorized access, and execution of malicious code. This could lead to breaches of sensitive data, service disruptions, and non-compliance with regulations like GDPR, HIPAA, or PCI-DSS.
  description: This rule checks whether Cloud Functions are configured to execute code only from trusted and verified sources, such as authorized repositories or storage locations. Administrators should verify the source of the code before deployment by ensuring proper IAM roles and permissions are applied, and by utilizing version control systems with strong access controls. Remediation involves using Google Cloud's IAM to restrict who can modify the function's source and employing signed URLs or checksums for integrity verification.
  references:
  - https://cloud.google.com/functions/docs/bestpractices/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.cloudfunctions.function.serverless_version_immutable
  service: cloudfunctions
  resource: function
  requirement: Serverless Version Immutable
  scope: cloudfunctions.function.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud Functions Use Immutable Serverless Versions
  rationale: Immutable serverless versions in Google Cloud Functions prevent unintended changes and ensure consistent, repeatable deployments. This minimizes the risk of unauthorized code changes, which could introduce vulnerabilities or disrupt services. Regulatory frameworks often require integrity and consistency in deployment processes to ensure compliance with standards like SOC2 and ISO 27001.
  description: This rule checks that Google Cloud Functions are configured to use immutable versions, ensuring that once deployed, the function code cannot be altered without deploying a new version. To verify, ensure that your deployment process includes specific versioning practices for Cloud Functions. Remediation involves setting up deployment pipelines using tools like Cloud Build, which automatically version functions and maintain deployment integrity.
  references:
  - https://cloud.google.com/functions/docs/versioning
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/architecture/serverless-best-practices
  - https://cloud.google.com/cloud-build/docs/configuring-builds/create-basic-configuration
- rule_id: gcp.cloudfunctions.function.serverless_vpc_private_networking_enabled
  service: cloudfunctions
  resource: function
  requirement: Serverless VPC Private Networking Enabled
  scope: cloudfunctions.function.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enable Serverless VPC Access for Cloud Functions
  rationale: Enabling Serverless VPC Access for Cloud Functions is crucial for securing network traffic and ensuring that function executions remain within a controlled network environment. This reduces the risk of unauthorized access over the public internet and helps comply with stringent data protection regulations by maintaining data within private network boundaries.
  description: This rule verifies that Google Cloud Functions have Serverless VPC Access enabled, allowing them to communicate privately with resources in a Virtual Private Cloud (VPC). To configure, associate your function with a VPC connector. This can be done via the Google Cloud Console by navigating to the 'VPC Access' section under Cloud Functions or by using the gcloud CLI. Remediation involves creating a serverless VPC access connector if one does not exist and updating the function configuration to use this connector.
  references:
  - https://cloud.google.com/functions/docs/networking/network-settings
  - https://cloud.google.com/functions/docs/networking/connecting-vpc
  - https://cloud.google.com/architecture/serverless-vpc-access
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
- rule_id: gcp.cloudfunctions.function.url_public
  service: cloudfunctions
  resource: function
  requirement: Url Public
  scope: cloudfunctions.function.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to GCP Cloud Functions URLs
  rationale: Allowing public access to Cloud Functions URLs can expose sensitive data and operations to unauthorized users, leading to potential data breaches and service disruptions. Unrestricted access could be exploited by attackers to execute malicious actions or access confidential information, thereby violating compliance with regulations such as PCI-DSS, HIPAA, and SOC2.
  description: This rule checks for GCP Cloud Functions that are configured with publicly accessible URLs, which can be a significant security risk. To verify, ensure that Cloud Functions are only accessible to authorized users and service accounts using IAM policies. Remediation involves updating IAM settings to restrict access to specific users and roles, and removing 'allUsers' or 'allAuthenticatedUsers' from the function's permissions.
  references:
  - https://cloud.google.com/functions/docs/securing/managing-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
- rule_id: gcp.cloudidentity.group.identity_access_attached_policies_not_admin_star
  service: cloudidentity
  resource: group
  requirement: Identity Access Attached Policies Not Admin Star
  scope: cloudidentity.group.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Admin * Policies on Cloud Identity Groups
  rationale: Ensuring that Cloud Identity groups do not have overly permissive admin policies is crucial to maintaining the principle of least privilege. Overly broad permissions can lead to unauthorized access and potential data breaches, impacting the organization's security posture and compliance with regulations such as PCI-DSS and SOC2.
  description: This rule checks for Cloud Identity groups with attached policies that include administrative access ('admin' roles with '*') which may grant excessive permissions. Administrators should review and revise these policies to ensure they align with the principle of least privilege. Remediation involves auditing the policies attached to each group and removing or constraining permissions that are too broad.
  references:
  - https://cloud.google.com/identity/docs/admin-overview
  - https://cloud.google.com/iam/docs/understanding-roles#primitive_roles
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.isaca.org/resources/cobit
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
- rule_id: gcp.cloudidentity.group.identity_access_membership_review_enabled_where_supported
  service: cloudidentity
  resource: group
  requirement: Identity Access Membership Review Enabled Where Supported
  scope: cloudidentity.group.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Identity Access Membership Review in Cloud Identity Groups
  rationale: Enabling identity access membership review helps to ensure that only authorized users have access to sensitive resources, minimizing the risk of unauthorized access and potential data breaches. This practice supports compliance with regulations by maintaining up-to-date access controls and auditing capabilities, enhancing the organization's security posture.
  description: This rule checks if identity access membership review is enabled for Cloud Identity groups where supported. Regular reviews of group memberships are crucial to verify that users' access levels align with their current roles and responsibilities. To enable this, navigate to the Cloud Identity console, select the group, and configure the membership settings to require periodic reviews. Remediation involves setting an appropriate review frequency and ensuring responsible parties are notified of upcoming reviews.
  references:
  - https://cloud.google.com/identity/docs/manage-access
  - https://cloud.google.com/security/compliance/cis#section_8
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices/identity-and-access-management
  - https://cloud.google.com/iam/docs/audit-config
- rule_id: gcp.cloudidentity.group.identity_access_no_inline_policies
  service: cloudidentity
  resource: group
  requirement: Identity Access No Inline Policies
  scope: cloudidentity.group.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure No Inline Policies on Cloud Identity Groups
  rationale: Inline policies directly attached to Cloud Identity Groups can lead to hidden permissions that are difficult to track and audit, increasing the risk of unauthorized access. By enforcing centralized policy management through IAM roles, organizations can better adhere to compliance requirements such as PCI-DSS and ISO 27001, and reduce the attack surface by ensuring consistent access controls.
  description: This rule checks for the presence of inline policies directly attached to Cloud Identity Groups, which can bypass the standard IAM roles and permissions model. Inline policies should be avoided to simplify policy management and auditing. Remediation involves transitioning any inline policies to managed IAM roles and ensuring that all permissions are granted through these roles. Verification can be done by reviewing the configuration of Cloud Identity Groups and ensuring no inline policies are present.
  references:
  - https://cloud.google.com/identity/docs/concepts/overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.cloudidentity.membership.identity_access_access_keys_rotated_90_days_or_less__present
  service: cloudidentity
  resource: membership
  requirement: Identity Access Access Keys Rotated 90 Days Or Less Present
  scope: cloudidentity.membership.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Access Keys Are Rotated Every 90 Days
  rationale: Regular rotation of access keys mitigates the risk of credential compromise, which can lead to unauthorized access to sensitive resources. This practice supports compliance with industry standards such as NIST and ISO 27001, reducing the risk of data breaches and ensuring a strong security posture against credential-based attacks.
  description: This rule checks whether access keys associated with Cloud Identity memberships have been rotated within the past 90 days. Regularly rotating access keys reduces the window of opportunity for malicious actors to exploit compromised credentials. To verify compliance, review the access key creation or last rotation dates within the Cloud Console or via the command line. Remediation involves scheduling regular key rotations and implementing automated notifications for keys nearing the rotation threshold.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.cloudidentity.membership.identity_access_console_password_present_only_if_required
  service: cloudidentity
  resource: membership
  requirement: Identity Access Console Password Present Only If Required
  scope: cloudidentity.membership.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Console Passwords Set Only When Necessary
  rationale: Unnecessary console passwords increase the risk of unauthorized access and potential data breaches. In environments where they are not required, they can introduce vulnerabilities that bypass more secure authentication methods. Minimizing password use aligns with least privilege principles and helps comply with regulations like PCI-DSS and ISO 27001, reducing attack surfaces and improving overall security posture.
  description: This rule checks that Identity Access Console passwords are present only when explicitly required for operational purposes. It ensures that console access is secured using alternative, more secure authentication methods like OAuth or SAML where possible. To verify, audit the membership settings in Cloud Identity to ensure that passwords are not set unless justified. If passwords are found without necessity, remove them and implement stronger authentication mechanisms.
  references:
  - https://cloud.google.com/identity/docs/setup
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices/identity-access-management
- rule_id: gcp.cloudidentity.membership.identity_access_inactive_90_days_disabled
  service: cloudidentity
  resource: membership
  requirement: Identity Access Inactive 90 Days Disabled
  scope: cloudidentity.membership.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Disable Inactive Identity Access After 90 Days
  rationale: Inactive identities pose a security risk as they can be exploited by unauthorized users to gain access to sensitive systems. Regularly disabling inactive accounts reduces the potential attack surface and helps organizations comply with regulatory standards like NIST and ISO 27001 which mandate secure management of user identities.
  description: This rule checks if user accounts in Google Cloud Identity have been inactive for over 90 days and ensures they are disabled. Inactive accounts are those that have not been logged into or used for any activities. Administrators can verify account activity through the Google Cloud Console under the 'Identity' section and disable inactive accounts manually or automate the process using identity management tools. Ensuring accounts are disabled after a period of inactivity mitigates the risk of unauthorized access.
  references:
  - https://cloud.google.com/identity
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-cloud-identity-security
- rule_id: gcp.cloudidentity.membership.identity_access_mfa_required
  service: cloudidentity
  resource: membership
  requirement: Identity Access MFA Required
  scope: cloudidentity.membership.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA for Cloud Identity Membership Access
  rationale: Requiring multi-factor authentication (MFA) for all members of Cloud Identity ensures that unauthorized access through compromised credentials is mitigated, reducing the risk of data breaches. This is critical for protecting sensitive information and ensuring compliance with security regulations such as PCI-DSS and HIPAA, which mandate strong authentication mechanisms.
  description: This rule checks whether all members in Cloud Identity have MFA enabled to access resources. MFA adds an extra layer of security by requiring users to provide two or more verification factors. To verify compliance, audit the identity provider settings or use the Google Cloud Console to ensure MFA is enforced. If MFA is not enabled, configure identity policies to require it for all users accessing the Cloud Identity services.
  references:
  - https://cloud.google.com/identity/docs/concepts/mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.cloudidentity.membership.identity_access_no_inline_policies_attached
  service: cloudidentity
  resource: membership
  requirement: Identity Access No Inline Policies Attached
  scope: cloudidentity.membership.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Inline Policies on Cloud Identity Memberships
  rationale: Attaching inline policies directly to Cloud Identity memberships can lead to inconsistent and difficult-to-manage access controls. This increases the risk of unintentional privilege escalation and makes it challenging to audit and comply with regulatory standards such as NIST and ISO 27001. By avoiding inline policies, organizations can ensure a more streamlined and secure access management strategy.
  description: This rule checks for the presence of inline policies directly attached to Cloud Identity memberships. Inline policies can lead to security risks by creating policy sprawl and making it difficult to maintain proper access controls. To verify, review membership configurations and ensure policies are attached through managed roles instead. Remediate by detaching inline policies and applying necessary permissions via predefined roles in IAM to improve manageability and security.
  references:
  - https://cloud.google.com/identity/docs
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudkms.crypto_key.cmk_rotation_enabled
  service: cloudkms
  resource: crypto_key
  requirement: CMK Rotation Enabled
  scope: cloudkms.crypto_key.key_rotation
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure CMK Rotation is Enabled for Crypto Keys
  rationale: Regular rotation of Customer Managed Keys (CMKs) mitigates the risk of data breaches and unauthorized access by limiting the lifespan of any single encryption key. This practice is crucial for maintaining compliance with data protection regulations like PCI-DSS and HIPAA, which mandate robust encryption management policies to safeguard sensitive data.
  description: This rule checks whether CMK rotation is enabled for Google Cloud KMS crypto keys. Enabling automatic rotation helps in minimizing potential exposure by automatically generating new encryption keys at regular intervals. To verify this setting, navigate to the Google Cloud Console, go to Cloud KMS, select the crypto key, and ensure the 'Rotation period' is configured. If not enabled, set an appropriate rotation period, such as 90 days, to enhance security posture.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 4.3
  - NIST SP 800-57 Part 1 Revision 5
  - PCI-DSS v3.2.1 Requirement 3.6.4
  - https://cloud.google.com/security/encryption-at-rest
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudkms.crypto_key.encryption_enabled
  service: cloudkms
  resource: crypto_key
  requirement: Encryption Enabled
  scope: cloudkms.crypto_key.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Cloud KMS Crypto Keys are Encrypted
  rationale: Ensuring encryption for Cloud KMS keys protects sensitive data from unauthorized access and potential breaches. Encryption is critical for maintaining data integrity and confidentiality, mitigating risks associated with data exposure. Compliance with regulations such as PCI-DSS and HIPAA often mandates encryption, ensuring that businesses meet legal requirements and avoid financial and reputational damages.
  description: This rule checks that Cloud KMS crypto keys are encrypted using appropriate algorithms. Ensure that all crypto keys are set up with encryption at rest enabled. Verification involves inspecting the key's configuration in the Google Cloud Console or via the gcloud command-line tool. Remediation includes configuring or updating crypto keys to enforce encryption settings. Use the GCP Console or gcloud commands to verify and adjust settings as needed.
  references:
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
- rule_id: gcp.cloudkms.crypto_key.expiration_set
  service: cloudkms
  resource: crypto_key
  requirement: Expiration Set
  scope: cloudkms.crypto_key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Expiration is Set for Cloud KMS Crypto Keys
  rationale: Setting an expiration date for Cloud KMS crypto keys is crucial to prevent the use of outdated keys, which can pose security risks such as unauthorized access to encrypted data. It helps in managing key lifecycle effectively, reducing the risk of key compromise and ensuring compliance with data protection regulations such as PCI-DSS and NIST SP 800-57, which mandate key rotation and expiration strategies.
  description: This rule checks whether an expiration date is set for all Cloud KMS crypto keys. By configuring an expiration date, organizations can ensure that keys are rotated or decommissioned after their intended usage period, preventing potential misuse. To verify, review the Cloud KMS key properties in the GCP Console or via gcloud command-line tool. Remediate by setting an expiration date through the GCP Console or using the gcloud command-line tool with the 'expiration-time' parameter.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://cloud.google.com/kms/docs/reference/rest/v1/projects.locations.keyRings.cryptoKeys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-4/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.cloudkms.crypto_key.key_iam_policy_public_access_configured
  service: cloudkms
  resource: crypto_key
  requirement: Key IAM Policy Public Access Configured
  scope: cloudkms.crypto_key.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Access to Cloud KMS Crypto Keys
  rationale: Allowing public access to Cloud KMS crypto keys poses significant security risks, including unauthorized data decryption and potential data breaches. Such access violates best practices and compliance standards like PCI-DSS and ISO 27001, which mandate strict control over cryptographic keys to safeguard sensitive data. Misconfigured access can lead to financial losses and damage to organizational reputation.
  description: This rule checks for configurations that grant public access to Cloud KMS crypto keys by reviewing IAM policies attached to the keys. A key that is publicly accessible can be exploited by unauthorized users, leading to potential data exposure. To remediate, ensure no IAM policy grants 'allUsers' or 'allAuthenticatedUsers' roles that allow access to the crypto key. Regularly audit key permissions using the GCP Console or gcloud command-line tool to maintain strict access controls.
  references:
  - https://cloud.google.com/kms/docs/iam
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/kms/docs/securing
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.cloudkms.crypto_key.lifecycle_policy
  service: cloudkms
  resource: crypto_key
  requirement: Lifecycle Policy
  scope: cloudkms.crypto_key.lifecycle_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Implement and Maintain KMS Key Lifecycle Management
  rationale: Proper lifecycle management of Cloud KMS keys is critical to ensuring data privacy and security. Without automated rotation and expiration policies, keys may become vulnerable to unauthorized access or cryptographic attacks, potentially leading to data breaches. Compliance with regulations such as PCI-DSS and HIPAA often mandates strict key management practices to protect sensitive information.
  description: This rule checks whether Cloud KMS keys have a defined lifecycle policy that includes key rotation and expiration schedules. Ensure each key has a 'nextRotationTime' and 'rotationPeriod' set to enforce automatic key rotation, reducing the risk of key compromise. Remediation involves configuring the key with a rotation period, typically every 90 days, via the GCP Console or gcloud command-line tool. Verify lifecycle settings by reviewing key properties in the Cloud KMS section of the GCP Console.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://cloud.google.com/security/compliance/cis#gcp_cis
  - https://www.pcisecuritystandards.org
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.cloudkms.crypto_key.not_publicly_accessible
  service: cloudkms
  resource: crypto_key
  requirement: Not Publicly Accessible
  scope: cloudkms.crypto_key.public_access
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: critical
  title: Ensure KMS Crypto Keys Are Not Publicly Accessible
  rationale: Publicly accessible KMS crypto keys pose a significant risk as unauthorized access could lead to data breaches, compromising sensitive information and violating data protection regulations. Ensuring these keys are not publicly accessible protects against unauthorized decryption of critical data and aligns with compliance requirements such as PCI-DSS and HIPAA.
  description: This rule checks the IAM policies associated with KMS crypto keys to ensure that no public access is granted. Verify that no 'allUsers' or 'allAuthenticatedUsers' entries exist in the key's IAM policy. To remediate, review the IAM policy of each crypto key and remove any public access entries. Implement principle of least privilege by granting access only to necessary users or service accounts.
  references:
  - https://cloud.google.com/kms/docs/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.cloudkms.crypto_key.not_scheduled_for_deletion
  service: cloudkms
  resource: crypto_key
  requirement: Not Scheduled For Deletion
  scope: cloudkms.crypto_key.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure GCP KMS Keys Are Not Scheduled for Deletion
  rationale: Preventing KMS keys from being scheduled for deletion is critical to maintaining data integrity and availability. If a key is deleted, any data encrypted with that key becomes irretrievable, which can lead to data loss and business disruption. Moreover, unnecessary deletion of encryption keys can breach compliance requirements in frameworks like PCI-DSS and HIPAA that mandate data protection and retention.
  description: This rule checks for Google Cloud KMS keys that are scheduled for deletion to ensure they remain available for decrypting data. To verify this, inspect the key's lifecycle state using the Cloud Console or gcloud CLI. Remediation involves canceling the deletion operation if the key is scheduled for deletion by using the 'gcloud kms keys cancel-deletion' command. Ensure key management policies include regular audits to prevent accidental deletion.
  references:
  - https://cloud.google.com/kms/docs/key-states#key_scheduled_for_deletion
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/best-practices
- rule_id: gcp.cloudkms.crypto_key.privacy_encryption_at_rest_cmek_enabled
  service: cloudkms
  resource: crypto_key
  requirement: Privacy Encryption At Rest Cmek Enabled
  scope: cloudkms.crypto_key.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure CMEK is Enabled for KMS Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encrypting data at rest in Google Cloud's Key Management Service (KMS) enhances control over cryptographic operations and key lifecycle management. This reduces the risk of unauthorized access to sensitive data and ensures compliance with stringent data protection regulations such as GDPR and HIPAA. Without CMEK, organizations may face increased risk of data breaches and non-compliance penalties.
  description: This rule verifies that all KMS crypto keys are configured to use Customer-Managed Encryption Keys (CMEK) for encryption at rest. It checks the key configuration settings to ensure that CMEK is enabled and properly managed. To remediate, ensure that all crypto keys are created or updated with CMEK settings via the Cloud Console or gcloud CLI by specifying a user-managed key ring and key. Regular audits should be conducted to maintain compliance and security posture.
  references:
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/architecture/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/creating-keys
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudkms.crypto_key.privacy_encryption_in_transit_tls_min_1_2
  service: cloudkms
  resource: crypto_key
  requirement: Privacy Encryption In Transit TLS Min 1 2
  scope: cloudkms.crypto_key.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure TLS 1.2+ for Cloud KMS Crypto Key Encryption
  rationale: Using a minimum of TLS 1.2 for encryption in transit protects sensitive key material from interception and man-in-the-middle attacks, which can lead to unauthorized data access and breaches. This is crucial for maintaining the confidentiality and integrity of cryptographic operations and adhering to compliance standards like PCI-DSS and HIPAA.
  description: This rule checks that Google Cloud KMS Crypto Keys are using at least TLS 1.2 for encryption in transit. To verify, inspect the network configuration settings to ensure TLS 1.2 or higher is enforced. Remediation involves updating the TLS policies and ensuring all clients and services interacting with Cloud KMS support TLS 1.2 or newer versions. It is essential to regularly review and update configurations to comply with industry standards and best practices.
  references:
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudkms.crypto_key.project_no_secrets_in_variables_gcp_cloud_build_proj_logging
  service: cloudkms
  resource: crypto_key
  requirement: Project No Secrets In Variables Gcp Cloud Build Proj Logging
  scope: cloudkms.crypto_key.logging
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Prevent Secrets in Cloud Build Variables and Enable Logging
  rationale: Inadvertently storing secrets in Cloud Build environment variables can lead to unauthorized access to sensitive information, posing a significant security risk. Such exposure can result in data breaches, non-compliance with regulations like PCI-DSS and HIPAA, and reputational damage. Effective logging ensures that access and modifications to keys are monitored, providing an audit trail for compliance and forensic investigations.
  description: This rule checks that no sensitive secrets are stored in the environment variables of Cloud Build projects and that logging is enabled for cryptographic key activities. Verify that Cloud Build configuration files do not contain hardcoded secrets. Instead, use Google Cloud's Secret Manager. Ensure Cloud KMS key logging is enabled in Cloud Audit Logs to track access and usage. To remediate, audit Cloud Build project configurations, use Secret Manager for sensitive data, and configure logging in the Google Cloud Console under 'Logging' to monitor key usage.
  references:
  - https://cloud.google.com/build/docs/securing-builds/store-secrets
  - https://cloud.google.com/kms/docs/logging
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudkms.crypto_key.rotation_compliance_configured
  service: cloudkms
  resource: crypto_key
  requirement: Rotation Compliance Configured
  scope: cloudkms.crypto_key.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure Crypto Key Rotation Policy is Configured
  rationale: Regular rotation of cryptographic keys is crucial for maintaining security hygiene and minimizing the risk of key compromise. Without a defined rotation policy, keys may remain vulnerable to prolonged exposure, increasing the likelihood of unauthorized access. Compliance with standards like NIST and PCI-DSS often requires key rotation to ensure ongoing data protection and integrity.
  description: This rule checks if a rotation policy is set for each Cloud KMS crypto key, ensuring keys are automatically rotated at regular intervals. To verify, review the 'rotationPeriod' field in the crypto key's properties; it should be configured to a period that aligns with your security policy (e.g., every 90 days). To remediate, update the crypto key settings to include an appropriate 'rotationPeriod' using the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.cloudkms.crypto_key.rotation_enabled
  service: cloudkms
  resource: crypto_key
  requirement: Rotation Enabled
  scope: cloudkms.crypto_key.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure Cloud KMS Crypto Key Rotation is Enabled
  rationale: Enabling key rotation for Cloud KMS crypto keys reduces the risk of key compromise by limiting the duration a key is used to encrypt data. This practice is critical for maintaining data confidentiality and integrity, especially in environments with high compliance requirements such as PCI-DSS and HIPAA, where regular key rotation is mandated to protect sensitive data.
  description: This rule checks if automatic rotation is enabled for Cloud KMS crypto keys, which should be configured to rotate at least every 90 days. To verify, navigate to the Cloud KMS section in the GCP Console, select the crypto key, and check the rotation schedule settings. To enable rotation, set an automatic rotation schedule through the GCP Console or gcloud command-line tool. Regularly rotated keys minimize exposure time and help adhere to compliance standards.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57pt1r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudkms.crypto_key.secrets_kms_key_deletion_requires_waiting_period
  service: cloudkms
  resource: crypto_key
  requirement: Secrets KMS Key Deletion Requires Waiting Period
  scope: cloudkms.crypto_key.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Require Waiting Period for KMS Key Deletion
  rationale: Enforcing a waiting period for KMS key deletion prevents accidental or malicious data loss by allowing time to recover keys before permanent deletion. This reduces the risk of data breaches associated with lost encryption keys and complies with security standards that mandate data protection and integrity. Implementing this control aids in meeting regulatory requirements such as GDPR and PCI-DSS which emphasize data confidentiality and availability.
  description: This rule checks if a waiting period is configured for the deletion of Cloud KMS keys used to encrypt secrets. Specifically, it ensures that a minimum waiting period of 7 days is enforced, allowing sufficient time to recover keys if a deletion request was made in error. Verification can be done by reviewing the key lifecycle settings in the Google Cloud Console or using the Google Cloud SDK. Remediation involves setting a 'deleteProtection' attribute on the crypto key to true, ensuring a waiting period is enforced before deletion.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, 4.3
  - https://www.nist.gov/publications/nist-special-publication-800-57-part-1-revision-5
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.cloudkms.crypto_key.secrets_kms_key_not_publicly_accessible
  service: cloudkms
  resource: crypto_key
  requirement: Secrets KMS Key Not Publicly Accessible
  scope: cloudkms.crypto_key.public_access
  domain: secrets_and_key_management
  subcategory: key_management
  severity: critical
  title: Ensure KMS Keys Are Not Publicly Accessible
  rationale: Publicly accessible KMS keys pose a severe risk as they can be exploited by malicious actors to access sensitive data. Unauthorized access to keys can lead to data breaches and non-compliance with standards like PCI-DSS and HIPAA, resulting in legal and financial repercussions. Ensuring keys are private protects against these threats and maintains trust.
  description: This rule checks that KMS keys used for encrypting secrets are not publicly accessible. Public access should be disabled by configuring IAM policies to restrict permissions to only trusted users and services. Verify key access policies via the GCP Console or gcloud CLI, and remove any public bindings. Restrict access using the principle of least privilege to prevent unauthorized access.
  references:
  - https://cloud.google.com/kms/docs/securing-keys
  - https://cloud.google.com/kms/docs/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudkms.crypto_key.secrets_kms_key_policy_least_privilege
  service: cloudkms
  resource: crypto_key
  requirement: Secrets KMS Key Policy Least Privilege
  scope: cloudkms.crypto_key.policy_management
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure KMS Key Policies Enforce Least Privilege Access
  rationale: Applying least privilege principles to KMS key policies is crucial to minimize unauthorized access and potential data breaches. Overly permissive key policies can lead to misuse of encryption keys, resulting in exposure of sensitive data and non-compliance with regulations like GDPR and PCI-DSS. Ensuring restrictive access helps maintain data confidentiality and integrity, reducing potential attack vectors.
  description: This rule checks that Cloud KMS key policies grant only the necessary permissions to intended users or service accounts. Review and adjust the IAM policies associated with your KMS keys to ensure they follow the principle of least privilege. Audit the roles assigned to each key and remove any excessive permissions. Use the GCP Console or CLI to examine IAM policy bindings and update configurations to align with best practices.
  references:
  - https://cloud.google.com/kms/docs/resource-hierarchy-access-control
  - https://www.cisecurity.org/controls/cis-google-cloud-computing-foundations-benchmark-v1-2-0
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.cloudkms.crypto_key.secrets_kms_key_rotation_enabled
  service: cloudkms
  resource: crypto_key
  requirement: Secrets KMS Key Rotation Enabled
  scope: cloudkms.crypto_key.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Enable Rotation for Secrets KMS Crypto Keys
  rationale: Regular key rotation for Secrets KMS keys reduces the risk of key compromise by limiting the exposure of cryptographic keys over time. It aligns with compliance mandates such as PCI-DSS and NIST SP 800-57, which require periodic key changes to enhance security posture and protect sensitive data from unauthorized access.
  description: This rule verifies that all CryptoKeys used for secrets management have automatic rotation enabled in Google Cloud KMS. Automatic key rotation helps to mitigate the risk of key exposure and ensures keys are regularly updated without manual intervention. To remediate, set a rotation period for each CryptoKey using the Google Cloud Console or gcloud CLI, specifying a rotation period of no more than 365 days.
  references:
  - https://cloud.google.com/kms/docs/key-rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudkms.key_ring.secrets_kms_default_keys_disabled_where_cmek_required
  service: cloudkms
  resource: key_ring
  requirement: Secrets KMS Default Keys Disabled Where Cmek Required
  scope: cloudkms.key_ring.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Disable Default KMS Keys for Secrets When CMEK is Required
  rationale: Using default KMS keys for secrets management can expose your organization to potential data breaches, as these keys are not sufficiently isolated and may not meet compliance requirements for data encryption. By disabling default keys where Customer-Managed Encryption Keys (CMEK) are required, you ensure that sensitive data is protected with stronger, customer-controlled encryption, reducing the risk of unauthorized access and aligning with regulatory standards.
  description: This rule checks that default KMS keys are disabled for managing secrets in environments where Customer-Managed Encryption Keys (CMEK) are mandated. Organizations should configure their key rings to use CMEK instead of default Google-managed keys to maintain full control over encryption keys and compliance. Verify this by checking the key ring configuration settings in Cloud KMS and ensure that only CMEK are used. To remediate, update your Cloud KMS settings to disable default keys and configure CMEK usage as per your security policy.
  references:
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.cloudkms.key_ring.secrets_kms_logging_enabled
  service: cloudkms
  resource: key_ring
  requirement: Secrets KMS Logging Enabled
  scope: cloudkms.key_ring.logging
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Cloud KMS Key Ring Logging is Enabled
  rationale: Enabling logging for Cloud KMS key rings is crucial for monitoring access and changes to encryption keys. Without logging, unauthorized access or misuse of keys might go unnoticed, posing a significant security risk. Compliance with frameworks like PCI-DSS and HIPAA requires detailed record-keeping of key management activities to ensure data protection and accountability.
  description: This rule verifies that logging is enabled for Cloud KMS key rings to track all access and modifications. To ensure logging is active, check that key ring activity is being recorded in Cloud Audit Logs. Remediation involves configuring the appropriate logging settings in the Google Cloud Console under the 'Logging' section for each key ring, ensuring audit logs are set to capture 'Admin Read', 'Admin Write', and 'Data Access' events.
  references:
  - https://cloud.google.com/kms/docs/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.cloudsql.backup.db_cluster_snapshot_cross_account_sharing_restricted
  service: cloudsql
  resource: backup
  requirement: Db Cluster Snapshot Cross Account Sharing Restricted
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Restrict Cross-Account Sharing of Cloud SQL Backups
  rationale: Restricting cross-account sharing of Cloud SQL backups is crucial to prevent unauthorized access to sensitive database content, which can lead to data breaches and compromise business operations. This control ensures that backups are only accessible by authorized accounts, mitigating risks of accidental exposure and aligning with compliance standards such as GDPR and PCI-DSS.
  description: This rule checks if Cloud SQL backups are shared across different GCP accounts without explicit authorization. Ensure that all snapshots are configured to deny access to external accounts unless explicitly required for business processes. To verify, review the IAM policies and permissions on Cloud SQL backups and ensure they are restricted to necessary accounts only. If cross-account access is detected, adjust the IAM settings to limit access to authorized users and accounts.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.backup.db_cluster_snapshot_cross_region_copy_encrypted
  service: cloudsql
  resource: backup
  requirement: Db Cluster Snapshot Cross Region Copy Encrypted
  scope: cloudsql.backup.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Snapshots Are Encrypted for Cross-Region Copies
  rationale: Encrypting database cluster snapshot copies across regions protects sensitive data from unauthorized access during transit and at rest. This measure mitigates the risk of data breaches, reduces potential exposure to legal liabilities, and aligns with compliance mandates such as GDPR and HIPAA, which require robust data protection strategies.
  description: This rule checks whether your Cloud SQL database snapshots that are copied across regions are encrypted. To verify this, ensure that the 'encryptionConfiguration' field is set in the Cloud SQL instance settings. If not enabled, configure Cloud SQL to use customer-managed encryption keys (CMEK) for snapshots. This ensures the data is encrypted using keys that you control, providing an additional layer of security.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/iam/docs/using-cmek
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.backup.db_cluster_snapshot_encrypted
  service: cloudsql
  resource: backup
  requirement: Db Cluster Snapshot Encrypted
  scope: cloudsql.backup.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Backup Snapshots Are Encrypted
  rationale: Encrypting Cloud SQL backup snapshots protects sensitive data from unauthorized access and potential data breaches. It mitigates risks related to data exfiltration and aligns with regulatory compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate encryption of data at rest. Unencrypted backups can expose sensitive information, leading to legal penalties and damage to organizational reputation.
  description: This rule checks that all Cloud SQL database backup snapshots are encrypted using Customer-Managed Encryption Keys (CMEK). To verify, ensure that the `encryptionConfiguration` field is set in the Cloud SQL instance settings. If not configured, enable encryption by setting up a CMEK in Google Cloud's Key Management Service (KMS) and associate it with your Cloud SQL instances. Regularly audit backup configurations to maintain compliance.
  references:
  - https://cloud.google.com/sql/docs/mysql/encrypt-data
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.cloudsql.backup.db_cluster_snapshot_not_publicly_shared
  service: cloudsql
  resource: backup
  requirement: Db Cluster Snapshot Not Publicly Shared
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Prevent Public Sharing of Cloud SQL Backup Snapshots
  rationale: Publicly shared database backups can expose sensitive data to unauthorized parties, increasing the risk of data breaches and non-compliance with regulations like GDPR and HIPAA. Properly securing these backups is critical to maintaining data integrity and ensuring that disaster recovery processes do not inadvertently lead to data leaks.
  description: This rule checks that Cloud SQL database cluster snapshots are not publicly shared. Public access to backups can be controlled by reviewing and configuring access policies in the Cloud SQL instance settings. To remediate, ensure that IAM policies do not grant public access to these resources and regularly audit permissions to maintain compliance.
  references:
  - https://cloud.google.com/sql/docs/mysql/manage-sql-instances#backup
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.backup.db_snapshot_cross_account_sharing_restricted
  service: cloudsql
  resource: backup
  requirement: Db Snapshot Cross Account Sharing Restricted
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Restrict Cross-Account Sharing of Cloud SQL Backups
  rationale: Restricting cross-account sharing of Cloud SQL backups is essential to prevent unauthorized access and potential data breaches. Sharing backups across accounts can expose sensitive data to unintended parties, increasing the risk of data exfiltration and compliance violations, particularly under regulations such as GDPR or HIPAA. Limiting sharing helps maintain data sovereignty and integrity, ensuring that only authorized personnel have access to critical backups.
  description: This rule checks for configurations that allow Cloud SQL database backups to be shared across different GCP accounts. To verify compliance, review the IAM policies associated with Cloud SQL backup resources and ensure that permissions for sharing are restricted to trusted accounts only. Remediation involves configuring IAM roles and policies to limit access strictly to necessary accounts, thereby reducing the risk of data leaks. It is advisable to audit and update these permissions regularly to align with the principle of least privilege.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.cloudsql.backup.db_snapshot_cross_region_copy_encrypted
  service: cloudsql
  resource: backup
  requirement: Db Snapshot Cross Region Copy Encrypted
  scope: cloudsql.backup.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region SQL Backups are Encrypted
  rationale: Encrypting cross-region backups of Cloud SQL instances protects sensitive data from unauthorized access during transit and storage in different geographic locations. This measure reduces the risk of data breaches that can lead to financial loss, reputational damage, and non-compliance with data protection regulations like GDPR and CCPA.
  description: This rule verifies that all cross-region copies of Cloud SQL backups are encrypted using Customer Managed Encryption Keys (CMEK). To ensure compliance, configure the Cloud SQL instance to use CMEK for backups and verify encryption settings in the GCP Console under the 'Backups' section for each instance. Remediation involves enabling CMEK and ensuring that the key is available and managed properly.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudsql.backup.db_snapshot_encrypted
  service: cloudsql
  resource: backup
  requirement: Db Snapshot Encrypted
  scope: cloudsql.backup.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Snapshots are Encrypted at Rest
  rationale: Encrypting database snapshots is crucial to prevent unauthorized access to sensitive data, which could lead to data breaches and non-compliance with regulatory standards such as GDPR and PCI-DSS. Without encryption, any snapshot could be accessed by malicious actors, posing a threat to data integrity and privacy.
  description: This rule checks whether Cloud SQL snapshots are encrypted at rest using customer-managed encryption keys (CMEK) or Google-managed encryption keys. To verify, ensure that the 'backups' setting for your Cloud SQL instance is enabled and that encryption configurations are properly set up. Remediation involves configuring your Cloud SQL instance to use the appropriate encryption keys and verifying the encryption status of existing snapshots.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.backup.db_snapshot_not_publicly_shared
  service: cloudsql
  resource: backup
  requirement: Db Snapshot Not Publicly Shared
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Prevent Public Access to Cloud SQL Backup Snapshots
  rationale: Publicly shared database snapshots can lead to unauthorized access to sensitive data, posing significant security risks and potential compliance violations. Unauthorized access could result in data breaches, affecting the organization's reputation and potentially leading to regulatory fines under frameworks such as GDPR and HIPAA.
  description: This rule checks that Cloud SQL backup snapshots are not publicly shared, ensuring they are protected from unauthorized access. To verify, review the access control settings in the Cloud SQL backup configuration and ensure that no public IAM policies or ACLs are applied. Remediation involves updating permissions to restrict access only to necessary users and roles, following the principle of least privilege.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/overview#policy_hierarchy
  - https://cloud.google.com/sql/docs/mysql/security
- rule_id: gcp.cloudsql.backup.snapshots_public_access_configured
  service: cloudsql
  resource: backup
  requirement: Snapshots Public Access Configured
  scope: cloudsql.backup.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Ensure Cloud SQL Snapshots Do Not Have Public Access
  rationale: Public access to Cloud SQL snapshots poses significant security risks, including unauthorized data exposure and potential data breaches. This can lead to the compromise of sensitive information, financial loss, and non-compliance with data protection regulations such as GDPR and HIPAA. Protecting snapshots by restricting public access is crucial to maintaining the confidentiality and integrity of your database backups.
  description: This rule checks if any Cloud SQL backups have been configured with public access. Ensuring that snapshots are not publicly accessible prevents unauthorized entities from retrieving sensitive data. Verify the backup's access settings in the Google Cloud Console under Cloud SQL > Backups, and ensure that no public IPs are listed. Remediate by configuring access restrictions to allow only authorized users and IP addresses.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/security
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudsql.instance.auto_minor_version_upgrade_configured
  service: cloudsql
  resource: instance
  requirement: Auto Minor Version Upgrade Configured
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Auto Minor Version Upgrade for Cloud SQL Instances
  rationale: Configuring auto minor version upgrades for Cloud SQL instances helps maintain the security and stability of databases by automatically applying the latest minor updates and patches. This reduces the risk of vulnerabilities being exploited in outdated database versions, which can lead to data breaches, service disruptions, and non-compliance with industry regulations.
  description: This rule verifies that auto minor version upgrades are enabled for Cloud SQL instances. It checks the instance configuration to ensure that the 'auto_minor_version_upgrade' setting is set to true. To enable this setting, navigate to the Cloud SQL instance settings in the Google Cloud Console, edit the instance, and enable the 'Auto Minor Version Upgrade' option. This ensures that your databases receive critical security updates automatically, minimizing manual intervention.
  references:
  - https://cloud.google.com/sql/docs/mysql/upgrade-db-versions
  - https://cloud.google.com/sql/docs/mysql/instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.automated_backups
  service: cloudsql
  resource: instance
  requirement: Automated Backups
  scope: cloudsql.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Enable Automated Backups for Cloud SQL Instances
  rationale: Automated backups are crucial for data recovery in case of accidental data loss, corruption, or system failure. Enabling this feature helps ensure business continuity by minimizing downtime and data loss, meeting regulatory requirements for data protection and resilience in frameworks like SOC 2 and ISO 27001.
  description: This rule checks whether automated backups are enabled for Cloud SQL instances. Automated backups must be configured to run at regular intervals, ensuring that recent copies of the database are available for restoration. To verify, navigate to the Cloud SQL Instance settings and ensure that automated backups are activated. Remediation involves enabling automated backups through the GCP Console or using the gcloud command line tool, specifying the desired backup window and retention period.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/sql/docs/postgres/backup-recovery/backups
  - CIS GCP Benchmark v1.3.0, Section 7.5 - Ensure that automated backups are enabled
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.cloudsql.instance.automated_backups_enabled
  service: cloudsql
  resource: instance
  requirement: Automated Backups Enabled
  scope: cloudsql.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Enable Automated Backups for Cloud SQL Instances
  rationale: Automated backups are crucial for protecting data against accidental deletion, corruption, and other forms of data loss. They ensure business continuity by allowing for quick data recovery, thereby minimizing downtime and potential revenue loss. Moreover, they are often a requirement for compliance with regulations such as GDPR and HIPAA, which mandate data protection and recovery processes.
  description: This rule checks if automated backups are enabled for Cloud SQL instances. Automated backups should be configured to occur daily to ensure data is consistently protected. To verify, navigate to the Google Cloud Console, select the SQL instance, and ensure that the 'Automated backups' setting is enabled under the 'Backups' tab. If not enabled, adjust the settings to activate automated backups. This ensures that all critical data is regularly backed up and can be restored in case of failure.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.backup_enabled
  service: cloudsql
  resource: instance
  requirement: Backup Enabled
  scope: cloudsql.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Cloud SQL Instances Have Backups Enabled
  rationale: Enabling backups for Cloud SQL instances is crucial for data resilience and disaster recovery. Without regular backups, you risk data loss in events such as accidental deletions, data corruption, or system failures. This practice supports business continuity and meets compliance requirements like PCI-DSS and ISO 27001, which mandate data protection measures.
  description: This rule checks if automatic backups are enabled for Cloud SQL instances. Backups provide a point-in-time snapshot of the database, which is essential for restoring data in case of incidents. To verify, navigate to the Cloud SQL instances page in the GCP Console and ensure that automatic backups are enabled under the 'Backups' section. Remediation involves configuring a backup schedule and enabling automatic backups through the console or using gcloud CLI commands.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.backup_encrypted
  service: cloudsql
  resource: instance
  requirement: Backup Encrypted
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Backups are Encrypted at Rest
  rationale: Encrypting backups of Cloud SQL instances at rest protects sensitive data from unauthorized access and potential data breaches. This is crucial for maintaining the confidentiality and integrity of data, meeting regulatory compliance requirements such as GDPR, HIPAA, and PCI-DSS, and mitigating risks associated with data leaks or theft.
  description: This rule checks whether backups for Cloud SQL instances are encrypted using Google-managed or customer-managed keys. To verify, navigate to the Cloud SQL instance settings in the GCP Console and ensure the 'Backups' section indicates that encryption is enabled. Remediation involves configuring encryption settings during the creation of the instance or by updating existing instances to use encrypted backups.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/security-foundations
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.cloudsql.instance.backup_retention_configured
  service: cloudsql
  resource: instance
  requirement: Backup Retention Configured
  scope: cloudsql.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Cloud SQL Backup Retention is Properly Configured
  rationale: Configuring backup retention for Cloud SQL instances is crucial for ensuring data can be restored in the event of accidental deletion, corruption, or other data loss scenarios. It helps maintain business continuity and meets compliance requirements for data protection and disaster recovery, such as those outlined by PCI-DSS and SOC2.
  description: This rule checks whether a backup retention policy is configured for Cloud SQL instances. Without this, data recovery may not be possible, leading to potential data loss. To verify, ensure that the 'backupConfiguration.enabled' setting is true and that a retention period is defined. Remediation involves setting an appropriate retention period in the Google Cloud Console under the SQL instance settings or using the gcloud CLI.
  references:
  - https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
  - https://cloud.google.com/sql/docs/postgres/backup-recovery/backups
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Section 6.1
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/soc2report.html
- rule_id: gcp.cloudsql.instance.contained_db_authentication_disabled
  service: cloudsql
  resource: instance
  requirement: Contained Db Authentication Disabled
  scope: cloudsql.instance.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Contained Database Authentication for Cloud SQL
  rationale: Disabling contained database authentication for Cloud SQL instances helps prevent unauthorized access by ensuring that authentication is managed at the instance level rather than within individual databases. This reduces the risk of privilege escalation and unauthorized access to sensitive data, aligning with compliance requirements for data protection and access control frameworks.
  description: This rule checks if contained database authentication is disabled for Cloud SQL instances. Contained database authentication allows users to authenticate directly within a database, which can bypass centralized access controls. To ensure stronger security management, it is recommended to disable this feature. Verification can be done by checking the instance settings, and remediation involves modifying the instance configuration to disable contained database authentication.
  references:
  - https://cloud.google.com/sql/docs/mysql/authentication
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.cloudsql.instance.cross_db_ownership_chaining_off
  service: cloudsql
  resource: instance
  requirement: Cross Db Ownership Chaining Off
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable Cross Database Ownership Chaining in Cloud SQL
  rationale: Disabling cross-database ownership chaining reduces the risk of unauthorized access between databases. This setting prevents users with permissions on one database from exploiting trust relationships to access objects in another database, thereby enhancing the overall security posture and supporting compliance with data separation policies.
  description: This rule checks if cross-database ownership chaining is disabled on Cloud SQL instances. Cross-database ownership chaining can be a security risk as it allows users to access objects they do not own across databases. To verify, inspect Cloud SQL instance settings for the 'cross_db_ownership_chaining' parameter and ensure it is set to 'off'. Remediation involves updating the instance configuration to disable this feature.
  references:
  - https://cloud.google.com/sql/docs/postgres/configure-instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://docs.microsoft.com/en-us/sql/relational-databases/security/database-engine-permissions-grants?view=sql-server-ver15
  - https://cloud.google.com/sql/docs/mysql/security
- rule_id: gcp.cloudsql.instance.db_cluster_audit_logging_enabled
  service: cloudsql
  resource: instance
  requirement: Db Cluster Audit Logging Enabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Cloud SQL Audit Logging is Enabled for Instances
  rationale: Enabling audit logging for Cloud SQL instances is crucial for tracking access and changes to your database configurations and data. It helps in identifying unauthorized access attempts and potential malicious activities, thus safeguarding sensitive data and meeting compliance requirements such as PCI-DSS and HIPAA. Without audit logs, organizations may face challenges in forensic investigations and risk non-compliance with regulatory standards.
  description: This rule checks whether audit logging is enabled for Cloud SQL instances in your GCP environment. Audit logs provide detailed insights into who accessed the database, what actions were taken, and when they occurred. To verify, navigate to the 'Logging' section under your Cloud SQL instance settings in the GCP Console and ensure 'Enable audit logs' is selected. Remediation involves configuring the Cloud SQL instance to enable audit logging via the GCP Console or using gcloud command-line tools.
  references:
  - https://cloud.google.com/sql/docs/mysql/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/instance-settings
- rule_id: gcp.cloudsql.instance.db_cluster_deletion_protection_enabled
  service: cloudsql
  resource: instance
  requirement: Db Cluster Deletion Protection Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Deletion Protection for Cloud SQL Instances
  rationale: Enabling deletion protection for Cloud SQL instances prevents accidental or malicious deletion of database clusters, which can lead to data loss, service downtime, and potential financial and reputational damage. This protection is crucial for maintaining data integrity and ensuring business continuity, especially in environments where multiple users have access to infrastructure resources.
  description: This rule checks if deletion protection is enabled for Cloud SQL instances. Deletion protection adds an additional layer of security by requiring a confirmation step before the instance can be deleted. To verify, navigate to the Google Cloud Console, access the SQL section, select your instance, and ensure that deletion protection is toggled on. Remediation involves enabling this setting in the instance configuration either via the console or using gcloud command-line tools.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings#deletion-protection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/architecture/security-best-practices
  - https://cloud.google.com/sql/docs/security-overview
- rule_id: gcp.cloudsql.instance.db_cluster_encryption_at_rest_cmek
  service: cloudsql
  resource: instance
  requirement: Db Cluster Encryption At Rest Cmek
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable CMEK for Cloud SQL Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for Cloud SQL ensures that organizations retain control over the encryption keys used to protect sensitive data at rest. This reduces the risk of unauthorized data access and enables compliance with regulations that require customer control over encryption keys, such as GDPR, HIPAA, and PCI-DSS.
  description: This rule checks that Cloud SQL instances are configured to use CMEK for encryption at rest. To verify, ensure that each Cloud SQL instance has a CMEK enabled in the settings. Remediation involves creating a Cloud Key Management Service (KMS) key and configuring the Cloud SQL instances to use this key for encrypting data at rest.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-cmek
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/sql/docs/mysql/encryption-at-rest
- rule_id: gcp.cloudsql.instance.db_cluster_minor_version_auto_upgrade_enabled
  service: cloudsql
  resource: instance
  requirement: Db Cluster Minor Version Auto Upgrade Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Auto Upgrade for Cloud SQL Minor Versions
  rationale: Enabling automatic minor version upgrades for Cloud SQL ensures that database instances receive the latest security patches and enhancements. This minimizes exposure to vulnerabilities and reduces the risk of data breaches that can lead to financial losses and reputational damage. It also aids in maintaining compliance with regulatory standards that require up-to-date software.
  description: This rule checks if Cloud SQL instances have the minor version auto upgrade feature enabled. This ensures that instances are automatically updated with the latest minor version releases, which often include critical security patches and performance improvements. To verify, check the Cloud SQL instance settings in the Google Cloud Console or use the gcloud CLI. If not enabled, configure the instance to allow minor version auto upgrades through the instance settings.
  references:
  - https://cloud.google.com/sql/docs/mysql/upgrade-db-instance
  - https://cloud.google.com/sql/docs/postgres/db-versions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings#auto-updates
- rule_id: gcp.cloudsql.instance.db_cluster_private_networking_enforced
  service: cloudsql
  resource: instance
  requirement: Db Cluster Private Networking Enforced
  scope: cloudsql.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Cloud SQL Instances Use Private IP Networking
  rationale: Enforcing private networking for Cloud SQL instances mitigates the risk of unauthorized access by ensuring that databases are accessible only through private IPs within a VPC. This reduces attack surface and aligns with compliance requirements like PCI-DSS and HIPAA, which mandate secure access controls for sensitive data.
  description: This rule checks if Cloud SQL instances are configured to use private IP addresses within a Virtual Private Cloud (VPC). By default, instances may be accessible over the public internet, increasing exposure to potential attacks. To remediate, enable private IP for the SQL instances under the 'Connections' settings in the Google Cloud Console, ensuring they are only accessible within authorized VPC networks.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://www.hipaajournal.com/hipaa-security-rule/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/security
- rule_id: gcp.cloudsql.instance.db_deletion_protection_enabled
  service: cloudsql
  resource: instance
  requirement: Db Deletion Protection Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Deletion Protection for Cloud SQL Instances
  rationale: Enabling deletion protection for Cloud SQL instances helps prevent accidental or malicious deletion of critical databases, which can lead to data loss, service downtime, and potential non-compliance with data retention policies. Protecting databases from deletion is crucial for maintaining business continuity and safeguarding sensitive data against unauthorized alterations.
  description: This rule checks if deletion protection is enabled for Cloud SQL instances. Deletion protection ensures that an instance cannot be deleted without first disabling this setting, providing a safeguard against unintended data loss. To verify, access the Cloud SQL instance settings in the Google Cloud Console and ensure the 'Deletion Protection' option is enabled. If not, update the instance configuration to enable this feature through the console or via gcloud command-line tool.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings#deletion_protection
  - https://cloud.google.com/security/compliance/cis#section-5.3
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/sql/docs/security
  - https://cloud.google.com/architecture/securing-sql-instances
- rule_id: gcp.cloudsql.instance.db_encryption_at_rest_enabled
  service: cloudsql
  resource: instance
  requirement: Db Encryption At Rest Enabled
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Encryption at Rest for Cloud SQL Instances
  rationale: Encryption at rest protects sensitive data stored in Cloud SQL instances from unauthorized access and potential data breaches. This is crucial for meeting compliance requirements such as PCI-DSS and HIPAA, and it helps prevent data theft in the event of physical hardware compromise or unauthorized database access.
  description: This rule checks if a Cloud SQL instance has encryption at rest enabled, which ensures that the data stored on the disk is encrypted using Google-managed encryption keys by default. To verify this setting, access the Cloud SQL instance settings in the Google Cloud Console and ensure that encryption is enabled. To remediate, create a new Cloud SQL instance with the 'Enable Encryption' option selected, or use customer-managed encryption keys (CMEK) for additional control.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings#enable-encryption
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/sites/default/files/hipaa-simplified.pdf
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/iam/docs/using-cmek
- rule_id: gcp.cloudsql.instance.db_iam_or_managed_identity_auth_enabled_where_supported
  service: cloudsql
  resource: instance
  requirement: Db IAM Or Managed Identity Auth Enabled Where Supported
  scope: cloudsql.instance.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable IAM or Managed Identity Auth for Cloud SQL Instances
  rationale: Enabling IAM or managed identity authentication for Cloud SQL instances enhances security by leveraging GCP's identity management capabilities. This reduces the risk of unauthorized access through better control and auditing of database access. It also aligns with compliance standards that require strict access management and audit trails for sensitive data stored in databases.
  description: This rule checks whether Cloud SQL instances have IAM or managed identity authentication enabled, which is crucial for securely managing database access. To verify, ensure that Cloud SQL instances are configured to use Cloud IAM or Managed Identity for database authentication. Remediation involves updating the database settings to enable these authentication methods, thereby improving access control and monitoring capabilities.
  references:
  - https://cloud.google.com/sql/docs/mysql/authentication
  - https://cloud.google.com/sql/docs/postgres/authentication
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - 'PCI-DSS Requirement 8: Identify and authenticate access to system components'
  - 'NIST SP 800-53: AC-2 Account Management'
- rule_id: gcp.cloudsql.instance.db_insecure_extensions_not_enabled
  service: cloudsql
  resource: instance
  requirement: Db Insecure Extensions Not Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable Insecure Extensions in Cloud SQL Instances
  rationale: Insecure database extensions can introduce vulnerabilities, allowing attackers to exploit these weaknesses to gain unauthorized access or escalate privileges. This is crucial for maintaining data integrity and protecting sensitive information, aligning with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks for the presence of insecure extensions in Cloud SQL instances. It ensures that such extensions, which are not recommended due to security risks, are disabled. To verify, review the list of enabled extensions in the Cloud SQL instance settings and disable any that are deemed insecure. Remediation involves updating the instance configuration to exclude these extensions, thereby reducing the attack surface.
  references:
  - https://cloud.google.com/sql/docs/mysql/extensions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/sql/docs/postgres/security-overview
- rule_id: gcp.cloudsql.instance.db_only_approved_extensions_enabled
  service: cloudsql
  resource: instance
  requirement: Db Only Approved Extensions Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Only Approved Extensions Enabled on Cloud SQL Instances
  rationale: Enabling only approved database extensions in Cloud SQL reduces the attack surface by preventing potentially vulnerable or unnecessary features from being used. This minimizes the risk of data breaches and ensures compliance with internal security policies and external regulations, such as PCI-DSS and HIPAA, which require minimizing the exposure of sensitive data to unvetted code.
  description: This rule verifies that only approved extensions are enabled on Cloud SQL instances. Administrators should maintain a list of allowed extensions, ensuring that any unapproved extensions are disabled. To verify, audit the extensions enabled on the instance via the Google Cloud Console or gcloud command-line tool, and compare them against the approved list. Remediation involves disabling unapproved extensions and revisiting database functionality to ensure business operations are not disrupted.
  references:
  - https://cloud.google.com/sql/docs/mysql/extensions
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-compliance-requirements/
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.db_parameter_group_audit_logging_enabled_where_supported
  service: cloudsql
  resource: instance
  requirement: Db Parameter Group Audit Logging Enabled Where Supported
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for Cloud SQL Instances Where Supported
  rationale: Audit logging for Cloud SQL instances helps track unauthorized access and changes, which is crucial for identifying potential security incidents and maintaining accountability. It supports compliance with regulations such as PCI-DSS and HIPAA by providing a detailed audit trail of database activities. Without audit logging, organizations may face undetected data breaches, leading to financial and reputational damage.
  description: This rule checks whether audit logging is enabled for Cloud SQL instances where supported. To verify, ensure the 'cloudsql.googleapis.com/sql/admin_audit' log is active in the Google Cloud Console under Logging > Logs Explorer. Remediation involves enabling audit logs by configuring the appropriate database flags and ensuring IAM roles have sufficient permissions for logging. This setting helps capture detailed logs of database activities for security monitoring and incident response.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-logs
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/logging/docs/audit/configure-data-access
- rule_id: gcp.cloudsql.instance.db_parameter_group_insecure_features_disabled_whe_applicable
  service: cloudsql
  resource: instance
  requirement: Db Parameter Group Insecure Features Disabled Whe Applicable
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable Insecure Features in Cloud SQL DB Parameter Groups
  rationale: Disabling insecure features in Cloud SQL DB parameter groups helps mitigate potential vulnerabilities that could be exploited by attackers to compromise database integrity or confidentiality. This is crucial for organizations handling sensitive data, as it reduces the risk of data breaches and aids in compliance with regulatory requirements such as PCI-DSS and HIPAA.
  description: This rule checks whether insecure features, such as unencrypted communication protocols and weak authentication mechanisms, are disabled in Cloud SQL DB parameter groups. Administrators should verify that these features are turned off by reviewing the DB parameter group's settings via the Cloud Console or using gcloud commands. Remediation involves adjusting the parameters to disable insecure options and enabling secure alternatives, such as SSL/TLS for data in transit and strong password policies.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://cloud.google.com/security/
  - https://cloud.google.com/sql/docs/mysql/security
- rule_id: gcp.cloudsql.instance.db_parameter_group_require_ssl
  service: cloudsql
  resource: instance
  requirement: Db Parameter Group Require SSL
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Instances Enforce SSL Connections
  rationale: Enforcing SSL connections for Cloud SQL instances is critical to protect data in transit from eavesdropping and man-in-the-middle attacks. This helps maintain data confidentiality and integrity, which is essential for meeting compliance requirements such as PCI-DSS and HIPAA. Without SSL, sensitive data may be exposed to unauthorized parties, leading to potential data breaches and financial loss.
  description: This rule verifies that Cloud SQL instances are configured to require SSL connections by checking the 'require_ssl' parameter in the database's parameter group. To ensure compliance, navigate to the Cloud SQL instance settings in the GCP Console and under 'Connections', enable 'SSL connections only'. Remediation involves updating the parameter group or applying a new one that enforces SSL, and ensuring that all client applications are configured to use SSL.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/sql/docs/mysql/security-overview
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.cloudsql.instance.db_public_access_disabled
  service: cloudsql
  resource: instance
  requirement: Db Public Access Disabled
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Access for Cloud SQL Instances
  rationale: Exposing Cloud SQL instances to the public internet increases the risk of unauthorized access, data breaches, and potential exploitation of database vulnerabilities. By disabling public access, organizations can ensure that only authorized internal networks can access database services, reducing the attack surface and aligning with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks if Cloud SQL instances have public IP addresses assigned, which allows connections from any IP address on the internet. To verify, inspect the SQL instance settings via the GCP Console or use gcloud commands to list instances with public IPs. Remediation involves configuring instances to use private IPs by modifying network settings, ensuring access is restricted to specific internal networks or VPNs.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.db_require_tls_in_transit
  service: cloudsql
  resource: instance
  requirement: Db Require TLS In Transit
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce TLS for Cloud SQL Instance Connections
  rationale: Requiring TLS for Cloud SQL instances ensures data is encrypted in transit, protecting against man-in-the-middle attacks and eavesdropping. This is crucial for maintaining data confidentiality and integrity, meeting compliance requirements such as PCI-DSS and HIPAA, and upholding customer trust.
  description: This rule verifies that Cloud SQL instances are configured to enforce TLS for all incoming connections. Administrators should enable 'require SSL' settings to ensure encrypted connections, thus safeguarding sensitive data against interception during transit. Verification can be done via the GCP Console or CLI by checking the instance's connection settings. Remediation involves updating the instance configuration to enforce SSL connections.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/sql/docs/postgres/configure-ssl-instance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.instance.db_subnet_group_private_subnets_only
  service: cloudsql
  resource: instance
  requirement: Db Subnet Group Private Subnets Only
  scope: cloudsql.instance.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Cloud SQL Instances Use Private Subnets Only
  rationale: Using private subnets for Cloud SQL instances reduces exposure to potential attacks from the internet and unauthorized access. It aligns with best practices for protecting sensitive data against breaches and complies with regulatory requirements such as HIPAA and PCI-DSS, which mandate strict control over network access to databases.
  description: This rule checks whether Cloud SQL instances are configured to use private subnets only within their Db Subnet Group, ensuring that instances do not have public IP addresses and are isolated from internet traffic. Verify that the 'Private Service Connect' is enabled, and ensure the instance is part of a VPC with private IP enabled. Remediation involves updating the instance's settings to disable public IP and configure networking to use private IPs only according to GCP documentation.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-private-ip
  - https://cloud.google.com/sql/docs/postgres/configure-private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/sqlserver/configure-private-ip
- rule_id: gcp.cloudsql.instance.default_admin_gcp_sql_instance_default_admin
  service: cloudsql
  resource: instance
  requirement: Default Admin Gcp Sql Instance Default Admin
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Disable Default Admin Account on GCP SQL Instances
  rationale: The presence of default admin accounts in Cloud SQL instances poses significant security risks, such as unauthorized access and potential data breaches. Attackers can exploit these accounts to gain full control over the database, compromising sensitive information. Ensuring these accounts are disabled aligns with compliance requirements like PCI-DSS and ISO 27001, which mandate strict access controls.
  description: This rule checks for the presence of default admin accounts on Cloud SQL instances, which should be disabled to prevent unauthorized access. Administrators should create custom accounts with the least privilege necessary for their operations. To verify, inspect the user list on each SQL instance and ensure no default admin accounts are enabled. Remediation involves disabling these accounts and setting up custom admin accounts with strong, unique credentials.
  references:
  - https://cloud.google.com/sql/docs/mysql/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings
- rule_id: gcp.cloudsql.instance.deletion_protection
  service: cloudsql
  resource: instance
  requirement: Deletion Protection
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Deletion Protection for Cloud SQL Instances
  rationale: Deletion protection prevents accidental or malicious deletion of Cloud SQL instances, which can lead to data loss and service disruption. This feature is crucial for maintaining data integrity and availability, especially for instances containing critical business or customer data. Compliance with data protection regulations may also necessitate safeguarding data against unintended deletions.
  description: This rule checks if deletion protection is enabled on Cloud SQL instances. Deletion protection is a setting that prevents the instance from being deleted via the console, gcloud command-line tool, or API. To verify, ensure the 'deletionProtection' field is set to true in the instance configuration. Remediation involves updating the instance settings to enable deletion protection using the Cloud Console, gcloud command-line tool, or REST API.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings#deletion-protection
  - https://cloud.google.com/sql/docs/postgres/instance-settings#deletion-protection
  - https://cloud.google.com/sql/docs/sqlserver/instance-settings#deletion-protection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudsql.instance.dynamodb_table_encryption_enabled
  service: cloudsql
  resource: instance
  requirement: Dynamodb Table Encryption Enabled
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Cloud SQL Instances Have Encryption at Rest Enabled
  rationale: Enabling encryption at rest for Cloud SQL instances protects sensitive data from unauthorized access and data breaches. It is crucial for maintaining data confidentiality and integrity, especially for workloads subject to regulatory standards such as PCI-DSS and HIPAA. This measure mitigates risks associated with data theft or loss in case of physical or logical breaches.
  description: This rule verifies that Google Cloud SQL instances have encryption at rest enabled. By default, Google automatically encrypts all data before it is written to disk using AES-256 encryption. To ensure compliance, users should verify that encryption settings align with organizational policies and regulatory requirements. Remediation involves confirming the use of Google's managed encryption keys or configuring customer-managed encryption keys (CMEK) for advanced control over encryption processes.
  references:
  - https://cloud.google.com/sql/docs/mysql/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.cloudsql.instance.encryption_at_rest_enabled
  service: cloudsql
  resource: instance
  requirement: Encryption At Rest Enabled
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Cloud SQL Instances Have Encryption at Rest Enabled
  rationale: Enabling encryption at rest for Cloud SQL instances is crucial for protecting sensitive data from unauthorized access and potential data breaches. It mitigates risks associated with data theft or exposure in cases of physical storage compromise. Compliance with standards such as PCI-DSS, HIPAA, and SOC2 often mandates encryption at rest for safeguarding sensitive information.
  description: This rule checks that all Cloud SQL instances have encryption at rest enabled by utilizing Google-managed keys or customer-managed keys. Verify this setting by accessing the Cloud SQL instance settings in the Google Cloud Console or using the gcloud CLI. To remediate, ensure that each Cloud SQL instance has the 'Enable Encryption at Rest' option checked during creation or update the instance to enable this option.
  references:
  - https://cloud.google.com/sql/docs/mysql/encryption-at-rest
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.cloudsql.instance.enhanced_monitoring_enabled
  service: cloudsql
  resource: instance
  requirement: Enhanced Monitoring Enabled
  scope: cloudsql.instance.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Enhanced Monitoring for Cloud SQL Instances
  rationale: Enabling enhanced monitoring for Cloud SQL instances provides deeper insights into system performance, which is critical for identifying and mitigating potential security threats and ensuring optimal operation. It helps in meeting compliance requirements by providing detailed metrics and logs, which are necessary for audit trails and forensic investigations in case of a security incident.
  description: This rule checks whether enhanced monitoring is enabled for Cloud SQL instances, which involves configuring the instances to export detailed performance metrics to Cloud Monitoring. To verify, check the 'Database' section in the Google Cloud Console and ensure 'Enable Cloud Monitoring' is activated. Remediation involves enabling this feature, which can be done via the Cloud Console or gcloud command-line tool, ensuring that the Cloud SQL Admin API is enabled and proper IAM roles are assigned.
  references:
  - https://cloud.google.com/sql/docs/mysql/monitoring
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/sql/docs/postgres/monitoring
  - https://cloud.google.com/sql/docs/sqlserver/monitoring
- rule_id: gcp.cloudsql.instance.external_scripts_disabled
  service: cloudsql
  resource: instance
  requirement: External Scripts Disabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable External Scripts for Cloud SQL Instances
  rationale: Disabling external scripts execution in Cloud SQL reduces the potential attack surface by preventing malicious code from running within the database environment. This enhances data integrity and helps comply with regulatory frameworks such as PCI-DSS and HIPAA, which require secure data handling practices. It also mitigates the risk of unauthorized data access and compromise through external scripts that could exploit database vulnerabilities.
  description: This rule checks whether external scripts are disabled for Cloud SQL instances, ensuring that scripts cannot be executed from external sources. To verify compliance, review the Cloud SQL instance settings in the GCP Console or via gcloud CLI to ensure the 'external_scripts' flag is set to 'off'. Remediation involves updating the instance configuration to disable external script execution, which can be done through the GCP Console under the instance's 'Flags' section or using the gcloud command line tool.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.high_availability
  service: cloudsql
  resource: instance
  requirement: High Availability
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud SQL Instances Have High Availability Configured
  rationale: Configuring high availability for Cloud SQL instances is crucial to minimize service downtime and ensure business continuity in case of zone failures. Without high availability, database instances are vulnerable to disruptions, potentially leading to data loss and financial impacts. High availability is often a requirement for compliance with standards such as PCI-DSS and ISO 27001, which demand resilient and fault-tolerant systems.
  description: This rule checks that Cloud SQL instances are configured with high availability, which involves setting up a primary instance and one or more replicas in different zones. To verify, ensure that the 'availabilityType' is set to 'REGIONAL' in the Cloud SQL instance settings. If not configured, enable high availability by modifying the instance settings to include multi-zone replication. This ensures automatic failover and data redundancy across zones.
  references:
  - https://cloud.google.com/sql/docs/mysql/high-availability
  - https://cloudsecurityalliance.org/download/gcp-security-best-practices/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/sql/docs/mysql/configure-ha
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.cloudsql.instance.iam_authentication_enabled
  service: cloudsql
  resource: instance
  requirement: IAM Authentication Enabled
  scope: cloudsql.instance.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enable IAM Authentication for Cloud SQL Instances
  rationale: Enabling IAM authentication for Cloud SQL instances ensures that database access is integrated with Google Cloud's identity management, enhancing access control and reducing the risk of unauthorized access. This is crucial for protecting sensitive data, meeting compliance requirements such as GDPR and HIPAA, and mitigating potential data breaches and insider threats.
  description: This rule checks if IAM authentication is enabled for Cloud SQL instances, which allows users to connect to databases using their Google Cloud credentials. To verify, inspect the instance settings in the Google Cloud Console or use the gcloud CLI to confirm that IAM DB authentication is activated. Remediation involves configuring the instance to enable IAM authentication through the Cloud SQL settings panel or via the gcloud sql instances patch command.
  references:
  - https://cloud.google.com/sql/docs/mysql/authentication
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/iam-authentication
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.cloudsql.instance.instance_enabled
  service: cloudsql
  resource: instance
  requirement: Instance Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud SQL Instances Are Enabled and Configured Correctly
  rationale: Enabled Cloud SQL instances are crucial for maintaining business continuity and data accessibility. Disabling instances can lead to service disruption, affecting application availability and integrity. Properly configured instances ensure compliance with data protection regulations and mitigate risks of unauthorized access or data breaches.
  description: This rule verifies that Cloud SQL instances are enabled and operational. It checks the 'state' attribute of each Cloud SQL instance to confirm it is not in a 'disabled' state, which could indicate an unintentional service interruption. To remediate, ensure instances are correctly started and configured using the GCP Console or CLI. Regularly audit instance states to prevent accidental downtime.
  references:
  - https://cloud.google.com/sql/docs/mysql/start-stop-restart-instance
  - https://cloud.google.com/sql/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/instance-settings
  - https://cloud.google.com/sql/docs/postgres/instance-settings
- rule_id: gcp.cloudsql.instance.local_infile_flag_disabled
  service: cloudsql
  resource: instance
  requirement: Local Infile Flag Disabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Local Infile Flag is Disabled on Cloud SQL Instances
  rationale: Disabling the local infile flag in Cloud SQL prevents unauthorized data import operations, reducing the risk of data exfiltration and malicious data manipulation. This control is crucial for maintaining the integrity and confidentiality of data, especially for organizations that must adhere to stringent data protection regulations.
  description: This rule checks if the local infile flag is disabled on Cloud SQL instances, which is necessary to prevent potential security vulnerabilities associated with data import operations. To verify compliance, review the database flags in the Cloud SQL instance settings and ensure 'local_infile' is set to 'OFF'. Remediation involves configuring the database flags accordingly via the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.instance.log_connections_flag_configured
  service: cloudsql
  resource: instance
  requirement: Log Connections Flag Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Cloud SQL Log Connections Flag is Enabled
  rationale: Enabling the log connections flag for Cloud SQL instances is crucial for monitoring access and detecting potential unauthorized attempts. This setting aids in compliance with regulations that mandate logging of access to sensitive data, such as PCI-DSS and ISO 27001, and helps in forensic investigations by providing detailed connection logs.
  description: This rule checks whether the 'log_connections' flag is enabled for Cloud SQL instances, which logs all connection requests to the database. To verify, access the Cloud SQL instance settings in the GCP Console, navigate to the 'Flags' section, and ensure 'log_connections' is set to 'on'. If not enabled, modify the instance configuration to include this flag, which can be done via the GCP Console, gcloud command-line tool, or Terraform scripts.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://cloud.google.com/sql/docs/mysql/monitoring
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/sql/docs/mysql/configure-advanced-settings#flags
- rule_id: gcp.cloudsql.instance.log_disconnections_flag_enabled
  service: cloudsql
  resource: instance
  requirement: Log Disconnections Flag Enabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Log Disconnections for Cloud SQL Instances
  rationale: Logging disconnections in Cloud SQL helps identify potential issues such as network interruptions, unauthorized access attempts, or performance bottlenecks. This can be crucial for maintaining the integrity and reliability of database operations. Additionally, it aids in meeting compliance requirements by providing a detailed audit trail of all disconnection events.
  description: This rule checks if the 'log_disconnections' flag is enabled for Cloud SQL instances, ensuring that disconnection events are logged. To verify, review the instance configuration for the 'log_disconnections' setting. If not enabled, update the instance settings to include this flag. Logging disconnections can be done through the Google Cloud Console or using the gcloud command-line tool, providing visibility into database access patterns and potential anomalies.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/operations
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.cloudsql.instance.log_error_verbosity_configured
  service: cloudsql
  resource: instance
  requirement: Log Error Verbosity Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Cloud SQL Log Error Verbosity is Configured
  rationale: Configuring log error verbosity in Cloud SQL is essential for capturing detailed error logs, which help in diagnosing issues, ensuring accountability, and maintaining transparency. This setup aids in forensic investigations and is crucial for compliance with regulations like PCI-DSS and SOC 2, which require detailed logging for audit purposes.
  description: This rule checks whether the log error verbosity setting is configured on Cloud SQL instances. Proper configuration ensures that error logs are verbose enough to provide meaningful insights into database operations and issues. To verify, check the 'log_error_verbosity' setting in the database flags. Remediation involves setting the verbosity level to 'VERBOSE' or 'ERROR' as per the security policy requirements.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/sql/docs/postgres/flags
- rule_id: gcp.cloudsql.instance.log_min_duration_statement_disabled
  service: cloudsql
  resource: instance
  requirement: Log Min Duration Statement Disabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Cloud SQL Log Min Duration Statement is Enabled
  rationale: Without logging the minimum duration of SQL statements, it becomes difficult to identify and troubleshoot long-running queries which may indicate performance issues or potential security incidents, impacting business operations and data integrity. Enabling this feature aids in compliance with regulatory requirements for auditing and monitoring, such as PCI-DSS and SOC2, which mandate detailed logging.
  description: This rule checks if the 'log_min_duration_statement' setting is disabled on Cloud SQL instances. When disabled, SQL statement execution times are not logged, obscuring performance insights and security audit trails. To verify, ensure the 'log_min_duration_statement' is set to an appropriate threshold in the PostgreSQL or MySQL configuration of the Cloud SQL instance. Remediate by enabling and configuring this setting through the GCP Console or gcloud CLI to capture statements exceeding a specified duration.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/sql/docs/postgres/audit-logging
- rule_id: gcp.cloudsql.instance.log_min_error_statement_compliance_configured
  service: cloudsql
  resource: instance
  requirement: Log Min Error Statement Compliance Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Log Min Error Statement Configured for Cloud SQL Instances
  rationale: Configuring the 'log_min_error_statement' setting in Cloud SQL ensures that all error statements meeting or exceeding a specified severity level are logged. This helps in early detection of potential issues, supports forensic investigations, and adheres to compliance regulations like PCI-DSS and SOC2 that require detailed error logging for auditing purposes.
  description: This rule checks whether the 'log_min_error_statement' configuration is set appropriately on Cloud SQL instances to log minimum error statements. Proper configuration involves setting the parameter to a severity level (e.g., ERROR, WARNING) that aligns with your organization's logging policy. To verify, access the Cloud SQL instance settings in the GCP Console, navigate to 'Flags', and ensure 'log_min_error_statement' is configured. Remediation involves setting this flag to a suitable level and applying the changes.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.cloudsql.instance.log_min_messages_compliance_configured
  service: cloudsql
  resource: instance
  requirement: Log Min Messages Compliance Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Configure Cloud SQL Instances for Minimum Log Level Compliance
  rationale: Properly configuring the minimum log level for Cloud SQL instances is crucial to ensure that relevant security events are captured while reducing noise from less critical log messages. This aids in timely detection of anomalies and compliance with standards such as PCI-DSS and ISO 27001, which require effective logging mechanisms to track access and changes.
  description: This rule checks whether the 'log_min_messages' setting for Cloud SQL instances is set to a compliance-friendly level, such as 'ERROR' or 'WARNING'. This configuration is intended to log only significant events that might indicate potential security issues or system malfunctions. Verification can be performed by reviewing the Cloud SQL instance settings in the GCP Console under 'Logs' or using the gcloud command-line tool. Remediation involves updating the 'log_min_messages' setting in the Cloud SQL instance configuration to a more appropriate level if it is not already set.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags#mysql_flags
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/sql/docs/mysql/logging
- rule_id: gcp.cloudsql.instance.log_statement_flag_configured
  service: cloudsql
  resource: instance
  requirement: Log Statement Flag Configured
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Cloud SQL Log Statement Flag is Configured
  rationale: Configuring the log statement flag in Cloud SQL is crucial for auditing and troubleshooting. It helps detect suspicious activities by capturing SQL statements executed on the database, which can aid in forensic investigations and compliance with standards like PCI-DSS and SOC2. Without logging, potential security incidents might go unnoticed, increasing the risk of data breaches.
  description: This rule checks if the log_statement flag is configured for Cloud SQL instances. The flag should be set to 'all' or 'mod' to capture all SQL statements or modifications, respectively. To verify, access the Cloud SQL instance settings and ensure the log_statement parameter is configured appropriately. If not configured, update the instance settings to enable logging. This enhances visibility into database operations, helping to monitor access and changes.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.cloudsql.instance.logging_enabled
  service: cloudsql
  resource: instance
  requirement: Logging Enabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for Cloud SQL Instances
  rationale: Enabling logging on Cloud SQL instances is crucial for monitoring access and activity, which helps in detecting unauthorized access and potential data breaches. Logging is also important for compliance with regulatory frameworks like PCI-DSS and HIPAA, which mandate detailed logging of access to sensitive data. Without logging, it is difficult to audit actions taken within the database, increasing the risk of undetected malicious activities.
  description: This rule checks whether logging is enabled for Cloud SQL instances, ensuring that all database operations are recorded. To verify, check the instance settings in the Google Cloud Console and ensure that the 'Enable Logging' option is activated. If not, navigate to the SQL instance settings and enable logging to capture all activities. This action allows for detailed auditing and issue resolution by maintaining an operational log of database activities.
  references:
  - https://cloud.google.com/sql/docs/mysql/instance-settings#enable-logging
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/logging/docs/audit/#cloudsql
  - https://cloud.google.com/sql/docs/mysql/maintain-log-files
- rule_id: gcp.cloudsql.instance.multi_az
  service: cloudsql
  resource: instance
  requirement: Multi Az
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud SQL Instances Use Multi-Region Deployment
  rationale: Deploying Cloud SQL instances across multiple availability zones (AZs) enhances fault tolerance and availability. This configuration protects against zonal outages, ensuring business continuity and minimizing downtime, which is critical for applications requiring high availability. It also aligns with compliance frameworks demanding robust failover mechanisms as part of disaster recovery strategies.
  description: This rule checks if Cloud SQL instances are configured to use multi-region (multi-AZ) deployment. To verify, inspect the instance settings in the Google Cloud Console or via the gcloud command-line tool to confirm that the 'region' setting includes multiple AZs. Remediation involves enabling the high-availability (HA) option during instance setup or modifying existing instances to enable HA, ensuring they are spread across multiple AZs for redundancy.
  references:
  - https://cloud.google.com/sql/docs/mysql/high-availability
  - https://cloud.google.com/sql/docs/postgres/high-availability
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.multi_az_enabled
  service: cloudsql
  resource: instance
  requirement: Multi Az Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Cloud SQL Instances are Configured for Multi-AZ Deployment
  rationale: Enabling Multi-AZ deployment for Cloud SQL instances is crucial for high availability and fault tolerance, minimizing downtime risk. This configuration helps protect against zone failures and ensures continuous database service, which is vital for business continuity and meeting SLAs. It also supports compliance with regulations requiring robust disaster recovery plans.
  description: This rule checks if Cloud SQL instances have Multi-AZ configuration enabled, meaning they are deployed across multiple availability zones. Multi-AZ deployment allows automatic failover to a standby instance in a different zone in case of a failure. To verify, review the instance settings in the Google Cloud Console or use the gcloud CLI. Remediation involves modifying the instance to enable high availability settings, which can be done during instance creation or via configuration updates.
  references:
  - https://cloud.google.com/sql/docs/mysql/high-availability
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/instance-settings#availability
- rule_id: gcp.cloudsql.instance.no_public_access
  service: cloudsql
  resource: instance
  requirement: No Public Access
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public IP for Cloud SQL Instances
  rationale: Exposing Cloud SQL instances to the public internet increases the risk of unauthorized access and potential data breaches. Public IP access can be exploited by attackers to launch brute force attacks or exploit vulnerabilities. Restricting access to private IPs helps maintain data confidentiality and aligns with compliance requirements such as PCI-DSS and HIPAA, which mandate the protection of sensitive data.
  description: This rule checks if a Cloud SQL instance is configured with a public IP, which should be disabled to prevent unauthorized internet access. Verify that all instances use private IP connections by checking the 'IP configuration' settings in the GCP Console or via the gcloud command-line tool. To remediate, modify the instance settings to disable public IP and enable private IP connectivity, ensuring that access is controlled through authorized networks and identity-based access management.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 6.4
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.cloudsql.instance.no_public_access_gcp_compute_securitygroup_allow__configured
  service: cloudsql
  resource: instance
  requirement: No Public Access Gcp Compute Securitygroup Allow Configured
  scope: cloudsql.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Prevent Public Access to Cloud SQL Instances
  rationale: Allowing public access to Cloud SQL instances can expose sensitive data to unauthorized users, leading to potential data breaches. It increases the risk of attacks such as SQL injection and unauthorized data exfiltration. Compliance with regulations such as PCI-DSS, HIPAA, and SOC2 often mandates restricted access to secure databases, thereby ensuring data privacy and integrity.
  description: This rule checks for Cloud SQL instances that have public IP addresses or are configured to allow public access through GCP compute security groups. It is essential to ensure that these instances are not accessible from the public internet and are instead restricted to internal IP addresses or specific trusted networks. To remediate, disable public IP addresses in the instance settings and configure VPC firewall rules to limit access to authorized IP ranges only.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://cloud.google.com/sql/docs/mysql/sql-proxy
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.cloudsql.instance.paas_db_encryption_at_rest_enabled
  service: cloudsql
  resource: instance
  requirement: Paas Db Encryption At Rest Enabled
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption at Rest for GCP Cloud SQL Instances
  rationale: Encryption at rest protects sensitive data by ensuring it is not readable without proper authorization, reducing the risk of data breaches. It mitigates threats from unauthorized access and meets compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate data protection through encryption.
  description: This rule checks if Google Cloud SQL instances have encryption at rest enabled, which safeguards data stored within the databases. To verify, ensure that the 'disk_encryption_configuration' settings in the Cloud SQL instance configuration are applied. Remediation involves using the Google Cloud Console or gcloud CLI to set up Customer-Managed Encryption Keys (CMEK) or let Google manage encryption keys to secure data at rest.
  references:
  - https://cloud.google.com/sql/docs/mysql/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/encryption-at-rest/customer-managed-encryption
- rule_id: gcp.cloudsql.instance.paas_db_public_access_disabled
  service: cloudsql
  resource: instance
  requirement: Paas Db Public Access Disabled
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Access to Cloud SQL Instances
  rationale: Exposing Cloud SQL instances to the public internet poses significant security risks, including unauthorized access and potential data breaches. Disabling public access reduces the attack surface and helps comply with regulatory requirements such as PCI-DSS and HIPAA, which mandate the protection of sensitive data from unauthorized exposure.
  description: This rule checks whether Cloud SQL instances are configured to deny public IP access, ensuring they are only accessible via private IP addresses within Google Cloud VPCs. To verify, inspect the instance settings in the GCP Console and ensure 'Public IP' is not enabled. If necessary, modify the instance to use private IPs and configure appropriate VPC peering or VPN connections to maintain secure access.
  references:
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.cloudsql.instance.paas_db_require_tls_in_transit
  service: cloudsql
  resource: instance
  requirement: Paas Db Require TLS In Transit
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Instances Use TLS for Data in Transit
  rationale: Enforcing TLS for data in transit protects against man-in-the-middle attacks, ensuring the confidentiality and integrity of data. This is crucial for maintaining trust with customers and complying with regulatory standards such as PCI-DSS and HIPAA, which mandate encryption of sensitive data during transmission.
  description: This rule checks that all Cloud SQL instances have TLS enabled for data in transit, ensuring secure communication between clients and databases. To verify, ensure the database instance is configured to require SSL/TLS connections. Remediation involves setting the 'require_ssl' flag to 'on' in the Cloud SQL instance settings and updating client configurations to use SSL/TLS for connections.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/encryption-in-transit/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.cloudsql.instance.pg_audit_logging_enabled
  service: cloudsql
  resource: instance
  requirement: Pg Audit Logging Enabled
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Pg Audit Logging is Enabled for Cloud SQL Instances
  rationale: Enabling Pg Audit logging for Cloud SQL instances is crucial for monitoring and tracking access to sensitive database resources. It helps in detecting unauthorized access attempts and potential data breaches, thereby safeguarding business-critical data. Compliance with regulations such as GDPR, PCI-DSS, and HIPAA often mandates comprehensive logging of database activities.
  description: This rule verifies that Pg Audit logging is enabled for PostgreSQL instances in Google Cloud SQL. Pg Audit provides detailed logging of database activities, including connection attempts, disconnections, and data access queries. To enable Pg Audit logging, configure the 'cloudsql.enable_pg_audit' flag for your Cloud SQL instance. This can be done via the GCP Console, gcloud command-line tool, or API. Ensure logs are regularly reviewed and stored securely for audit purposes.
  references:
  - https://cloud.google.com/sql/docs/postgres/pg-audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/sql/docs/postgres/configure-logs
- rule_id: gcp.cloudsql.instance.postgres_log_connections_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Connections Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Postgres Log Connections Flag is Enabled
  rationale: Enabling the Postgres log_connections flag helps in auditing and monitoring PostgreSQL database access patterns, which is crucial for detecting unauthorized access attempts and ensuring compliance with security policies. This logging capability supports forensic analysis and can assist in identifying potential security incidents or misuse.
  description: This rule checks for the activation of the log_connections flag on Cloud SQL PostgreSQL instances. When enabled, this flag logs each successful connection, aiding in the auditing of database access. To verify, navigate to the 'Flags' tab in the Cloud SQL instance settings and ensure 'log_connections' is set to 'on'. Remediation involves enabling this flag in the Cloud SQL instance configuration, either through the GCP console, gcloud CLI, or the API.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/postgres
  - https://cloud.google.com/sql/docs/postgres/logging
  - https://cloud.google.com/sql/docs/postgres/flags#log_connections
- rule_id: gcp.cloudsql.instance.postgres_log_disconnections_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Disconnections Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Postgres Log Disconnections for Cloud SQL Instances
  rationale: Logging disconnections in PostgreSQL helps in tracking user sessions and identifying unexpected disconnections, which can indicate potential security incidents or misconfigurations. This is crucial for forensic analysis and compliance with regulations that require detailed activity logs, such as PCI-DSS and SOC2.
  description: This rule checks whether the 'log_disconnections' flag is enabled for PostgreSQL instances on Cloud SQL. Enabling this flag ensures that all disconnection events are logged, providing visibility into session terminations. To verify, navigate to the Cloud SQL instance settings in the Google Cloud Console and ensure that 'log_disconnections' is set to 'on'. Remediation involves setting this flag to 'on' via the Cloud Console or using gcloud CLI.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags#postgres-flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/postgres/logging
- rule_id: gcp.cloudsql.instance.postgres_log_error_verbosity_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Error Verbosity Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Postgres Log Error Verbosity is Set to 'Default' or Lower
  rationale: Configuring the Postgres log error verbosity to an appropriate level is crucial for effective monitoring and troubleshooting. Excessive verbosity can lead to log flooding, obscuring important information and potentially leading to non-compliance with data protection regulations by exposing sensitive information. Ensuring the correct verbosity level helps maintain a balance between sufficient logging for security auditing and avoiding unnecessary exposure of sensitive data.
  description: This rule checks if the Postgres log error verbosity flag on Cloud SQL instances is set to 'Default' or lower. By default, this setting ensures that only essential error messages are logged, minimizing the risk of sensitive data exposure while maintaining necessary operational insights. To verify, access the Cloud SQL instance settings and review the 'log_error_verbosity' configuration. If set to 'Verbose', change it to 'Default' or 'Terse' to comply with best practices.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/postgres/configure-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-iec-27001-information-security.html
- rule_id: gcp.cloudsql.instance.postgres_log_min_duration_statement_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Min Duration Statement Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Postgres Log Min Duration Statement Configuration
  rationale: Configuring the log_min_duration_statement flag in PostgreSQL instances helps identify slow queries that might indicate performance bottlenecks or potential security vulnerabilities, such as SQL injection attempts. This setting aids in proactive database monitoring and optimization, aligning with compliance requirements for audit logging and performance monitoring.
  description: This rule checks whether the 'log_min_duration_statement' flag is set for PostgreSQL instances in Cloud SQL. The flag must be adjusted to log queries exceeding a specified execution time, aiding in the detection of inefficient queries and potential anomalies. To verify and remediate, access the Cloud SQL instance settings, navigate to the 'Flags' section, and set 'log_min_duration_statement' to a reasonable value, such as 1000 milliseconds, based on your application needs.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-working-with-postgresql-on-google-cloud
  - https://www.postgresql.org/docs/current/runtime-config-logging.html
- rule_id: gcp.cloudsql.instance.postgres_log_min_error_statement_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Min Error Statement Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Postgres Log Min Error Statement is Configured Correctly
  rationale: Proper configuration of the Postgres log_min_error_statement flag helps in identifying and troubleshooting SQL errors by capturing error statements in the logs. This is crucial for detecting potential security incidents and operational issues. Misconfiguration can lead to incomplete logging, hindering forensic investigations and failing to meet compliance standards such as PCI-DSS and HIPAA.
  description: This rule checks if the Postgres 'log_min_error_statement' flag is set to 'ERROR' or a more detailed level on Cloud SQL instances. This setting ensures that all SQL statements that generate errors, including syntax errors and permission denials, are logged. To verify, inspect the database flags configuration in the GCP Console or via gcloud CLI. Remediate by updating the flag in the Cloud SQL instance settings to 'ERROR' or 'ALL', and restart the instance for changes to take effect.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/postgres/diagnose-issues
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/postgres/configure-logging
  - https://cloud.google.com/sql/docs/postgres/audit-logging
- rule_id: gcp.cloudsql.instance.postgres_log_min_messages_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Min Messages Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Postgres Log Min Messages Flag is Configured
  rationale: Configuring the Postgres log_min_messages flag is crucial as it determines the minimum level of messages that are logged. This can help in identifying potential security incidents by capturing relevant logs. Not setting this flag appropriately can lead to insufficient logging, missing critical events, and potentially violating regulatory compliance requirements such as those outlined by HIPAA or PCI-DSS.
  description: This rule checks if the Postgres log_min_messages flag for Cloud SQL instances is set to a level that ensures adequate logging of important events, such as 'WARNING' or 'ERROR'. It is important to configure this setting to capture essential logs for security monitoring and incident response. To verify, navigate to the GCP Console, access the Cloud SQL instance settings, and ensure the log_min_messages flag is set appropriately. For remediation, update this setting under the 'Flags' section of your Cloud SQL instance.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://cloud.google.com/sql/docs/postgres/diagnose-issues
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.cloudsql.instance.postgres_log_statement_flag
  service: cloudsql
  resource: instance
  requirement: Postgres Log Statement Flag
  scope: cloudsql.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Postgres Log Statement for Cloud SQL Instances
  rationale: Enabling the log_statement flag in Cloud SQL for PostgreSQL instances is critical for auditing and monitoring activities within your database. This setting provides visibility into SQL queries, helping identify potential malicious activities such as SQL injection attempts or unauthorized data access. It supports compliance requirements by maintaining a clear record of database interactions, which is essential for audits and forensic investigations.
  description: This rule checks if the log_statement flag is enabled for PostgreSQL instances in Cloud SQL. This setting logs SQL statements executed against the database, which can be set to log all statements, none, or specific types of statements. To ensure comprehensive logging, configure the log_statement parameter to 'all'. This can be verified and set in the instance's database flags configuration. If not enabled, update the Cloud SQL instance settings to include the log_statement flag set to 'all' to ensure complete logging of database activities.
  references:
  - https://cloud.google.com/sql/docs/postgres/flags#log_statement
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/sql/docs/audit-logs
  - https://cloud.google.com/sql/docs/postgres/configure-pg-flags
- rule_id: gcp.cloudsql.instance.public_access
  service: cloudsql
  resource: instance
  requirement: Public Access
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to Cloud SQL Instances
  rationale: Allowing public access to Cloud SQL instances exposes them to potential unauthorized access and attacks, such as brute force or SQL injection. This increases the risk of data breaches, compromising sensitive information, and potentially violating compliance standards like PCI-DSS, SOC2, or HIPAA, which mandate strict access controls.
  description: This rule checks if Cloud SQL instances are configured to allow public IP access. Instances should be configured to use private IPs and accessed via a secure, private network or VPN. To remediate, ensure that the 'Public IP' setting is disabled in the GCP console under the SQL instance settings or configure a private IP on the instance. Regularly audit and monitor network configurations to ensure no public endpoints are exposed.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/sql/docs/mysql/private-ip
- rule_id: gcp.cloudsql.instance.public_ip
  service: cloudsql
  resource: instance
  requirement: Public Ip
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Cloud SQL Instances Do Not Use Public IPs
  rationale: Public IP exposure of Cloud SQL instances can lead to unauthorized access and data breaches, increasing the attack surface. Limiting access to private IPs helps in compliance with regulations such as PCI-DSS and HIPAA that mandate secure data environments and reduces the risk of exposure to external threats.
  description: This rule checks for Cloud SQL instances configured with a public IP address, which can be accessed from the internet. To enhance security, disable public IP and configure a private IP within a VPC. Verification can be done through the GCP Console by navigating to the SQL instances and checking the IP configuration. Remediation involves switching the instance to use only private IPs and ensuring access through secure, internal networks.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/architecture/using-private-google-access-and-cloud-nat
  - https://cloud.google.com/sql/docs/mysql/sql-proxy
- rule_id: gcp.cloudsql.instance.public_ip_configured
  service: cloudsql
  resource: instance
  requirement: Public Ip Configured
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Cloud SQL Instances Do Not Use Public IP Addresses
  rationale: Exposing Cloud SQL instances to the internet via public IPs increases the risk of unauthorized access and data breaches. This can lead to sensitive information exposure, non-compliance with regulations like GDPR and HIPAA, and potential financial and reputational damage to the organization. Limiting access to private IPs within VPC networks enhances security by reducing the attack surface.
  description: This rule checks if a Cloud SQL instance is configured with a public IP address. Public IPs allow direct internet access, which can be exploited by attackers. To verify, check the instance's IP configuration in the GCP Console or via gcloud CLI. Remediation involves disabling the public IP and ensuring the instance is accessed through a private IP within a VPC network, possibly utilizing Cloud SQL Proxy for secure connections.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-private-ip
  - https://cloud.google.com/sql/docs/mysql/private-ip
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/sql-proxy
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.cloudsql.instance.public_ip_whitelist_configured
  service: cloudsql
  resource: instance
  requirement: Public Ip Whitelist Configured
  scope: cloudsql.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce Public IP Whitelisting for Cloud SQL Instances
  rationale: Unrestricted public access to Cloud SQL instances can expose sensitive data to unauthorized users, leading to potential data breaches and compliance violations. Restricting access through IP whitelisting helps mitigate risks from unauthorized access, ensuring only trusted networks can connect, aligning with regulatory requirements such as PCI-DSS and GDPR.
  description: This rule verifies that public IP addresses accessing Cloud SQL instances are restricted to a defined set of authorized IP ranges. To ensure security, configure the 'Authorized networks' settings in the Cloud SQL instance to include only trusted IP addresses. Regularly review and update the IP whitelist to reflect changes in trusted network infrastructure. Remediation involves navigating to the Google Cloud Console, accessing the Cloud SQL instance settings, and updating the public IP whitelist under 'Connections'.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/gdpr/
  - https://cloud.google.com/security/compliance/pci-dss/
  - https://cloud.google.com/sql/docs/postgres/authorized-networks
  - https://cloud.google.com/sql/docs/sqlserver/configure-ip
- rule_id: gcp.cloudsql.instance.remote_access_flag_configured
  service: cloudsql
  resource: instance
  requirement: Remote Access Flag Configured
  scope: cloudsql.instance.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Cloud SQL Remote Access is Securely Configured
  rationale: Improper configuration of remote access settings in Cloud SQL can expose databases to unauthorized access, leading to data breaches or loss. Configuring remote access controls helps in mitigating risks associated with unauthorized data exposure and ensures compliance with data protection regulations such as PCI-DSS and HIPAA.
  description: This rule checks if Cloud SQL instances have the 'allow only SSL connections' flag enabled, ensuring that all remote connections use SSL encryption. To verify, navigate to the Cloud SQL instance settings in the GCP Console and confirm that the 'Require SSL' option is activated under 'Connections'. To remediate, enable SSL enforcement and configure SSL certificates for client connections. This enhances data security by encrypting data in transit.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.require_ssl_enforcement
  service: cloudsql
  resource: instance
  requirement: Require SSL Enforcement
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce SSL for Cloud SQL Instances
  rationale: Enforcing SSL for Cloud SQL instances prevents unauthorized interception and tampering of data in transit, which is crucial for protecting sensitive information and maintaining privacy. This measure mitigates the risk of man-in-the-middle attacks and is often required for compliance with data protection regulations such as PCI-DSS and HIPAA.
  description: This rule checks if SSL is enforced for all Cloud SQL instances, ensuring that database connections are encrypted. To verify, inspect the 'require_ssl' setting in the Cloud SQL instance configuration. If not enabled, configure the instance to require SSL by setting the 'require_ssl' option to true in the Google Cloud Console or using gcloud command-line tools. This action ensures that all incoming connections use SSL for encryption.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://cloud.google.com/security/encryption-in-transit/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-52r1.pdf
- rule_id: gcp.cloudsql.instance.skip_show_database_flag_configured
  service: cloudsql
  resource: instance
  requirement: Skip Show Database Flag Configured
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure 'skip_show_database' Flag is Properly Configured in Cloud SQL
  rationale: The 'skip_show_database' flag limits the information an unauthorized user can retrieve from the server, reducing the risk of data exposure and potential information leakage. This is crucial for protecting sensitive metadata about databases, which could otherwise be exploited in targeted attacks. Ensuring this flag is configured aligns with data protection policies and helps meet compliance requirements related to data privacy standards.
  description: This rule checks if the 'skip_show_database' flag is set on Cloud SQL instances. When enabled, this flag prevents users from seeing all available databases on the instance, enhancing security by limiting information exposure. To verify, navigate to the Cloud SQL instance settings and confirm that the flag is set. Remediation involves enabling this setting through the Google Cloud Console, gcloud command-line tool, or API. It is critical for maintaining data confidentiality and minimizing the risk of unauthorized access to database metadata.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/configure-ip
- rule_id: gcp.cloudsql.instance.ssl_connections
  service: cloudsql
  resource: instance
  requirement: SSL Connections
  scope: cloudsql.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure SSL Connections for Cloud SQL Instances
  rationale: Enforcing SSL connections for Cloud SQL instances mitigates the risk of data interception and unauthorized access to sensitive data during transit. This is crucial for maintaining data confidentiality and integrity, especially for organizations handling regulated data under compliance frameworks like PCI-DSS and HIPAA. Failure to encrypt data in transit can lead to severe business impacts, including data breaches and non-compliance penalties.
  description: This rule checks whether SSL connections are enforced for Cloud SQL instances. To verify, ensure that the 'Require SSL' option is enabled in the Cloud SQL instance settings. This can be done via the Google Cloud Console, gcloud command-line tool, or REST API. Remediation involves enabling SSL enforcement and distributing the necessary SSL certificates to all database clients to secure the data transmission between clients and the Cloud SQL instance.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/sql/docs/security-overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.cloudsql.instance.storage_encrypted
  service: cloudsql
  resource: instance
  requirement: Storage Encrypted
  scope: cloudsql.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL Instances Have Storage Encryption Enabled
  rationale: Encrypting storage of Cloud SQL instances is crucial for protecting sensitive data from unauthorized access, particularly in the event of a data breach or physical media loss. This is vital for maintaining customer trust, adhering to data privacy regulations such as GDPR, and fulfilling compliance requirements of standards like PCI-DSS and HIPAA, which mandate encryption of data at rest.
  description: This rule verifies that Cloud SQL instances have storage encryption enabled, ensuring data is encrypted at rest using Google-managed keys. To check, verify the 'diskEncryptionConfiguration' parameter in the Cloud SQL instance settings. If not configured, enable encryption by navigating to the Cloud SQL instance settings in the GCP Console and selecting 'Enable Encryption' under storage options. This is an essential step in securing data and preventing unauthorized access.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/sql/docs/mysql/instance-settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.cloudsql.instance.trace_flag_3625_enabled
  service: cloudsql
  resource: instance
  requirement: Trace Flag 3625 Enabled
  scope: cloudsql.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Trace Flag 3625 for Cloud SQL Instances
  rationale: Enabling Trace Flag 3625 on SQL Server instances in Cloud SQL helps limit the amount of sensitive information logged in error messages, reducing the risk of data leaks. This is particularly important for protecting personally identifiable information (PII) and complying with data protection regulations such as GDPR and CCPA.
  description: This rule checks whether Trace Flag 3625 is enabled on your Cloud SQL for SQL Server instances. Trace Flag 3625 suppresses detailed error messages that could reveal sensitive information, ensuring error logs do not inadvertently disclose PII. To verify, review the Cloud SQL instance settings and ensure the flag is active. Remediation involves enabling the flag via a Cloud SQL instance configuration change, which can be done through the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/sql/docs/sqlserver/flags
  - https://cloud.google.com/sql/docs/sqlserver/manage-flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
- rule_id: gcp.cloudsql.instance.user_connections_flag_non_limiting
  service: cloudsql
  resource: instance
  requirement: User Connections Flag Non Limiting
  scope: cloudsql.instance.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit User Connections in Cloud SQL Instances
  rationale: Unrestricted user connections to Cloud SQL instances can lead to resource exhaustion, potentially causing service disruptions and increased vulnerability to denial-of-service attacks. Limiting connections helps maintain availability, ensures fair resource usage, and complies with best practices for access management. It also aids in meeting compliance requirements for data protection and system integrity.
  description: This rule checks whether the 'max_connections' flag is set to a limiting value for Cloud SQL instances. A non-limiting configuration can allow excessive simultaneous connections, impacting performance and security. To verify, ensure that the 'max_connections' flag is configured with a value appropriate for your workload and usage patterns. Remediation involves accessing the Cloud SQL instance settings and setting a reasonable limit based on expected traffic and resource capacity.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://cloud.google.com/sql/docs/postgres/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.cloudsql.instance.user_options_flag_not_configured
  service: cloudsql
  resource: instance
  requirement: User Options Flag Not Configured
  scope: cloudsql.instance.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Cloud SQL User Options Flag is Properly Configured
  rationale: Improper configuration of the user options flag in Cloud SQL instances can lead to unauthorized access and potential data breaches. This setting controls user-defined parameters that can affect the security posture of the database. Ensuring it is configured appropriately helps mitigate risks associated with unauthorized database modifications and enhance regulatory compliance with standards such as PCI-DSS and HIPAA.
  description: This rule verifies that the user options flag is correctly configured on Cloud SQL instances to prevent security vulnerabilities. It checks whether any user-defined options that could compromise security are enabled. To verify, review the SQL instance settings in the GCP Console or use the gcloud command-line tool. Remediation involves setting a secure configuration by adjusting the user options flag via the Cloud SQL settings interface or by using gcloud CLI commands.
  references:
  - https://cloud.google.com/sql/docs/mysql/flags
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/sql/docs/mysql/security-overview
  - https://cloud.google.com/sql/docs/mysql/configure-instance-settings
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.cloudsql.ssl_cert.db_sg_egress_restricted
  service: cloudsql
  resource: ssl_cert
  requirement: Db Sg Egress Restricted
  scope: cloudsql.ssl_cert.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Egress for Cloud SQL SSL Certificates
  rationale: Restricting egress traffic for SSL certificates in Cloud SQL is crucial to prevent unauthorized data exfiltration and to ensure compliance with data protection regulations. Unrestricted egress can lead to potential exposure of sensitive data and can increase the risk of data breaches, affecting business continuity and trust.
  description: This rule checks if egress traffic from Cloud SQL SSL certificates is restricted to specific IPs or networks. Proper configuration involves setting up firewall rules that limit outbound connections to only trusted destinations. Verify that network security groups associated with Cloud SQL instances have specific egress rules. Remediation includes configuring egress rules in the VPC firewall to permit only necessary traffic, ensuring that all data transfers are encrypted and securely transmitted.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
  - https://cloud.google.com/sql/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.cloudsql.ssl_cert.db_sg_no_0_0_0_0_ingress_on_db_ports
  service: cloudsql
  resource: ssl_cert
  requirement: Db Sg No 0 0 0 0 Ingress On Db Ports
  scope: cloudsql.ssl_cert.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cloud SQL Ingress from 0.0.0.0/0 on DB Ports
  rationale: Allowing ingress to Cloud SQL from 0.0.0.0/0 poses a significant security risk as it exposes databases to potential attacks from any IP address on the internet. This can lead to unauthorized access, data breaches, or exploitation of database vulnerabilities. Compliance frameworks such as PCI-DSS and HIPAA require restricted access to sensitive data, ensuring only trusted sources can connect.
  description: This rule checks for Cloud SQL instances allowing ingress traffic from 0.0.0.0/0 on database ports. To secure your database, configure the instance's authorized networks to allow connections only from trusted IP ranges or VPNs. Review and modify the VPC firewall rules associated with your Cloud SQL instances to ensure they do not permit unrestricted access. Regularly audit and update these settings to comply with best practices and regulatory requirements.
  references:
  - https://cloud.google.com/sql/docs/mysql/configure-ip
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.cloudsql.ssl_cert.db_sg_only_required_ports_open
  service: cloudsql
  resource: ssl_cert
  requirement: Db Sg Only Required Ports Open
  scope: cloudsql.ssl_cert.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cloud SQL SSL Certs Have Only Required Ports Open
  rationale: Limiting open ports for Cloud SQL SSL certificates reduces the attack surface, mitigating risks of unauthorized access and potential data breaches. This is crucial for protecting sensitive data hosted in databases, ensuring compliance with data privacy regulations such as GDPR and HIPAA, and maintaining operational integrity by preventing service disruptions from malicious activities.
  description: This rule checks that only necessary ports are open for Cloud SQL instances utilizing SSL certificates, in accordance with security best practices. Administrators should verify that the database security groups (SG) only allow traffic on ports required for application functionality, typically port 3306 for MySQL or port 5432 for PostgreSQL. Remediation involves reviewing and updating firewall rules to restrict open ports to only those essential for legitimate database operations, minimizing exposure to potential threats.
  references:
  - https://cloud.google.com/sql/docs/mysql/authorize-networks
  - https://cloud.google.com/sql/docs/postgres/configure-ssl-instance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.user.db_approved_list_of_superusers_only
  service: cloudsql
  resource: user
  requirement: Db Approved List Of Superusers Only
  scope: cloudsql.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Cloud SQL Superusers to Approved List Only
  rationale: Limiting superuser access to Cloud SQL databases to an approved list mitigates the risk of unauthorized data access and potential breaches. Superusers have elevated privileges, making them prime targets for exploitation in cyber attacks. Ensuring only vetted individuals have such access helps maintain data integrity and supports compliance with regulations like PCI-DSS and HIPAA.
  description: This rule checks that only users from a predefined, approved list have superuser privileges in Cloud SQL instances. It verifies that no unauthorized users can escalate privileges to a superuser level, which could lead to unauthorized access and data manipulation. To remediate, regularly audit user roles and update the approved list in your IAM policy settings, ensuring only necessary personnel have superuser access. Use the Google Cloud Console or gcloud CLI to manage and verify these configurations.
  references:
  - https://cloud.google.com/sql/docs/mysql/users
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/sql/docs/mysql/roles-and-permissions
- rule_id: gcp.cloudsql.user.db_no_unused_or_default_superusers
  service: cloudsql
  resource: user
  requirement: Db No Unused Or Default Superusers
  scope: cloudsql.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Remove Unused or Default Superusers in Cloud SQL
  rationale: Leaving unused or default superuser accounts active in Cloud SQL can increase the risk of unauthorized access, data breaches, and privilege escalation. Superusers have elevated privileges, and if left unsecured, they can be exploited by attackers to gain complete control over the database. Regulatory frameworks such as PCI-DSS and HIPAA emphasize the principle of least privilege, making it essential to manage superuser accounts diligently.
  description: This rule checks for the presence of unused or default superuser accounts in Cloud SQL databases. Default accounts like 'root' or other vendor-provided superusers should be renamed, disabled, or deleted if not required. Additionally, regularly review user activity logs to identify and remove unused accounts. Remediation involves connecting to the database and executing SQL commands to manage or delete these accounts, ensuring that only necessary and active superuser accounts are retained.
  references:
  - https://cloud.google.com/sql/docs/mysql/users
  - https://cloud.google.com/sql/docs/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.cloudsql.user.db_password_auth_hardened_or_iam_auth_preferred_wh_supported
  service: cloudsql
  resource: user
  requirement: Db Password Auth Hardened Or IAM Auth Preferred Wh Supported
  scope: cloudsql.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Cloud SQL Users Use Hardened Passwords or IAM Authentication
  rationale: Weak database password authentication increases the risk of unauthorized access to sensitive data, potentially leading to data breaches and non-compliance with regulations like PCI-DSS and HIPAA. Enforcing IAM authentication or hardened password policies reduces these risks by implementing stronger access controls and centralized identity management.
  description: This rule checks Cloud SQL user accounts to ensure they are using either IAM authentication, where supported, or database passwords that meet hardened security requirements. Verify that IAM authentication is enabled for databases that support it, or enforce strong password policies, such as a minimum length of 12 characters with complexity requirements, for database password authentication. To remediate, configure IAM roles for database access or update the password policy settings in the Google Cloud Console for Cloud SQL.
  references:
  - https://cloud.google.com/sql/docs/mysql/authentication
  - https://cloud.google.com/sql/docs/postgres/authentication
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
- rule_id: gcp.compute.address.attached
  service: compute
  resource: address
  requirement: Attached
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Static IP Addresses Are Attached to Resources
  rationale: Unattached static IP addresses may lead to unexpected billing and may be unintentionally left unused, posing a security risk if mistakenly allocated to unauthorized resources. Ensuring IP addresses are attached helps maintain an accurate inventory of network resources and supports compliance with network configuration management best practices.
  description: This rule checks for any static IP addresses in Google Cloud Platform's Compute Engine that are not attached to any resources, such as virtual machines or load balancers. Unattached IP addresses should be reviewed and either attached to the necessary resources or released if no longer needed. This can be verified through the Google Cloud Console under VPC Network > External IP addresses or using the gcloud command-line tool. Remediation involves attaching the IP to an appropriate resource or releasing it to avoid unnecessary charges and potential misuse.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address
  - https://cloud.google.com/compute/docs/instances/connecting-vm-instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.compute.address.edge_ip_set_cidrs_valid_and_minimized
  service: compute
  resource: address
  requirement: Edge Ip Set Cidrs Valid And Minimized
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge IP CIDRs Are Valid and Minimized
  rationale: Proper configuration of IP address CIDR ranges ensures that resources are not exposed to unnecessary network traffic, reducing the attack surface and preventing unauthorized access. Minimizing CIDR ranges helps in adhering to the principle of least privilege and can mitigate risks of data breaches and unauthorized access, which are critical for regulatory compliance with frameworks like PCI-DSS and SOC 2.
  description: This rule checks that the CIDR ranges assigned to edge IP addresses in GCP are valid, non-overlapping, and as narrow as possible. It involves verifying current CIDR configurations against access needs and adjusting them to allow only the required IP ranges. Remediation includes reviewing the IP address configurations in the GCP Console under 'VPC network' settings and using 'gcloud' commands to update CIDR ranges, ensuring they are appropriately limited to necessary IPs only.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.compute.address.edge_ip_set_cross_account_sharing_restricted
  service: compute
  resource: address
  requirement: Edge Ip Set Cross Account Sharing Restricted
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict Cross-Account Sharing of Edge IP Sets
  rationale: Restricting cross-account sharing of edge IP sets reduces the risk of unauthorized access and potential data breaches. It ensures that sensitive network configurations are not inadvertently exposed to other accounts, which could be exploited by malicious actors to compromise the network. This practice aligns with compliance requirements for data protection and access control.
  description: This rule checks if edge IP sets are shared across different GCP accounts. To verify, review the IAM policies and ensure that edge IP sets are not granted access to external accounts. Remediation involves adjusting IAM policies to limit access to trusted accounts only. This helps in maintaining network integrity and adheres to best practices for least privilege access.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#ephemeral
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloudsecurityalliance.org/research/guidance/
- rule_id: gcp.compute.address.edge_ip_set_not_empty
  service: compute
  resource: address
  requirement: Edge Ip Set Not Empty
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge IP Set on Compute Addresses is Configured
  rationale: Having an empty Edge IP set on a compute address can lead to potential security risks including unauthorized access or exposure of sensitive data. Proper configuration ensures that only designated IPs can access resources, aligning with security best practices and compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that the Edge IP set for compute addresses in Google Cloud Platform is not empty, ensuring that access is limited to specified IP ranges. To verify, inspect the configuration of each address in the GCP Console or via gcloud CLI to confirm that Edge IPs are properly set. Remediation involves updating the Edge IP set configuration to include all necessary IP ranges, ensuring compliance with your organization's access policies.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/docs/security-compliance
- rule_id: gcp.compute.address.edge_ip_set_used_by_at_least_one_rule
  service: compute
  resource: address
  requirement: Edge Ip Set Used By At Least One Rule
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge IP Sets are Actively Used by Firewall Rules
  rationale: Unused edge IP sets may indicate misconfiguration or unused resources, leading to potential security risks such as unintended exposure of network interfaces or inefficient resource utilization. Ensuring that edge IP sets are used by at least one firewall rule reduces attack surface and aligns with best practice for resource management and security compliance.
  description: This rule checks if edge IP sets in Google Cloud Platform's Compute service are actively used by at least one firewall rule. Unused IP sets should be reviewed and either associated with a relevant firewall rule or decommissioned. This ensures that all network configurations are intentional and necessary, reducing risk of exposure or unauthorized access. To verify, review the list of edge IP sets and corresponding firewall rules in the GCP Console or via gcloud CLI, and update configurations as needed.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
- rule_id: gcp.compute.address.ip_attached_configured
  service: compute
  resource: address
  requirement: Ip Attached Configured
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Attached IP Addresses Are Properly Configured
  rationale: Proper configuration of attached IP addresses is critical to maintaining network security and preventing unauthorized access or data leakage. Misconfigured IP addresses can lead to exposure of internal services to the internet, increasing the risk of attacks and non-compliance with standards such as PCI-DSS and ISO 27001.
  description: This rule checks that all attached IP addresses in Google Cloud Platform are correctly configured according to best practices. It ensures that IP addresses are not unnecessarily exposed to the public internet and are restricted to specific services or resources that require them. Verification involves reviewing the network interface settings on instances to ensure IP addresses are appropriately allocated and configured. Remediation may include re-assigning IP addresses to private networks or adjusting firewall rules to limit exposure.
  references:
  - https://cloud.google.com/vpc/docs/using-routes
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/ip-addresses
- rule_id: gcp.compute.address.ip_unassigned
  service: compute
  resource: address
  requirement: Ip Unassigned
  scope: compute.address.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Unassigned IP Addresses in Google Compute Engine
  rationale: Unassigned IP addresses in Google Compute Engine can lead to inefficient resource utilization and increased costs. They may also pose a security risk if accidentally re-assigned to unauthorized or malicious workloads, potentially enabling network exploitation or unauthorized access to resources. Proper management of IP addresses is crucial for compliance with resource governance and budgetary controls.
  description: This rule checks for IP addresses in Google Compute Engine that have been allocated but are not currently assigned to any resource. Unassigned IP addresses should be regularly reviewed and released if not in use to prevent unnecessary charges. To remediate, identify unassigned IP addresses via the GCP Console or CLI and release them using the 'gcloud compute addresses delete' command or the Console UI to avoid incurring charges.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/compute/docs/instances/networking
  - https://cloud.google.com/docs/overview/security
- rule_id: gcp.compute.address.network_endpoint_policy_least_privilege
  service: compute
  resource: address
  requirement: Network Endpoint Policy Least Privilege
  scope: compute.address.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Network Endpoint Policies
  rationale: Applying the principle of least privilege to network endpoint policies minimizes access to only what is necessary, reducing potential attack vectors and preventing unauthorized access. This is crucial for protecting sensitive data and maintaining compliance with standards like NIST and PCI-DSS, which require strict access control measures.
  description: This rule checks if network endpoint policies associated with compute addresses are configured with least privilege in mind. Verify that IAM roles assigned to these policies are limited to the minimum permissions necessary. Remediation involves auditing the current roles and permissions and adjusting them to ensure they only allow essential actions, thus minimizing potential security risks.
  references:
  - https://cloud.google.com/compute/docs/reference/rest/v1/networkEndpointGroups
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.compute.address.network_endpoint_private_dns_enabled_where_supported
  service: compute
  resource: address
  requirement: Network Endpoint Private Dns Enabled Where Supported
  scope: compute.address.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enable Private DNS for Network Endpoints in GCP Compute
  rationale: Enabling Private DNS for network endpoints enhances security by ensuring that DNS queries are resolved within the private network, reducing exposure to DNS spoofing and man-in-the-middle attacks. This configuration is critical for maintaining data confidentiality and integrity, especially for organizations handling sensitive information. Additionally, it aligns with compliance requirements for data protection under standards like PCI-DSS and HIPAA.
  description: This rule checks if Private DNS is enabled for network endpoints in GCP Compute where supported. Verification involves checking the DNS configuration of network endpoints to ensure they use private DNS zones. To remediate, configure network endpoints to use private DNS by specifying a private DNS policy in the endpoint's settings, ensuring secure and private resolution of DNS queries.
  references:
  - https://cloud.google.com/dns/docs/private-zones
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-security-and-privacy-controls
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.address.public_address_shodan_configured
  service: compute
  resource: address
  requirement: Public Address Shodan Configured
  scope: compute.address.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Public IPs Are Not Indexed by Shodan
  rationale: Exposing public IP addresses to Shodan poses a significant security risk as it enables potential attackers to easily identify and target these resources. This increases the likelihood of unauthorized access, data breaches, and exploitation. Compliance with security standards often requires minimizing exposure of sensitive resources to the internet to protect the organization's assets and reputation.
  description: This rule checks if public IP addresses associated with Google Cloud Compute resources are indexed by Shodan, a search engine for internet-connected devices. Public IPs should be configured to minimize exposure and prevent indexing by Shodan. To verify, ensure that firewall rules restrict access to trusted IP ranges and that unnecessary services are disabled. Remediation involves auditing and adjusting firewall rules, and employing Cloud Armor to filter unwanted traffic.
  references:
  - https://cloud.google.com/vpc/docs/using-firewall-rules
  - https://cloud.google.com/compute/docs/ip-addresses
  - 'CIS GCP Benchmark: 4.1 Ensure that there are no public IPs that are not needed'
  - 'NIST SP 800-53: AC-3 Access Enforcement'
  - 'PCI-DSS Requirement 1.2: Build and Maintain a Secure Network and Systems'
  - https://www.shodan.io/
- rule_id: gcp.compute.autoscaler.autoscaler_configured
  service: compute
  resource: autoscaler
  requirement: Autoscaler Configured
  scope: compute.autoscaler.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Autoscaler is Configured for Compute Instances
  rationale: Properly configured autoscalers optimize resource allocation and cost efficiency while maintaining application performance. Failure to configure autoscalers may lead to resource exhaustion or unnecessary expenses, impacting business operations and violating cost management policies. Inadequate scaling can also result in service interruptions, affecting user experience and compliance with service level agreements.
  description: This rule checks if the autoscaler is properly configured for compute instances by verifying the presence of scaling policies and thresholds. Ensure that scaling policies align with workload demands and are set to automatically adjust resources based on metrics such as CPU utilization or request load. To verify, review the autoscaler settings in the Google Cloud Console under Compute Engine > Autoscaler. Ensure that policies are active and thresholds are set appropriately. Remediation involves configuring or adjusting autoscaling policies to match application performance and cost objectives.
  references:
  - https://cloud.google.com/compute/docs/autoscaler
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/instance-groups/autoscaling-considerations
  - https://cloud.google.com/blog/topics/developers-practitioners/understanding-autoscaling-in-google-cloud
  - https://cloud.google.com/compute/docs/instance-groups/using-autoscaler
- rule_id: gcp.compute.backend_service.balancer_ssl_policy_check_secure_ciphers
  service: compute
  resource: backend_service
  requirement: Balancer SSL Policy Check Secure Ciphers
  scope: compute.backend_service.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Secure Ciphers in Backend Service SSL Policies
  rationale: Using secure ciphers in SSL policies is crucial to protect communication between clients and backend services from interception and attacks like man-in-the-middle. Insecure ciphers can lead to vulnerabilities allowing unauthorized access to sensitive information, impacting confidentiality and compliance with standards such as PCI-DSS and HIPAA.
  description: This rule checks if the SSL policies attached to backend services enforce the use of secure ciphers. A secure SSL policy should exclude deprecated ciphers like MD5, RC4, or anonymous ciphers. To verify, review the SSL policy configuration in the GCP Console under Compute Engine, ensuring compliance with recommended secure cipher suites. Remediation involves updating the SSL policy to include only approved ciphers, which can be done through the GCP Console or using gcloud commands.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/load-balancing/ssl-policies
- rule_id: gcp.compute.backend_service.balancing_deletion_protection_configured
  service: compute
  resource: backend_service
  requirement: Balancing Deletion Protection Configured
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Backend Service Deletion Protection is Configured
  rationale: Deletion protection on backend services prevents accidental or unauthorized deletion, which could lead to service downtime and disrupt business operations. By configuring deletion protection, organizations mitigate the risk of data loss and service interruption, which is crucial for maintaining customer trust and meeting regulatory requirements.
  description: This rule verifies that deletion protection is enabled for Google Cloud Platform compute backend services. Deletion protection ensures that these services cannot be deleted without explicit consent, reducing the risk of accidental disruptions. To check this setting, navigate to the Google Cloud Console, select the backend service, and verify that deletion protection is activated. If not enabled, update the service settings to enable deletion protection.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.backend_service.balancing_desync_mitigation_mode_configured
  service: compute
  resource: backend_service
  requirement: Balancing Desync Mitigation Mode Configured
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Desync Mitigation Mode is Configured for Backend Services
  rationale: Configuring the Desync Mitigation Mode for backend services is critical to prevent desynchronization attacks that can lead to data corruption or unauthorized access. This configuration helps in maintaining consistent data flow and integrity across distributed systems, reducing the risk of downtime and potential breaches. Ensuring this setting aligns with compliance requirements such as PCI-DSS and ISO 27001, which mandate secure configurations for network communications.
  description: This rule checks if the Desync Mitigation Mode is enabled for Google Cloud Platform backend services. Desync Mitigation Mode helps protect against HTTP desync attacks by enforcing strict checks on HTTP/1.x request and response headers. To verify this setting, ensure that the 'desyncMitigationMode' field in the backend service configuration is set to 'ENABLED' or 'STRICT'. If this is not configured, update the backend service settings using the Google Cloud Console or gcloud command-line tool to enable this feature.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/load-balancing/docs/https
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.compute.backend_service.edge_cdn_access_logging_enabled
  service: compute
  resource: backend_service
  requirement: Edge Cdn Access Logging Enabled
  scope: compute.backend_service.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Edge CDN Access Logging for Backend Services
  rationale: Enabling Edge CDN access logging enhances visibility into request patterns and access behaviors, helping to identify potential security threats and performance issues. It also supports compliance with regulations that require logging of access to critical resources, thereby reducing the risk of undetected malicious activity and providing a clear audit trail for forensic investigations.
  description: This rule checks if Edge CDN access logging is enabled for Google Cloud backend services. To verify, ensure that the 'logging' configuration of each backend service includes the 'logConfig' field with 'enable' set to true. Remediation involves updating the backend service configuration to enable logging, which can be done via the GCP Console or command-line interface by setting the appropriate logging parameters.
  references:
  - https://cloud.google.com/load-balancing/docs/https/edge-logging
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-analyzing-http-s-logs
  - https://cloud.google.com/logging/docs/audit
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.backend_service.edge_cdn_cache_key_excludes_sensitive_headers
  service: compute
  resource: backend_service
  requirement: Edge Cdn Cache Key Excludes Sensitive Headers
  scope: compute.backend_service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Exclude Sensitive Headers from Edge CDN Cache Keys
  rationale: Excluding sensitive headers from Edge CDN cache keys is crucial to prevent exposure of sensitive information such as authentication tokens, cookies, or personally identifiable information (PII). This helps mitigate the risk of unauthorized access or data leakage, which can lead to security breaches, loss of customer trust, and non-compliance with data protection regulations like GDPR and HIPAA.
  description: This rule checks if sensitive headers are excluded from Edge CDN cache keys in GCP Backend Services. It ensures that headers containing sensitive data are not used in cache key creation, thereby reducing the risk of sensitive data exposure. To verify, inspect the backend service settings and ensure sensitive headers are not part of the CDN cache key configuration. Remediation involves updating the backend service configuration to explicitly exclude sensitive headers from cache key settings.
  references:
  - https://cloud.google.com/cdn/docs/caching#cache-key
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security
- rule_id: gcp.compute.backend_service.edge_cdn_cache_key_minimal_query_and_cookie_variants
  service: compute
  resource: backend_service
  requirement: Edge Cdn Cache Key Minimal Query And Cookie Variants
  scope: compute.backend_service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Minimal Query and Cookie Variants for CDN Cache Keys
  rationale: This rule helps prevent cache poisoning attacks by ensuring that Google Cloud's CDN cache keys are minimally affected by query strings and cookies. By minimizing these variants, the backend service reduces risk of unauthorized data exposure and ensures consistent content delivery, which is crucial for maintaining data integrity and meeting compliance requirements such as PCI-DSS and GDPR.
  description: This check ensures that the CDN cache key configuration for Google Cloud Backend Services is set to use minimal query and cookie variants. This involves verifying the 'includeHttpHeaders', 'includeNamedCookies', and 'queryStringWhitelist' settings in the backend service configuration. To remediate, adjust these settings to only include necessary cookies and query parameters, or use a whitelist approach to limit exposure. This minimizes cache key variability, enhancing security and performance.
  references:
  - https://cloud.google.com/cdn/docs/caching
  - https://cloud.google.com/architecture/best-practices-for-using-cdn#cache_keys
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Control 4.9
  - PCI-DSS Requirement 6.5 - Develop security guidelines and procedures
  - 'NIST SP 800-53 Rev. 5: SI-10 - Information Input Validation'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.backend_service.edge_cdn_hsts_enabled
  service: compute
  resource: backend_service
  requirement: Edge Cdn Hsts Enabled
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable HSTS for Edge CDN on Backend Services
  rationale: Enabling HTTP Strict Transport Security (HSTS) for Edge CDN on backend services mitigates man-in-the-middle attacks by ensuring browsers interact with the server using a secure HTTPS connection. This configuration is crucial for protecting sensitive data, maintaining user trust, and achieving compliance with data protection regulations such as GDPR and PCI-DSS.
  description: This rule checks if HSTS is enabled for the Edge CDN on GCP Compute backend services. HSTS ensures that all communications between clients and the service are conducted over secure HTTPS connections, preventing downgrade attacks. To verify this setting, check the 'cdnPolicy' configuration of the backend service in the Google Cloud Console or use the gcloud command-line tool. Remediation involves setting 'cdnPolicy.edgeSecurityPolicy' to enforce HSTS.
  references:
  - https://cloud.google.com/cdn/docs/edge-security-policy
  - https://cloud.google.com/security-compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://datatracker.ietf.org/doc/html/rfc6797
  - https://cloud.google.com/architecture/best-practices-for-securing-cloud-cdn
- rule_id: gcp.compute.backend_service.edge_cdn_https_only_viewer_protocol
  service: compute
  resource: backend_service
  requirement: Edge Cdn HTTPS Only Viewer Protocol
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enforce HTTPS for Edge CDN Viewer Protocol on Backend Services
  rationale: Enforcing HTTPS for edge CDN viewer protocols minimizes the risk of data interception and tampering by ensuring that data in transit is encrypted. This is crucial for maintaining user trust and meeting compliance requirements such as PCI-DSS, which mandate secure transmission of sensitive information. Failing to enforce HTTPS could result in data breaches, legal liabilities, and damage to the organization's reputation.
  description: This rule checks whether the 'Edge CDN' uses only HTTPS as the viewer protocol for backend services. To verify, ensure that the 'protocol' field for the backend service is set to 'HTTPS'. To remediate, update the backend service configuration to enforce HTTPS by setting the 'protocol' option appropriately. This ensures that all data between clients and the CDN is encrypted, protecting against potential man-in-the-middle attacks.
  references:
  - https://cloud.google.com/cdn/docs/using-cdn#protocol
  - https://cloud.google.com/security/compliance/pci-dss
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.backend_service.edge_cdn_origin_access_restricted
  service: compute
  resource: backend_service
  requirement: Edge Cdn Origin Access Restricted
  scope: compute.backend_service.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Edge CDN Origin Access for Backend Services
  rationale: Restricting access to the origins of Edge CDN prevents unauthorized entities from bypassing caching layers to access backend services directly. This reduces the risk of data breaches, service disruptions, and unnecessary load on backend systems, aligning with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks if the backend services in GCP Compute are configured to restrict origin access when using Edge CDN. Ensure that origin access is limited to trusted sources by configuring appropriate identity and access management (IAM) policies. Verify that backend services have settings that enforce origin access restrictions and update IAM roles and policies to limit permissions to necessary entities only.
  references:
  - https://cloud.google.com/cdn/docs/overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.backend_service.edge_cdn_signed_urls_or_headers_required_for_private_content
  service: compute
  resource: backend_service
  requirement: Edge Cdn Signed Urls Or Headers Required For Private Content
  scope: compute.backend_service.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Require Signed URLs/Headers for Private CDN Content
  rationale: Ensuring that edge CDN traffic is verified using signed URLs or headers helps protect private content from unauthorized access and reduces the risk of data breaches. This control is crucial for maintaining data confidentiality and integrity, especially for businesses subject to compliance requirements like PCI-DSS or HIPAA, where unauthorized data exposure can lead to significant financial and reputational damage.
  description: This rule checks if Google Cloud's Backend Services within the private networking scope require signed URLs or headers for edge CDN access to private content. To verify, ensure that the 'signedUrlKeyNames' or 'signedUrlCacheMaxAgeSec' fields are set when configuring backend services. Remediation involves updating the backend service configuration to enforce signed URL or header requirements, thereby ensuring that only authenticated requests can access sensitive content.
  references:
  - https://cloud.google.com/cdn/docs/private-content#signed-urls
  - https://cloud.google.com/architecture/best-practices-for-content-delivery-networking
  - 'PCI-DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - https://cloud.google.com/security/compliance
  - 'NIST SP 800-53 AC-3: Access Enforcement'
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.backend_service.edge_cdn_tls_min_1_2_enforced
  service: compute
  resource: backend_service
  requirement: Edge Cdn TLS Min 1 2 Enforced
  scope: compute.backend_service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS 1.2+ for Edge CDN in Backend Services
  rationale: Enforcing a minimum of TLS 1.2 for Edge CDN connections in backend services protects data in transit from interception and tampering by ensuring strong encryption protocols. This is crucial for maintaining data integrity and confidentiality, especially in compliance-driven industries such as finance and healthcare, where robust encryption standards are mandated by regulations like PCI-DSS and HIPAA.
  description: This rule checks whether backend services in Google Cloud's Compute Engine enforce a minimum of TLS 1.2 for connections through the Edge CDN. To verify compliance, ensure that the 'minTlsVersion' attribute in the backend service configuration is set to 'TLS_1_2' or higher in the Google Cloud Console or via the gcloud command-line tool. Remediate non-compliance by updating the backend service settings to enforce TLS 1.2 or higher, thereby enhancing data in transit protection.
  references:
  - https://cloud.google.com/cdn/docs/https
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.compute.backend_service.edge_cdn_valid_trusted_certificate_attached
  service: compute
  resource: backend_service
  requirement: Edge Cdn Valid Trusted Certificate Attached
  scope: compute.backend_service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Valid Trusted Certificates for Edge CDN in Backend Services
  rationale: Attaching a valid trusted certificate to your backend service's edge CDN is critical for ensuring encrypted data transmission over the network. Without a trusted certificate, data may be intercepted by attackers, leading to potential data breaches and non-compliance with regulations such as GDPR and PCI-DSS. This not only affects customer trust but can also result in significant financial and reputational damage.
  description: This rule checks whether a valid trusted certificate is attached to the edge CDN of a GCP backend service. To verify, ensure that your backend service has a certificate issued by a recognized certificate authority (CA) and that it is not expired or self-signed. Remediation involves acquiring a valid certificate from a CA and configuring your backend service to use this certificate for its edge CDN. This ensures that data in transit is encrypted and secure, thereby protecting sensitive information.
  references:
  - https://cloud.google.com/cdn/docs/using-ssl-certificates
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.nist.gov/cyberframework
- rule_id: gcp.compute.backend_service.edge_cdn_waf_web_acl_attached
  service: compute
  resource: backend_service
  requirement: Edge Cdn Waf Web ACL Attached
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge CDN WAF Web ACL is Attached to Backend Service
  rationale: Attaching a Web Application Firewall (WAF) ACL to an Edge CDN backend service helps protect web applications from common threats such as SQL injection, cross-site scripting, and other OWASP top 10 vulnerabilities. This enhances security by inspecting traffic and blocking malicious requests, reducing the risk of data breaches and ensuring compliance with security standards like PCI-DSS and NIST.
  description: This rule checks if a Web ACL is attached to a backend service in GCP's Edge CDN. Without this configuration, backend services are vulnerable to web-based attacks. To verify, inspect the backend service's configuration in the GCP Console or via the gcloud CLI to ensure a WAF Web ACL is associated. Remediation involves configuring or attaching an existing Web ACL to the backend service, which can be done through the Cloud Armor interface in the console.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/architecture/best-practices-for-ddos-resiliency
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.compute.backend_service.logging_enabled_gcp_storage_bucket_server_access_log_logging
  service: compute
  resource: backend_service
  requirement: Logging Enabled Gcp Storage Bucket Server Access Log Logging
  scope: compute.backend_service.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for Backend Service Access to GCP Storage Buckets
  rationale: Enabling server access logging for backend services that interact with GCP Storage Buckets is crucial for monitoring and auditing access patterns. This logging provides visibility into access operations, aiding in the detection of unauthorized access or anomalies. It supports compliance with regulations such as PCI-DSS and HIPAA, which require detailed access logs for sensitive data.
  description: This rule checks if logging is enabled for backend services that access GCP Storage Buckets. To verify, ensure that the 'logConfig' field is configured in the backend service with 'enable' set to true. Remediation involves updating the backend service configuration to include 'logConfig', thereby enabling logging. This ensures audit trails are maintained for all access requests, supporting both security and compliance efforts.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/storage/docs/access-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.compute.backend_service.network_tg_health_checks_tls_where_supported
  service: compute
  resource: backend_service
  requirement: Network Tg Health Checks TLS Where Supported
  scope: compute.backend_service.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS for Health Checks on Network Tg Where Supported
  rationale: Implementing TLS for network target group health checks enhances data integrity and confidentiality by encrypting the health check data in transit. This reduces the risk of man-in-the-middle attacks, ensuring that health checks cannot be intercepted or altered, thus maintaining the reliability and security of backend services. Compliance with regulations such as PCI-DSS and HIPAA often require encryption in transit, making TLS implementation crucial for meeting these standards.
  description: This rule checks if TLS is enforced on network target group health checks where supported, ensuring that sensitive data is encrypted in transit. Verify that the backend services utilizing network target groups have health checks configured with TLS. If not configured, update the health checks to support and enforce TLS by specifying the 'useSsl' attribute to 'true' in the health check configuration. This configuration can be reviewed and modified in the GCP Console or via the gcloud command-line tool.
  references:
  - https://cloud.google.com/load-balancing/docs/health-checks
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/encryption-in-transit/
  - https://cloud.google.com/load-balancing/docs/https/
  - https://cloud.google.com/storage/docs/encryption
- rule_id: gcp.compute.backend_service.network_tg_targets_in_private_subnets
  service: compute
  resource: backend_service
  requirement: Network Tg Targets In Private Subnets
  scope: compute.backend_service.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Backend Service Targets Are in Private Subnets
  rationale: Configuring backend services to target instances within private subnets reduces exposure to potential external threats and unauthorized access. This setup is crucial for protecting sensitive data and maintaining compliance with network security standards such as PCI-DSS and ISO 27001. Failure to properly isolate backend services can lead to data breaches and compromise service integrity.
  description: This rule checks that all network traffic targets for backend services are located within private subnets, preventing direct exposure to the public internet. Verify configurations by ensuring that backend instances are assigned private IPs in VPC subnets. Remediation involves modifying backend services to ensure only private subnets are used, adjusting the network configuration in the GCP console under VPC settings.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/overview/whitepapers
  - https://cloud.google.com/vpc/docs/subnets
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.compute.backend_service.service_https_logging_enabled
  service: compute
  resource: backend_service
  requirement: Service HTTPS Logging Enabled
  scope: compute.backend_service.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable HTTPS Logging for GCP Compute Backend Services
  rationale: Enabling HTTPS logging for backend services in GCP is critical for monitoring and auditing traffic patterns, detecting anomalies, and ensuring compliance with regulatory frameworks such as PCI-DSS and SOC2. Without HTTPS logging, organizations may miss crucial insights into service usage, potentially leading to unmonitored security breaches or non-compliance issues.
  description: This rule checks whether HTTPS logging is enabled for Google Cloud Platform Compute Backend Services. To verify, ensure that the 'logConfig.enable' field is set to true in the backend service's configuration. Remediation involves updating the backend service settings to enable logging, which can be done via the GCP Console, gcloud command-line tool, or Terraform scripts. This setup allows for detailed logging of HTTP(S) requests, facilitating enhanced security monitoring and auditing.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.backend_service.waf_acl_attached
  service: compute
  resource: backend_service
  requirement: Waf ACL Attached
  scope: compute.backend_service.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure WAF ACL is Attached to Backend Services
  rationale: Attaching a Web Application Firewall (WAF) Access Control List (ACL) to backend services helps protect against common web exploits that could compromise data integrity and availability. Without this protection, an application is more vulnerable to attacks such as SQL injection and cross-site scripting (XSS), which can lead to data breaches and service disruptions. Compliance with security standards like PCI-DSS and NIST requires robust application layer protections, which WAFs provide.
  description: This check verifies that a WAF ACL is attached to Google Cloud Platform's backend services. A WAF ACL helps filter and monitor HTTP requests, providing an additional layer of security by allowing or blocking requests based on predefined rules. To verify, access the GCP Console, navigate to the Compute Engine section, and ensure each backend service has an associated WAF ACL. Remediation involves creating or associating a WAF ACL with the backend service if it is not already attached.
  references:
  - https://cloud.google.com/load-balancing/docs/backend-service
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/security/compliance/pci-dss
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.compute.disk.cmk_cmek_configured
  service: compute
  resource: disk
  requirement: CMK Cmek Configured
  scope: compute.disk.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure CMK or CMEK Configured for Compute Disks
  rationale: Configuring Customer-Managed Keys (CMK) or Customer-Managed Encryption Keys (CMEK) for Compute Engine disks ensures that your data is encrypted with keys you control. This enhances data security by protecting sensitive information from unauthorized access and meets compliance requirements such as GDPR, HIPAA, and PCI-DSS which mandate strong encryption practices.
  description: This rule checks if Compute Engine disks are configured with CMK or CMEK. Disks not using these keys rely on Google-managed encryption keys, which may not meet certain regulatory or organizational security requirements. To verify, review your disk settings in the GCP Console under 'Encryption' to ensure CMK or CMEK is selected. Remediation involves creating a key in Cloud Key Management Service (KMS) and applying it to your disks.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.compute.disk.csek_status_configured
  service: compute
  resource: disk
  requirement: Csek Status Configured
  scope: compute.disk.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure CMEK is Configured for Compute Disks
  rationale: Configuring Customer-Supplied Encryption Keys (CSEK) for disks helps secure data at rest by allowing organizations to manage and control encryption keys. This mitigates risks associated with unauthorized data access and is essential for meeting compliance requirements like GDPR and HIPAA, where data protection is crucial.
  description: This rule checks if Customer-Supplied Encryption Keys (CSEK) are configured for Google Cloud Compute Engine disks. To verify, inspect the disk configuration for 'diskEncryptionKey' settings that specify a CSEK. Remediation involves updating disk settings to include a CSEK, ensuring the encryption keys used align with organizational security policies. This enhances data security by ensuring encryption keys are managed by the organization.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/docs/security/encryption-at-rest
  - https://cloud.google.com/security/encryption/default-encryption
- rule_id: gcp.compute.disk.data_protection_storage_volume_cmk_cmek_configured
  service: compute
  resource: disk
  requirement: Data Protection Storage Volume CMK Cmek Configured
  scope: compute.disk.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure CMEK Configured for Compute Engine Disks
  rationale: Configuring Customer Managed Encryption Keys (CMEK) for Compute Engine disks enhances control over cryptographic operations and key management policies, thereby reducing the risk of unauthorized access to sensitive data. It also aids in meeting compliance requirements for data protection and privacy regulations such as GDPR and HIPAA, which mandate strong data encryption standards.
  description: This rule checks if Compute Engine disks are configured with Customer Managed Encryption Keys (CMEK) rather than using Google-managed keys. To verify, ensure that the 'kmsKeyName' property is set for each disk resource. If not configured, update the disk configuration to specify a suitable CMEK, leveraging Cloud Key Management Service (KMS) to create and manage cryptographic keys. This configuration provides enhanced security by allowing organizations to manage their own encryption keys.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.disk.data_protection_storage_volume_encryption_at_rest_enabled
  service: compute
  resource: disk
  requirement: Data Protection Storage Volume Encryption At Rest Enabled
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Encryption at Rest for GCP Compute Disks
  rationale: Ensuring data stored on Google Cloud Platform Compute Engine disks is encrypted at rest protects sensitive information from unauthorized access and potential breaches. This is critical for maintaining customer trust, meeting regulatory requirements like GDPR and HIPAA, and mitigating the risk of data exposure in case of hardware theft or unauthorized administrative access.
  description: This rule checks whether encryption at rest is enabled for Google Cloud Platform Compute Engine disks. By default, GCP encrypts all data at rest; however, users can apply their own encryption keys for added security. To verify this, ensure that the Compute Engine disks have the 'encryptionKey' property configured. Remediation involves setting up Customer-Managed Encryption Keys (CMEK) through Google Cloud Key Management Service (KMS) and updating disk configurations to use these keys.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
- rule_id: gcp.compute.disk.data_protection_storage_volume_snapshots_encrypted
  service: compute
  resource: disk
  requirement: Data Protection Storage Volume Snapshots Encrypted
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Snapshots of Compute Disk are Encrypted
  rationale: Encrypting snapshots of Compute Engine disks is crucial to protect sensitive data from unauthorized access and potential exposure. This measure reduces the risk of data breaches, unauthorized data manipulation, and compliance violations with standards such as PCI-DSS, HIPAA, and GDPR. Encryption at rest ensures that even if physical security is compromised, data remains unreadable and secure.
  description: This rule checks whether snapshots of Compute Engine disks are encrypted using customer-managed or Google-managed encryption keys. Unencrypted snapshots can lead to data exposure if accessed by unauthorized users. To verify, ensure that all disk snapshots are created with encryption enabled by configuring the appropriate encryption keys in the Cloud Console or via the gcloud CLI. Remediate by updating existing snapshots to apply encryption and setting organization policies to enforce encryption on all new snapshots.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
- rule_id: gcp.compute.disk.disk_encrypted
  service: compute
  resource: disk
  requirement: Disk Encrypted
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Engine Disks are Encrypted at Rest
  rationale: Encrypting disks at rest protects sensitive data from unauthorized access and potential breaches, particularly if a disk is improperly decommissioned. This is critical for compliance with data protection regulations like GDPR and helps mitigate risks associated with data exposure or loss, ensuring that sensitive information remains secure even if physical security controls are bypassed.
  description: This rule checks whether Compute Engine disks are encrypted using Google-managed encryption keys or customer-managed encryption keys (CMEK). To verify, review the disk's properties in the GCP Console or use the `gcloud compute disks describe` command to check the 'encryption' field. Remediation involves enabling encryption for all disks during creation or updating existing disks with encryption by moving data to a new encrypted disk.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.disk.dr_replication_auth_roles_least_privilege
  service: compute
  resource: disk
  requirement: DR Replication Auth Roles Least Privilege
  scope: compute.disk.replication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for DR Replication Auth Roles
  rationale: Restricting the roles for disaster recovery (DR) replication to the least privilege necessary reduces the risk of unauthorized access and potential data breaches. Excessive permissions can lead to accidental or malicious data manipulation, impacting business continuity and potentially violating compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks that only the minimum necessary permissions are granted for roles involved in DR replication for compute disks. Ensure that roles are strictly defined to avoid excessive permissions that could be misused. Verification involves reviewing IAM policies assigned to resources managing DR replication and removing any permissions that are not essential. Remediation involves customizing roles to limit permissions to only those required for DR replication operations.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/compute/docs/disks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/least-privilege
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
- rule_id: gcp.compute.disk.dr_replication_cross_region_replication_encrypted
  service: compute
  resource: disk
  requirement: DR Replication Cross Region Replication Encrypted
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region Disk Replication is Encrypted
  rationale: Encrypting cross-region replicated disks safeguards sensitive data from unauthorized access and potential data breaches during disaster recovery scenarios. This practice mitigates risks of data exposure when data is transferred across regions and helps meet compliance requirements such as GDPR, which mandates data protection during transit and at rest.
  description: This rule checks that all cross-region replicated disks in GCP are encrypted at rest using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). Verify that the 'diskEncryptionKey' is specified in the disk's configuration. To remediate, enable encryption on disks by setting up CMEK or GMEK in the Compute Engine settings, ensuring that the 'diskEncryptionKey' is correctly configured to protect the data.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance
  - 'CIS GCP Benchmark: Section 4.4 - Ensure that Cloud Storage buckets are encrypted with Customer-Managed Keys (CMKs)'
  - 'NIST SP 800-53: SC-13 Cryptographic Protection'
  - 'PCI-DSS Requirement 3: Protect stored cardholder data'
  - ISO 27001:2013 A.10.1 Cryptographic controls
- rule_id: gcp.compute.disk.dr_replication_encryption_in_transit_tls_required
  service: compute
  resource: disk
  requirement: DR Replication Encryption In Transit TLS Required
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure TLS for DR Replication Encryption In Transit for Disks
  rationale: Encrypting data in transit using TLS during disaster recovery (DR) replication is crucial to protect sensitive information from interception and unauthorized access. This is essential to mitigate the risks of data breaches and ensure compliance with standards like PCI-DSS and HIPAA, which mandate encrypted data transfers. Failure to do so can lead to significant reputational damage and legal penalties.
  description: This rule checks whether Transport Layer Security (TLS) is enabled for encrypting data in transit during DR replication of Compute Engine disks. Ensure that TLS is configured by verifying the encryption settings in the Google Cloud Console or via the gcloud command-line tool. To remediate, update the disk replication settings to enforce TLS encryption when data is transferred between data centers.
  references:
  - https://cloud.google.com/compute/docs/disks/add-persistent-disk#encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
  - https://cloud.google.com/security/encryption-in-transit/
- rule_id: gcp.compute.disk.dr_replication_lag_monitoring_alerts_enabled
  service: compute
  resource: disk
  requirement: DR Replication Lag Monitoring Alerts Enabled
  scope: compute.disk.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Monitoring Alerts for DR Replication Lag on Disks
  rationale: Monitoring DR replication lag is crucial for ensuring data consistency and availability in disaster recovery scenarios. Without timely alerts, businesses risk data loss or corruption, which can impact service continuity and violate compliance mandates such as ISO 27001 and SOC2. Proactive monitoring helps mitigate these risks by allowing prompt corrective actions.
  description: This rule checks if alerts are configured for monitoring replication lag of disks in disaster recovery setups on GCP. Ensure that you have set up Cloud Monitoring alerts to notify you of any lag beyond acceptable thresholds. Verify by accessing the Cloud Monitoring dashboard and reviewing alert policies related to disk replication metrics. Remediation involves configuring alert policies to track relevant metrics and setting appropriate thresholds to trigger notifications.
  references:
  - https://cloud.google.com/compute/docs/disks/disaster-recovery
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/monitoring/support/notification-options
- rule_id: gcp.compute.disk.encryption_at_rest_enabled
  service: compute
  resource: disk
  requirement: Encryption At Rest Enabled
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption at Rest is Enabled for Compute Disks
  rationale: Enabling encryption at rest protects sensitive data stored on Compute Engine disks from unauthorized access and potential data breaches. This is critical for maintaining business reputation, preventing financial loss from data theft, and adhering to regulatory standards such as GDPR and HIPAA.
  description: This rule checks if Compute Engine disks have encryption at rest enabled using either Google-managed or customer-managed encryption keys. To verify, check the disk's encryption settings in the GCP Console or via the gcloud CLI. Remediation involves enabling encryption for disks without it, which can be done by configuring the disk to use Google-managed keys or by specifying a customer-managed key in Cloud KMS.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/compute/docs/disks#encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/
- rule_id: gcp.compute.disk.encryption_enabled
  service: compute
  resource: disk
  requirement: Encryption Enabled
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Compute Engine Disks Have Encryption Enabled
  rationale: Enabling encryption for Compute Engine disks protects sensitive data from unauthorized access and data breaches. It is essential for maintaining confidentiality and integrity of data-at-rest, and is often a requirement for regulatory compliance frameworks such as PCI-DSS and HIPAA. Unencrypted disks can expose data to risk if compromised, leading to potential financial loss and reputational damage.
  description: This rule verifies that all Compute Engine disks have encryption enabled, either with Google-managed or customer-supplied encryption keys. To check, navigate to the Google Cloud Console, select 'Compute Engine', and review disk settings for encryption status. Remediation involves configuring disk encryption during disk creation or updating existing disks to use encryption keys. Utilize customer-managed keys for enhanced security control.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/compute/docs/disks
  - https://cloud.google.com/iam/docs/using-cmek
- rule_id: gcp.compute.disk.in_use
  service: compute
  resource: disk
  requirement: In Use
  scope: compute.disk.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure GCP Compute Disks Are Attached to Active Instances
  rationale: Unused disks in GCP incur costs and can lead to potential security risks such as unauthorized access or data leaks. Ensuring disks are in use helps optimize resource allocation and reduces the attack surface by limiting exposure of data stored on unassociated disks. It also aids in maintaining compliance with data protection standards by ensuring data is not inadvertently exposed.
  description: This rule checks that all Google Cloud Platform Compute Engine disks are attached to active instances. Disks that are not in use should be evaluated and either attached to an instance or deleted if no longer needed. This helps in managing costs effectively and reducing security risks. To remediate, attach unattached disks to appropriate instances or delete them if they are redundant.
  references:
  - https://cloud.google.com/compute/docs/disks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances
- rule_id: gcp.compute.disk.network_mtls_required_for_internal_services_where_supported
  service: compute
  resource: disk
  requirement: Network Mtls Required For Internal Services Where Supported
  scope: compute.disk.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce mTLS for Internal Disk Services in Compute
  rationale: Requiring mutual TLS for internal disk services mitigates risks such as unauthorized access and data interception by ensuring that both the client and server authenticate each other. This is crucial for protecting sensitive data and maintaining compliance with standards like PCI-DSS and HIPAA, which mandate strong encryption and authentication mechanisms to safeguard data privacy.
  description: This rule checks that network mTLS is enabled for internal disk services where supported within Google Compute Engine. Verify that all internal services are configured to use mTLS by updating the load balancer settings or service configurations to require client certificates. Remediation involves configuring IAM roles and certificates in the GCP Console or using gcloud commands to enforce mTLS, ensuring secure communication channels.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://cloud.google.com/compute/docs/disks
  - CIS Google Cloud Computing Foundations Benchmark v1.0.0, Section 5.2
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.compute.disk.network_tls_min_1_2_enforced
  service: compute
  resource: disk
  requirement: Network TLS Min 1 2 Enforced
  scope: compute.disk.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Disk Network TLS Min Version is 1.2
  rationale: Enforcing a minimum TLS version of 1.2 for GCP Compute Disk network communication is crucial to protect data in transit from being intercepted or tampered with by attackers. This enhances data integrity and confidentiality, reducing the risk of man-in-the-middle attacks and ensuring compliance with industry standards like PCI-DSS and NIST SP 800-52.
  description: This rule checks that all network communications involving GCP Compute Disks enforce a minimum TLS version of 1.2. To verify, ensure that TLS policies are configured correctly in your network security settings. Remediation involves updating any outdated configurations to specify TLS 1.2 as the minimum version to secure data transfers, particularly for sensitive or regulated information.
  references:
  - https://cloud.google.com/compute/docs/disks
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.disk.public_snapshot
  service: compute
  resource: disk
  requirement: Public Snapshot
  scope: compute.disk.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure Compute Disk Snapshots Are Not Publicly Accessible
  rationale: Publicly accessible disk snapshots can expose sensitive data to unauthorized users, leading to potential data breaches. This can result in financial loss, reputational damage, and non-compliance with regulations such as GDPR and HIPAA. Ensuring snapshots are private mitigates the risk of unauthorized data access and enhances the security posture of your cloud environment.
  description: This rule checks if any Compute Engine disk snapshots are publicly accessible. Public snapshots can be accessed by anyone on the internet, increasing the risk of data exposure. To verify, review the 'isPublic' property of each snapshot in the GCP Console or use gcloud commands. Remediation involves modifying the snapshot permissions to restrict access, ensuring only authorized users can view or restore them.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/identity-security/understanding-google-cloud-security-and-compliance
- rule_id: gcp.compute.disk.snapshot
  service: compute
  resource: disk
  requirement: Snapshot
  scope: compute.disk.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Compute Disk Snapshots are Encrypted
  rationale: Encrypting disk snapshots is crucial to protect sensitive data from unauthorized access, especially in backup and recovery scenarios. Unencrypted snapshots can lead to data breaches, exposing critical business information and potentially violating compliance standards such as GDPR and HIPAA. By enforcing encryption, organizations mitigate risks associated with data theft and ensure data integrity and confidentiality.
  description: This rule checks whether snapshots of Compute Engine disks are encrypted using Customer-Managed Encryption Keys (CMEK) or Google-managed encryption keys. To verify, inspect the encryption status of existing disk snapshots in the GCP Console or via gcloud CLI. Remediation involves enabling encryption by specifying a CMEK during snapshot creation or ensuring current snapshots use Google-managed keys. Regular reviews and updates to encryption keys are recommended to maintain security posture.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots#encrypted_snapshots
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/compute/docs/disks/viewing-and-applying-resource-policies
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.disk.snapshots_encrypted
  service: compute
  resource: disk
  requirement: Snapshots Encrypted
  scope: compute.disk.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Snapshots Are Encrypted at Rest
  rationale: Encrypting snapshots at rest is crucial for protecting sensitive data from unauthorized access and potential breaches. Unencrypted snapshots can be exploited if compromised, leading to data leaks and violating compliance with standards like PCI-DSS, HIPAA, and GDPR. Encrypting data at rest mitigates the risk of data exposure and enhances data privacy and security posture.
  description: This rule checks whether snapshots of Compute Engine disks are encrypted using Google-managed or customer-managed encryption keys. Verify that all snapshots are encrypted by default by reviewing the encryption settings in the Google Cloud Console or using the gcloud command-line tool. To remediate, enable encryption for snapshots through the Compute Engine settings, ensuring compliance with encryption standards and enhancing data protection.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis#section-5.4
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/index.html
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.disk.snapshots_not_public
  service: compute
  resource: disk
  requirement: Snapshots Not Public
  scope: compute.disk.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure Compute Disk Snapshots Are Not Publicly Accessible
  rationale: Publicly accessible disk snapshots expose sensitive data to unauthorized users, increasing the risk of data breaches and non-compliance with regulations such as GDPR and PCI-DSS. Unauthorized access could lead to data leaks, financial loss, and reputational damage to the organization.
  description: This rule checks if Google Cloud Platform Compute Disk snapshots are publicly accessible. It ensures that no snapshots are set to have 'allUsers' or 'allAuthenticatedUsers' in their access control lists. To verify, review the IAM policies associated with each snapshot and remove any public access entries. Ensure that only necessary and authorized personnel have access to these snapshots.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.allow_ingress_from_internet_to_all_ports_gcp_api_ga_attached
  service: compute
  resource: firewall
  requirement: Allow Ingress From Internet To All Ports Gcp API Ga Attached
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Ingress from Internet to All Ports on GCP Firewall
  rationale: Allowing unrestricted ingress from the internet to all ports poses significant security risks, including potential unauthorized access, data breaches, and exposure to attacks such as DDoS. This configuration can lead to non-compliance with standards like PCI-DSS and ISO 27001, which require controlled access to protect sensitive data.
  description: This rule checks for firewall rules that allow ingress traffic from the internet on all ports, which can expose resources to unauthorized access. Verify that firewall rules specify limited source IP ranges and only open necessary ports. Remediation involves restricting ingress rules to specific IP addresses and limiting ports to those essential for operations. Use the GCP Console or gcloud CLI to review and update firewall rules accordingly.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.allow_ingress_tcp_port_22_gcp_compute_securitygro_configured
  service: compute
  resource: firewall
  requirement: Allow Ingress Tcp Port 22 Gcp Compute Securitygro Configured
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Ingress TCP Port 22 to Specific IPs in GCP Firewall
  rationale: Allowing unrestricted access to TCP port 22 (SSH) can expose virtual machines to unauthorized access attempts and brute-force attacks, compromising sensitive data. Implementing strict access controls aligns with regulatory requirements such as PCI-DSS and helps mitigate potential security breaches.
  description: This rule checks for GCP firewall configurations that permit ingress traffic on TCP port 22 from any source IP. It is crucial to limit access to known IP addresses to reduce the attack surface. Verify firewall rules and update them to allow SSH access only from trusted IP addresses or ranges. Remediation involves modifying the firewall rule to specify allowed source IPs, ensuring enhanced security while maintaining necessary access.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.firewall.data_warehouse_sg_egress_egress_restricted
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Egress Egress Restricted
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Egress Traffic for Data Warehouse Firewall
  rationale: Unrestricted egress traffic from data warehouse resources can expose sensitive data to unauthorized networks, leading to data breaches and non-compliance with data protection regulations such as GDPR and CCPA. Restricting egress traffic minimizes the attack surface and ensures that only legitimate communication channels are used, reducing the risk of data exfiltration.
  description: This rule checks if egress traffic from the data warehouse firewall is restricted to necessary IP ranges and ports. To verify, ensure that egress rules specify allowed destination IPs and protocols, and block all others by default. Remediation involves updating firewall rules to only allow traffic to known, trusted destinations necessary for business operations, and logging all egress activities for audit purposes.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.data_warehouse_sg_egress_restricted
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Egress Restricted
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Data Warehouse Firewall Egress Traffic
  rationale: Unrestricted egress traffic from data warehouses can lead to data exfiltration and unauthorized access to sensitive data, posing significant business risks. Ensuring egress restrictions aligns with compliance requirements such as PCI-DSS and HIPAA, and mitigates threats like data breaches and insider threats.
  description: This rule checks whether egress traffic from Data Warehouse security groups is appropriately restricted to prevent unauthorized data flows. It requires configuring firewall rules to limit outbound connections to only trusted IP ranges and necessary ports. To verify, review the firewall settings in the GCP Console under VPC Network > Firewall, and ensure egress rules are defined for the data warehouse subnet with least privilege access. Remediation involves updating firewall rules to enforce these restrictions.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.data_warehouse_sg_ingress_no_0_0_0_0
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Ingress No 0 0 0 0
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Disallow Wide Open Ingress to Data Warehouse Firewalls
  rationale: Allowing ingress from 0.0.0.0/0 to data warehouse systems can expose sensitive data to unauthorized access, increasing the risk of data breaches. This is particularly critical for organizations handling regulated data, where compliance with standards like PCI-DSS, HIPAA, or GDPR is necessary to avoid fines and reputational damage.
  description: This rule checks for firewall rules that permit ingress traffic from any IP address (0.0.0.0/0) to data warehouse resources. Such configurations should be restricted to mitigate unauthorized access risks. To remediate, modify the firewall rule to limit ingress to trusted IP ranges or utilize identity-aware proxies. Verification can be done via the Google Cloud Console by reviewing firewall rules under the Compute Engine section.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.data_warehouse_sg_ingress_only_required_ports
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Ingress Only Required Ports
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Data Warehouse Ingress to Required Ports
  rationale: Allowing only necessary ports for ingress to your data warehouse minimizes the attack surface, reducing the likelihood of unauthorized access or data breaches. This is critical for protecting sensitive data and meeting compliance requirements such as PCI-DSS and HIPAA, which mandate strict access controls to safeguard sensitive information.
  description: This rule checks whether ingress traffic to Google Cloud data warehouses is limited to only the required ports. By default, all traffic should be denied except for specific ports used by your applications or services. To verify, review the firewall rule settings to ensure only necessary ports are open, such as port 5432 for PostgreSQL. Remediation involves updating or creating firewall rules to restrict inbound traffic to these specified ports, ensuring all other ingress traffic is denied.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.data_warehouse_sg_no_0_0_0_0_ingress
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg No 0 0 0 0 Ingress
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Ingress Traffic to Data Warehouse Firewalls
  rationale: Allowing ingress from 0.0.0.0/0 exposes the data warehouse to potential unauthorized access, increasing the risk of data breaches. This misconfiguration can lead to non-compliance with data protection regulations such as PCI-DSS and GDPR, which mandate restricted access to sensitive data. Properly scoped firewall rules are crucial to safeguarding data and maintaining customer trust.
  description: This rule checks for firewall configurations allowing ingress traffic from any IP address (0.0.0.0/0) to data warehouses. To verify compliance, review firewall rules in the GCP Console or via the gcloud CLI to ensure that ingress traffic is restricted to specific IPs or ranges. Remediation involves updating firewall rules to specify allowed IP addresses, reducing exposure to only trusted sources.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/best-practices-for-enterprise-organizations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://ec.europa.eu/info/law/law-topic/data-protection_en
- rule_id: gcp.compute.firewall.data_warehouse_sg_only_required_ports_open
  service: compute
  resource: firewall
  requirement: Data Warehouse Sg Only Required Ports Open
  scope: compute.firewall.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Ports for Data Warehouse Firewalls
  rationale: Opening only necessary ports for data warehouse firewalls minimizes the attack surface, reducing the risk of unauthorized access or data breaches. This practice helps protect sensitive data and supports compliance with regulations like PCI-DSS and HIPAA, which require stringent access controls and data protection mechanisms.
  description: This rule checks if firewall rules for data warehouse instances are configured to allow only the necessary ports. Specifically, it ensures that ports such as 5432 for PostgreSQL or 3306 for MySQL are open while unnecessary ports are closed. To verify, review firewall rules in the GCP Console, ensuring only required ports are listed. Remediate by modifying firewall rules to restrict ports to those essential for data warehouse operations.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.network_default_deny_between_tiers
  service: compute
  resource: firewall
  requirement: Network Default Deny Between Tiers
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Default Deny Policy Between Network Tiers
  rationale: Implementing a default deny policy between network tiers minimizes the risk of lateral movement by attackers and helps prevent unauthorized access to sensitive resources. This is critical for maintaining data integrity and confidentiality, as it ensures that only explicitly permitted communication is allowed, aligning with regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that firewall rules enforce a default deny policy between different network tiers within GCP. By default, all inbound and outbound traffic between tiers should be denied, and only specific, necessary communications should be allowed. To verify compliance, review firewall rule settings in the GCP Console or using the gcloud command-line tool. Remediation involves configuring firewall rules to explicitly allow only required traffic while denying all other traffic by default.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-41r1.pdf
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#security_and_compliance
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.compute.firewall.network_exception_approvals_required
  service: compute
  resource: firewall
  requirement: Network Exception Approvals Required
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Firewall Rules Require Network Exception Approvals
  rationale: Requiring approvals for network exceptions minimizes the risk of unauthorized access and potential data breaches. It ensures that firewall rule changes are vetted for security impact, aligning with compliance needs such as PCI-DSS and ISO 27001, which mandate strict access controls and auditing. This practice helps prevent accidental or malicious misconfigurations that could expose sensitive data.
  description: This rule checks that any exceptions to standard network access control policies in GCP firewall rules have documented approvals. Specifically, it ensures that any rule allowing traffic from less secure networks or any 'allow' rules with a broad IP range have been reviewed and approved by a security team. To verify, review the audit logs for evidence of approval and ensure that all firewall changes are tracked in a change management system. Remedy any unapproved rules by conducting a security review and obtaining necessary approvals or removing the rule.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.network_logging_enabled
  service: compute
  resource: firewall
  requirement: Network Logging Enabled
  scope: compute.firewall.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Logging is Enabled for Firewall Rules
  rationale: Enabling network logging on firewall rules is critical for monitoring and auditing network traffic, helping to identify unauthorized access attempts or breaches. This enhances visibility into network operations, supports incident response efforts, and aids in meeting various compliance requirements such as PCI-DSS and SOC2 by providing detailed records of data flow across network boundaries.
  description: This rule checks if network logging is enabled for all firewall rules within your GCP project. Network logging should be enabled to capture logs of connections passing through your firewall, providing crucial data for security analysis and incident response. To remediate, ensure that logging is turned on for each firewall rule by setting the 'enableLogging' field to true. Verification can be performed by reviewing the firewall rule settings in the Google Cloud Console or via the gcloud CLI.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://cloud.google.com/compute/docs/reference/rest/v1/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.firewall.network_microseg_endpoint_policies_applied
  service: compute
  resource: firewall
  requirement: Network Microseg Endpoint Policies Applied
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Microsegmentation Policies on GCP Firewall
  rationale: Applying network microsegmentation endpoint policies is crucial for minimizing lateral movement within the network, thereby reducing the attack surface. This enhances the security posture by ensuring that only necessary and explicit communications are allowed between network segments, which is essential for meeting regulatory requirements such as PCI-DSS and HIPAA.
  description: This rule checks for the application of network microsegmentation policies on GCP firewall configurations, ensuring that endpoint policies are properly applied to manage and restrict traffic flows. Verify that the firewall rules are configured to enforce microsegmentation by using network tags or service accounts to define granular access controls. Remediation involves auditing current configurations and applying policies that enforce strict traffic segregation between different network zones.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/architecture/network-security#microsegmentation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-207/final
- rule_id: gcp.compute.firewall.network_microseg_identity_aware_policies_enabled
  service: compute
  resource: firewall
  requirement: Network Microseg Identity Aware Policies Enabled
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enable Identity-Aware Policies for Network Microsegmentation
  rationale: Identity-aware policies enhance security by enforcing access controls based on user identity and context, reducing the risk of unauthorized access to sensitive data. This approach helps in mitigating insider threats and limits the attack surface by ensuring only authenticated and authorized entities can access specific network segments. It also aids in meeting compliance with regulations such as GDPR and HIPAA, which mandate stringent access controls.
  description: This rule checks if identity-aware policies are enabled for microsegmented networks in GCP. These policies should be configured to restrict access based on user identity and contextual attributes. Verification involves reviewing firewall rules to ensure they incorporate identity-based restrictions. To remediate, configure Identity-Aware Proxy (IAP) for your applications and define firewall rules that incorporate identity conditions to control access effectively.
  references:
  - https://cloud.google.com/iap/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/concepts-cspm
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Control 9.1
  - NIST SP 800-53 Rev. 5, AC-3 Access Enforcement
  - https://cloud.google.com/security/infrastructure/design
  - ISO/IEC 27001:2013, A.9.1.2 Access Control Policy
- rule_id: gcp.compute.firewall.network_nacl_egress_restricted
  service: compute
  resource: firewall
  requirement: Network Nacl Egress Restricted
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Egress Traffic with Network ACLs on GCP Firewall
  rationale: Restricting egress traffic in your Google Cloud environment is crucial to prevent data exfiltration and mitigate the risk of unauthorized external communications. Open egress rules can expose your network to potential threats, including data breaches and regulatory non-compliance, affecting business continuity and reputation.
  description: This rule checks that egress rules in GCP firewall configurations are restricted to only necessary IP ranges and ports. To verify, review firewall settings in the Google Cloud Console to ensure no overly permissive egress rules exist. Remediation involves updating firewall rules to limit outbound traffic to specific, trusted IP addresses and ports, aligning with least privilege principles.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.network_nacl_no_allow_all_rules
  service: compute
  resource: firewall
  requirement: Network Nacl No Allow All Rules
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Firewall Rules to Avoid Open Access
  rationale: Allowing open access through firewall rules can expose your network to unauthorized access and potential cyber threats, increasing the risk of data breaches and compromising sensitive information. Adhering to the principle of least privilege and preventing 'allow all' rules is crucial for maintaining security posture and ensuring compliance with regulations like PCI-DSS and ISO 27001.
  description: This rule checks for firewall rules in GCP that permit open access to all IP addresses. It ensures that no firewall rule is overly permissive by allowing traffic from any IP, which can be verified by reviewing the 'sourceRanges' field in firewall configurations. To remediate, replace overly broad source ranges with specific IP addresses or CIDR blocks that correspond to trusted sources, reducing the attack surface.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.compute.firewall.network_no_permit_all
  service: compute
  resource: firewall
  requirement: Network No Permit All
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Firewall Rules from Allowing Open Network Access
  rationale: Allowing unrestricted network access poses significant security risks, including unauthorized access, data breaches, and potential exploitation by attackers. This can lead to financial loss, reputational damage, and legal liabilities, especially if sensitive data is exposed. Ensuring firewall rules do not permit all traffic is crucial for maintaining a secure and compliant cloud environment.
  description: This rule checks for firewall configurations that allow open access (0.0.0.0/0) to networks. It verifies that firewall rules are not set to 'allow' all inbound or outbound traffic without proper restrictions. Remediation involves reviewing firewall rules and limiting access to specific IP ranges, services, or protocols as per business requirements. For verification, review firewall rule settings in the Google Cloud Console under 'VPC Network > Firewall'.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/network-connectivity/docs
  - https://cloud.google.com/architecture/security-foundations/network-architecture
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.compute.firewall.network_no_permit_any_any
  service: compute
  resource: firewall
  requirement: Network No Permit Any Any
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Any-Any Firewall Rules in GCP
  rationale: Allowing 'any-to-any' traffic in firewall rules can expose your network to unauthorized access and potential attacks. This configuration increases the risk of data breaches and non-compliance with security standards such as PCI-DSS and ISO 27001, which mandate strict access controls to protect sensitive data.
  description: This rule checks for firewall rules in GCP that allow unrestricted access to and from any IP addresses. To verify, review the firewall configurations in the Google Cloud Console under VPC network settings and ensure no rules permit '0.0.0.0/0' as both source and destination. Remediation involves updating these rules to specify only the necessary IP ranges and ports, implementing least privilege access principles.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.firewall.network_policies_present
  service: compute
  resource: firewall
  requirement: Network Policies Present
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Policies Are Applied to GCP Firewall Rules
  rationale: Implementing network policies in GCP firewall rules is crucial for defining and enforcing traffic ingress and egress controls. This reduces the risk of unauthorized access, data breaches, and lateral movement within the network. Properly configured network policies help meet compliance with standards such as PCI-DSS and ISO 27001, thereby protecting sensitive data and maintaining customer trust.
  description: This rule checks if network policies are applied to GCP firewall rules, ensuring that only authorized traffic can access network resources. Verify that all firewall rules have associated network policies specifying ingress and egress traffic controls. Remediation involves reviewing each firewall rule and applying appropriate network policies to restrict access based on business and security requirements. Adjust or create firewall rules with network policies using the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/network-security
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.compute.firewall.network_policy_store_encrypted
  service: compute
  resource: firewall
  requirement: Network Policy Store Encrypted
  scope: compute.firewall.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Network Policy Store is Encrypted at Rest
  rationale: Encrypting network policy data at rest protects sensitive configuration data from unauthorized access and potential data breaches. This is crucial for maintaining the integrity and confidentiality of network policies, which can prevent unauthorized network traffic and ensure compliance with data protection regulations like GDPR and CCPA.
  description: This rule checks if the network policy store in GCP's Compute Engine Firewall is encrypted at rest. Verify that encryption mechanisms are enabled for all firewall configurations by examining the firewall's metadata and ensuring that customer-managed encryption keys (CMEK) are utilized where applicable. Remediation involves configuring encryption settings in the GCP Console or using Terraform to enforce CMEK for enhanced security.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/encryption-at-rest/customer-managed-encryption-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.firewall.network_rule_groups_attached
  service: compute
  resource: firewall
  requirement: Network Rule Groups Attached
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Rule Groups Are Attached to Firewalls
  rationale: Attaching network rule groups to firewalls ensures that traffic is managed and monitored according to defined security policies. This reduces the risk of unauthorized access, data breaches, and non-compliance with regulatory requirements by controlling ingress and egress traffic effectively.
  description: This rule checks whether network rule groups are attached to GCP Compute Firewall resources, which are essential for applying consistent and comprehensive access control policies. Network rule groups allow for scalable management of firewall rules, enabling centralized updates and enforcement. To verify, ensure that all firewalls have associated rule groups via the GCP Console or CLI. Remediation involves creating or updating firewall rules to include network rule groups as applicable.
  references:
  - https://cloud.google.com/firewall/docs/network-rule-groups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/compute/docs/networking
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.firewall.network_sg_egress_restricted
  service: compute
  resource: firewall
  requirement: Network Sg Egress Restricted
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Egress Traffic in GCP Firewall Rules
  rationale: Unrestricted egress traffic from GCP firewall rules may lead to data exfiltration, unintentional exposure of internal services, and increased attack surface. By limiting egress traffic, organizations can prevent unauthorized data flow and comply with regulatory mandates such as GDPR and PCI-DSS, which require stringent data protection measures.
  description: This rule evaluates GCP firewall settings to ensure that egress traffic is restricted to necessary destinations. Check for overly permissive egress rules, such as allowing traffic to 0.0.0.0/0. To remediate, configure specific egress rules that only permit traffic to known, trusted IP ranges and ports required for business operations. Regularly review and update these rules to adapt to evolving threats.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.firewall.network_sg_no_0_0_0_0_ingress
  service: compute
  resource: firewall
  requirement: Network Sg No 0 0 0 0 Ingress
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Inbound Traffic from 0.0.0.0/0 in Firewall Rules
  rationale: Allowing inbound traffic from 0.0.0.0/0 exposes your resources to the internet, increasing the risk of unauthorized access, data breaches, and exploitation of vulnerabilities. This configuration can lead to non-compliance with industry standards and regulations like PCI-DSS and HIPAA, which mandate strict access control measures to protect sensitive data.
  description: This rule checks for firewall configurations that permit inbound traffic from 0.0.0.0/0, indicating unrestricted access. Such settings should be avoided unless absolutely necessary. To verify, inspect your GCP Firewall rules in the Cloud Console or via gcloud commands. Remediation involves restricting the source IP ranges to known, trusted networks and applying the principle of least privilege. Consider using identity-aware proxies or VPNs to limit access where appropriate.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.firewall.network_sg_only_required_ports_open
  service: compute
  resource: firewall
  requirement: Network Sg Only Required Ports Open
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Firewall to Open Only Necessary Ports
  rationale: Opening unnecessary ports in network security groups increases the attack surface, potentially allowing unauthorized access to systems and data. This can lead to breaches, data loss, and non-compliance with regulations such as PCI-DSS and HIPAA, which mandate strict access controls.
  description: This rule checks GCP firewall configurations to ensure that only required ports are open, minimizing exposure to potential threats. Firewall rules should be configured to allow traffic only on specific ports needed for business operations. Verification involves reviewing firewall rules in the Google Cloud Console or via gcloud commands and closing any ports not essential for application functionality. Remediation includes updating firewall rules to restrict access and logging rule changes for audit purposes.
  references:
  - https://cloud.google.com/vpc/docs/using-firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.firewall.network_tier_to_tier_policies_defined
  service: compute
  resource: firewall
  requirement: Network Tier To Tier Policies Defined
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Tier Policies Are Defined for Firewall Rules
  rationale: Defining network tier policies for firewall rules in GCP helps ensure that traffic flows are adequately managed between different network tiers, reducing the risk of unauthorized access and potential data breaches. This practice is essential for maintaining robust network segmentation, which mitigates lateral movement by attackers and helps in achieving compliance with standards that require network isolation and access control, such as PCI-DSS and ISO 27001.
  description: This rule checks if network tier-to-tier policies are defined for all firewall rules within GCP's Compute Engine. Proper configuration requires specifying which network tiers (Standard or Premium) can communicate with each other, ensuring traffic is only allowed where explicitly permitted. Verification involves reviewing the firewall rules and ensuring that tier policies are accurately configured. Remediation includes updating firewall configurations to include explicit tier policies, using the GCP Console or gcloud CLI, to prevent unintended traffic flows.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/blog/products/networking/achieve-better-control-over-your-network-with-gcp-firewall-rules
- rule_id: gcp.compute.firewall.networkacl_allow_ingress_any_port_configured
  service: compute
  resource: firewall
  requirement: Networkacl Allow Ingress Any Port Configured
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Ingress Traffic to Specific Ports on GCP Firewall
  rationale: Allowing ingress traffic from any port increases the attack surface and exposes the network to potential threats such as unauthorized access and data breaches. Limiting allowed ports helps in meeting compliance requirements by ensuring only necessary services are accessible, reducing the risk of exploitation.
  description: This rule checks for Google Cloud Platform firewall configurations that permit ingress traffic from any port. It is crucial to restrict this setting to only necessary ports to minimize exposure to potential threats. Verify by reviewing firewall rules in the GCP Console or via gcloud command-line tool. Remediate by updating the firewall rules to allow traffic only on specific, required ports and implementing strict access controls.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#network-security
- rule_id: gcp.compute.firewall.networkacl_unrestricted_ingress_configured
  service: compute
  resource: firewall
  requirement: Networkacl Unrestricted Ingress Configured
  scope: compute.firewall.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Restrict Unrestricted Ingress in Firewall Rules
  rationale: Allowing unrestricted ingress to your GCP network can expose critical resources to unauthorized access, leading to potential data breaches, service disruptions, and compliance violations. Attackers can exploit open ports to launch attacks or gain unauthorized access, impacting operational integrity and regulatory compliance with frameworks like PCI-DSS and NIST.
  description: This check ensures that GCP firewall rules are not configured with wide-open ingress settings. It assesses firewall rules for overly permissive source IP ranges (e.g., 0.0.0.0/0) that may expose services to the internet without proper access controls. To remediate, review and update these rules to specify more restrictive IP ranges or use identity-aware proxy configurations to limit access to trusted sources.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.compute.firewall.rdp_access_from_the_internet_allowed
  service: compute
  resource: firewall
  requirement: Rdp Access From The Internet Allowed
  scope: compute.firewall.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict RDP Access from the Internet in GCP Firewall
  rationale: Allowing unrestricted RDP access from the internet can expose your virtual machines to unauthorized access and potential attacks such as brute force or exploitation of RDP vulnerabilities. This poses a risk to sensitive data and critical systems, potentially leading to data breaches and compliance violations with standards like PCI-DSS and HIPAA.
  description: This rule checks for firewall rules that allow RDP access (TCP port 3389) from any IP address on the internet. It is recommended to restrict RDP access to specific IP addresses or ranges associated with trusted networks or VPNs. To remediate, modify the firewall rule to limit RDP access to known IP addresses or implement a bastion host to control access securely.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/solutions/best-practices-vpc-design#restricting_network_access
- rule_id: gcp.compute.firewall.rule_no_rdp_internet_access
  service: compute
  resource: firewall
  requirement: Rule No Rdp Internet Access
  scope: compute.firewall.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prohibit RDP Access from the Internet
  rationale: Allowing RDP access from the internet increases the risk of unauthorized access, brute force attacks, and exposure to vulnerabilities. This can lead to data breaches, service disruptions, and non-compliance with regulations such as PCI-DSS and ISO 27001, which require secure access controls and protection of sensitive data.
  description: This rule checks for firewall configurations that permit Remote Desktop Protocol (RDP) traffic from any source IP address, which poses security risks. Ensure that RDP access is restricted to specific IP addresses or ranges, preferably through a VPN or private network. Remediation involves modifying firewall rules to specify only trusted sources for RDP traffic, thereby reducing the attack surface.
  references:
  - https://cloud.google.com/compute/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.firewall.securitygroup_common_ports_restricted
  service: compute
  resource: firewall
  requirement: Securitygroup Common Ports Restricted
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Common Ports in GCP Firewall Rules
  rationale: Open common ports such as 22 (SSH), 80 (HTTP), and 443 (HTTPS) can expose your GCP resources to unauthorized access and potential attacks. Restricting access to these ports reduces the risk of exploitation from malicious actors, ensuring that only legitimate traffic reaches your services. This is crucial for maintaining compliance with security standards like NIST and ISO 27001 that require stringent access controls.
  description: This rule checks that GCP firewall rules are configured to restrict access to common ports to only trusted IP ranges or specific service accounts. It ensures that open ports are not accessible from the internet at large, minimizing attack surfaces. Verify by reviewing firewall rule settings in the GCP Console or using gcloud commands. Remediation involves adjusting firewall rules to limit access to these ports, ideally using private IP ranges or VPC Service Controls.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.firewall.securitygroup_default_restrict_traffic_configured
  service: compute
  resource: firewall
  requirement: Securitygroup Default Restrict Traffic Configured
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Default Firewall Rules in GCP Compute
  rationale: Default firewall rules in GCP can expose your network to unauthorized access, potentially leading to data breaches or service disruptions. By restricting these rules, organizations can minimize attack surfaces, comply with best practices, and adhere to regulatory standards such as PCI-DSS and ISO 27001.
  description: This rule checks for the existence and configuration of default firewall rules in GCP Compute that allow unrestricted traffic. It ensures these rules are modified to restrict access only to necessary services and IP ranges. To verify, review the 'default-allow' rules in your GCP Firewall settings and update them to use restrictive source ranges and protocols. Remediation involves adjusting the 'default' network's firewall settings to limit exposure.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/vpc/docs/firewall-rules-best-practices
- rule_id: gcp.compute.firewall.securitygroup_default_restricted
  service: compute
  resource: firewall
  requirement: Securitygroup Default Restricted
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Default Security Group Access
  rationale: Unrestricted firewall rules in the default security group can expose GCP resources to unauthorized access, leading to potential data breaches and service disruptions. Organizations often underestimate the risk posed by default configurations, which can be exploited by attackers to gain unauthorized entry into the network. Adhering to security best practices and compliance standards, such as NIST and PCI-DSS, requires limiting network access to only necessary services.
  description: This rule checks that the default security group in GCP Compute Engine does not allow overly permissive ingress and egress traffic, particularly from the internet. Firewall rules should be configured to deny non-essential traffic and limit access to specific IP addresses or ranges as needed. To remediate, review and modify the default firewall rules to ensure they align with the principle of least privilege, allowing only required traffic through. Verification can be done by inspecting the firewall rule configurations in the Google Cloud Console or via gcloud CLI.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#firewall
- rule_id: gcp.compute.firewall.securitygroup_rdp_restricted
  service: compute
  resource: firewall
  requirement: Securitygroup Rdp Restricted
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict RDP Access in GCP Firewall Rules
  rationale: Unrestricted RDP access over the internet can expose your instances to unauthorized access, leading to potential data breaches and system compromise. Limiting RDP access aligns with best practices for minimizing attack vectors and meets compliance requirements for secure network access control.
  description: This rule verifies that GCP firewall rules do not allow unrestricted RDP (TCP port 3389) access from the internet. It checks for firewall rules permitting `0.0.0.0/0` as a source for RDP. To remediate, configure the firewall rules to allow RDP access only from specific IP addresses or ranges. This can be done by updating source ranges in the GCP Console or using gcloud commands.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/solutions/best-practices-vpc-design
- rule_id: gcp.compute.firewall.securitygroup_ssh_restricted
  service: compute
  resource: firewall
  requirement: Securitygroup Ssh Restricted
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict SSH Access in GCP Firewall Rules
  rationale: Unrestricted SSH access can expose your infrastructure to unauthorized access and potential attacks, leading to data breaches and service disruptions. By limiting SSH access to specific IP ranges or using identity-aware proxy, you reduce attack vectors and comply with best practices such as the CIS GCP Benchmark, NIST, and PCI-DSS requirements.
  description: This check ensures that SSH access (port 22) is restricted in GCP firewall rules to only allow trusted IP addresses or ranges. Verify that firewall rules do not permit 0.0.0.0/0 for SSH and implement identity-aware proxy where possible. Remediation involves updating firewall rules to specify trusted IP addresses and auditing rules regularly to ensure compliance.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.firewall.ssh_access_from_the_internet_allowed
  service: compute
  resource: firewall
  requirement: Ssh Access From The Internet Allowed
  scope: compute.firewall.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: SSH Access from Internet Not Restricted
  rationale: Allowing SSH access from the internet exposes your VM instances to potential unauthorized access and brute force attacks, increasing the risk of data breaches and service disruption. Restricting SSH access to specific IP addresses mitigates these risks and helps maintain compliance with security standards such as PCI-DSS and ISO 27001.
  description: This rule checks for firewall rules that permit SSH access (TCP port 22) from any IP address on the internet. To minimize security risks, SSH access should be restricted to specific IP ranges. Verify by reviewing firewall rules with open access and update them to allow connections only from trusted IP addresses or networks. Consider using Identity-Aware Proxy or VPN for more secure access.
  references:
  - https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/securing-ssh
- rule_id: gcp.compute.firewall.ssh_internet_restriction_configured
  service: compute
  resource: firewall
  requirement: Ssh Internet Restriction Configured
  scope: compute.firewall.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict SSH Access from the Internet
  rationale: Allowing SSH access from the internet increases the risk of unauthorized access and potential exploits such as brute force attacks. Implementing restrictions helps protect sensitive data and resources, aligning with security best practices and compliance requirements like PCI-DSS and SOC2.
  description: This rule checks for firewall rules that allow SSH (port 22) access from any IP address. To enhance security, configure firewall rules to restrict SSH access to specific IP ranges or use identity-based access controls. Verify by reviewing firewall configurations in the GCP Console or using gcloud commands, and update rules to block 0.0.0.0/0 or to allow specific trusted IP addresses only.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org
  - https://www.sans.org/reading-room/whitepapers/cloud/securing-cloud-infrastructure-best-practices-38997
- rule_id: gcp.compute.forwarding_rule.network_listener_cipher_policy_secure
  service: compute
  resource: forwarding_rule
  requirement: Network Listener Cipher Policy Secure
  scope: compute.forwarding_rule.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Secure Cipher Policies for Network Listeners
  rationale: Using insecure cipher policies for network listeners can expose sensitive data to interception and tampering, leading to data breaches and non-compliance with security standards. Secure cipher configurations are crucial to protect data in transit, maintain customer trust, and meet regulatory requirements such as PCI-DSS and NIST guidelines.
  description: This rule checks that all forwarding rules in the GCP Compute service use secure cipher policies for network listeners. Insecure ciphers can be vulnerable to attacks like BEAST, POODLE, or Logjam. To verify, inspect the TLS policies associated with forwarding rules and ensure they only support strong, secure ciphers. Remediation involves updating the cipher policies to exclude weak ciphers and protocols, aligning with the latest security standards.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.forwarding_rule.network_listener_tls_min_1_2
  service: compute
  resource: forwarding_rule
  requirement: Network Listener TLS Min 1 2
  scope: compute.forwarding_rule.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS 1.2+ for Network Listener on Forwarding Rules
  rationale: Utilizing TLS version 1.2 or higher for network listeners on forwarding rules is critical to safeguard data in transit against interception and tampering. Older TLS versions are vulnerable to several known exploits that could compromise data integrity and confidentiality. This configuration aids in meeting compliance obligations under standards like PCI-DSS, which require strong encryption protocols.
  description: This check ensures that all forwarding rules in Google Cloud Compute Engine enforce a minimum TLS version of 1.2 for network listeners. Verify this by inspecting the 'minTlsVersion' attribute of each forwarding rule's associated HTTPS or SSL proxy. If the version is below 1.2, update the configuration via the Google Cloud Console or gcloud CLI to enforce TLS 1.2 or higher. This helps prevent downgrade attacks and ensures secure data transmission.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.health_check.dns_alerts_configured
  service: compute
  resource: health_check
  requirement: Dns Alerts Configured
  scope: compute.health_check.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure DNS Alerts Configured for Compute Health Checks
  rationale: Configuring DNS alerts for health checks is crucial for timely detection and response to potential disruptions in service availability. Without these alerts, businesses risk prolonged downtime, impacting customer satisfaction and potentially leading to financial losses. Additionally, this practice supports compliance with industry standards that mandate monitoring and alerting for critical infrastructure.
  description: This rule checks if DNS-based alerts are configured for Compute Engine health checks. Properly configured alerts ensure that any issues with DNS resolution impacting health checks are promptly notified to the relevant teams. To verify, ensure that alert policies are configured in Cloud Monitoring with conditions based on DNS health check metrics. Remediation involves setting up alert policies that trigger notifications when DNS resolution issues are detected for health checks.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/compute/docs/load-balancing/health-checks
  - https://cloud.google.com/monitoring/docs/alerting
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.health_check.dns_https_or_tls_used
  service: compute
  resource: health_check
  requirement: Dns HTTPS Or TLS Used
  scope: compute.health_check.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Health Checks Use DNS, HTTPS, or TLS
  rationale: Using DNS, HTTPS, or TLS for health checks is crucial to protect data in transit from interception or tampering, ensuring the reliability and security of services. Failure to use encrypted protocols can expose sensitive data to unauthorized access, leading to data breaches and non-compliance with regulations such as GDPR and HIPAA.
  description: This rule checks if Google Cloud Platform health checks are configured to use DNS, HTTPS, or TLS protocols, ensuring that data in transit is encrypted. Verify that health checks are not set to use unencrypted protocols such as HTTP. To remediate, modify health check configurations to use HTTPS or TLS by navigating to the Cloud Console, selecting the appropriate health check, and updating the protocol settings.
  references:
  - https://cloud.google.com/load-balancing/docs/health-checks
  - https://cloud.google.com/security/compliance/cis-gcp-1-0-0
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/itl/applied-cybersecurity/nist-special-publication-800-53
- rule_id: gcp.compute.health_check.dns_no_plaintext_credentials_in_url
  service: compute
  resource: health_check
  requirement: Dns No Plaintext Credentials In Url
  scope: compute.health_check.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Avoid Plaintext Credentials in Health Check URLs
  rationale: Including plaintext credentials in DNS health check URLs can lead to unauthorized access if intercepted, compromising system integrity and exposing sensitive data. This practice increases the risk of credential leakage, potentially violating compliance requirements such as ISO 27001 and PCI-DSS, and can result in severe business impacts including data breaches and financial loss.
  description: This rule checks that no plaintext credentials are embedded in DNS health check URLs within GCP Compute Engine configurations. Review and ensure that health check configurations do not include credentials in the URL or switch to using secure methods like OAuth tokens or service accounts. Remediation involves updating the health check settings and ensuring credentials are managed securely through GCP's Secret Manager or environment variables.
  references:
  - https://cloud.google.com/compute/docs/load-balancing/health-checks
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.compute.image.approved_image_allowlist_enforced
  service: compute
  resource: image
  requirement: Approved Image Allowlist Enforced
  scope: compute.image.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enforce Approved Image Allowlist for Compute Images
  rationale: Using unapproved images on GCP can lead to potential security vulnerabilities, compliance violations, and operational inefficiencies. Enforcing an approved image allowlist helps mitigate risks by ensuring that only vetted and compliant images are used, thus reducing the attack surface and aligning with industry standards and best practices.
  description: This rule verifies that all compute instances are launched using images from a predefined allowlist. It checks for the enforcement of image allowlists to prevent unauthorized or insecure images from being used. To remediate, implement IAM policies that restrict image creation and selection to approved images only. Regularly review and update the allowlist to include necessary images that comply with security and operational guidelines.
  references:
  - https://cloud.google.com/compute/docs/images
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/compute/docs/instances/image-management-best-practices
- rule_id: gcp.compute.image.encrypted_cmek
  service: compute
  resource: image
  requirement: Encrypted Cmek
  scope: compute.image.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Images Use Customer-Managed Encryption Keys
  rationale: Using Customer-Managed Encryption Keys (CMEK) to encrypt compute images provides greater control over data protection and access management, mitigating risks of unauthorized access and data breaches. This practice is crucial for meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strong encryption measures to protect sensitive data at rest.
  description: This rule checks whether Google Compute Engine images are encrypted with Customer-Managed Encryption Keys (CMEK). To verify, ensure the image's 'encryptionKey' parameter is set to a valid CMEK key. Remediation involves creating or using an existing Cloud Key Management Service (KMS) key and specifying it during the image creation or update process. This enhances security by allowing key rotation and revocation, thus maintaining strict data governance.
  references:
  - https://cloud.google.com/compute/docs/images
  - https://cloud.google.com/kms/docs/cmek
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.image.image_signed_and_verified
  service: compute
  resource: image
  requirement: Image Signed And Verified
  scope: compute.image.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Image is Signed and Verified
  rationale: Unsigned or unverified images pose security risks as they may contain malicious code, leading to compromised instances and data breaches. Ensuring images are signed and verified enhances trust and integrity in your compute environment, aligning with compliance frameworks that mandate secure software delivery practices.
  description: This rule checks that compute images used in the environment are cryptographically signed and verified. Images should be signed using a trusted certificate authority to ensure authenticity and integrity. Administrators can verify image signatures via GCP's Container Analysis API. Remediation involves signing images with a trusted key and configuring GCP to verify this signature before deployment.
  references:
  - https://cloud.google.com/container-registry/docs/using-container-analysis
  - https://cloud.google.com/security/best-practices-for-enterprise-organizations#implement_image_signing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.image.not_publicly_shared
  service: compute
  resource: image
  requirement: Not Publicly Shared
  scope: compute.image.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Access to GCP Compute Images
  rationale: Publicly shared compute images can expose sensitive data and lead to unauthorized access or resource misuse. Ensuring images are not publicly accessible reduces the risk of data breaches and aligns with compliance requirements such as PCI-DSS and ISO 27001, which mandate controlled access to sensitive resources.
  description: This rule checks if any Google Cloud Platform compute images are publicly accessible. To verify, examine the IAM policy bindings for each image to ensure 'allUsers' or 'allAuthenticatedUsers' are not granted 'roles/compute.imageUser'. Remediation involves updating the IAM policies to restrict access to only necessary users or groups, thereby preventing potential unauthorized access.
  references:
  - https://cloud.google.com/compute/docs/images
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.compute.image.vuln_scanned_no_critical
  service: compute
  resource: image
  requirement: Vuln Scanned No Critical
  scope: compute.image.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Images Are Scanned and Free of Critical Vulnerabilities
  rationale: Conducting vulnerability scans on images helps prevent the deployment of workloads with critical security flaws that could be exploited by attackers, thereby reducing the risk of data breaches and service disruptions. This practice is crucial for maintaining a secure cloud environment and meeting compliance requirements such as PCI-DSS and ISO 27001, which mandate vulnerability management and secure configurations.
  description: This rule checks that all Compute Engine images have undergone vulnerability scanning and contain no critical vulnerabilities. To verify, ensure that images in use are regularly scanned using tools like Google Cloud's Container Analysis or third-party security solutions. Remediation involves updating or patching images to address any critical vulnerabilities found, or replacing them with secure, compliant versions.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-container-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/images
  - https://cloud.google.com/container-registry/docs/container-analysis
- rule_id: gcp.compute.instance.backup_enabled
  service: compute
  resource: instance
  requirement: Backup Enabled
  scope: compute.instance.backup_recovery
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Backup is Enabled for Compute Instances
  rationale: Enabling backups for Compute instances is crucial for data resilience and business continuity. In the event of accidental deletion, corruption, or a security breach, having backups ensures that critical data can be restored, minimizing downtime and potential financial losses. Moreover, backups help meet regulatory compliance requirements, such as those specified by PCI-DSS and ISO 27001, which mandate data protection and recovery strategies.
  description: This rule checks that Compute Engine instances have backup mechanisms, such as snapshots or persistent disk backup policies, properly configured. To verify, ensure that instance disks have scheduled snapshots or are part of a managed backup plan. Remediation involves setting up Cloud Scheduler to automate disk snapshots or using Google Cloud's Backup and DR service to configure a backup policy that aligns with your RPO and RTO objectives.
  references:
  - https://cloud.google.com/compute/docs/disks/backup-and-restore
  - https://cloud.google.com/backup-and-disaster-recovery/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instance-templates
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
- rule_id: gcp.compute.instance.balancer_health_check_configured
  service: compute
  resource: instance
  requirement: Balancer Health Check Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Load Balancer Health Check is Configured for Instances
  rationale: Configuring a health check for instances behind a load balancer ensures that traffic is only routed to healthy instances, improving the availability and reliability of applications. Without proper health checks, there is a risk of sending traffic to unresponsive or malfunctioning instances, leading to application downtime and potential business disruptions. This is crucial for maintaining service level agreements and meeting compliance standards regarding uptime and reliability.
  description: This rule checks whether health checks are configured for instances behind Google Cloud Load Balancers. Health checks monitor the status of instances, ensuring that only healthy instances receive traffic. To verify, inspect the load balancer's configuration in the Google Cloud Console or use the gcloud CLI to confirm health checks are set. Remediation involves navigating to the Load Balancer settings and ensuring health checks are properly defined for each instance group attached to the load balancer.
  references:
  - https://cloud.google.com/load-balancing/docs/health-check-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/load-balancing/docs/backend-service
- rule_id: gcp.compute.instance.balancing_logging_enabled
  service: compute
  resource: instance
  requirement: Balancing Logging Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Load Balancer Logging for Compute Instances
  rationale: Enabling logging for load-balanced compute instances in GCP provides critical insights into traffic patterns and potential security threats. Without logging, anomalous activities and unauthorized access attempts may go undetected, increasing the risk of data breaches and non-compliance with regulatory standards such as PCI-DSS and SOC2. Logging aids in forensic analysis, operational troubleshooting, and ensures adherence to audit requirements.
  description: This rule checks if load balancing logging is enabled for compute instances. To verify, ensure that the HTTP(S) load balancer associated with the compute instance has logging configured in the Google Cloud Console under 'Network services' > 'Load balancing'. Remediation involves enabling logging at the load balancer level, specifying a Cloud Storage bucket for logs. This process includes setting up IAM permissions to allow the load balancer to write logs to the specified bucket.
  references:
  - https://cloud.google.com/load-balancing/docs/https/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/resources/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.compute.instance.block_project_wide_ssh_keys_disabled
  service: compute
  resource: instance
  requirement: Block Project Wide Ssh Keys Disabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Block Project-Wide SSH Keys Enabled for Instances
  rationale: Disabling project-wide SSH keys prevents unauthorized access to instances by limiting SSH key usage to instance-specific keys only. This reduces the risk of unauthorized access due to compromised keys that might be used across multiple instances, thus enhancing security posture. Ensuring this setting aligns with compliance requirements like PCI-DSS and HIPAA, which mandate strict access controls.
  description: This rule checks whether the 'blockProjectWideSshKeys' setting is enabled for GCP Compute Instances. Enabling this setting ensures that only instance-specific SSH keys are allowed, preventing the use of project-wide keys that could lead to unintended access. To verify, inspect the instance's metadata and ensure the 'blockProjectWideSshKeys' attribute is set to true. Remediation involves updating the instance metadata to enable this setting via the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/compute/docs/oslogin
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/connecting-advanced#project-wide-keys
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 4.3
  - PCI-DSS Requirement 7.1.2
  - ISO 27001 Annex A.9.4.2
- rule_id: gcp.compute.instance.confidential_computing_enabled
  service: compute
  resource: instance
  requirement: Confidential Computing Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Confidential Computing for Compute Instances
  rationale: Enabling Confidential Computing on GCP Compute Instances enhances data protection by encrypting data in use, reducing the risk of unauthorized access to sensitive information during processing. This is crucial for meeting regulatory compliance and safeguarding intellectual property from advanced threats and insider attacks.
  description: This rule checks if Confidential Computing is enabled for GCP Compute Instances, which ensures that data remains encrypted in memory during processing using secure enclaves. To verify, ensure that instances are configured with the Confidential VM option enabled. Remediation involves modifying instance settings to activate Confidential Computing, which can be done via the GCP Console or the CLI.
  references:
  - https://cloud.google.com/confidential-computing/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/nist-special-publication-800-53-revision-5
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/create-start-confidential-vm
- rule_id: gcp.compute.instance.default_service_account_full_access_configured
  service: compute
  resource: instance
  requirement: Default Service Account Full Access Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Avoid Default Service Account Full Access on Compute Instances
  rationale: Using the default service account with full access on compute instances can lead to over-privileged configurations, increasing the risk of unauthorized access and potential data breaches. This can compromise the principle of least privilege, leading to non-compliance with security standards such as CIS, NIST, and ISO 27001, which emphasize minimal access and role-based permissions.
  description: This rule checks whether GCP compute instances are configured to use the default service account with full access. Such a configuration can unintentionally grant more permissions than necessary, making it critical to customize service accounts with the least privilege principle in mind. To remediate, create a custom service account with only the necessary permissions and update the compute instance to use this account instead of the default. Verification can be done through the Google Cloud Console or using the gcloud command-line tool.
  references:
  - https://cloud.google.com/compute/docs/access/service-accounts
  - https://cloud.google.com/iam/docs/service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.compute.instance.default_service_account_in_use_with_full_api_access
  service: compute
  resource: instance
  requirement: Default Service Account In Use With Full API Access
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Avoid Default Service Account with Full API Access in Instances
  rationale: Using the default service account with full API access increases the risk of privilege escalation and unauthorized data access if the account is compromised. It can lead to extensive exposure of cloud resources and sensitive data, undermining compliance with security frameworks and increasing the attack surface for malicious actors.
  description: This rule checks if any Compute Engine instances are configured to use the default service account with full API access. Best practices recommend creating and using custom service accounts with the principle of least privilege applied. To verify, inspect the service account settings of each instance in the GCP Console or via the gcloud CLI, and adjust permissions as necessary by assigning a custom service account with only the necessary IAM roles.
  references:
  - https://cloud.google.com/compute/docs/access/service-accounts#default_service_account
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#service_accounts
- rule_id: gcp.compute.instance.detailed_monitoring_enabled
  service: compute
  resource: instance
  requirement: Detailed Monitoring Enabled
  scope: compute.instance.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Detailed Monitoring on Compute Instances
  rationale: Enabling detailed monitoring on compute instances allows for better visibility into system performance, enabling proactive identification of potential issues. This is crucial for maintaining system reliability and meeting compliance requirements for performance monitoring in frameworks like ISO 27001. Without detailed monitoring, businesses risk undetected performance degradation and potential loss of service availability.
  description: This rule checks whether detailed monitoring is enabled on Google Compute Engine instances. Detailed monitoring provides additional metrics that are crucial for performance analysis and capacity planning. To verify this setting, ensure that the 'monitoring' service is enabled on each instance. Remediation involves enabling the 'Stackdriver Monitoring' agent on the instance, which can be done via the Google Cloud Console or using gcloud commands.
  references:
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/compute/docs/instances
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Section 5.2
  - ISO 27001:2013 - A.12.1.3 Capacity management
  - https://cloud.google.com/monitoring/agent
  - NIST SP 800-53 Rev. 5 - CA-7 Continuous Monitoring
- rule_id: gcp.compute.instance.distributions_logging_enabled
  service: compute
  resource: instance
  requirement: Distributions Logging Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Compute Instances Have Distributions Logging Enabled
  rationale: Enabling distributions logging for compute instances is crucial for monitoring and analyzing the distribution of network traffic and resource usage, which helps in identifying potential security threats and performance bottlenecks. It also supports compliance with regulatory requirements by ensuring that all access and activity logs are captured and reviewed, reducing the risk of undetected breaches.
  description: This rule checks whether distribution logging is enabled for all compute instances, which requires configuring the appropriate logging settings in the GCP Console or using gcloud commands. To enable this, navigate to the 'Logging' section under the instance's settings and ensure that logging is activated. This setup allows for comprehensive visibility into the instance's operations and network traffic, facilitating timely threat detection and incident response.
  references:
  - https://cloud.google.com/logging/docs/setup/manage-logs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/compute/docs/logging
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.instance.dr_testing_execution_roles_least_privilege
  service: compute
  resource: instance
  requirement: DR Testing Execution Roles Least Privilege
  scope: compute.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for DR Testing Execution on Compute Instances
  rationale: Applying the principle of least privilege for disaster recovery (DR) testing roles minimizes the risk of unauthorized access, reducing potential data breaches and disruptions. This is crucial for maintaining business continuity and protecting sensitive data, ensuring compliance with regulations such as NIST and PCI-DSS, which mandate controlled access to critical infrastructure.
  description: This rule checks that roles assigned for DR testing on Compute instances are granted only the permissions necessary to perform their specific tasks, avoiding excessive privileges. Verify that IAM roles do not include permissions beyond what is strictly required for DR testing. To remediate, audit existing roles and adjust permissions to align with the principle of least privilege, using predefined roles or custom roles tailored to specific DR tasks.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/compute/docs/access/iam
  - CIS Google Cloud Platform Foundation Benchmark 1.1.0 - 1.6
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.compute.instance.dr_testing_logs_enabled
  service: compute
  resource: instance
  requirement: DR Testing Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable DR Testing Logs for Compute Instances
  rationale: Enabling disaster recovery (DR) testing logs for compute instances is crucial for ensuring that all DR test activities are recorded and auditable. This helps organizations identify potential issues in their DR plans, improve system resilience, and meet compliance requirements such as PCI-DSS and ISO 27001, which mandate logging and monitoring of DR activities.
  description: This rule checks if logging is enabled for disaster recovery testing activities on GCP Compute Engine instances. To verify, ensure that Stackdriver Logging is configured to capture DR test logs by setting up a log sink for relevant projects. Remediation involves creating a log sink in the GCP Console or via gcloud CLI that targets DR testing activities, ensuring that all DR tests are logged and stored for future analysis.
  references:
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/compute/docs/instances
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/dr-scenarios-planning-guide
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.compute.instance.dr_testing_results_storage_encrypted_and_private
  service: compute
  resource: instance
  requirement: DR Testing Results Storage Encrypted And Private
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DR Test Results Are Encrypted and Access is Restricted
  rationale: Encrypting and restricting access to DR testing results protects sensitive data from unauthorized access and potential breaches. This is crucial for maintaining data integrity and confidentiality, especially as these results may contain sensitive information critical for business continuity plans. Non-compliance with encryption standards can lead to significant financial penalties and damage to reputation.
  description: This rule checks that storage solutions used for Disaster Recovery (DR) testing results within Compute Engine instances are encrypted and have access controls in place. Verify that disks are encrypted using Customer-Managed Encryption Keys (CMEK) and that IAM policies restrict access to authorized personnel only. Remediation involves enabling encryption on all relevant storage resources and reviewing IAM policies to ensure least privilege access.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis#control_4.1
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.instance.encryption_with_csek_enabled
  service: compute
  resource: instance
  requirement: Encryption With Csek Enabled
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enforce Customer-Supplied Encryption Keys for VM Disks
  rationale: Utilizing customer-supplied encryption keys (CSEK) ensures that the data remains under the customer's control, even if Google Cloud Platform experiences a breach. This approach aligns with stringent compliance requirements such as GDPR and PCI-DSS, which mandate robust data protection measures. Without CSEK, sensitive data may be at risk of unauthorized access, leading to significant financial and reputational damage.
  description: This rule checks whether Google Compute Engine VM instances are configured to use customer-supplied encryption keys (CSEK) for disk encryption. To verify, check that the disks associated with the instances are encrypted with keys managed by the customer rather than Google. Remediation involves updating the disk encryption settings to use CSEK, which can be done via the GCP Console, CLI, or API by specifying the CSEK during instance creation or disk attachment.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.instance.env_vars_secret_configured
  service: compute
  resource: instance
  requirement: Env Vars Secret Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Secrets in Environment Variables are Securely Configured
  rationale: Storing sensitive data such as API keys, database credentials, or other secrets in environment variables without proper security measures can lead to unauthorized access and data breaches. This practice can expose critical business operations to cyber threats and compromise compliance with standards like PCI-DSS and HIPAA, which mandate strict controls over sensitive information handling.
  description: This rule checks if sensitive information is stored securely within the environment variables of Google Compute Engine (GCE) instances. It verifies that secrets are encrypted and managed through GCP Secret Manager rather than being stored in plaintext. To remediate, ensure that all secrets are moved to Secret Manager, and access them programmatically within your application using IAM roles and permissions. Follow the principle of least privilege to limit access to these secrets.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-security-rule/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.instance.flow_logs_enabled
  service: compute
  resource: instance
  requirement: Flow Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure VPC Flow Logs are Enabled for Compute Instances
  rationale: Enabling VPC Flow Logs for Google Cloud Compute Instances is crucial for monitoring network traffic, diagnosing network issues, and ensuring compliance with security policies. It provides visibility into traffic patterns and helps detect anomalies, which could indicate potential security threats or misuse. This logging capability supports incident response and forensic investigations, aligning with regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule verifies that VPC Flow Logs are enabled for each Google Cloud Compute Instance. Flow Logs capture IP traffic information flowing to and from network interfaces, which can be enabled by configuring the subnet containing the instance. To verify, access the Google Cloud Console, navigate to the VPC network settings, and ensure Flow Logs are set to 'On' for the respective subnets. Remediation involves enabling Flow Logs in the subnet configuration, specifying log aggregation options such as sampling rate and log format.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/blog/products/gcp/using-vpc-flow-logs-and-bigquery-for-network-analytics
- rule_id: gcp.compute.instance.gke_worker_nodes_firewall_restriction
  service: compute
  resource: instance
  requirement: Gke Worker Nodes Firewall Restriction
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict GKE Worker Nodes with Specific Firewall Rules
  rationale: Unrestricted network access to GKE worker nodes can expose them to potential threats such as unauthorized access, data breaches, and Denial-of-Service (DoS) attacks. Implementing specific firewall rules minimizes the attack surface by allowing only necessary traffic, thereby enhancing the security posture and ensuring compliance with standards like PCI-DSS and ISO 27001.
  description: This rule checks if GKE worker nodes have restrictive firewall rules that limit inbound and outbound traffic to only what is necessary for operations. Verifying this involves inspecting firewall configurations associated with the nodes to ensure they adhere to the principle of least privilege. Remediation includes defining and applying specific firewall rules that only allow required traffic, such as SSH from trusted IPs and inter-node communications, while blocking all other traffic.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_access_to_the_control_plane
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc/docs/firewalls
- rule_id: gcp.compute.instance.group_autoscaling
  service: compute
  resource: instance
  requirement: Group Autoscaling
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instance Groups Use Autoscaling
  rationale: Autoscaling optimizes resource usage and cost by automatically adjusting the number of VM instances in a group based on demand. Not using autoscaling can lead to over-provisioning, increased costs, and potential performance issues during periods of high demand, affecting business continuity and customer satisfaction.
  description: This rule checks if Compute Engine instance groups have autoscaling enabled. Autoscaling must be configured to automatically adjust the number of VM instances in response to workload demands. To verify, check the instance group settings in the Google Cloud Console under 'Compute Engine' > 'Instance groups'. If autoscaling is not enabled, configure it by setting up an autoscaling policy that suits your workload patterns. Consider factors such as target CPU utilization and custom metrics.
  references:
  - https://cloud.google.com/compute/docs/autoscaler
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-operating-your-cloud-environment
- rule_id: gcp.compute.instance.host_sharing_restricted
  service: compute
  resource: instance
  requirement: Host Sharing Restricted
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict Shared Host Usage for Compute Instances
  rationale: Restricting host sharing for compute instances minimizes the risk of cross-tenant data leakage and potential side-channel attacks. It enhances data protection and isolation, which is critical for maintaining confidentiality and integrity of sensitive workloads. This practice is particularly important for compliance with data protection regulations such as GDPR and HIPAA, which mandate strict data isolation controls.
  description: This rule checks whether Google Cloud Compute Instances are configured to restrict host sharing by using sole-tenant nodes. Sole-tenant nodes ensure that your instances do not share physical hardware with instances from other projects, reducing the risk of resource contention and vulnerabilities. To verify this, review your instance settings in the GCP Console or via the gcloud CLI and configure instances to utilize sole-tenant nodes if necessary. Remediation involves updating instance settings to select a specific node group dedicated to your project's instances.
  references:
  - https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/instances/
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.compute.instance.https_load_balancer_configured
  service: compute
  resource: instance
  requirement: HTTPS Load Balancer Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure HTTPS Load Balancer Configured for Compute Instances
  rationale: Configuring an HTTPS Load Balancer for compute instances ensures that data transmitted over the network is encrypted, protecting against interception and man-in-the-middle attacks. This is crucial for maintaining data confidentiality and integrity, especially for sensitive information. Additionally, using HTTPS aligns with compliance requirements such as PCI-DSS and HIPAA, reducing legal and financial risks.
  description: This rule verifies that compute instances are behind an HTTPS Load Balancer, which ensures all traffic is encrypted using TLS. To check this configuration, ensure that the target instances are part of a backend service linked to an HTTPS forwarding rule. Remediation involves setting up an HTTPS Load Balancer with a valid SSL certificate and associating it with the relevant backend services and instances. This configuration not only secures data in transit but also improves application availability and scalability.
  references:
  - https://cloud.google.com/load-balancing/docs/https
  - https://cloud.google.com/security/compliance/pci-dss
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/security-foundations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.compute.instance.instance_auto_placement_controlled
  service: compute
  resource: instance
  requirement: Instance Auto Placement Controlled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Control Instance Auto-Placement for Managed Deployments
  rationale: Controlling the auto-placement of compute instances is vital to ensure workloads are deployed in specific zones or regions, which can help meet data residency requirements, optimize resource utilization, and reduce latency. Uncontrolled placement may lead to compliance violations and inefficient resource allocation, impacting performance and incurring unnecessary costs.
  description: This rule checks that Google Cloud Compute Engine instances are configured with controlled auto-placement settings. Specifically, instances should have constraints set to deploy in designated zones or regions based on organizational policies. Verify by reviewing instance configurations in the Google Cloud Console or using gcloud commands. Remediation involves setting the desired zone or region during instance creation or updating existing instances to adhere to placement policies.
  references:
  - https://cloud.google.com/compute/docs/instances/creating-instance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - CIS Google Cloud Platform Foundation Benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/compute/docs/regions-zones
- rule_id: gcp.compute.instance.instance_multi_az
  service: compute
  resource: instance
  requirement: Instance Multi Az
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances are Configured for Multi-Region Availability
  rationale: Deploying compute instances across multiple zones enhances fault tolerance and business continuity by mitigating the risk of a single-zone failure. This configuration is crucial for maintaining service availability and meeting compliance with standards that require high availability and disaster recovery solutions.
  description: This rule checks if compute instances are configured to run in multiple zones within a region, ensuring redundancy and reliability. Instances should be part of managed instance groups with multi-zonal configurations. Verify this by reviewing instance group settings in the GCP Console or via CLI, ensuring that the 'distributionPolicy.zones' setting includes multiple zones. Remediation involves updating instance groups to include multiple zones or creating new multi-zonal configurations.
  references:
  - https://cloud.google.com/compute/docs/regions-zones
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/best-practices-for-building-fault-tolerant-applications
- rule_id: gcp.compute.instance.instance_older_than_specific_days
  service: compute
  resource: instance
  requirement: Instance Older Than Specific Days
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Identify and Manage Aging Compute Instances
  rationale: Compute instances that have been running longer than necessary may be using outdated software, posing security risks due to unpatched vulnerabilities. This can lead to increased exposure to exploitation and non-compliance with security policies that mandate regular instance updates. Regularly reviewing and updating such instances ensures alignment with security best practices and regulatory requirements.
  description: This rule checks for Google Cloud Compute instances that have been operational beyond a specified number of days. Instances that exceed this threshold should be reviewed for software updates, configuration changes, or decommissioning if no longer needed. Administrators can use the Google Cloud Console or CLI to list instances, assess their creation dates, and take appropriate remediation steps such as updating the instance or replacing it with a newer, patched version.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-securing-your-instances
- rule_id: gcp.compute.instance.inventory_api_enabled
  service: compute
  resource: instance
  requirement: Inventory API Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Inventory API is Enabled on Compute Instances
  rationale: Enabling the Inventory API on compute instances allows for better asset management and monitoring, helping to identify and mitigate vulnerabilities. This is crucial for maintaining compliance with industry standards like ISO 27001 and ensuring that only approved software is installed, reducing the risk of unauthorized or outdated configurations.
  description: This rule checks if the Inventory API is enabled on Google Compute Engine instances, which is essential for collecting data about installed packages and OS configurations. To verify this setting, access the Compute Engine API in the GCP Console and ensure the Inventory API is activated. If it is not enabled, navigate to the API & Services section, find the Inventory API, and activate it to ensure comprehensive instance monitoring and compliance with security policies.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/compute/docs/instances/viewing-managing-inventory
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.cisecurity.org/cis-benchmarks/
  - https://cloud.google.com/security/overview/whitepapers
- rule_id: gcp.compute.instance.ip_forwarding_configured
  service: compute
  resource: instance
  requirement: Ip Forwarding Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure IP Forwarding is Properly Configured for Instances
  rationale: Improper configuration of IP forwarding on instances can lead to unauthorized data transit, making the network susceptible to man-in-the-middle attacks and data exfiltration. Ensuring IP forwarding is correctly set aligns with security best practices and helps maintain compliance with industry standards such as PCI-DSS and NIST, which require stringent control over network traffic.
  description: This check verifies whether IP forwarding is enabled on Google Compute Engine instances, which may be necessary for specific network configurations such as NAT or VPN gateways. However, it should be disabled for instances not intended to forward packets to prevent unauthorized routing. To verify, review the 'canIpForward' setting in the instance configuration. Remediation involves disabling IP forwarding for non-gateway instances by setting 'canIpForward' to false in the instance's properties.
  references:
  - https://cloud.google.com/compute/docs/troubleshooting/general-tips#ipforwarding
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/create-start-instance
- rule_id: gcp.compute.instance.ip_forwarding_is_enabled
  service: compute
  resource: instance
  requirement: Ip Forwarding Is Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable IP Forwarding for Compute Instances
  rationale: IP forwarding is crucial for routing network traffic beyond the local instance, enabling it for specific instances can enhance network management but poses a security risk if enabled unnecessarily. Improper configuration can expose the instance to unauthorized access, leading to data breaches or network misuse. Compliance with network security standards requires careful control of IP forwarding settings.
  description: This rule checks if IP forwarding is enabled on Google Compute Engine instances, which should only be activated for instances acting as gateways or routers. Verification involves reviewing instance configurations to ensure IP forwarding is only enabled where necessary. Remediation includes disabling IP forwarding on instances where it is not required, thereby reducing attack surfaces and improving security posture.
  references:
  - https://cloud.google.com/compute/docs/networking
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.key_pair_disable_unused_keys
  service: compute
  resource: instance
  requirement: Key Pair Disable Unused Keys
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Disable Unused SSH Keys on Compute Instances
  rationale: Unused SSH keys on compute instances pose a significant security risk as they may allow unauthorized access if compromised. This is particularly critical in environments where key rotation and user deactivation are not consistently managed. Disabling unused keys reduces the attack surface and helps maintain compliance with security standards such as CIS benchmarks and regulatory frameworks like PCI-DSS and ISO 27001.
  description: This rule checks for and disables any SSH keys on GCP Compute Engine instances that have not been used for a predefined period, indicating potential obsolescence or abandonment. To verify, review the instance metadata for SSH keys and cross-reference with access logs to determine usage patterns. Remediation involves removing or disabling keys that have not been accessed within the organization's key management policy timeframe. This can be automated using IAM policies and regular audits of access logs.
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.compute.instance.key_pair_max_age_days
  service: compute
  resource: instance
  requirement: Key Pair Max Age Days
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Key Pair Age for Compute Instances is Limited
  rationale: Limiting the age of key pairs for compute instances is crucial to reduce the risk of key compromise. Older keys are more susceptible to being discovered and misused by attackers, potentially leading to unauthorized access. Regularly rotating key pairs aligns with compliance requirements and mitigates threats such as brute force attacks and credential theft.
  description: This rule checks that the key pairs used for Google Compute Engine instances do not exceed a specified maximum age, typically set to 90 days. To verify, audit the SSH key metadata associated with instances and ensure they are rotated within the defined period. Remediation involves updating the key pairs by generating new keys and replacing the old ones, ensuring minimal disruption and maintaining access control integrity.
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://cloud.google.com/security-compliance/cis-gcp-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
- rule_id: gcp.compute.instance.loadbalancer_logging_enabled
  service: compute
  resource: instance
  requirement: Loadbalancer Logging Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Load Balancer Logging is Enabled for Compute Instances
  rationale: Enabling load balancer logging for compute instances is crucial for maintaining a comprehensive audit trail of network activity, which aids in identifying and mitigating potential security threats. This capability supports forensic analysis, performance monitoring, and can help meet compliance requirements related to data protection and incident response. Without logging, organizations might miss critical insights into traffic patterns and potential anomalies, posing a risk to both security and operational efficiency.
  description: This rule verifies that load balancer logging is enabled for all Google Cloud compute instances, ensuring that all incoming and outgoing network requests are logged. To verify, navigate to the Google Cloud Console, select 'Network services', then 'Load balancing', and check that 'Enable logging' is turned on for each load balancer associated with your instances. If not enabled, configure each load balancer's logging settings to ensure all traffic is logged and stored in Cloud Logging for review and analysis.
  references:
  - https://cloud.google.com/load-balancing/docs/audit-logs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.instance.managed_by_os_config
  service: compute
  resource: instance
  requirement: Managed By OS Config
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances are Managed by OS Config
  rationale: Managing compute instances with OS Config ensures centralized control over configuration, compliance, and updates, reducing the risk of security vulnerabilities due to misconfiguration or outdated software. It provides a consistent approach to enforcing security policies and can help meet compliance requirements by ensuring instances are aligned with organizational security standards.
  description: This rule checks if GCP Compute Engine instances are managed by OS Config, which facilitates automated patch management, configuration compliance, and software management. To verify, ensure that the OS Config agent is installed and configured on each instance. Remediation involves installing the OS Config agent and enabling the necessary API permissions for the project to allow management through OS Config.
  references:
  - https://cloud.google.com/compute/docs/os-config-management
  - https://cloud.google.com/compute/docs/instances/os-config
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/osconfig-agent
- rule_id: gcp.compute.instance.managed_by_ssm
  service: compute
  resource: instance
  requirement: Managed By Ssm
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances Managed by Systems Manager
  rationale: Managing compute instances with a centralized service like AWS Systems Manager (SSM) enhances operational efficiency and security by enabling automated patching, configuration management, and resource monitoring. This mitigates risks associated with manual instance management, reducing the likelihood of configuration drift and non-compliance with security policies. It also aids in achieving compliance with frameworks that require demonstrable controls over compute resources.
  description: This rule checks if Google Cloud compute instances are managed by AWS Systems Manager, ensuring that they are configured for automated management tasks. Verify that the SSM agent is installed and properly configured on each instance, and that instances are registered with the Systems Manager service. To remediate, install the SSM agent on unmanaged instances and configure them to communicate with the Systems Manager, allowing for automated updates and configuration management.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.metadata_block_project_ssh_keys_enabled
  service: compute
  resource: instance
  requirement: Metadata Block Project Ssh Keys Enabled
  scope: compute.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Instance Metadata Blocks Project SSH Keys
  rationale: Allowing project-wide SSH keys can lead to unauthorized access if those keys are compromised, posing a significant risk to data integrity and confidentiality. Disabling project SSH keys at the instance level minimizes the attack surface and aligns with the principle of least privilege, thus enhancing the security of sensitive data and meeting compliance requirements such as PCI-DSS and HIPAA.
  description: 'This rule checks that the metadata key ''block-project-ssh-keys'' is set to true for each GCP compute instance, ensuring that project-wide SSH keys are not used. This configuration prevents users with project-level access from SSH-ing into instances, thus limiting access to those explicitly granted instance-level credentials. To verify this setting, review the instance metadata in the GCP Console or use the gcloud command-line tool. Remediation involves updating the instance metadata to include ''block-project-ssh-keys'': ''true''.'
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys#block-project-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/instances/metadata-protection
- rule_id: gcp.compute.instance.metadata_concealment_enabled
  service: compute
  resource: instance
  requirement: Metadata Concealment Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Metadata Concealment for Compute Instances
  rationale: Enabling metadata concealment for compute instances is crucial to prevent unauthorized access to sensitive data such as service account tokens and instance metadata, which can be exploited by attackers to escalate privileges or perform lateral movements. This configuration helps mitigate risks associated with data exposure and meets compliance requirements outlined in frameworks like NIST SP 800-53 and ISO 27001, ensuring robust protection against metadata-related vulnerabilities.
  description: This rule checks if metadata concealment is enabled for GCP Compute Engine instances, which prevents direct access to instance metadata from within the virtual machine. To verify, inspect the instance's metadata server configuration and ensure the 'enable-oslogin' metadata key is set to 'true'. Remediation involves configuring the instance's metadata server to enable concealment, which can be done via the Google Cloud Console, CLI, or API by updating instance settings to enable 'enable-oslogin'.
  references:
  - https://cloud.google.com/compute/docs/instances/access-advanced-configurations
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/metadata/overview
  - https://cloud.google.com/compute/docs/security
- rule_id: gcp.compute.instance.network_alerts_for_anomalies_configured
  service: compute
  resource: instance
  requirement: Network Alerts For Anomalies Configured
  scope: compute.instance.network_security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Enable Network Anomaly Alerts on GCP Compute Instances
  rationale: Configuring network alerts for anomalies on GCP Compute instances is critical for timely detection and response to potential security incidents such as unauthorized access or data exfiltration. Network anomalies can indicate compromised instances, leading to data breaches, financial loss, and non-compliance with regulations like PCI-DSS and HIPAA. Proactively monitoring network activity helps mitigate these risks and ensures business continuity.
  description: This rule checks whether network anomaly alerts are configured for GCP Compute instances. It ensures that Stackdriver Monitoring and Logging are set up to detect unusual network patterns, such as atypical traffic spikes or unexpected outbound connections. To verify, ensure that alerting policies are active in Stackdriver with conditions set for anomaly detection based on historical data. Remediation involves configuring Stackdriver to monitor network metrics and setting up alerts that notify security teams of any detected anomalies.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.network_eni_security_groups_present
  service: compute
  resource: instance
  requirement: Network Eni Security Groups Present
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network ENI Security Groups Are Configured for Instances
  rationale: Configuring security groups for network interfaces ensures that only authorized traffic can reach your instances, reducing the risk of unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, and for complying with regulations such as GDPR and PCI-DSS, which mandate strict access controls.
  description: This rule checks if each network interface (ENI) associated with a GCP Compute Engine instance has an appropriate security group configured. Security groups act as virtual firewalls, controlling inbound and outbound traffic to instances. To verify, inspect the instance's network interface settings in the GCP Console and ensure security groups are applied. Remediation involves assigning appropriate security groups to network interfaces to enforce the principle of least privilege.
  references:
  - https://cloud.google.com/vpc/docs/using-firewalls
  - CIS Google Cloud Platform Foundation Benchmark v1.0.0
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.instance.network_eni_security_groups_restrictive
  service: compute
  resource: instance
  requirement: Network Eni Security Groups Restrictive
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network ENI Security Groups are Restrictive
  rationale: Restrictive security group configurations minimize the attack surface by limiting unnecessary access to instances, reducing the risk of unauthorized access or data breaches. Properly configured security groups help comply with regulations such as PCI-DSS and HIPAA, which mandate robust network access controls.
  description: This rule checks that security groups associated with network interfaces (ENIs) of compute instances enforce restrictive ingress and egress rules. Specifically, it requires that only necessary ports and IP addresses are allowed. To verify, review security group rules for overly permissive settings, such as open ports to any IP. Remediation involves updating security group rules to follow the principle of least privilege, ensuring only trusted IPs can access necessary services.
  references:
  - https://cloud.google.com/vpc/docs/using-firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.instance.network_flow_logs_enabled
  service: compute
  resource: instance
  requirement: Network Flow Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Network Flow Logs for Compute Instances
  rationale: Enabling network flow logs for compute instances is crucial for monitoring and analyzing traffic patterns, which helps in identifying unusual activities that might indicate security threats. It also aids in meeting compliance requirements by providing visibility into network operations, supporting audits, and forensic investigations.
  description: This rule checks if network flow logging is enabled for Google Cloud Compute instances. Network flow logs capture metadata about the IP traffic going to and from VM instances, which is vital for security analysis and operational oversight. To enable, configure VPC Flow Logs for the relevant subnets in each VPC network. This can be done via the Google Cloud Console or gcloud CLI by setting the flow logs configuration to 'enabled'.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.no_public_ip
  service: compute
  resource: instance
  requirement: No Public Ip
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure Instances Do Not Have Public IP Addresses
  rationale: Exposing instances to the public internet significantly increases the risk of unauthorized access, data breaches, and exploitation by attackers. Instances with public IPs are more susceptible to attacks such as brute force, DDoS, and unauthorized data extraction. Ensuring instances remain private helps comply with regulations like PCI-DSS and HIPAA, which mandate strict control over network access to sensitive data.
  description: This rule checks if any compute instance is assigned a public IP address. Instances should be configured to use private IPs and access the internet through secure channels like Cloud NAT or VPN. To verify, review instance network interfaces for public IP associations. Remediation involves removing public IPs and utilizing VPNs or private connectivity options for necessary internet access.
  references:
  - https://cloud.google.com/compute/docs/instances/instance-config#public_ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/nat/docs/overview
  - https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.instance.older_than_specific_days
  service: compute
  resource: instance
  requirement: Older Than Specific Days
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances Are Not Older Than Specific Days
  rationale: Regularly updating and replacing older compute instances helps maintain security by ensuring that instances benefit from the latest security patches and updates. Older instances may have vulnerabilities that could be exploited by attackers, potentially leading to unauthorized access or data breaches. Maintaining a policy on instance age can also support compliance with standards that require up-to-date security measures.
  description: This rule checks for Google Cloud compute instances that have been running longer than a specified number of days, which could indicate outdated software or unpatched vulnerabilities. To verify compliance, review the creation date of instances and compare against the organizationâ€™s policy for instance lifecycle management. Instances identified as older than the specified threshold should be evaluated for updating or decommissioning. Implementing automated lifecycle management tools can assist in mitigating risks associated with outdated instances.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_services
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/compute/docs/instances/instance-life-cycle
- rule_id: gcp.compute.instance.os_config_compliance_configured
  service: compute
  resource: instance
  requirement: OS Config Compliance Configured
  scope: compute.instance.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure OS Config Compliance on GCP Compute Instances
  rationale: Configuring OS Config Compliance on instances is vital to maintaining a secure and up-to-date cloud environment. It helps mitigate security risks by ensuring that operating systems are patched, configurations are compliant with security policies, and any deviation is promptly addressed. This is crucial for meeting regulatory requirements like PCI-DSS and SOC2, which mandate regular updates and configuration management.
  description: This rule checks if OS Config Compliance is configured on GCP Compute Instances, which involves setting up the OS Config agent to manage patch compliance, configuration compliance, and software inventory. To verify, ensure the OS Config API is enabled and the agent is installed and running on each instance. Remediation involves enabling the OS Config API, installing the agent, and configuring it to report compliance status. This ensures that instances adhere to organizational security policies and compliance frameworks.
  references:
  - https://cloud.google.com/compute/docs/osconfig
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances
  - https://www.pcisecuritystandards.org/
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
- rule_id: gcp.compute.instance.paas_disk_encrypted
  service: compute
  resource: instance
  requirement: Paas Disk Encrypted
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Engine Disks Are Encrypted
  rationale: Encrypting disks at rest is critical to protect sensitive data from unauthorized access and potential data breaches. It mitigates risks associated with data exposure if the disk is compromised, aligning with compliance requirements such as PCI-DSS and HIPAA that mandate encryption of sensitive information.
  description: This rule verifies that all persistent disks attached to Google Compute Engine instances have encryption enabled. By default, GCP encrypts data at rest using Google-managed keys, but customers can use Customer-Managed Encryption Keys (CMEK) for additional control. To ensure compliance, verify that CMEK is applied where necessary and review the encryption status of disks through the GCP Console or CLI. If encryption is not enabled, configure CMEK by creating a key in Cloud KMS and associating it with the disks.
  references:
  - https://cloud.google.com/compute/docs/disks#encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.compute.instance.paas_no_public_ip_unless_required
  service: compute
  resource: instance
  requirement: Paas No Public Ip Unless Required
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Prevent Unnecessary Public IPs on PaaS Compute Instances
  rationale: Public IPs on PaaS compute instances increase the risk of unauthorized access and potential data breaches, exposing critical workloads to the internet. This can lead to compliance violations with regulations such as PCI-DSS and HIPAA, and increase the attack surface for cyber threats. Ensuring instances do not have public IPs unless necessary mitigates these risks and supports secure architecture practices.
  description: This rule checks for PaaS compute instances assigned with public IP addresses without explicit justification. Instances should be configured to use internal IPs unless access from the internet is essential and properly secured. To remediate, review network settings in the Google Cloud Console or via the CLI, removing public IPs where not needed, and consider using Cloud NAT for outbound internet access without exposing instances directly.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/nat/docs/overview
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://www.hipaajournal.com/hipaa-compliance-google-cloud-platform/
- rule_id: gcp.compute.instance.paas_ssh_password_auth_disabled
  service: compute
  resource: instance
  requirement: Paas Ssh Password Auth Disabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable SSH Password Authentication on Compute Instances
  rationale: Disabling SSH password authentication reduces the risk of unauthorized access by preventing brute force attacks that exploit weak passwords. This practice enhances security by enforcing key-based authentication, which is significantly more secure and aligns with compliance requirements like PCI-DSS and ISO 27001, protecting sensitive data and maintaining customer trust.
  description: This rule checks whether SSH password authentication is disabled on Google Compute Engine instances. To verify, examine the SSH configuration file to ensure 'PasswordAuthentication no' is set. Remediation involves updating the SSH configuration and restarting the SSH service to apply changes, thereby enforcing the use of SSH keys for authentication. This step is crucial for minimizing attack vectors on cloud resources.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-to-instance
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/compute/docs/instances/setting-instance-access
  - https://csrc.nist.gov/publications/detail/sp/800-123/final
- rule_id: gcp.compute.instance.patch_compliance_configured
  service: compute
  resource: instance
  requirement: Patch Compliance Configured
  scope: compute.instance.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Compliance Configured for Compute Instances
  rationale: Unpatched virtual machine instances can expose your cloud environment to security vulnerabilities, leading to potential data breaches, service disruptions, or unauthorized access. This could result in significant business impact, including financial losses and damage to reputation. Ensuring patch compliance is also a critical requirement for meeting various regulatory standards such as PCI-DSS and ISO 27001.
  description: This rule checks if patch compliance is configured for GCP Compute Engine instances. It verifies that instances are set to automatically apply security patches, ensuring they are protected against known vulnerabilities. To configure patch compliance, enable 'OS patch management' in the Google Cloud Console and ensure that the 'Automatic patching' option is set to 'Enabled'. Remediation involves auditing your instance configurations and adjusting them to meet compliance requirements.
  references:
  - https://cloud.google.com/compute/docs/instances/create-start-instance
  - https://cloud.google.com/compute/docs/instance-templates#os-patch-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security-privacy/compliance/
  - https://cloud.google.com/compute/docs/instance-groups/autohealing-instances
- rule_id: gcp.compute.instance.profile_attached
  service: compute
  resource: instance
  requirement: Profile Attached
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Verify Compute Instances Have Service Accounts Attached
  rationale: Attaching a service account to a Compute Engine instance is essential for controlling permissions and access to other Google Cloud services. Without a service account, instances may operate with excessive privileges or lack necessary access, increasing the risk of unauthorized actions or service disruptions. Adhering to this practice supports compliance with security frameworks by ensuring least privilege access.
  description: This rule checks whether all Compute Engine instances have a service account profile attached. Instances without a service account lack the ability to access Google Cloud APIs securely, potentially leading to security vulnerabilities. To verify, inspect the 'serviceAccounts' property of the instance configuration. If missing, attach an appropriate service account with minimal permissions required for the instance's operations. This can be done via the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances
  - https://cloud.google.com/iam/docs/service-accounts
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, 5.1
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.project_os_login_enabled
  service: compute
  resource: instance
  requirement: Project OS Login Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure OS Login is Enabled for Compute Instances
  rationale: Enabling OS Login enhances security by centralizing identity management and providing consistent access controls across instances. This reduces the risk of unauthorized access and simplifies compliance with regulatory frameworks such as PCI-DSS and ISO 27001, which require robust identity and access management practices. It also mitigates the threat of compromised SSH keys by eliminating the need for local key management.
  description: 'This rule checks whether OS Login is enabled at the project level for Google Compute Engine instances. OS Login uses IAM roles to manage SSH access, replacing traditional SSH keys with Google-managed keys, thereby improving security and auditability. To verify, check if the metadata ''enable-oslogin'' is set to ''TRUE'' at the project level. Remediation involves enabling OS Login via the Google Cloud Console or using the gcloud command-line tool: `gcloud compute project-info add-metadata --metadata enable-oslogin=TRUE`.'
  references:
  - https://cloud.google.com/compute/docs/oslogin
  - https://cloud.google.com/compute/docs/instances/enable-oslogin
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.compute.instance.project_source_repo_url_no_sensitive_credentials_g_variables
  service: compute
  resource: instance
  requirement: Project Source Repo Url No Sensitive Credentials G Variables
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure No Sensitive Credentials in GCP Project Source Repo URLs
  rationale: Embedding sensitive credentials in source repository URLs can lead to unauthorized access and data breaches, significantly impacting business operations and client trust. Such exposure can also result in non-compliance with regulatory frameworks like PCI-DSS, HIPAA, and SOC2, leading to potential legal penalties and reputational damage.
  description: This rule checks for the presence of sensitive credentials within environment variables that reference project source repository URLs in GCP Compute Engine instances. It ensures that no sensitive information, such as API keys or passwords, is included in URLs. To verify, review instance metadata and environment variables, and remove any credentials embedded in URLs. Instead, use IAM roles and service accounts for secure authentication.
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/architecture/best-practices-for-using-service-accounts
- rule_id: gcp.compute.instance.protection
  service: compute
  resource: instance
  requirement: Protection
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances Have Shielded VM Enabled
  rationale: Enabling Shielded VMs helps protect your instances from rootkits, bootkits, and other persistent threats by verifying the integrity of the VM's boot process. Without this protection, instances are vulnerable to advanced attacks that can lead to data breaches, compliance violations, and operational disruptions.
  description: This rule checks that all Google Compute Engine instances have Shielded VM features enabled. Shielded VMs provide verifiable integrity of the boot process and help prevent and detect unauthorized changes. To verify, ensure 'vulnerabilityProtection' is enabled in the instance's configuration. Remediate by configuring existing instances to use Shielded VM features via the GCP Console or CLI, and configure new instances with Shielded VM settings from the start.
  references:
  - https://cloud.google.com/shielded-vm/docs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/compute/docs/instances/enable-shielded-vm
- rule_id: gcp.compute.instance.public_ip
  service: compute
  resource: instance
  requirement: Public Ip
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure Compute Instances Do Not Have Public IP Addresses
  rationale: Exposing compute instances to the public internet can significantly increase the attack surface, making them vulnerable to unauthorized access, data breaches, and distributed denial-of-service (DDoS) attacks. This exposure can lead to compliance violations under standards such as PCI-DSS and GDPR, which require stringent data protection measures. Limiting public access helps safeguard sensitive data and maintain service availability.
  description: This rule checks for Google Cloud Compute instances configured with public IP addresses. Instances with public IPs are accessible from the internet, posing potential security risks. To verify, inspect the network interface configurations for each instance and ensure no public IPs are assigned. Remediation involves either removing the public IP or replacing it with a private IP and using a VPN or Cloud NAT for necessary external communications.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-advanced#no-public-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/nat/docs/overview
  - https://cloud.google.com/vpc/docs/using-vpn
- rule_id: gcp.compute.instance.resilience_dr_approvals_required_for_changes
  service: compute
  resource: instance
  requirement: Resilience DR Approvals Required For Changes
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure DR Approval for Compute Instance Changes
  rationale: Requiring Disaster Recovery (DR) approvals for changes to compute instances ensures that modifications are assessed for their impact on system resilience and business continuity. This process minimizes the risk of unauthorized changes that could lead to downtime or data loss, which is critical for maintaining service availability and meeting compliance requirements such as ISO 27001 and SOC 2.
  description: This rule verifies that any configuration changes to GCP compute instances, such as network settings or instance types, require formal approval from designated DR personnel. Organizations should implement a change management process where all changes are logged and approved by relevant stakeholders. To remediate, configure a Cloud Function triggered by Cloud Audit Logs that enforces approval workflows for instance modifications.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://cloud.google.com/architecture/dr-scenarios-planning-guide
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.resilience_dr_change_audit_logging_enabled
  service: compute
  resource: instance
  requirement: Resilience DR Change Audit Logging Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Audit Logging for DR Changes on Compute Instances
  rationale: Enabling audit logging for disaster recovery (DR) changes on compute instances is crucial to track and understand changes that could impact system resilience. Without logging, unauthorized or accidental modifications may go unnoticed, potentially leading to increased downtime or data loss. This practice supports compliance with regulations such as GDPR and assists in forensic investigations.
  description: This rule checks whether audit logging is enabled for changes related to disaster recovery on GCP Compute Engine instances. To verify, ensure that the necessary audit logs are enabled in Google Cloud's IAM & Admin under Audit Logs for Compute Engine. Remediation involves configuring the logging settings to include 'ADMIN_READ' and 'DATA_WRITE' log types for the 'compute.googleapis.com' service, thus capturing relevant DR configuration changes.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/audit-logging
  - https://cloud.google.com/iam/docs/auditing
- rule_id: gcp.compute.instance.resilience_dr_logs_enabled
  service: compute
  resource: instance
  requirement: Resilience DR Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Compute Instance Resilience DR Logs are Enabled
  rationale: Enabling resilience disaster recovery (DR) logging for compute instances is crucial for maintaining business continuity during unforeseen events. These logs provide critical insights into system performance and data recovery processes, helping to mitigate risks such as data loss or extended downtime. Additionally, they support compliance with industry regulations that require logging and monitoring of disaster recovery activities.
  description: This rule checks if resilience disaster recovery logs are enabled on GCP Compute Engine instances. To verify, ensure that the Compute Engine instances are configured to send logs to Google Cloud Logging by enabling the 'logs' field in the VM instance's metadata or by using the --logging flag during instance creation. Remediation involves updating instance configurations to include logging settings that capture DR activities, which can be achieved through the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/logging/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/logging
  - https://cloud.google.com/architecture/dr-scenarios
- rule_id: gcp.compute.instance.resilience_dr_network_private_only
  service: compute
  resource: instance
  requirement: Resilience DR Network Private Only
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Compute Instances Use Private Networks for DR Resilience
  rationale: Limiting compute instance network interfaces to private networks enhances security by reducing exposure to external threats, thereby supporting business continuity through resilient disaster recovery (DR) strategies. This practice mitigates risks such as data breaches and unauthorized access, and aligns with compliance requirements that mandate controlled and limited network access.
  description: This rule checks that all Google Cloud Compute instances configured for disaster recovery (DR) are connected only to private networks. Instances should not have public IP addresses to avoid exposure to the internet. To verify, inspect the network interfaces of your instances to confirm they are attached only to private subnets. Remediation involves configuring instances without public IPs and ensuring they are associated with VPC networks configured for private connectivity.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#networkinterfaces
  - https://cloud.google.com/security/compliance/cis#section_4.0
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/vpc/docs/vpc
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.resilience_dr_roles_least_privilege
  service: compute
  resource: instance
  requirement: Resilience DR Roles Least Privilege
  scope: compute.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Compute Instance DR Roles
  rationale: Implementing least privilege for Disaster Recovery (DR) roles on GCP Compute Instances minimizes the risk of unauthorized access and data breaches. Overprivileged accounts can lead to misuse or exploitation during an incident, increasing the potential for data loss and non-compliance with regulations such as PCI-DSS and ISO 27001. Least privilege helps ensure that only necessary permissions are granted, supporting both security and compliance objectives.
  description: This rule checks if DR roles associated with compute instances are granted only the permissions essential for their function, avoiding excessive privileges. Verify role assignments by reviewing IAM policies and ensure that roles like 'Compute Instance Admin' are not overly broad. Remediation involves auditing roles with 'gcloud' commands and refining IAM policies to adhere strictly to the principle of least privilege, assigning custom roles if necessary.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/compute/docs/access/
- rule_id: gcp.compute.instance.resilience_dr_source_agent_to_service_tls_required
  service: compute
  resource: instance
  requirement: Resilience DR Source Agent To Service TLS Required
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enforce TLS for Resilience DR Source Agent Communications
  rationale: Requiring TLS for communications between the Resilience DR source agent and service ensures data integrity and confidentiality during transmission. Without TLS, sensitive data could be exposed to interception or tampering, posing a risk to operational continuity and potentially leading to non-compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks if TLS encryption is enforced for communications between the Resilience DR source agent and its corresponding service on GCP instances. To verify, ensure that all network traffic between these components uses TLS by configuring the agent and service endpoints to support HTTPS connections. Remediation involves updating the instance configurations and network policies to mandate TLS usage for these communications.
  references:
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/compute/docs/instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-95.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.resilience_dr_source_encryption_at_rest_cmek_where_supported
  service: compute
  resource: instance
  requirement: Resilience DR Source Encryption At Rest Cmek Where Supported
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure CMEK for Resilience DR Source Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encrypting data at rest in Compute Engine enhances security by giving you control over key management. This reduces the risk of unauthorized access and data breaches, and helps meet compliance requirements such as GDPR and HIPAA by ensuring data is protected with user-managed keys.
  description: This rule checks if Compute Engine instances used as sources for disaster recovery (DR) have their data encrypted at rest using CMEK where supported. Verify by ensuring the encryption key settings of the instances are configured to use user-managed keys. Remediate by updating the instance settings to specify a CMEK in the relevant Google Cloud Key Management Service (KMS) key ring.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.instance.resilience_dr_source_min_telemtry_required_no_pii
  service: compute
  resource: instance
  requirement: Resilience DR Source Min Telemtry Required No Pii
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Non-PII Telemetry for Compute Instance Resilience
  rationale: Ensuring that telemetry data from compute instances used for disaster recovery (DR) is devoid of personally identifiable information (PII) mitigates privacy risks and data breaches. This practice supports compliance with privacy regulations such as GDPR, enhances data protection, and reduces the risk of sensitive data exposure in DR scenarios.
  description: This rule verifies that compute instances configured for resilience and disaster recovery in GCP do not include PII in their telemetry data. It checks the telemetry configuration to ensure data minimization principles are followed. Administrators should review and configure telemetry settings to exclude PII, using GCP's logging and monitoring services to filter and control data flow. Remediation involves auditing telemetry configurations and applying filters or exclusions to sensitive data fields.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/monitoring/docs
- rule_id: gcp.compute.instance.resilience_dr_storage_encrypted_and_private
  service: compute
  resource: instance
  requirement: Resilience DR Storage Encrypted And Private
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Instance DR Storage is Encrypted and Private
  rationale: Encryption of data at rest is crucial to protect sensitive information from unauthorized access, especially in disaster recovery scenarios. Unencrypted or publicly accessible storage can lead to data breaches, compromising confidentiality and integrity. Compliance with standards such as PCI-DSS and HIPAA often mandates encryption to safeguard data and maintain trust.
  description: This rule checks that all disaster recovery storage associated with compute instances in GCP is encrypted and not publicly accessible. Ensure that Compute Engine instances use customer-managed encryption keys (CMEK) or Google-managed keys for all attached disks and that these disks are not shared with public IPs or unauthorized networks. Remediation involves configuring disk encryption settings in the GCP Console or using the gcloud CLI, and setting firewall rules to restrict access.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/
- rule_id: gcp.compute.instance.resilience_private_network_only
  service: compute
  resource: instance
  requirement: Resilience Private Network Only
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Compute Instances Use Private Networks for Resilience
  rationale: Restricting compute instances to private networks reduces exposure to potential external threats and minimizes attack surfaces. This isolation is crucial for protecting sensitive data and maintaining service availability, particularly for systems handling critical workloads or sensitive information. Compliance with regulatory standards often mandates network segmentation to safeguard data integrity and confidentiality.
  description: This rule verifies that all Google Compute Engine instances are configured to operate solely on private networks. Instances should not have external IP addresses, ensuring they are not accessible from the internet directly. To remediate, review your instances' network interface settings and ensure they are aligned with private network configurations. Utilize VPC firewall rules to further control instance access and maintain strong network segmentation.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/compute/docs/ip-addresses#networkaddresses
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.instance.resilience_security_groups_restrictive
  service: compute
  resource: instance
  requirement: Resilience Security Groups Restrictive
  scope: compute.instance.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Restrict Inbound Rules for Compute Instance Network Security
  rationale: Restricting inbound network access to compute instances minimizes the attack surface, reducing the risk of unauthorized access and potential data breaches. This is crucial for maintaining compliance with security frameworks such as PCI-DSS and HIPAA, which mandate strict access controls to protect sensitive data. By ensuring minimal and necessary network paths are open, organizations can significantly lower the likelihood of exploitation through network-based attacks.
  description: This rule checks compute instances for overly permissive network security groups, specifically those with wide-ranging inbound rules. A restrictive security group should limit access to only necessary ports and IP ranges. To verify, review the security group's inbound rules and ensure they align with the principle of least privilege, allowing only required traffic. Remediation involves adjusting security group rules to close unnecessary ports and limit IP ranges to trusted sources.
  references:
  - https://cloud.google.com/vpc/docs/using-firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.resilience_volumes_encrypted
  service: compute
  resource: instance
  requirement: Resilience Volumes Encrypted
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Instance Resilience Volumes are Encrypted
  rationale: Encrypting resilience volumes on Compute Instances is crucial for protecting sensitive data at rest from unauthorized access and potential breaches. Failure to encrypt could lead to data exposure in case of media theft or improper disposal, and may result in non-compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks whether resilience volumes attached to GCP Compute Instances are encrypted. Encryption should be enabled by default using Google-managed keys or customer-managed keys for enhanced control. Verify encryption status through the GCP Console or gcloud CLI. To remediate, update the instance's volume settings to ensure encryption is enabled, which can be configured at instance creation or by modifying existing volumes.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-111/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.instance.restrict_public_access
  service: compute
  resource: instance
  requirement: Restrict Public Access
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Restrict Public IPs on Compute Instances
  rationale: Allowing public IP access on compute instances increases the risk of unauthorized access, exposing sensitive applications and data to potential attackers, which can lead to data breaches and compliance violations. Restricting public access helps protect against common attack vectors such as brute force attacks and reduces the attack surface, aligning with compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks whether Google Cloud Compute instances have external IP addresses assigned, which can be accessed over the internet. Instances should be configured without public IPs, instead utilizing private IPs and appropriate network configurations like VPNs or Bastion Hosts for secure access. To verify, review instance settings in the GCP Console or use the gcloud command-line tool. Remediation involves removing the external IP and ensuring access through secure internal networks.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-to-instance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#security
  - https://cloud.google.com/vpc/docs/best-practices
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.serial_port_access_disabled
  service: compute
  resource: instance
  requirement: Serial Port Access Disabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable Serial Port Access on Compute Instances
  rationale: Enabling serial port access on virtual machines can expose sensitive information and increase the risk of unauthorized access, as attackers can exploit this to gain insights into system operations or bypass security controls. Disabling this feature helps in adhering to security best practices and compliance requirements, such as PCI-DSS and ISO 27001, which emphasize minimizing unnecessary exposure of system interfaces.
  description: This rule checks whether serial port access is disabled on Google Compute Engine instances. Serial port access should be restricted as it can be used to expose sensitive data and configuration details. To verify, ensure that 'enable-serial-port' is set to 'false' in the instance metadata. Remediation involves updating the instance settings via the Google Cloud Console or gcloud command-line tool to set 'enable-serial-port' to 'false'.
  references:
  - https://cloud.google.com/compute/docs/instances/interacting-with-serial-console
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.serial_ports_in_use
  service: compute
  resource: instance
  requirement: Serial Ports In Use
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Serial Ports Are Disabled for Compute Instances
  rationale: Enabling serial ports on Google Cloud Compute instances can expose sensitive information and create potential entry points for unauthorized access. This setting should be disabled to mitigate risks associated with data leaks and unauthorized configuration changes, which can lead to compliance violations with standards such as PCI-DSS and ISO 27001.
  description: This rule checks for the presence of enabled serial ports on GCP Compute Engine instances. To ensure secure configuration, serial ports should be disabled unless explicitly required for troubleshooting purposes. Users can verify this setting by checking the metadata of each instance and can disable serial ports by setting the 'enable-serial-port' metadata key to 'false'. Remediation involves updating the instance metadata to disable serial ports across all instances.
  references:
  - https://cloud.google.com/compute/docs/instances/interacting-with-serial-console
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.service_account_non_default_configured
  service: compute
  resource: instance
  requirement: Service Account Non Default Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Custom Service Accounts for Compute Instances
  rationale: Assigning non-default service accounts to compute instances limits permissions to only those necessary for the instance to function, reducing the risk of privilege escalation and lateral movement within the cloud environment. This practice helps to minimize the attack surface and supports compliance with least privilege access principles, which are vital for meeting regulatory requirements such as PCI-DSS and SOC2.
  description: This rule checks if Google Cloud Compute Engine instances are configured with a non-default service account. By default, Compute Engine instances use the default service account, which often has more permissions than necessary. To verify, inspect the instance metadata and ensure a custom service account is specified. Remediation involves creating a custom service account with the minimal required IAM roles and updating the instance configuration to use this account.
  references:
  - https://cloud.google.com/compute/docs/access/service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloudsecurityalliance.org/guidance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/cyberframework
  - https://www.isaca.org/resources/cobit
- rule_id: gcp.compute.instance.shielded_vm_compliance_configured
  service: compute
  resource: instance
  requirement: Shielded VM Compliance Configured
  scope: compute.instance.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Shielded VM Compliance for Compute Instances
  rationale: Shielded VMs protect against rootkits and bootkits by ensuring instances boot with verified images, thus minimizing the risk of unauthorized code execution. This is crucial for maintaining data integrity and compliance with security standards such as PCI-DSS and HIPAA, particularly in environments handling sensitive information.
  description: This rule verifies that Compute Engine instances have Shielded VM features enabled, including Secure Boot, vTPM, and Integrity Monitoring. To check compliance, review the instance configurations in the GCP Console or via gcloud CLI, ensuring these features are activated. Remediation involves enabling Shielded VM options under the instance's security settings to enhance threat resistance.
  references:
  - https://cloud.google.com/shielded-vm
  - https://cloud.google.com/compute/docs/instances/verifying-shielded-vm
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.compute.instance.shielded_vm_enabled
  service: compute
  resource: instance
  requirement: Shielded VM Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Shielded VM is Enabled for Compute Instances
  rationale: Shielded VMs provide verifiable integrity of virtual machines, protecting against rootkits and bootkits, which can compromise the integrity of your instances. Enabling Shielded VM safeguards against unauthorized alterations and enhances compliance with security frameworks like PCI-DSS and ISO 27001, mitigating risks of data breaches and unauthorized access.
  description: This rule checks if Shielded VM features are enabled on Google Compute Engine instances, ensuring that vTPM, integrity monitoring, and secure boot are active. To verify, navigate to the VM instance settings in the GCP console and confirm that Shielded VM options are enabled. Remediation involves editing the instance settings and enabling these features, or recreating the instance with Shielded VM enabled.
  references:
  - https://cloud.google.com/shielded-vm
  - https://cloud.google.com/compute/docs/instances/verifying-shielded-vm
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.compute.instance.spot_instance_instance_profile_least_privilege
  service: compute
  resource: instance
  requirement: Spot Instance Instance Profile Least Privilege
  scope: compute.instance.least_privilege
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Spot Instance Profiles Use Least Privilege
  rationale: Applying the principle of least privilege to Spot Instance profiles minimizes the risk of unauthorized access and potential data breaches. Incorrectly configured instance profiles can lead to privilege escalation, allowing attackers to exploit permissions not needed for the instance's operation. This practice is crucial for maintaining a secure and compliant cloud environment, especially under regulatory frameworks like PCI-DSS and ISO 27001.
  description: This rule checks if Spot Instances are assigned instance profiles with excessive permissions beyond what is necessary for their operation. Ensure that IAM roles linked to Spot Instances only include permissions essential for their specific task. Regularly review and update IAM policies to align with current workload requirements and remove any superfluous permissions. Remediation involves adjusting the IAM roles assigned to Spot Instances to enforce the least privilege principle.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.spot_instance_no_public_ip_assigned
  service: compute
  resource: instance
  requirement: Spot Instance No Public Ip Assigned
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Spot Instances Must Not Have Public IPs Assigned
  rationale: Assigning public IPs to spot instances increases the attack surface, exposing these ephemeral resources to potential unauthorized access and attacks. This can lead to data breaches, unauthorized access to sensitive workloads, and service disruptions. Complying with security frameworks like NIST and PCI-DSS necessitates minimizing public exposure of cloud resources to protect against potential threats.
  description: This rule checks if any spot instances in your GCP environment have public IP addresses assigned. Spot instances are usually short-lived and should be tightly controlled to prevent exposure to the internet. Verification involves ensuring that spot instances do not have 'networkInterfaces.accessConfigs' configured. Remediation requires removing any public IPs assigned to spot instances by updating the network interface settings to restrict to private IPs only.
  references:
  - https://cloud.google.com/compute/docs/instances/spot
  - https://cloud.google.com/vpc/docs/compute-instances#no-external-ip
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.compute.instance.spot_instance_uses_approved_launch_template
  service: compute
  resource: instance
  requirement: Spot Instance Uses Approved Launch Template
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Spot Instances Use Approved Launch Templates
  rationale: Using approved launch templates for Spot Instances ensures that instances are created with consistent and secure configurations. It mitigates risks associated with misconfigurations which could lead to vulnerabilities, data exposure, or service disruptions. This is particularly important for maintaining compliance with security standards and reducing the operational costs associated with manual configuration errors.
  description: This rule checks that all Spot Instances in your GCP environment are launched using pre-approved launch templates. These templates should include security settings such as firewall rules, IAM roles, and instance metadata settings. To verify, review instance configurations in the GCP Console or use the gcloud CLI to ensure templates are used consistently. If discrepancies are found, update your launch processes to enforce template usage and document any exceptions.
  references:
  - https://cloud.google.com/compute/docs/instances/spot
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 3.1
  - NIST SP 800-53 Rev. 5
  - ISO/IEC 27001:2013
- rule_id: gcp.compute.instance.ssh_key_authentication
  service: compute
  resource: instance
  requirement: Ssh Key Authentication
  scope: compute.instance.authentication
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Enforce SSH Key Authentication on Compute Instances
  rationale: SSH key authentication enhances security by eliminating password-based logins, reducing the risk of brute force attacks and unauthorized access. It is crucial for maintaining the confidentiality and integrity of sensitive data processed by compute instances, and is often a requirement for compliance with standards like PCI-DSS and ISO 27001.
  description: This rule checks if SSH key authentication is enforced on Google Compute Engine instances. Instances should be configured to accept only SSH keys instead of passwords for remote access. To verify, ensure the 'Block project-wide SSH keys' setting is disabled, and instance-specific SSH keys are configured. Remediation involves updating instance metadata to include only authorized public SSH keys and disabling password authentication in the SSH daemon configuration.
  references:
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.ssm_enabled
  service: compute
  resource: instance
  requirement: Ssm Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure SSM Agent is Enabled on GCP Compute Instances
  rationale: Enabling the SSM (Systems Manager) agent on GCP Compute instances enhances security by allowing administrators to manage and automate configurations securely without direct SSH access. This reduces the attack surface by limiting open ports and direct access, mitigating risks such as unauthorized access and potential data breaches. It also ensures compliance with security policies requiring centralized management and monitoring of virtual machines.
  description: This rule checks if the SSM agent is installed and running on GCP Compute instances. Verify that the agent is enabled by reviewing instance metadata or using gcloud command-line tools. If not enabled, install the SSM agent using the appropriate package manager for the instance's operating system and ensure it is configured to start on boot. This can be automated across instances using startup scripts or configuration management tools.
  references:
  - https://cloud.google.com/compute/docs/instances/access-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/automating-secure-infrastructure
- rule_id: gcp.compute.instance.ssm_managed
  service: compute
  resource: instance
  requirement: Ssm Managed
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compute Instances are SSM Managed
  rationale: Configuring instances to be SSM managed enhances security by allowing centralized management and automation of patching, configuration, and compliance policies, reducing the risk of vulnerabilities and misconfigurations. This is critical for maintaining the integrity and security of workloads and ensuring compliance with industry standards.
  description: This rule checks if Google Compute Engine instances are managed by AWS Systems Manager (SSM), which allows for automated patching, monitoring, and configuration management. To verify, ensure that the SSM agent is installed and configured correctly on each instance. Remediation involves installing the SSM agent on instances and configuring the necessary IAM roles for SSM access, facilitating efficient management and security compliance.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/compute/docs/storing-retrieving-metadata
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.stopped_instance_configured
  service: compute
  resource: instance
  requirement: Stopped Instance Configured
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Stopped Instances Are Properly Configured
  rationale: Unconfigured stopped instances may lead to unexpected charges and security risks if accidentally started with insecure settings. Ensuring that stopped instances are configured with necessary security measures helps prevent unauthorized access and data leakage, aligning with compliance requirements and safeguarding organizational assets.
  description: This rule checks that all stopped Google Compute Engine instances have security configurations such as firewall rules, IAM policies, and network settings appropriately set. Administrators should verify that no critical data is stored on local disks and that instances have minimal access permissions. Remediating involves reviewing and updating instance configurations to meet security policies before they are started again.
  references:
  - https://cloud.google.com/compute/docs/instances
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/instances/instance-life-cycle
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc/docs/firewalls
- rule_id: gcp.compute.instance.subnet_flow_logs_enabled
  service: compute
  resource: instance
  requirement: Subnet Flow Logs Enabled
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Subnet Flow Logs are Enabled for Compute Instances
  rationale: Enabling subnet flow logs provides visibility into the network traffic entering and leaving your compute instances, which is crucial for detecting anomalous activities and unauthorized access attempts. This visibility helps in identifying security threats and maintaining compliance with regulations that require detailed logging and monitoring of network activities.
  description: This rule checks whether subnet flow logs are enabled for Google Cloud Compute instances. Subnet flow logs capture information about network traffic, which is essential for auditing and troubleshooting. To verify, navigate to the VPC network in the Google Cloud Console and ensure that flow logs are enabled for each subnet used by compute instances. Remediation involves modifying the subnet settings to enable flow logs, providing crucial data for network analysis and security auditing.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.compute.instance.url_authentication
  service: compute
  resource: instance
  requirement: Url Authentication
  scope: compute.instance.authentication
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Enable Secure URL Authentication for GCP Compute Instances
  rationale: Enabling secure URL authentication on GCP Compute Instances mitigates the risk of unauthorized access to sensitive resources. It prevents potential data breaches and unauthorized usage, ensuring compliance with regulatory standards such as PCI-DSS and HIPAA. Failing to secure URL endpoints can lead to exposure of critical services, impacting business operations and customer trust.
  description: This rule checks that GCP Compute Instances have URL authentication mechanisms properly configured. It ensures that only authorized users and applications can access instance endpoints via secure URLs. Verification involves reviewing instance metadata and network configuration settings to confirm authentication is enabled. Remediation includes configuring IAM policies to restrict access and implementing OAuth 2.0 or signed URLs for additional security.
  references:
  - https://cloud.google.com/compute/docs/security
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-in-the-cloud/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.compute.instance.vm_data_volumes_encrypted_cmek
  service: compute
  resource: instance
  requirement: VM Data Volumes Encrypted Cmek
  scope: compute.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure VM Data Volumes Use CMEK for Encryption
  rationale: Encrypting VM data volumes with Customer-Managed Encryption Keys (CMEK) enhances data protection by allowing organizations to control key rotation, revocation, and management. This mitigates risks such as unauthorized access and data breaches, which can have significant financial and reputational impacts. Compliance with regulatory frameworks like PCI-DSS and HIPAA often requires strict encryption controls.
  description: This rule checks if Google Cloud Platform Compute Engine VM instances use Customer-Managed Encryption Keys for encrypting attached data volumes. To verify, review the 'properties.disks' field in the VM instance configurations for the 'kmsKeyName' attribute. If it's absent, update the instance configuration to include a CMEK. Remediation involves specifying a KMS key in the VM instance settings, ensuring that data is encrypted using organization-controlled keys.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.compute.instance.vm_imds_hardened
  service: compute
  resource: instance
  requirement: VM Imds Hardened
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Harden Metadata Server Access on VM Instances
  rationale: Properly securing access to the VM Instance Metadata Server (IMDS) mitigates risks of unauthorized access to sensitive metadata, which could lead to privilege escalation or data exfiltration. This is crucial for maintaining the integrity and confidentiality of workloads running in Google Cloud, and aligns with regulatory requirements like PCI-DSS and ISO 27001.
  description: This rule checks that VM instances have restricted access to the metadata server by utilizing GCP firewall rules or configuring the metadata concealment feature. To verify, ensure that the metadata server is only accessible from trusted sources or has been disabled where not needed. Remediation involves setting up network tags and firewall rules that block access to the metadata server from unauthorized sources, or using GCP's metadata concealment feature to limit exposure.
  references:
  - https://cloud.google.com/compute/docs/storing-retrieving-metadata
  - https://cloud.google.com/compute/docs/security-best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.compute.instance.vm_no_public_ip_assigned
  service: compute
  resource: instance
  requirement: VM No Public Ip Assigned
  scope: compute.instance.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure VM Instances Have No Public IP Assigned
  rationale: Public IP addresses on VM instances expose them to the internet, increasing the risk of unauthorized access, DDoS attacks, and data breaches. This can lead to loss of sensitive data, service disruption, and non-compliance with regulations such as PCI-DSS and GDPR, which mandate secure handling of data.
  description: This rule checks whether any VM instances have public IP addresses assigned, which should be avoided to reduce exposure to external attacks. Ensure that VM instances are only accessible through secure internal networks or via a VPN. To remediate, configure instances to use private IPs and access them through a bastion host or secure gateway. You can verify this setting in the Google Cloud Console under the instance's network interface settings.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#ephemeraladdress
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://cloud.google.com/architecture/best-practices-vpc-design
- rule_id: gcp.compute.instance.vm_root_volume_encrypted_cmek
  service: compute
  resource: instance
  requirement: VM Root Volume Encrypted Cmek
  scope: compute.instance.encryption
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure VM Root Volume is Encrypted with Customer-Managed Keys
  rationale: Encrypting the VM root volume with Customer-Managed Encryption Keys (CMEK) provides enhanced data security by allowing customers to control and manage the encryption keys. This is crucial for protecting sensitive data and ensuring compliance with regulatory frameworks such as PCI-DSS and HIPAA, which require strong encryption controls. Without CMEK, organizations face increased risk of unauthorized access and potential data breaches.
  description: This rule checks if the VM root volume is encrypted using Customer-Managed Encryption Keys on Google Cloud Platform. To verify, ensure that CMEK is enabled for the disk encryption settings during the creation of the VM instance. Remediation involves setting up a Cloud Key Management Service (KMS) key and applying it to the VM root volume. This ensures that encryption keys are under customer control, providing better security and compliance with industry standards.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/compute/docs/disks/add-persistent-disk#encrypted
  - https://cloud.google.com/compute/docs/disks/viewing-and-applying-encryption-keys
- rule_id: gcp.compute.instance.vm_secure_boot_enabled
  service: compute
  resource: instance
  requirement: VM Secure Boot Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure VM Instances Have Secure Boot Enabled
  rationale: Enabling Secure Boot on VM instances minimizes the risk of unauthorized code execution during the boot process, protecting against rootkits and low-level malware. This is crucial for maintaining the integrity and trustworthiness of the system's boot sequence, thus ensuring compliance with security standards like NIST and ISO 27001, which require strong boot-time security controls.
  description: 'This rule checks if Secure Boot is enabled for Google Compute Engine VM instances. Secure Boot is a feature that helps prevent malicious software and unauthorized operating systems from loading during the system start-up process. To verify, ensure the VM instance''s boot configuration includes ''secureBoot: true''. If not enabled, modify the instance configuration by setting the secure boot option in the Shielded VM settings. This action enhances the security posture by enforcing trusted boot paths.'
  references:
  - https://cloud.google.com/compute/docs/instances/enable-secure-boot
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/shielded-cloud
- rule_id: gcp.compute.instance.vm_security_group_inbound_restricted
  service: compute
  resource: instance
  requirement: VM Security Group Inbound Restricted
  scope: compute.instance.network_security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict Inbound Traffic to VM Instances
  rationale: Restricting inbound traffic to VM instances reduces the attack surface, preventing unauthorized access and potential breaches. This control is crucial for safeguarding sensitive data and maintaining compliance with security frameworks such as PCI-DSS and HIPAA, which mandate strict network access controls.
  description: This rule checks that security groups associated with VM instances in Google Cloud Platform have restrictive inbound rules, allowing only necessary traffic. It verifies that no overly permissive rules (e.g., 0.0.0.0/0 allowing all IPs) exist, especially for sensitive ports like SSH (22) or RDP (3389). Remediation involves reviewing and updating firewall rules to specify only trusted IP ranges for inbound access, utilizing identity-aware proxies or VPNs where feasible.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/identity-aware-proxy/docs/concepts-overview
- rule_id: gcp.compute.instance.vm_serial_console_access_restricted
  service: compute
  resource: instance
  requirement: VM Serial Console Access Restricted
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict VM Serial Console Access on Compute Instances
  rationale: Restricting VM serial console access is crucial to prevent unauthorized users from accessing sensitive information or executing commands on virtual machines. Unrestricted access could lead to potential data breaches, unauthorized configurations, and compromise of critical workloads, impacting business continuity and compliance with standards like ISO 27001 and PCI-DSS.
  description: This rule checks if serial console access is disabled for GCP Compute instances. By default, the VM serial console should be disabled as it allows users to connect to the VM even if they do not have SSH access. To verify, ensure the 'enable-serial-port' metadata is set to false on instances. Remediation involves updating the instance metadata to disable serial port access, which can be done via the GCP Console, CLI, or API.
  references:
  - https://cloud.google.com/compute/docs/instances/interacting-with-serial-console
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.compute.instance.vm_ssh_key_based_auth_required
  service: compute
  resource: instance
  requirement: VM Ssh Key Based Auth Required
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Enforce SSH Key-Based Authentication for GCP VMs
  rationale: Relying solely on password-based authentication for VM instances can expose systems to brute force attacks, leading to unauthorized access and potential data breaches. Enforcing SSH key-based authentication mitigates these risks by requiring cryptographic keys that are significantly more secure and align with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks if SSH key-based authentication is enforced on all GCP VM instances. SSH keys provide a more secure method of accessing instances by using asymmetric encryption keys instead of passwords. To ensure compliance, administrators should disable password authentication and configure instances to accept SSH keys only. This can be verified using the GCP Console or CLI by checking the instance metadata settings and ensuring 'block-project-ssh-keys' is not set, and the 'ssh-keys' metadata entry is configured.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-advanced
  - https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance.vm_ssh_password_auth_disabled
  service: compute
  resource: instance
  requirement: VM Ssh Password Auth Disabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Disable SSH Password Authentication for VM Instances
  rationale: Disabling SSH password authentication reduces the risk of brute-force attacks and unauthorized access, as passwords can be easily guessed or stolen. This practice aligns with security best practices and compliance requirements by enforcing the use of stronger authentication methods like SSH keys, thereby enhancing the security posture of cloud workloads.
  description: This rule checks if SSH password authentication is disabled on Google Cloud VM instances. Enabling SSH key-based authentication and disabling password-based access helps secure the VM from unauthorized logins. Verify the 'PasswordAuthentication' setting in the SSH configuration file is set to 'no'. Remediation involves updating the SSH configuration and restarting the SSH service on the VM instance.
  references:
  - https://cloud.google.com/compute/docs/instances/connecting-advanced#disable-ssh-password
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.compute.instance.vm_user_data_no_secrets
  service: compute
  resource: instance
  requirement: VM User Data No Secrets
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure VM User Data Does Not Contain Secrets
  rationale: Storing secrets such as API keys, passwords, or private keys in VM user data poses significant security risks. Unauthorized access to these secrets can lead to data breaches, financial loss, or compromised services. Regulatory compliance frameworks like PCI-DSS and HIPAA mandate secure handling of sensitive information, making it crucial to avoid embedding secrets in VM configurations.
  description: This rule checks that VM user data does not contain any sensitive information such as secrets or credentials. User data is often used for instance initialization scripts, but it should never include confidential data. Verify user data configurations through the GCP Console or CLI and ensure encryption and secret management solutions like Google Secret Manager are used for sensitive data. Remediation involves auditing user data and removing any embedded secrets, replacing them with secure reference calls to managed secret services.
  references:
  - https://cloud.google.com/compute/docs/metadata/overview
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance.vm_vtpm_enabled
  service: compute
  resource: instance
  requirement: VM Vtpm Enabled
  scope: compute.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure VM Instances have vTPM Enabled
  rationale: Enabling the virtual Trusted Platform Module (vTPM) on VM instances enhances the security by providing hardware-based cryptographic operations. This helps protect sensitive data and application workloads from unauthorized access and tampering, reducing the risk of data breaches. Compliance with security standards like PCI-DSS and HIPAA often require such measures to ensure data confidentiality and integrity.
  description: This rule checks whether vTPM is enabled for Google Cloud VM instances. The vTPM feature helps in securing VM instances by providing a hardware root of trust. To verify, navigate to the VM instance settings in the Google Cloud Console and ensure vTPM is enabled under the 'Shielded VM' options. Remediation involves activating vTPM on existing instances or configuring it during the creation of new instances.
  references:
  - https://cloud.google.com/shielded-vm/docs/shielded-vm-concepts
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance.vpc_subnet_flow_logs_compliance_configured
  service: compute
  resource: instance
  requirement: VPC Subnet Flow Logs Compliance Configured
  scope: compute.instance.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure VPC Subnet Flow Logs Are Enabled for Instances
  rationale: Enabling VPC subnet flow logs is crucial for monitoring network traffic, detecting anomalous behavior, and fulfilling audit requirements. It helps in identifying potential security threats, such as data exfiltration or unauthorized access patterns, thereby reducing the risk of data breaches. Compliance with regulatory frameworks often mandates detailed logging for forensic analysis and reporting purposes.
  description: This rule checks if flow logs are enabled for VPC subnets associated with instances in your GCP environment. VPC flow logs capture information about the ingress and egress IP traffic, which can be used for performance analysis and security insights. To verify, ensure that the 'Enable flow logs' option is active on your VPC subnets. Remediation involves navigating to the VPC network details in the GCP Console, selecting 'Edit' for the subnet, and enabling flow logs, specifying the desired log format and sampling rate.
  references:
  - https://cloud.google.com/vpc/docs/flow-logs
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/logging/docs/reference
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance_group.instance_profile_least_privilege
  service: compute
  resource: instance_group
  requirement: Instance Profile Least Privilege
  scope: compute.instance_group.least_privilege
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Enforce Least Privilege for Compute Instance Profiles
  rationale: Implementing least privilege for instance profiles minimizes the risk of unauthorized access and potential data breaches by ensuring that compute resources have only the permissions necessary for their function. This approach reduces the attack surface and helps comply with regulatory standards like PCI-DSS and ISO 27001, which mandate access controls to protect sensitive information.
  description: This rule verifies that GCP Compute Instance Groups are configured with the minimum permissions necessary for their intended purpose. It checks for overly permissive roles attached to instance profiles and suggests replacing them with custom roles that grant only essential permissions. Remediation involves auditing instance roles and adjusting IAM policies to align with the principle of least privilege, using the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/creating-custom-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://cloud.google.com/compute/docs/access/iam
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
- rule_id: gcp.compute.instance_group.no_public_ip_assigned
  service: compute
  resource: instance_group
  requirement: No Public Ip Assigned
  scope: compute.instance_group.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Prevent Public IP Assignment to GCP Instance Groups
  rationale: Assigning public IPs to instance groups can expose your infrastructure to the internet, increasing the risk of unauthorized access and potential data breaches. Public IPs can be exploited by attackers to perform reconnaissance or launch attacks on your instances, leading to potential business disruptions, data loss, and non-compliance with regulations such as PCI-DSS and SOC2 that mandate secure access controls.
  description: This rule checks that no public IP addresses are assigned to any instances within a GCP instance group. Instances with public IPs should be reconfigured to use private IPs to leverage Google's VPC for secure communication. To verify, review the instance group's network interface settings in the Google Cloud Console or via the gcloud CLI for IP address configurations. Remediation involves removing public IP assignments and setting up a Cloud NAT or VPN for secure external communications.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#assigning_ip_addresses
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/vpc/docs/vpc#vpc_networks_and_subnets
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance_group.uses_approved_launch_template
  service: compute
  resource: instance_group
  requirement: Uses Approved Launch Template
  scope: compute.instance_group.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Instance Groups Use Approved Launch Templates
  rationale: Using approved launch templates for instance groups ensures that all instances adhere to organizational security standards, minimizing the risk of misconfigurations that could lead to vulnerabilities. This practice helps maintain consistency across deployments, reducing the attack surface and supporting compliance with regulations like PCI-DSS, which require stringent control over system configurations.
  description: This rule checks if all instance groups within your GCP environment utilize launch templates that have been approved by your organization's security team. Approved launch templates contain predefined configurations that meet security policies, such as specific network settings, firewall rules, and instance metadata configurations. To verify compliance, review the launch template IDs associated with your instance groups and compare them against the list of approved templates. If discrepancies are found, update the instance group to use an approved launch template, ensuring it matches security requirements.
  references:
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/compute/docs/instance-groups/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/compute/docs/instances/instance-life-cycle
- rule_id: gcp.compute.instance_group.volumes_encrypted
  service: compute
  resource: instance_group
  requirement: Volumes Encrypted
  scope: compute.instance_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Compute Instance Group Volumes are Encrypted
  rationale: Encrypting volumes protects sensitive data from unauthorized access and potential breaches by ensuring that data at rest is unreadable without the proper decryption keys. This is crucial for meeting regulatory compliance requirements like GDPR and HIPAA, which mandate data protection standards to safeguard personal and sensitive information.
  description: This rule checks whether the persistent disks attached to instance groups in GCP Compute are encrypted using Customer-Managed Encryption Keys (CMEK) or Google-managed encryption keys. To verify, review the encryption settings of each disk in the instance group via the Google Cloud Console or gcloud CLI. If disks are not encrypted, configure encryption under 'Disks' by selecting an encryption key. This can be done during disk creation or by modifying existing disks.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/publications/nist-special-publication-800-53-revision-5-security-and-privacy-controls
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/docs/security-overview
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.instance_template.dr_artifacts_encrypted_and_private
  service: compute
  resource: instance_template
  requirement: DR Artifacts Encrypted And Private
  scope: compute.instance_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DR Artifacts in Instance Templates Are Encrypted and Private
  rationale: Encrypting disaster recovery (DR) artifacts in instance templates protects sensitive data from unauthorized access and exposure. Without encryption and privacy controls, there is a heightened risk of data breaches, which can lead to financial loss and damage to reputation. Additionally, compliance with regulations like GDPR and CCPA requires robust data protection measures, including encryption at rest.
  description: This rule checks that all DR artifacts associated with GCP Compute Engine instance templates are encrypted using Customer-Managed Encryption Keys (CMEK) and are not publicly accessible. Verify that the instance templates specify CMEK for disk encryption. Remediation involves updating the instance template to include CMEK and ensuring appropriate IAM roles are applied to restrict access. This helps maintain data confidentiality and integrity.
  references:
  - https://cloud.google.com/compute/docs/instances/disks#encrypted_disks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.instance_template.dr_change_audit_logging_enabled
  service: compute
  resource: instance_template
  requirement: DR Change Audit Logging Enabled
  scope: compute.instance_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DR Change Audit Logging is Enabled for Instance Templates
  rationale: Enabling audit logging for disaster recovery (DR) changes in instance templates is crucial to track any unauthorized or inappropriate modifications. This helps in maintaining the integrity of DR plans, preventing data loss, and ensuring business continuity. It also aids in meeting compliance requirements by providing a verifiable trail of changes.
  description: This rule checks if audit logging is enabled for changes made to instance templates, specifically those related to disaster recovery configurations. To verify, ensure that the 'enableLogging' attribute is set to true in the instance template's configuration. If logging is not enabled, update the instance template settings to include logging for all change activities. This helps in auditing access and modifications to DR settings.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/compute/docs/instance-templates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://hipaa.jotform.com/resources/hipaa-compliance-guide/
- rule_id: gcp.compute.instance_template.dr_execution_roles_least_privilege
  service: compute
  resource: instance_template
  requirement: DR Execution Roles Least Privilege
  scope: compute.instance_template.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege on DR Execution Roles
  rationale: Implementing least privilege for Disaster Recovery (DR) execution roles is critical to prevent unauthorized access and potential misuse of compute resources. Over-permissioned roles increase the risk of insider threats and exposure to external attacks, potentially leading to data breaches and service disruptions. Compliance with standards such as NIST SP 800-53 and ISO 27001 mandates stringent access controls to safeguard sensitive data and maintain operational integrity.
  description: This rule checks if instance templates are configured with the minimal necessary IAM roles required for DR operations. Ensure that roles assigned to instance templates do not exceed the permissions necessary for their intended function. Verify configurations through IAM policy review and use GCP tools to audit permissions. Remediation involves adjusting IAM policies to remove unnecessary roles while validating functionality remains intact.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/resource-manager/docs/access-control-org
- rule_id: gcp.compute.instance_template.launch_template_imds_hardened
  service: compute
  resource: instance_template
  requirement: Launch Template Imds Hardened
  scope: compute.instance_template.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Harden IMDS Access in Compute Instance Templates
  rationale: Ensuring that the Instance Metadata Service (IMDS) is properly configured in GCP Compute instance templates is crucial for preventing unauthorized access to sensitive metadata, which could lead to potential data breaches. By hardening IMDS access, organizations can mitigate risks such as metadata exposure and privilege escalation, thus enhancing their security posture and aligning with compliance requirements like SOC2 and ISO 27001.
  description: This rule checks if the Instance Metadata Service (IMDS) access is properly restricted in GCP Compute instance templates. It verifies that the 'metadataFlavor' header is set to 'Google' and metadata server requests are secured to prevent unauthorized access. To remediate, ensure that the instance template's metadata options are configured with 'enable-oslogin' set to 'TRUE' and 'block-project-ssh-keys' set to 'TRUE'. Regular audits and updates to these settings are recommended to maintain security.
  references:
  - https://cloud.google.com/compute/docs/instances/accessing-instance-metadata
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.instance_template.launch_template_instance_profile_least_privilege
  service: compute
  resource: instance_template
  requirement: Launch Template Instance Profile Least Privilege
  scope: compute.instance_template.least_privilege
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure Instance Templates Use Least Privilege Profiles
  rationale: Ensuring instance templates use the least privilege principle minimizes the attack surface by restricting unnecessary permissions, reducing the risk of exploitation in case of a compromised instance. This approach mitigates potential data breaches and aligns with compliance frameworks that require stringent access controls to protect sensitive information.
  description: This rule checks that instance templates in GCP do not associate with overly permissive IAM roles. It verifies that the service accounts tied to instance templates have the minimum required permissions. To remediate, review the permissions granted through IAM roles and adjust them to ensure only essential permissions are assigned. This involves using custom roles tailored to the specific needs of applications running on the instances.
  references:
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.compute.instance_template.launch_template_no_public_ip_default
  service: compute
  resource: instance_template
  requirement: Launch Template No Public Ip Default
  scope: compute.instance_template.public_access
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: critical
  title: Ensure Instance Templates Do Not Use Public IP by Default
  rationale: Public IP addresses expose instances directly to the internet, increasing the risk of unauthorized access, data breaches, and other security threats. Limiting public IP usage helps protect sensitive workloads and aligns with compliance requirements such as CIS Benchmarks, which advocate for minimizing public exposure to reduce attack surfaces.
  description: This rule checks that instance templates do not have a public IP address set as default upon launch. Verify by inspecting the 'networkInterfaces' configuration in the instance template to ensure 'accessConfigs' is not set with a public IP. To remediate, modify the instance template to use private IP addresses only, and utilize Cloud NAT or VPNs for external connectivity if needed.
  references:
  - https://cloud.google.com/compute/docs/ip-addresses#types
  - https://cloud.google.com/compute/docs/instance-templates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/nat/docs/overview
  - https://cloud.google.com/vpn/docs/concepts/overview
- rule_id: gcp.compute.instance_template.launch_template_root_volume_encrypted_by_default
  service: compute
  resource: instance_template
  requirement: Launch Template Root Volume Encrypted By Default
  scope: compute.instance_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: critical
  title: Ensure Root Volume Encryption in Instance Templates
  rationale: Encrypting the root volume of instance templates by default is crucial for protecting sensitive data at rest. Without encryption, unauthorized access to storage media could lead to data breaches, violating compliance with standards like PCI-DSS and HIPAA, and potentially resulting in financial and reputational damage.
  description: This rule checks whether the root volumes in GCP instance templates are encrypted by default. It ensures that all new instances launched from these templates have their root volumes encrypted using Google-managed or customer-managed encryption keys. To verify, review the instance template settings for disk encryption properties. Remediation involves enabling encryption for disks in the instance template settings via the GCP Console or CLI.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/encryption-at-rest/
- rule_id: gcp.compute.instance_template.launch_template_security_groups_restrictive
  service: compute
  resource: instance_template
  requirement: Launch Template Security Groups Restrictive
  scope: compute.instance_template.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Launch Templates Use Restrictive Security Groups
  rationale: Restrictive security groups in launch templates minimize exposure to unauthorized access by explicitly defining allowed traffic. This reduces the risk of unauthorized data access and potential breaches, aligning with compliance mandates such as PCI-DSS and NIST that require strict access controls around sensitive data handling.
  description: This rule checks if GCP instance templates are configured with security groups that limit inbound and outbound traffic to only necessary protocols and ports. It examines the security group configurations associated with launch templates and flags those that allow overly permissive access. To remediate, restrict security group rules to the minimum necessary, ensuring only trusted IP addresses and essential services are permitted.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/pci-dss
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.compute.instance_template.launch_template_user_data_no_secrets
  service: compute
  resource: instance_template
  requirement: Launch Template User Data No Secrets
  scope: compute.instance_template.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Ensure No Secrets in Instance Template User Data
  rationale: Storing secrets such as passwords, API keys, or sensitive configuration data in instance templates can lead to unauthorized access and data breaches. This poses significant security risks, including potential exposure of sensitive data and violation of compliance standards like PCI-DSS and HIPAA. Protecting these secrets is crucial to maintaining the integrity and confidentiality of your systems.
  description: This rule checks that sensitive data is not included in the user data field of GCP compute instance templates. Storing secrets in this field can lead to exposure as the user data is accessible to any user with read access to the instance template. To remediate, use secret management tools like Google Secret Manager to securely store and access sensitive data, and ensure instance templates are configured without embedding secrets directly.
  references:
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/secret-manager/docs
  - 'CIS GCP Benchmark: 1.0.0 - Ensure Instance Templates Do Not Contain Sensitive Data'
  - 'NIST SP 800-53: AC-6 Least Privilege'
  - 'PCI-DSS Requirement 3: Protect Stored Cardholder Data'
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance_template.network_change_audit_logging_enabled
  service: compute
  resource: instance_template
  requirement: Network Change Audit Logging Enabled
  scope: compute.instance_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Network Change Audit Logging for Instance Templates
  rationale: Enabling network change audit logging for instance templates is crucial for maintaining visibility over modifications that could impact your cloud infrastructure. Without these logs, unauthorized or erroneous changes could go undetected, leading to potential exposure of resources, disruptions in service, and non-compliance with regulatory standards such as PCI-DSS and SOC 2. Monitoring network changes helps in quickly identifying and responding to suspicious activities, thereby reducing the risk of data breaches and ensuring operational integrity.
  description: This rule checks whether audit logging for network changes is enabled on Google Cloud instance templates. Specifically, it verifies that audit logs are configured to capture events related to network alterations such as updates to firewall rules or changes to network configurations. To ensure compliance, navigate to the 'Audit Logs' section in the Google Cloud Console, select the appropriate service and resource, and enable logging for 'Admin Read' and 'Admin Write' activities. This can be automated using Terraform or gcloud CLI commands. Regularly review these logs for unauthorized access or anomalies.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/compute/docs/instance-templates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.instance_template.network_remediation_roles_least_privilege
  service: compute
  resource: instance_template
  requirement: Network Remediation Roles Least Privilege
  scope: compute.instance_template.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Network Roles in Instance Templates
  rationale: Assigning least privilege roles for network remediation in instance templates minimizes the risk of unauthorized access and data breaches. It reduces the attack surface by ensuring that users and services have only the necessary permissions, aligning with compliance requirements such as least privilege principles in NIST and ISO 27001. This approach mitigates the potential impact of compromised credentials or insider threats, protecting sensitive network configurations and data.
  description: This rule checks that network remediation roles assigned to instance templates do not exceed the minimum permissions required. Review and adjust IAM policies to ensure roles are designed according to the principle of least privilege. Verify that no overly permissive roles are attached to instance templates, and modify them to limit access to only the actions necessary for remediation tasks. Regular audits and policy reviews should be conducted to maintain compliance and security posture.
  references:
  - https://cloud.google.com/compute/docs/instance-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.compute.network.acl_unused
  service: compute
  resource: network
  requirement: ACL Unused
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Identify and Remove Unused Network ACLs in GCP
  rationale: Unused Network ACLs in GCP can lead to potential security risks by allowing outdated or unnecessary access configurations to persist in the environment. This increases the attack surface and can inadvertently expose resources to unauthorized access. Regularly reviewing and removing unused ACLs is crucial for maintaining a secure and compliant cloud environment by ensuring that only necessary access permissions are in place.
  description: This rule checks for Network ACLs in GCP that are configured but not actively used by any resources. Unused ACLs should be reviewed and removed to reduce complexity and potential misconfigurations. Verification involves checking the association of each ACL with network resources such as subnets or VMs. Remediation involves deleting the ACLs that are not in use, ensuring that no active resources depend on them, and documenting any changes made for audit purposes.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.network.check_default_absence
  service: compute
  resource: network
  requirement: Check Default Absence
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Default Network Is Not Present in GCP Projects
  rationale: The default network in GCP is automatically created with permissive firewall rules and is not tailored to specific security needs, posing a potential security risk. Organizations may inadvertently use it, leading to exposure to unauthorized access and data breaches. Removing it helps align with security best practices and compliance requirements, reducing the attack surface.
  description: This rule checks for the presence of the default network in your GCP projects. By default, GCP creates a network with open firewall rules that may not adhere to your organization's security policies. To enhance security, delete the default network if it exists and create custom networks with specific firewall rules that align with your security posture. Verify by using the GCP Console or gcloud CLI to list available networks and ensure the default is absent. Remediate by deleting the default network via the console or `gcloud compute networks delete default` command.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/vpc/docs/configure-firewall-rules
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.compute.network.default_in_use
  service: compute
  resource: network
  requirement: Default In Use
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Disable Default Network Usage in GCP
  rationale: Using the default network in GCP can lead to security risks as it is automatically created with preconfigured firewall rules that allow broad access. This may expose your resources to unwanted traffic, increasing the attack surface and potential for unauthorized access. Ensuring custom network configurations helps meet compliance requirements and reduce vulnerabilities.
  description: This rule checks if the default network is in use within your Google Cloud Platform project. The default network comes with pre-established firewall rules that may not adhere to the principle of least privilege, posing a security risk. To mitigate this, create custom networks with specific firewall rules tailored to your application's security needs. Verification involves checking the network configurations and ensuring no resources are deployed on the default network. Remediation includes migrating existing resources to custom networks and deleting the default network if not needed.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/solutions/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.network.dns_logging_enabled
  service: compute
  resource: network
  requirement: Dns Logging Enabled
  scope: compute.network.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DNS Logging is Enabled for GCP Networks
  rationale: Enabling DNS logging on Google Cloud Platform networks allows for comprehensive visibility into DNS queries, which is crucial for detecting and responding to potential security threats such as data exfiltration or network attacks. It helps in auditing and monitoring network activities for compliance with regulations like GDPR and PCI DSS, which mandate detailed logging for data protection and integrity.
  description: This rule checks if DNS logging is enabled for networks within Google Cloud Platform. DNS logs provide valuable insights into network traffic, helping identify unusual patterns or unauthorized access. To verify, review the network's logging configuration in the Google Cloud Console or use the gcloud command-line tool. Remediation involves enabling DNS logging in the network settings under the 'Logging' section, ensuring that logs are sent to a designated Cloud Logging project for analysis and storage.
  references:
  - https://cloud.google.com/dns/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf
- rule_id: gcp.compute.network.legacy_configured
  service: compute
  resource: network
  requirement: Legacy Configured
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Avoid Using Legacy Networks in GCP Compute
  rationale: Legacy networks in GCP lack granular control over network security policies, which increases the risk of unauthorized access and data breaches. Utilizing modern network configurations enhances the ability to implement tighter access controls and comply with regulatory requirements such as PCI-DSS and HIPAA. Failure to update legacy networks could lead to security vulnerabilities and non-compliance with industry standards.
  description: This rule checks for the presence of legacy networks configured within GCP Compute environments. Legacy networks do not support subnetting or modern firewall rules, which are critical for fine-grained access control. To verify, navigate to the VPC Networks section in the GCP Console and ensure no networks are labeled as 'legacy'. Remediation involves migrating legacy networks to VPC networks, which offer improved security features and manageability.
  references:
  - https://cloud.google.com/vpc/docs/using-vpc
  - https://landing.google.com/securebydesign/
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/document_library
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.compute.network.network_automated_isolation_supported
  service: compute
  resource: network
  requirement: Network Automated Isolation Supported
  scope: compute.network.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Automated Isolation is Enabled in GCP
  rationale: Automated network isolation helps contain security incidents and limits unauthorized access. It mitigates risks associated with compromised resources by automatically isolating affected networks. This approach reduces the potential impact on business operations and supports compliance with security frameworks that require incident containment strategies.
  description: This rule verifies if automated isolation is supported and enabled within Google Cloud's network settings. It checks for configurations like VPC Service Controls that facilitate automated isolation of resources based on predefined conditions. Ensure you have set up appropriate VPC Service Control boundaries and enabled audit logging for network activities to monitor and respond to suspicious behaviors. Remediation involves configuring VPC Service Controls and enabling network policies that support automated isolation.
  references:
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://cloud.google.com/security/best-practices/identity-access-management
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 5.1
  - 'NIST SP 800-53: SI-4 Information System Monitoring'
  - 'PCI-DSS Requirement 11: Regularly test security systems and processes'
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.network.network_quarantine_network_defined
  service: compute
  resource: network
  requirement: Network Quarantine Network Defined
  scope: compute.network.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Define Quarantine Network for Isolating Untrusted Resources
  rationale: Defining a quarantine network is crucial to isolate potentially malicious or untrusted assets, preventing lateral movement and reducing the risk of data breaches. This practice mitigates the impact of compromised resources by containing threats and preventing them from affecting other parts of the network. It is an essential component for adhering to regulatory frameworks that mandate network segmentation and containment strategies.
  description: This rule checks if a designated quarantine network is defined and utilized within your GCP environment to isolate untrusted or compromised resources. Ensure that a virtual network with strict ingress and egress rules is configured to limit communication with other network segments. To verify, review your VPC network configurations and ensure a network labeled 'quarantine' or equivalent exists, with appropriate firewall rules. Remediation involves creating a new VPC network specifically for quarantine purposes and configuring firewall rules to restrict traffic.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/network-connectivity/docs
- rule_id: gcp.compute.network.not_legacy
  service: compute
  resource: network
  requirement: Not Legacy
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Avoid Use of Legacy Networks in GCP
  rationale: Legacy networks in GCP lack the advanced features and controls of VPC networks, potentially exposing systems to security risks such as unauthorized access and limited traffic management capabilities. Using non-legacy networks supports business continuity by providing enhanced scalability, better integration with modern services, and compliance with industry standards, reducing the risk of non-compliance with regulations such as PCI-DSS and ISO 27001.
  description: This check verifies that no legacy networks are in use within GCP projects. Legacy networks are identified by their flat global IP address space, which lacks the segmentation capabilities of Virtual Private Cloud (VPC) networks. To ensure compliance, administrators should migrate any legacy networks to VPC networks, which offer subnet-level segmentation, custom routes, and firewall rules. Verification can be done through the GCP Console or CLI by listing all networks and ensuring 'VPC' is used. Remediation involves creating VPC networks and migrating resources from legacy networks.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/network-connectivity/docs/overview
- rule_id: gcp.compute.network.subnet_flow_logs_compliance_configured
  service: compute
  resource: network
  requirement: Subnet Flow Logs Compliance Configured
  scope: compute.network.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Subnet Flow Logs Are Configured in GCP
  rationale: Enabling subnet flow logs helps capture IP traffic information, which is crucial for monitoring network activity, diagnosing network issues, and detecting potential security incidents. Without flow logs, organizations may have blind spots in their network security posture, increasing the risk of undetected anomalies and non-compliance with regulatory standards such as PCI-DSS and SOC 2.
  description: This rule verifies that subnet flow logs are enabled for each Google Cloud Platform VPC network. To ensure compliance, navigate to the 'VPC network' section in the GCP Console, select the desired subnet, and verify that flow logs are set to 'On'. If not enabled, adjust the settings accordingly to capture and analyze network data for better security monitoring and incident response capabilities.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.network.vpc_dns_hostnames_and_support_configured
  service: compute
  resource: network
  requirement: VPC Dns Hostnames And Support Configured
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure VPC DNS Hostnames and Support are Properly Configured
  rationale: Configuring VPC DNS hostnames and support is crucial for smooth internal DNS resolution, which enhances network efficiency and security. Without proper configuration, instances may face difficulties in resolving internal hostnames, leading to potential service disruptions. This setting is also vital for meeting certain regulatory requirements that mandate proper network configuration and access controls.
  description: This rule checks whether VPC networks have DNS hostnames and support enabled, facilitating internal DNS resolution for instances. Verify this configuration by navigating to the 'VPC Network' settings in the Google Cloud Console and ensuring that both 'DNS Hostnames' and 'DNS Resolution' are enabled. Remediate by enabling these options to ensure seamless connectivity and compliance with network security best practices.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/vpc/docs/configure-dns
- rule_id: gcp.compute.network.vpc_flow_logs_enabled
  service: compute
  resource: network
  requirement: VPC Flow Logs Enabled
  scope: compute.network.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enable VPC Flow Logs for Enhanced Network Monitoring
  rationale: Enabling VPC Flow Logs provides detailed visibility into network traffic, helping to detect anomalies and potential security threats. This is crucial for identifying unauthorized access attempts and ensuring data integrity, thereby reducing the risk of data breaches and supporting forensic investigations. Additionally, it aids in meeting compliance requirements by providing necessary audit trails for regulatory standards.
  description: This rule checks if VPC Flow Logs are enabled for all networks in GCP. VPC Flow Logs capture information about the IP traffic going to and from network interfaces within a VPC, allowing for detailed analysis of network flows. To verify, ensure that VPC Flow Logs are configured in the Google Cloud Console under the VPC network settings. Remediation involves enabling Flow Logs for each subnet, specifying the desired aggregation interval, sampling rate, and logging metadata. This setup enhances the ability to monitor network activity effectively.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.network.vpc_route_tables_no_unintended_internet_paths
  service: compute
  resource: network
  requirement: VPC Route Tables No Unintended Internet Paths
  scope: compute.network.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Prevent Unintended Internet Paths in VPC Route Tables
  rationale: Unintended internet exposure from misconfigured VPC route tables can lead to unauthorized access, data breaches, and potential compliance violations. Ensuring that only intended internet paths are configured reduces the risk of exposing sensitive resources, aligning with best practices for network security and meeting regulatory requirements.
  description: This rule checks for unintended internet paths in VPC route tables, specifically looking for default routes that may inadvertently expose internal resources to the internet. To verify, review route tables for any 0.0.0.0/0 routes and ensure they direct traffic only through secure gateways like Cloud NAT or specific VPN connections. Remediation involves removing or correcting any routes that allow unintended internet access, ensuring traffic is routed through secure, intended paths.
  references:
  - https://cloud.google.com/vpc/docs/routes
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.3
  - 'NIST SP 800-53 Rev. 5: AC-3 Access Enforcement'
  - 'PCI DSS v3.2.1: Requirement 1.3.2'
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/network-connectivity/docs
- rule_id: gcp.compute.packet_mirroring.network_alert_destinations_configured
  service: compute
  resource: packet_mirroring
  requirement: Network Alert Destinations Configured
  scope: compute.packet_mirroring.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network Alert Destinations for Packet Mirroring are Configured
  rationale: Proper configuration of network alert destinations for packet mirroring is essential to ensure that traffic anomalies and potential security threats are detected and addressed promptly. Without configured alert destinations, critical security events may go unnoticed, leading to data breaches or service disruptions. Compliance with standards such as PCI-DSS and NIST requires proper monitoring and alerting mechanisms to safeguard sensitive information.
  description: This rule checks if network alert destinations are configured for Packet Mirroring in GCP Compute. It ensures that mirrored traffic is monitored and that alerts are sent to designated destinations, such as Security Operations Centers (SOCs), for analysis. To verify, ensure alert destinations are set in the Packet Mirroring configuration under the 'MirrorConfig' settings. Remediation involves specifying valid alert destinations in the Packet Mirroring policy to ensure all mirrored traffic is properly monitored.
  references:
  - https://cloud.google.com/vpc/docs/packet-mirroring
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.packet_mirroring.network_ids_ips_enabled_where_supported
  service: compute
  resource: packet_mirroring
  requirement: Network Ids Ips Enabled Where Supported
  scope: compute.packet_mirroring.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enable Network IDs and IPs in Packet Mirroring
  rationale: Enabling network IDs and IPs in packet mirroring is crucial for ensuring that mirrored traffic is accurately tagged and can be properly analyzed. Without this, there is a risk of incomplete traffic analysis, which can lead to undetected anomalies or breaches. This is particularly important for organizations that must comply with security standards such as NIST, PCI-DSS, or HIPAA, where network monitoring and incident detection are critical components.
  description: This rule checks that network IDs and IPs are enabled for packet mirroring configurations where supported. To verify, access the Google Cloud Console, navigate to the Compute Engine section, and review packet mirroring configurations to ensure that network IDs and IPs are included. Remediation involves updating the packet mirroring settings to include these identifiers, ensuring comprehensive traffic analysis and improved incident response capabilities.
  references:
  - https://cloud.google.com/vpc/docs/packet-mirroring
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.project.os_login_compliance_configured
  service: compute
  resource: project
  requirement: OS Login Compliance Configured
  scope: compute.project.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure OS Login is Enabled for Project Compliance
  rationale: Enabling OS Login enhances security by centralizing user management via IAM, reducing the risk of unauthorized access through SSH keys. It aids in compliance with security frameworks by ensuring consistent access control and audit logging across instances, providing traceability and accountability for access activities.
  description: This rule checks if OS Login is configured for all VM instances in the project. OS Login enforces IAM-based user and SSH key management, improving security and simplifying access control. To verify, ensure 'enable-oslogin' metadata is set to 'true' at the project level. Remediation involves updating project metadata to include 'enable-oslogin=true' and verifying IAM roles and permissions for users.
  references:
  - https://cloud.google.com/compute/docs/oslogin
  - https://cloud.google.com/compute/docs/instances/managing-instance-access
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/docs/security/compliance/nist
  - https://cloud.google.com/docs/security/compliance/iso-27001
  - https://cloud.google.com/docs/security/compliance/soc-2
- rule_id: gcp.compute.reservation.cost_approval_workflow_required
  service: compute
  resource: reservation
  requirement: Cost Approval Workflow Required
  scope: compute.reservation.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Mandatory Cost Approval for Compute Reservations
  rationale: Implementing a cost approval workflow for compute reservations ensures that financial expenditures are monitored and validated by appropriate stakeholders, reducing the risk of unexpected charges and budget overruns. This is essential for organizations to maintain financial accountability and align with budgetary constraints, particularly in environments with multiple users and teams. It also aids in detecting and preventing unauthorized or frivolous resource allocations that could lead to financial waste.
  description: This rule checks if a cost approval workflow is in place before compute reservations are finalized in GCP. It requires configurations in Google Cloud's billing settings to enforce an approval process, such as using workflows to route reservation requests to financial controllers or managers for review. To verify, audit the billing account settings to ensure an approval policy exists. Remediation involves setting up a workflow using Google Cloud Functions and Cloud Pub/Sub to trigger notifications and approvals based on reservation requests.
  references:
  - https://cloud.google.com/billing/docs/how-to/billing-access
  - https://cloud.google.com/iam/docs/using-workload-identity-federation
  - https://cloud.google.com/functions/docs/calling/pubsub
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0 - Section 4.10
  - NIST SP 800-53 Rev. 5 - CM-5 Access Restrictions for Change
  - https://cloud.google.com/architecture/cost-management
- rule_id: gcp.compute.reservation.cost_billing_admins_mfa_required
  service: compute
  resource: reservation
  requirement: Cost Billing Admins MFA Required
  scope: compute.reservation.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce MFA for Cost Billing Admins on Compute Reservations
  rationale: Requiring multi-factor authentication (MFA) for Cost Billing Admins mitigates the risk of unauthorized access to financial resources and sensitive configurations. Without MFA, attackers with stolen credentials can access and manipulate billing data, leading to financial loss and compliance violations under frameworks like PCI-DSS and SOC2.
  description: This rule checks if Cost Billing Admins managing compute reservations have MFA enabled. Ensure that all accounts with billing access are secured with two-step verification by navigating to 'IAM & Admin' > 'IAM', selecting the relevant account, and enabling MFA. Remediation involves configuring MFA in the Google Account settings and enforcing it through 'Access Context Manager' policies.
  references:
  - https://cloud.google.com/identity-platform/docs/mfa-overview
  - https://cloud.google.com/iam/docs/best-practices-for-enterprise-organizations
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.compute.reservation.cost_purchase_permissions_restricted
  service: compute
  resource: reservation
  requirement: Cost Purchase Permissions Restricted
  scope: compute.reservation.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Restrict Cost Purchase Permissions for Reservations
  rationale: Restricting permissions for cost purchases on reservations mitigates the risk of unauthorized or inadvertent financial commitments, which can lead to unexpected cost overruns. It ensures that only designated personnel can make such purchases, aligning with financial governance and compliance requirements like SOC2 and ISO 27001.
  description: This check verifies that only authorized users or roles have permissions to purchase committed use discounts on Compute Engine reservations. To ensure compliance, review IAM policies to confirm that the 'roles/compute.reservationAdmin' role is only granted to trusted accounts. Remediation involves auditing IAM policies and revoking permissions from unauthorized users, thereby preventing unintended financial liabilities.
  references:
  - https://cloud.google.com/compute/docs/instances/reserving-zonal-resources
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.resource_policy.snapshot_schedule_configured
  service: compute
  resource: resource_policy
  requirement: Snapshot Schedule Configured
  scope: compute.resource_policy.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Snapshot Schedule is Configured for Compute Resource Policies
  rationale: Configuring snapshot schedules for resource policies is crucial for maintaining data integrity and availability in GCP environments. It helps mitigate risks associated with data loss due to accidental deletion or system failures, ensuring business continuity and compliance with industry standards like ISO 27001 and SOC2.
  description: This rule checks whether a snapshot schedule is configured for each resource policy in Google Cloud's Compute service. Without a snapshot schedule, data may not be regularly backed up, leading to potential data loss. To verify, review the resource policy settings in the GCP Console or via the gcloud command-line tool. Remediation involves creating or updating resource policies to include regular snapshot schedules, ensuring backups are taken at appropriate intervals.
  references:
  - https://cloud.google.com/compute/docs/disks/scheduled-snapshots
  - https://cloud.google.com/architecture/disaster-recovery-planning-guide
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.compute.route.network_table_no_0_0_0_0_from_private_subnets
  service: compute
  resource: route
  requirement: Network Table No 0 0 0 0 From Private Subnets
  scope: compute.route.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Prevent 0.0.0.0/0 Routes from Private Subnets
  rationale: Allowing routes to 0.0.0.0/0 from private subnets can expose sensitive data and resources to the entire internet, increasing the risk of unauthorized access and data breaches. This configuration could lead to non-compliance with standards like PCI-DSS and HIPAA, which require stringent access controls for sensitive data.
  description: This rule checks for routes in Google Cloud Platform that allow traffic from private subnets to be directed to 0.0.0.0/0, which represents the entire internet. Ensure that your routes are configured to restrict such access unless explicitly required and securely managed. To remediate, audit your routing tables to ensure that no route from a private subnet is set to 0.0.0.0/0, and adjust the routes to ensure they target only necessary and secure destinations.
  references:
  - https://cloud.google.com/vpc/docs/routes
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.route.network_table_vpc_endpoints_used_for_saas_where_supported
  service: compute
  resource: route
  requirement: Network Table VPC Endpoints Used For Saas Where Supported
  scope: compute.route.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Use VPC Endpoints for SaaS in Network Routes
  rationale: Using VPC endpoints for SaaS applications reduces exposure to the public internet, minimizing the risk of data interception and unauthorized access. This is crucial for protecting sensitive data and maintaining compliance with regulations such as GDPR and HIPAA, which require stringent data protection controls.
  description: This rule checks whether VPC endpoints are utilized in routing tables for SaaS applications where supported. Ensuring that traffic to SaaS applications is routed through VPC endpoints can enhance security by keeping traffic within the managed Google Cloud network. To verify, inspect the routing tables for routes pointing to VPC endpoints for SaaS, ensuring they are correctly configured. Remediation includes setting up VPC endpoints and updating routing tables to leverage these endpoints for SaaS traffic.
  references:
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/privacy
- rule_id: gcp.compute.security_policy.balancing_waf_acl_attached_configured
  service: compute
  resource: security_policy
  requirement: Balancing Waf ACL Attached Configured
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure WAF ACLs are Attached and Configured on Load Balancers
  rationale: Proper configuration of Web Application Firewall (WAF) Access Control Lists (ACLs) on load balancers is crucial to protect web applications from common threats such as SQL injection and cross-site scripting. Failing to configure these ACLs can lead to security vulnerabilities, exposing sensitive data and potentially violating compliance requirements such as PCI-DSS and GDPR.
  description: This rule checks whether a WAF ACL is both attached and configured on Google Cloud Load Balancers within the security policies. It verifies the presence of security policies that include WAF rules tailored to the application's threat model. Remediation involves reviewing and updating security policies to include appropriate WAF ACLs, ensuring they are actively blocking or mitigating known vulnerabilities.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/armor/docs/rules
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.security_policy.edge_waf_block_actions_not_count_only
  service: compute
  resource: security_policy
  requirement: Edge Waf Block Actions Not Count Only
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Edge WAF Policies Enforce Block Actions
  rationale: Implementing block actions in your Edge WAF policies is crucial for actively preventing malicious traffic from reaching your applications. While 'count' actions allow monitoring and analysis of potential threats, they do not stop attacks in real-time, increasing the risk of data breaches and service disruptions. Organizations must enforce block actions to comply with security standards and mitigate threats effectively.
  description: This rule checks that security policies in Google Cloud Platform's Edge WAF are configured to enforce block actions, rather than just logging or counting suspicious activity. Without active blocking, potential threats are only monitored, which does not prevent them from causing harm. To verify, check the security policies in your GCP account to ensure that they include rules with action set to 'block'. If only 'count' actions are present, update the policy to enforce blocking for critical threat vectors. This ensures compliance with security best practices and reduces the risk of successful attacks.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/armor/docs/rules-language-reference
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.security_policy.edge_waf_ip_rate_limit_rules_configured_where_supported
  service: compute
  resource: security_policy
  requirement: Edge Waf Ip Rate Limit Rules Configured Where Supported
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Edge WAF IP Rate Limit Rules Configured
  rationale: Configuring IP rate limit rules on Edge WAF helps mitigate denial-of-service attacks and limits the potential for abuse from malicious actors. This is crucial for maintaining availability and ensuring that legitimate traffic is not disrupted, thereby supporting business continuity and compliance with security standards that require protection against such threats.
  description: This rule checks whether IP rate limit rules are configured on GCP Edge WAF within security policies where supported. To verify, inspect your security policy settings in the GCP Console or via gcloud commands to ensure rate limiting is enabled. If not configured, establish rate limits based on expected traffic patterns to prevent abuse. Remediation involves modifying security policies to include rate limiting rules, which can be done through the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/load-balancing/docs/using-firewall-rules
  - https://cloud.google.com/security-command-center/docs/concepts-cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security
  - https://cloud.google.com/armor/docs/configure-security-policies
- rule_id: gcp.compute.security_policy.edge_waf_logging_enabled
  service: compute
  resource: security_policy
  requirement: Edge Waf Logging Enabled
  scope: compute.security_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Edge WAF Logging for Security Policies
  rationale: Enabling Edge WAF logging is crucial for detecting and investigating potential security incidents, as it provides visibility into web application attacks targeted at your infrastructure. Without logging, threat actors could exploit vulnerabilities undetected, leading to data breaches, service disruptions, or non-compliance with regulations like PCI-DSS and ISO 27001 which mandate audit trails.
  description: This rule checks if logging is enabled for Edge WAF within your security policies on GCP. To verify, ensure that 'enableLogging' is set to true in the security policy configuration. If logging is not enabled, configure your WAF settings to log all traffic, providing critical data for analysis. This can be done via the GCP Console under 'Security Policies' or using the gcloud command line tool. Enabling logging facilitates proactive monitoring and response to security threats.
  references:
  - https://cloud.google.com/armor/docs/security-policy-concepts#logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/armor/docs/configure-security-policies
- rule_id: gcp.compute.security_policy.edge_waf_managed_rule_sets_enabled
  service: compute
  resource: security_policy
  requirement: Edge Waf Managed Rule Sets Enabled
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Edge WAF Managed Rule Sets on Security Policies
  rationale: Enabling Edge WAF Managed Rule Sets mitigates security risks by providing automatic protection against common web vulnerabilities such as SQL injection and cross-site scripting (XSS). This enhances the security posture of applications by reducing the risk of data breaches and downtime caused by malicious attacks. It also aids in meeting compliance requirements like PCI-DSS by ensuring that web applications are protected against known threats.
  description: This rule checks if Edge WAF Managed Rule Sets are enabled on Google Cloud Platform security policies. Managed rule sets provide predefined rules that detect and block malicious web traffic, which can be crucial for safeguarding applications from a wide range of attacks. To verify, inspect the security policies in the GCP console under 'Network Security' and ensure that managed rule sets are applied. If not enabled, configure the security policy to include Edge WAF Managed Rule Sets to enhance protection.
  references:
  - https://cloud.google.com/armor/docs/security-policy-overview
  - https://cloud.google.com/security-compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/armor/docs/managed-protection
  - https://www.nist.gov/cyberframework
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.security_policy.edge_waf_web_acl_attached_to_cdn
  service: compute
  resource: security_policy
  requirement: Edge Waf Web ACL Attached To Cdn
  scope: compute.security_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Edge WAF Web ACL is Attached to GCP CDN
  rationale: Attaching an Edge WAF Web ACL to a CDN in GCP helps mitigate web-based attacks such as SQL injection and cross-site scripting, reducing the risk of data breaches and service disruptions. This is critical for maintaining the integrity and availability of web applications and ensuring compliance with industry standards like PCI-DSS and ISO 27001.
  description: This rule checks if an Edge WAF Web ACL is attached to a Google Cloud CDN, ensuring incoming traffic is filtered through configured security policies. Verify by checking the security policy configuration in the Cloud Console or via gcloud CLI. If missing, attach an appropriate Web ACL to your CDN distribution to filter out malicious requests. Remediation involves creating or selecting a Web ACL with rules tailored to your application's security needs and associating it with your CDN.
  references:
  - https://cloud.google.com/cdn/docs/security
  - https://cloud.google.com/armor/docs/security-policy-concepts
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/armor/docs/rule-tuning
- rule_id: gcp.compute.security_policy.network_anomaly_destinations_encrypted
  service: compute
  resource: security_policy
  requirement: Network Anomaly Destinations Encrypted
  scope: compute.security_policy.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Network Anomaly Destinations Use Encrypted Channels
  rationale: Encrypting network anomaly destinations protects against unauthorized data interception and tampering, which can lead to data breaches and compliance violations. This encryption is critical for maintaining the confidentiality and integrity of data in transit, especially for organizations handling sensitive or regulated information.
  description: This rule verifies that all network anomaly destinations defined in GCP Compute Engine security policies use encrypted channels such as TLS for data transmission. To ensure compliance, review security policy configurations to confirm that all destinations are set to accept only secure, encrypted connections. Remediation involves updating security policies to enforce encryption and confirming that all endpoints support secure protocols.
  references:
  - https://cloud.google.com/vpc/docs/firewalls
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/security/encryption/default-encryption
- rule_id: gcp.compute.security_policy.network_anomaly_detectors_enabled
  service: compute
  resource: security_policy
  requirement: Network Anomaly Detectors Enabled
  scope: compute.security_policy.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Network Anomaly Detectors in Security Policies
  rationale: Network anomaly detectors are essential for identifying unusual patterns that could indicate potential security threats such as DDoS attacks or unauthorized access attempts. Enabling these detectors helps organizations to proactively mitigate risks, comply with regulatory standards like PCI-DSS and ISO 27001, and protect sensitive data from breaches.
  description: This rule checks whether network anomaly detectors are enabled in GCP security policies for Compute Engine. To verify, access the Google Cloud Console, navigate to 'Security Policies' under the 'Compute Engine' service, and ensure that anomaly detection is activated. Remediation involves enabling this feature in the security policy settings to enhance threat detection and response capabilities.
  references:
  - https://cloud.google.com/armor/docs/security-policy-concepts
  - https://cloud.google.com/security-command-center/docs/how-to-anomalies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.snapshot.dr_backup_destination_private_only
  service: compute
  resource: snapshot
  requirement: DR Backup Destination Private Only
  scope: compute.snapshot.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Snapshots for DR Are Stored in Private Buckets
  rationale: Storing backup snapshots in private destinations minimizes the risk of unauthorized access and data breaches, which can lead to significant financial and reputational damage. It ensures compliance with data protection regulations by preventing public exposure of sensitive data stored in backups. This practice supports business continuity by securing critical recovery data.
  description: This rule checks whether Compute Engine snapshots used for disaster recovery (DR) are stored in Cloud Storage buckets with private access settings. Public access to these buckets should be disabled to protect sensitive backup data. To verify, review the bucket's IAM policy and ensure 'allUsers' or 'allAuthenticatedUsers' are not granted access. Remediation involves updating the bucket's permissions to restrict access to specific service accounts or users who require it.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.snapshot.dr_backup_encrypted_at_rest_cmek
  service: compute
  resource: snapshot
  requirement: DR Backup Encrypted At Rest Cmek
  scope: compute.snapshot.encryption
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Snapshots Are Encrypted with Customer-Managed Keys
  rationale: Encrypting snapshots using Customer-Managed Encryption Keys (CMEK) enhances data confidentiality and control, reducing the risk of unauthorized access during a disaster recovery scenario. This approach aligns with compliance standards that require strong encryption methods and empowers organizations to manage encryption keys, crucial for sectors handling sensitive data.
  description: This rule verifies that all Compute Engine snapshots are encrypted at rest using Customer-Managed Encryption Keys (CMEK). By default, Google Cloud encrypts data with Google-managed keys, but using CMEK provides greater control and allows for key rotation and revocation policies. To comply, ensure that snapshot configurations specify CMEK, which can be verified via the GCP Console or gcloud CLI. Remediation involves updating snapshot settings to refer to the appropriate CMEK.
  references:
  - https://cloud.google.com/compute/docs/disks/customer-supplied-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
- rule_id: gcp.compute.snapshot.dr_backup_immutable_or_worm_enabled_where_supported
  service: compute
  resource: snapshot
  requirement: DR Backup Immutable Or Worm Enabled Where Supported
  scope: compute.snapshot.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Snapshots Have Immutable or WORM Backups Enabled
  rationale: Enabling immutable or Write Once Read Many (WORM) backups for snapshots prevents accidental or malicious data modification. This is crucial for maintaining data integrity and ensuring that disaster recovery processes can rely on untampered backups. It also supports compliance with regulations that require data retention and protection against unauthorized changes.
  description: This rule verifies that snapshots in GCP have immutable or WORM backup settings enabled where supported. Enabling these settings ensures that once data is written, it cannot be altered, providing a secure backup solution. Administrators should configure snapshot settings in the GCP Console or via the API to enable immutability where available. Regular audits and configuration checks should be conducted to ensure continuous compliance.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance/cis-gcp-1-2-0
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.snapshot.dr_backup_schedule_defined_min_frequency
  service: compute
  resource: snapshot
  requirement: DR Backup Schedule Defined Min Frequency
  scope: compute.snapshot.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Min Frequency for Compute Snapshot DR Backups
  rationale: Defining a minimum frequency for disaster recovery (DR) backups of snapshots is critical to minimize data loss and ensure data availability in the event of a failure. Regular backups enhance resilience by allowing businesses to quickly restore operations, thereby reducing downtime and mitigating financial and reputational damage. Compliance with industry regulations often mandates regular backup schedules to safeguard sensitive data against loss due to disasters.
  description: This rule checks if a minimum frequency for DR backup schedules is defined for Google Cloud Compute snapshots. It ensures that snapshots are regularly backed up according to a predefined schedule, which is crucial for effective disaster recovery. To verify, review the configuration of snapshot schedules in the Google Cloud Console or via gcloud CLI, ensuring that they meet the minimum frequency requirements. If not configured, establish a backup schedule that aligns with your organizationâ€™s data recovery and business continuity objectives.
  references:
  - https://cloud.google.com/compute/docs/disks/scheduled-snapshots
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/compute/docs/disaster-recovery
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.compute.snapshot.dr_restore_cross_account_restore_blocked_by_policy
  service: compute
  resource: snapshot
  requirement: DR Restore Cross Account Restore Blocked By Policy
  scope: compute.snapshot.policy_management
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Prevent Cross-Account Restore for Compute Snapshots
  rationale: Preventing cross-account restores of snapshots is crucial to minimizing unauthorized access and potential data breaches. By enforcing policies that restrict such actions, organizations can ensure that sensitive data remains within the intended account boundaries, reducing the risk of data leakage and aiding in compliance with data protection regulations.
  description: This rule checks for policies that block cross-account restoration of Compute Engine snapshots, ensuring that snapshots cannot be restored in unauthorized accounts. To verify compliance, ensure that IAM policies and organization policies are configured to restrict snapshot sharing and restoration to trusted accounts only. Remediation involves reviewing and updating policy configurations in the GCP Console, ensuring proper permissions are set to prevent unauthorized data access.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.snapshot.dr_restore_from_encrypted_backups_only
  service: compute
  resource: snapshot
  requirement: DR Restore From Encrypted Backups Only
  scope: compute.snapshot.encryption
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Snapshots are Restored from Encrypted Backups Only
  rationale: Restoring from encrypted backups ensures data confidentiality and integrity, mitigating the risk of data breaches during disaster recovery. This practice protects sensitive information from unauthorized access, and aligns with compliance requirements such as GDPR and PCI-DSS, which mandate strong encryption for data at rest.
  description: This rule verifies that all Compute Engine snapshots used for disaster recovery are created with encryption. To ensure compliance, check that snapshots are encrypted using either Google-managed or customer-supplied encryption keys. To remediate, enable encryption on existing snapshots by creating new encrypted snapshots from the original disks, and ensure all future snapshots are encrypted by default.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.snapshot.dr_restore_logs_and_artifacts_encrypted
  service: compute
  resource: snapshot
  requirement: DR Restore Logs And Artifacts Encrypted
  scope: compute.snapshot.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Snapshots Encryption for DR Logs & Artifacts
  rationale: Encrypting snapshots used for disaster recovery (DR) ensures that sensitive data remains protected even in backup and recovery scenarios, reducing the risk of unauthorized data access. This practice is crucial for meeting compliance requirements such as PCI-DSS and HIPAA, which mandate encryption of data at rest to protect customer data and maintain privacy standards.
  description: This rule verifies that all Compute Engine snapshots, particularly those containing disaster recovery logs and artifacts, are encrypted using Google-managed or customer-managed encryption keys. To comply, navigate to the GCP Console, select 'Snapshots', and ensure the 'Encryption' field indicates the use of a valid key. Remediation involves configuring existing snapshots to use encryption keys and ensuring new snapshots are automatically encrypted.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/security/encryption/default-encryption
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.snapshot.dr_restore_roles_least_privilege
  service: compute
  resource: snapshot
  requirement: DR Restore Roles Least Privilege
  scope: compute.snapshot.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Snapshot Restoration Roles
  rationale: Assigning the least privilege necessary for snapshot restoration reduces the risk of unauthorized access to sensitive data and potential data loss. Over-privileged roles can lead to accidental or malicious data exposure, violating compliance with regulations like GDPR and HIPAA, and increasing the attack surface of your cloud infrastructure.
  description: This rule checks if roles assigned for snapshot restoration have the minimum permissions required. Ensure that only essential personnel have the ability to restore snapshots by using custom roles or predefined roles with limited permissions. Regularly audit IAM policies to verify compliance with the principle of least privilege and remove excessive permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-custom-roles
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.compute.snapshot.public_access_blocked
  service: compute
  resource: snapshot
  requirement: Public Access Blocked
  scope: compute.snapshot.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Public Access is Disabled for Compute Snapshots
  rationale: Snapshots containing system and user data, if exposed publicly, can lead to unauthorized data access, potentially resulting in data breaches, non-compliance with data protection regulations, and reputational damage. Securing snapshots is essential to prevent malicious actors from exploiting sensitive information and to maintain the integrity and confidentiality of critical business data.
  description: This rule checks for Google Cloud Compute snapshots that are configured with public access permissions. Snapshots should not be publicly accessible as they may contain sensitive information. To verify, review the snapshot's IAM policies and ensure that no public users (i.e., 'allUsers' or 'allAuthenticatedUsers') have access. Remediation involves updating the IAM policy to restrict access to only necessary and authorized accounts or groups.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0 - 5.3
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 5 CM-8
  - PCI-DSS Requirement 1.2.1
- rule_id: gcp.compute.snapshot.public_snapshot_access_restricted
  service: compute
  resource: snapshot
  requirement: Public Snapshot Access Restricted
  scope: compute.snapshot.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: critical
  title: Restrict Public Access to Compute Snapshots
  rationale: Allowing public access to Compute Engine snapshots exposes sensitive data, leading to potential data breaches and unauthorized data manipulation. It violates compliance requirements such as PCI-DSS and HIPAA, which mandate stringent access controls. Restricting access minimizes the risk of data leakage and ensures data integrity essential for business continuity.
  description: This rule checks for any Google Cloud Compute snapshots with public access permissions. Snapshots should be configured with private access, employing Identity and Access Management (IAM) policies to limit visibility and modifications to authorized users only. Remediation involves reviewing and modifying IAM policies to remove 'allUsers' or 'allAuthenticatedUsers' roles from snapshot resources. Use the Google Cloud Console or gcloud CLI to adjust permissions and confirm settings.
  references:
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.compute.snapshot.rdp_port_3389_blocked
  service: compute
  resource: snapshot
  requirement: Rdp Port 3389 Blocked
  scope: compute.snapshot.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure RDP Port 3389 is Blocked on Compute Snapshots
  rationale: Exposing RDP port 3389 can lead to unauthorized access and potential data breaches, especially if sensitive information is stored on instances. Blocking this port mitigates the risk of brute force attacks and unauthorized access, aligning with compliance and governance standards that require minimizing unnecessary exposure of services.
  description: This check ensures that RDP port 3389 is not open on any snapshots of compute instances. Snapshots should be configured to block inbound traffic on port 3389 to prevent unauthorized remote desktop access. Verify by reviewing firewall rules associated with the snapshots and adjust the rules to deny access on this port. Remediation involves updating firewall settings to explicitly block port 3389 for all instances derived from these snapshots.
  references:
  - https://cloud.google.com/compute/docs/firewalls
  - https://www.cisecurity.org/cis-benchmarks/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/instances/windows/connecting-rdp
- rule_id: gcp.compute.ssl_policy.balancing_insecure_ssl_ciphers_configured
  service: compute
  resource: ssl_policy
  requirement: Balancing Insecure SSL Ciphers Configured
  scope: compute.ssl_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Configure Secure SSL Ciphers in SSL Policies
  rationale: Using insecure SSL ciphers can expose your application to vulnerabilities such as man-in-the-middle attacks, compromising data confidentiality and integrity. Organizations must adhere to regulatory standards, like PCI-DSS and ISO 27001, which mandate the use of strong encryption methods to protect sensitive information. Proper configuration of SSL policies helps mitigate risks associated with outdated encryption protocols.
  description: This rule checks if SSL policies in Google Cloud's Compute Engine are configured to use secure ciphers only. To verify, ensure that your SSL policies exclude weak ciphers such as RC4 or any ciphers using less than 128-bit encryption. Remediation involves updating the SSL policy to include only strong ciphers, which can be done via the Google Cloud Console or gcloud command-line tool. This ensures compliance with security best practices and regulatory requirements.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.ssllabs.com/projects/best-practices/
- rule_id: gcp.compute.ssl_policy.distributions_using_deprecated_ssl_protocols_configured
  service: compute
  resource: ssl_policy
  requirement: Distributions Using Deprecated SSL Protocols Configured
  scope: compute.ssl_policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SSL Policies Avoid Deprecated Protocols
  rationale: Using deprecated SSL protocols can expose your systems to vulnerabilities such as man-in-the-middle attacks, potentially leading to data breaches and loss of customer trust. Compliance with industry standards and regulations, like PCI-DSS and NIST, often requires the use of secure protocols to protect sensitive data in transit.
  description: This rule checks for SSL policies in Google Cloud Platform's Compute service that are configured with deprecated SSL protocols. Deprecated protocols lack modern security features and are more susceptible to exploitation. To mitigate this risk, update your SSL policies to use only supported and secure protocols such as TLS 1.2 or 1.3. Verification can be done by reviewing the SSL policy settings in the GCP Console or using the gcloud command-line tool. Remediation involves modifying the SSL policy to exclude deprecated protocols.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.subnetwork.data_warehouse_subnet_group_private_subnets_only
  service: compute
  resource: subnetwork
  requirement: Data Warehouse Subnet Group Private Subnets Only
  scope: compute.subnetwork.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Data Warehouse Subnets Are Private Only
  rationale: Limiting Data Warehouse subnets to private IP addresses minimizes exposure to the internet, reducing risk of unauthorized access and data breaches. This is crucial for maintaining data confidentiality and meeting compliance requirements like PCI-DSS and GDPR, which mandate strict controls over access to sensitive data.
  description: This rule checks that subnets within the Data Warehouse subnet group are configured to use private IP ranges only. Verify that 'IPV6 access type' is set to 'INTERNAL' and no external IP addresses are assigned. To remediate, ensure that all subnets in the Data Warehouse group are created with 'Private Google Access' enabled and do not attach external IPs to any instances within these subnets.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.subnetwork.logs_enabled
  service: compute
  resource: subnetwork
  requirement: Logs Enabled
  scope: compute.subnetwork.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Subnetwork Flow Logs in GCP Compute Engine
  rationale: Enabling flow logs for subnetworks is crucial for monitoring network traffic, detecting anomalies, and identifying potential security threats. It provides visibility into network flows, aiding in incident response and forensic investigations. Moreover, it supports compliance with regulatory standards that require detailed logging of network activity.
  description: This rule checks if flow logs are enabled for GCP Compute Engine subnetworks. Flow logs capture metadata about network traffic, which can be used for analysis and monitoring. To verify, navigate to the Google Cloud Console, select 'VPC networks', and check the 'Flow logs' status for each subnetwork. To enable flow logs, edit the subnetwork settings and set 'Flow logs' to 'On'.
  references:
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.compute.subnetwork.network_subnet_private_subnets_no_igw_route
  service: compute
  resource: subnetwork
  requirement: Network Subnet Private Subnets No Igw Route
  scope: compute.subnetwork.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure No Internet Gateway Route for Private Subnets
  rationale: Allowing internet access to private subnets via an internet gateway route can lead to unauthorized data exposure and potential attacks from public networks. This exposure can increase the risk of data breaches, non-compliance with regulations like PCI-DSS, and violation of internal security policies. Maintaining strict control over network routes helps preserve data confidentiality and integrity.
  description: This rule checks that private subnets in GCP do not have routes directing traffic through an internet gateway. Ensure that private subnets only communicate through internal or VPN/Interconnect routes, avoiding public internet exposure. To verify, inspect the routing table of each subnetwork and ensure that no route with a destination of 0.0.0.0/0 is pointing to an internet gateway. As a remediation step, remove any such routes and restrict the route table to internal addresses only.
  references:
  - https://cloud.google.com/vpc/docs/routes
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/vpc/docs/vpc-best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.subnetwork.network_subnet_public_subnets_use_nacl_restrictions
  service: compute
  resource: subnetwork
  requirement: Network Subnet Public Subnets Use Nacl Restrictions
  scope: compute.subnetwork.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Enforce Network ACLs on Public Subnets
  rationale: Public subnets without proper Network ACLs pose significant security risks, as they can allow unauthorized access to resources within the network. This can lead to data breaches, unauthorized data exfiltration, and potential non-compliance with regulatory frameworks such as PCI-DSS and GDPR which mandate strict access controls. Ensuring that public subnets are protected by Network ACLs mitigates these risks by controlling inbound and outbound traffic effectively.
  description: This rule checks that all public subnets in your GCP environment have Network ACLs (Access Control Lists) applied to restrict unauthorized access. Network ACLs should be configured to allow only the necessary traffic to and from your public subnets. To verify, ensure that each public subnet has an ACL applied with rules that define explicit allow and deny permissions. Remediation involves creating or updating ACLs to include rules that limit exposure to critical resources by denying unnecessary traffic.
  references:
  - https://cloud.google.com/vpc/docs/vpc#subnet-ranges
  - https://cloud.google.com/vpc/docs/firewalls#firewall_rules_implementation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.compute.subnetwork.network_subnet_route_table_association_present
  service: compute
  resource: subnetwork
  requirement: Network Subnet Route Table Association Present
  scope: compute.subnetwork.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Subnet Route Table Association in GCP
  rationale: Associating a subnet with a route table is critical for defining how network traffic is directed within your GCP environment. Without proper association, there could be a risk of unauthorized access or data exfiltration as traffic routes may not comply with security policies. This is essential for maintaining compliance with regulatory frameworks that require stringent network access controls.
  description: This rule checks that each Google Cloud subnetwork is associated with a route table, ensuring that traffic is routed according to defined security policies. To verify, review the subnet configurations in the GCP Console under VPC Network settings to ensure route tables are assigned correctly. Remediation involves associating the missing route table with the subnetwork to ensure all ingress and egress traffic is appropriately controlled.
  references:
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/network-connectivity/docs
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.target_https_proxy.balancing_ssl_listeners_configured
  service: compute
  resource: target_https_proxy
  requirement: Balancing SSL Listeners Configured
  scope: compute.target_https_proxy.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure SSL Listeners Configured for HTTPS Proxies
  rationale: Configuring SSL listeners for target HTTPS proxies is critical to encrypt data in transit, preventing unauthorized interception and data breaches. Without SSL, data is vulnerable to man-in-the-middle attacks, potentially leading to sensitive information exposure. This configuration also aligns with compliance requirements for data protection and privacy, such as PCI-DSS and HIPAA.
  description: This rule checks that all target HTTPS proxies have SSL listeners properly configured to ensure secure communication. To verify, inspect the target HTTPS proxy settings in your GCP environment and ensure that SSL certificates are applied to each listener. Remediation involves updating the proxy configuration to include valid SSL certificates and enforcing HTTPS for all incoming requests.
  references:
  - https://cloud.google.com/load-balancing/docs/https/
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/
  - https://cloud.google.com/security/encryption-in-transit
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.url_map.alb_https_redirect_configured
  service: compute
  resource: url_map
  requirement: Alb HTTPS Redirect Configured
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure HTTPS Redirect is Configured for ALB URL Maps
  rationale: Configuring HTTPS redirects for Application Load Balancer URL maps ensures that all traffic to your web applications is encrypted, protecting sensitive data from eavesdropping and man-in-the-middle attacks. This is essential for maintaining data confidentiality, integrity, and securing your applications against potential threats. It also helps in meeting compliance requirements such as PCI-DSS which mandate secure transmission of data.
  description: This rule checks if URL maps in Google Cloud's Application Load Balancer have HTTPS redirects configured. Without this configuration, traffic can be transmitted over insecure HTTP, exposing it to interception. To verify, inspect each URL map configuration for HTTPS redirect settings. Remediation involves updating the URL map to enforce HTTPS by setting up appropriate redirect response configurations within the load balancing settings.
  references:
  - https://cloud.google.com/load-balancing/docs/https/setting-up-https-redirect
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.url_map.edge_cache_policy_allowlists_minimal_query_headers_cookies
  service: compute
  resource: url_map
  requirement: Edge Cache Policy Allowlists Minimal Query Headers Cookies
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Edge Cache Policy for Minimal Query Headers and Cookies
  rationale: Allowing excessive query headers and cookies in an edge cache policy can expose sensitive information, increase the attack surface, and lead to cache poisoning attacks. Restricting this ensures that only the necessary data is cached, reducing the risk of unauthorized access and data breaches, while complying with security frameworks like PCI-DSS and HIPAA which mandate minimal data exposure.
  description: This rule checks that the Edge Cache Policy in `url_map` resources minimizes the use of query headers and cookies, allowing only essential ones. Verify that the configuration explicitly specifies which headers and cookies are permitted. Remediation involves reviewing the `url_map` configuration and updating the `edge_cache_policy` to restrict query headers and cookies to only those required for functionality. This reduces potential security vulnerabilities and limits data exposure.
  references:
  - https://cloud.google.com/load-balancing/docs/https/edge-cache
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.compute.url_map.edge_cache_policy_no_sensitive_headers_cached
  service: compute
  resource: url_map
  requirement: Edge Cache Policy No Sensitive Headers Cached
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Caching of Sensitive Headers in Edge Cache Policies
  rationale: Caching sensitive headers at the edge can expose confidential information such as authentication tokens or personal data, increasing the risk of unauthorized access and data breaches. This practice can compromise user privacy and violate compliance standards like GDPR, PCI-DSS, and HIPAA, leading to potential legal penalties and reputational damage.
  description: This rule checks if URL maps in GCP Compute Engine have edge cache policies that improperly cache sensitive headers. Configuration should exclude headers like Authorization or Set-Cookie from being cached. To verify, inspect the edge cache configuration within each URL map and ensure sensitive headers are omitted. Remediation involves updating the edge cache settings to exclude sensitive headers and aligning with best practices for secure data handling.
  references:
  - https://cloud.google.com/load-balancing/docs/https/traffic-management#cache-key-policy
  - https://cloud.google.com/security/compliance/cis#gcp-compute-engine-2-0
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices#data_protection
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.url_map.edge_cache_policy_respects_cache_control_no_store_private
  service: compute
  resource: url_map
  requirement: Edge Cache Policy Respects Cache Control No Store Private
  scope: compute.url_map.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Edge Cache Policy Honors Cache Control Directives
  rationale: Respecting Cache-Control directives such as 'no-store' and 'private' is crucial to prevent sensitive data from being cached at the edge, which can lead to unauthorized data access or data leaks. This practice minimizes the risk of exposing private information and aligns with privacy regulations and security best practices, ensuring that end-user data is handled securely.
  description: 'This rule verifies that the Edge Cache Policy for URL Maps in Google Cloud respects ''Cache-Control: no-store'' and ''Cache-Control: private'' directives. To check, ensure that URL Maps are correctly configured to prevent caching of sensitive data when these directives are present. Remediation involves adjusting Edge Cache Settings in the URL Map to comply with these directives, preventing potentially sensitive information from being stored in edge caches.'
  references:
  - https://cloud.google.com/load-balancing/docs/https/edge-cache
  - https://cloud.google.com/architecture/best-practices-for-content-delivery#cache-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org
  - https://www.hipaajournal.com
- rule_id: gcp.compute.url_map.edge_origin_request_policy_forward_cookies_minimal
  service: compute
  resource: url_map
  requirement: Edge Origin Request Policy Forward Cookies Minimal
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Minimal Cookie Forwarding in URL Map Policy
  rationale: Minimizing cookie forwarding in URL Map policies reduces the risk of exposing sensitive data to unintended endpoints, thereby mitigating potential lateral movement or data breaches. This control is crucial for maintaining a secure and compliant environment, especially under regulations like GDPR, which emphasize data protection and privacy.
  description: This rule verifies that URL Map configurations in GCP's Compute Engine enforce minimal cookie forwarding settings. This involves checking that only essential cookies are forwarded to the origin, preventing unnecessary exposure of sensitive information. To remediate, configure the URL Map to restrict cookie forwarding by selecting the 'Minimal' option under 'Edge Origin Request Policy'. Verification can be done through the Google Cloud Console or gcloud CLI by reviewing URL Map settings.
  references:
  - https://cloud.google.com/load-balancing/docs/https/https-load-balancer-url-map
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.compute.url_map.edge_origin_request_policy_forward_headers_minimal
  service: compute
  resource: url_map
  requirement: Edge Origin Request Policy Forward Headers Minimal
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Minimize Forwarded Headers in Edge Origin Requests
  rationale: Minimizing the headers forwarded in edge origin requests reduces the risk of exposing sensitive information and limits the attack surface. This is crucial for maintaining secure communication channels and protecting data integrity. Ensuring minimal header forwarding aligns with best practices for data protection and supports compliance with privacy regulations such as GDPR and CCPA.
  description: This rule checks that Compute Engine URL Maps are configured to forward only necessary headers to origins. To verify, inspect the URL Map settings in the GCP Console or via gcloud CLI to ensure the 'edgeSecurityPolicy' is set to forward minimal headers. Remediation involves adjusting the policy to exclude unnecessary headers, thus enhancing data privacy and security.
  references:
  - https://cloud.google.com/load-balancing/docs/https/policies
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.compute.url_map.edge_origin_request_policy_forward_querystrings_minimal
  service: compute
  resource: url_map
  requirement: Edge Origin Request Policy Forward Querystrings Minimal
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Minimal Forwarding of Querystrings in Edge Origin Requests
  rationale: Forwarding unnecessary querystrings in edge origin requests can expose sensitive data and lead to security vulnerabilities such as information leakage. This configuration minimizes data exposure, reducing the risk of attacks such as query injection and helps maintain compliance with data protection regulations by ensuring only essential data is transmitted.
  description: This rule checks whether your GCP URL Maps are configured to forward only the minimal necessary set of querystrings in edge origin requests. To verify, review the URL Maps in your GCP project and ensure the 'forwardQueryString' setting is minimized. Remediation involves modifying the URL Map configuration to restrict querystring forwarding to only those necessary for application functionality, which can be done through the GCP Console or gcloud command line tool.
  references:
  - https://cloud.google.com/load-balancing/docs/https/setting-up-https-server#url-map
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/docs/security
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.url_map.edge_origin_request_policy_no_secret_headers_forwarded
  service: compute
  resource: url_map
  requirement: Edge Origin Request Policy No Secret Headers Forwarded
  scope: compute.url_map.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Prevent Forwarding Secret Headers in URL Map Policies
  rationale: Forwarding secret headers in URL map configurations can expose sensitive information to unauthorized parties, leading to data breaches and compliance violations. Proper handling of such headers is crucial to prevent threat actors from exploiting them to gain unauthorized access or perform malicious actions, aligning with compliance requirements like PCI-DSS and HIPAA.
  description: This rule checks whether URL map configurations in GCP's Compute Engine are forwarding secret headers to the origin. The absence of restrictions on forwarding secret headers can lead to security vulnerabilities. To ensure security, verify the URL map's edge origin request policies to ensure no secret headers are forwarded. Remediation involves updating the URL map configuration to explicitly disallow forwarding of sensitive headers.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/load-balancing/docs/https
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.compute.url_map.edge_regex_set_no_overly_broad_patterns
  service: compute
  resource: url_map
  requirement: Edge Regex Set No Overly Broad Patterns
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure URL Maps Do Not Use Overly Broad Regex Patterns
  rationale: Overly broad regex patterns in URL maps can inadvertently expose sensitive endpoints, leading to unauthorized access and potential data breaches. This can increase the attack surface and complicate incident response efforts. Ensuring precise regex patterns aligns with security best practices and helps meet compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks URL maps for the presence of overly broad regex patterns that could match unintended paths. To verify, examine the 'pathMatcher' configuration in your URL map resources to ensure regex patterns are specific and targeted. Remediation involves refining regex patterns to only match the intended paths, reducing potential exposure of sensitive resources.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map-concepts
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.compute.url_map.edge_regex_set_not_empty
  service: compute
  resource: url_map
  requirement: Edge Regex Set Not Empty
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Edge Regex Set is Defined for URL Maps
  rationale: Defining an edge regex set for URL maps in GCP is crucial to controlling traffic routing accurately. Without it, misrouted traffic could lead to security vulnerabilities, such as exposure to unauthorized access or data leakage. Proper configuration helps in meeting compliance requirements like PCI-DSS by ensuring data flows through secure and verified paths.
  description: This rule checks that the edge regex set for URL maps is not empty, ensuring that traffic is routed according to defined policies. To verify, inspect the URL map configuration in the GCP Console and ensure that a regex set is specified for edge routing. Remediation involves updating the URL map to include a comprehensive regex set that aligns with your routing and security policies, preventing potential misrouting or unauthorized access.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map-concepts
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.url_map.edge_regex_set_used_by_at_least_one_rule
  service: compute
  resource: url_map
  requirement: Edge Regex Set Used By At Least One Rule
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Regex Sets Are Utilized in URL Map Rules
  rationale: Utilizing edge regex sets in URL maps enhances the security and performance of load-balanced applications by enabling more precise routing and traffic management. Failure to employ regex can result in inefficient routing rules, potential exposure to malformed requests, and reduced security posture. This practice aligns with regulatory requirements for protecting data in transit and maintaining a secure configuration baseline.
  description: This rule checks if at least one rule in a URL map utilizes an edge regex set for matching requests. An edge regex set allows for advanced pattern matching, ensuring that traffic is directed based on specific and complex criteria. To verify, review the load balancer configuration for URL maps and ensure regex patterns are used where applicable. Remediation involves updating the URL map to include regex sets in the rules to improve traffic management and security.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.compute.url_map.edge_rule_group_no_permit_all_rule
  service: compute
  resource: url_map
  requirement: Edge Rule Group No Permit All Rule
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Prevent Open Access in Edge Rule Groups for URL Maps
  rationale: Allowing unrestricted access in edge rule groups can expose critical application endpoints to potential attacks such as DDoS and unauthorized data access. Controlling access ensures that only legitimate traffic reaches your applications, thereby mitigating security risks and helping comply with industry standards like PCI-DSS and ISO 27001.
  description: This rule checks for 'permit all' configurations in edge rule groups of URL maps within your Google Cloud Platform environment. A 'permit all' rule can lead to exposing your application to the internet without restrictions, leading to security vulnerabilities. To remediate, configure specific allow or deny rules based on IP ranges, headers, or other attributes to limit access. Verification can be done by reviewing the URL map edge rule groups in the GCP Console or using gcloud commands to ensure no overly permissive rules exist.
  references:
  - https://cloud.google.com/load-balancing/docs/https#url_map
  - CIS GCP Foundation Benchmark v1.3.0 - Section 7.2
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.compute.url_map.edge_rule_group_references_only_approved_managed_sets_w_used
  service: compute
  resource: url_map
  requirement: Edge Rule Group References Only Approved Managed Sets W Used
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure URL Maps Reference Approved Managed Sets
  rationale: Using only approved managed sets in URL Maps helps mitigate the risk of unauthorized access and configuration drift, which can lead to potential exposure of sensitive data and service disruption. This practice supports maintaining consistent security policies across the cloud environment, aligns with compliance requirements, and helps prevent misconfigurations that could be exploited by attackers.
  description: This rule checks that URL Maps in your GCP project reference only approved managed sets for edge rule groups. By enforcing this control, organizations ensure that only vetted and authorized configurations are applied, reducing the risk of security vulnerabilities. To verify, inspect the URL Maps configurations to confirm they are exclusively linked to the pre-approved sets. Remediate by updating URL Maps to reference only those managed sets that have been reviewed and approved by your security team.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/blog/products/identity-security/ensuring-compliance-in-the-cloud
- rule_id: gcp.compute.url_map.edge_rule_group_unique_priorities
  service: compute
  resource: url_map
  requirement: Edge Rule Group Unique Priorities
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Unique Priorities for Edge Rule Groups in URL Maps
  rationale: Unique priorities for edge rule groups within a URL map are crucial to preventing conflicts and ensuring predictable routing behavior. This reduces the risk of misrouting traffic, which can lead to application downtime, loss of service availability, and potential security vulnerabilities. Additionally, maintaining unique priorities aligns with regulatory requirements for system integrity and availability.
  description: This rule checks that each edge rule group within a Google Cloud Platform URL map has a unique priority. Edge rule groups with non-unique priorities can cause conflicts in traffic routing, leading to unexpected behavior. To verify, review the URL map configuration and ensure each rule group has a distinct priority. Remediation involves adjusting the priorities so that no two edge rule groups share the same value, ensuring proper traffic flow and application functionality.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.url_map.edge_rule_group_visibility_config_enabled
  service: compute
  resource: url_map
  requirement: Edge Rule Group Visibility Config Enabled
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Edge Rule Group Visibility for URL Maps
  rationale: Enabling Edge Rule Group Visibility for URL Maps is crucial for maintaining transparency and control over HTTP(S) load balancing configurations. This setting helps detect unauthorized changes and ensures that traffic rules align with organizational policies, reducing the risk of misconfigurations that could lead to data breaches or service disruptions. Furthermore, it supports compliance with regulatory standards by providing a clear audit trail of load balancing configurations.
  description: This check verifies that the Edge Rule Group Visibility Config is enabled for URL Maps within Google Cloud's HTTP(S) Load Balancing. When enabled, this feature allows administrators to see and audit the rules applied at the edge, ensuring proper configuration and facilitating troubleshooting. To enable this setting, navigate to the Google Cloud Console, select the appropriate URL Map, and activate the visibility configuration under the advanced settings. This action enhances visibility and management of traffic routing rules.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/load-balancing/docs/https
- rule_id: gcp.compute.url_map.edge_rule_has_effective_action_not_count_only
  service: compute
  resource: url_map
  requirement: Edge Rule Has Effective Action Not Count Only
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure URL Map Edge Rule Has Action Beyond Count Only
  rationale: Using 'count only' as the sole action in URL map edge rules can lead to inadequate response measures against potential threats. This configuration might result in missed opportunities for automated security enforcement, increasing the risk of malicious traffic going unchecked and potentially impacting application availability and data integrity.
  description: This rule checks URL map edge rules to ensure that they include actions beyond 'count only'. Edge rules should be configured with effective actions such as 'deny', 'redirect', or 'allow' to actively manage and mitigate malicious traffic. To verify, inspect the URL map configurations in the GCP Console or via gcloud CLI for edge rules without effective actions and update them accordingly to enhance security measures.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.url_map.edge_rule_priority_unique_within_web_acl
  service: compute
  resource: url_map
  requirement: Edge Rule Priority Unique Within Web ACL
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Unique Edge Rule Priority in URL Maps
  rationale: Unique priorities for edge rules within a URL map are crucial to ensuring predictable traffic routing and security rule application. Duplicate priorities can lead to conflicts and unexpected behavior, potentially exposing services to unauthorized access or data leakage. Maintaining unique priorities helps in complying with organizational security policies and regulatory requirements by ensuring clarity and control over network traffic management.
  description: This rule checks that each edge rule within a URL map in Google Cloud has a unique priority. URL maps are used to route requests to appropriate backend services, and edge rules define specific behaviors based on request attributes. Conflicting priorities can lead to ambiguous routing decisions, so it is important to assign distinct priorities to each rule. To remediate, review and update the URL map configurations to ensure each edge rule has a unique priority value, and test routing behavior post-update.
  references:
  - https://cloud.google.com/load-balancing/docs/url-map-concepts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.compute.url_map.edge_rule_references_valid_ip_or_regex_sets_only
  service: compute
  resource: url_map
  requirement: Edge Rule References Valid Ip Or Regex Sets Only
  scope: compute.url_map.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Validate IP/Regex Sets in URL Map Edge Rules
  rationale: Ensuring edge rules in URL maps reference valid IP or regex sets helps prevent unauthorized access and potential misconfigurations that could lead to exposure of sensitive data. This is crucial for maintaining the integrity of network traffic management and aligning with security best practices. Proper validation supports compliance with regulatory standards by ensuring that only intended traffic patterns are matched and processed.
  description: This rule checks that edge rules in GCP URL maps only reference valid IP or regex sets. Invalid references can lead to unintended access or misdirected traffic, compromising the security of applications. To verify compliance, review your URL map configurations in the Google Cloud Console or via the gcloud command-line tool, ensuring all edge rules are correctly referencing existing and authorized IP or regex sets. Remediate by updating any invalid references to point to correct and authorized sets.
  references:
  - https://cloud.google.com/load-balancing/docs/https/https-load-balancer-url-map
  - https://cloud.google.com/load-balancing/docs/https/setting-up-https
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-cloud-security-controls
- rule_id: gcp.compute.url_map.network_lb_access_logs_enabled
  service: compute
  resource: url_map
  requirement: Network Lb Access Logs Enabled
  scope: compute.url_map.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Network Load Balancer Access Logs
  rationale: Enabling access logs for Network Load Balancers provides critical visibility into traffic patterns and potential security threats, allowing for timely detection and response to unauthorized access attempts or anomalies. This is essential for compliance with auditing requirements and helps in forensic investigations and operational oversight.
  description: This rule checks whether access logging is enabled for Network Load Balancers using URL maps in Google Cloud Platform. Logs capture important details about incoming requests, which can be used for monitoring and troubleshooting. To enable logging, ensure that the 'logConfig' field is set within the URL map's configuration in the Google Cloud Console or via gcloud CLI. Regularly review logs to identify any suspicious activity and maintain a secure environment.
  references:
  - https://cloud.google.com/load-balancing/docs/network/network-logging
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/security-foundations
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.compute.url_map.network_lb_listener_tls_min_1_2
  service: compute
  resource: url_map
  requirement: Network Lb Listener TLS Min 1 2
  scope: compute.url_map.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Use TLS 1.2 or Higher for Network LB Listener on URL Maps
  rationale: Ensuring TLS 1.2 or higher for Network Load Balancer listeners mitigates risks from outdated encryption protocols, protecting data in transit from interception and tampering. This is essential for maintaining the confidentiality and integrity of data, meeting compliance standards such as PCI-DSS and HIPAA, and preventing potential data breaches.
  description: This rule checks that Network Load Balancer listeners associated with URL maps enforce a minimum TLS version of 1.2. TLS 1.0 and 1.1 have known vulnerabilities that can be exploited by attackers. To verify, inspect the URL map configurations in the GCP Console or via the gcloud CLI, ensuring 'minTlsVersion' is set to 'TLS_1_2'. If not compliant, update the configuration to enforce TLS 1.2 or higher to ensure secure communications.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
- rule_id: gcp.compute.url_map.network_lb_valid_certificate_attached
  service: compute
  resource: url_map
  requirement: Network Lb Valid Certificate Attached
  scope: compute.url_map.network_security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Valid TLS Certificate on GCP Network Load Balancer
  rationale: Ensuring that a valid TLS certificate is attached to a Google Cloud Platform (GCP) Network Load Balancer is critical for protecting data in transit. Without a valid certificate, sensitive data may be exposed to interception or tampering, leading to potential data breaches and non-compliance with standards such as PCI-DSS and HIPAA, which require secure data transmission.
  description: This rule checks that a valid TLS certificate is attached to the URL map of a Network Load Balancer in GCP. A URL map with an invalid or missing certificate can lead to unencrypted traffic, exposing data to security threats. To verify, ensure that the URL map configuration includes a valid SSL certificate. Remediation involves updating or attaching a valid certificate to the load balancer using the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/load-balancing/docs/ssl-certificates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.compute.url_map.network_lb_waf_attached_where_supported
  service: compute
  resource: url_map
  requirement: Network Lb Waf Attached Where Supported
  scope: compute.url_map.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Network LB WAF is Configured on Supported URL Maps
  rationale: Attaching a Web Application Firewall (WAF) to Network Load Balancer URL Maps mitigates security threats such as SQL injection and cross-site scripting by filtering and monitoring HTTP traffic. This reduces the risk of application-layer attacks, which can lead to data breaches or service disruptions, ensuring compliance with security frameworks like PCI-DSS and enhancing overall system resilience.
  description: This rule verifies that a Web Application Firewall is attached to URL Maps where supported within Network Load Balancer configurations. A WAF protects applications by monitoring and filtering HTTP traffic between a web application and the Internet. To ensure compliance, inspect the URL Map settings in the Google Cloud Console and confirm that a WAF is enabled for each applicable URL Map. Remediate by configuring a WAF via the Console or gcloud CLI to enhance security posture.
  references:
  - https://cloud.google.com/load-balancing/docs/https/setting-up-https#waf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security
  - https://cloud.google.com/armor/docs/rules-overview
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.compute.vpn_tunnel.network_cgw_ip_addresses_valid_and_owned
  service: compute
  resource: vpn_tunnel
  requirement: Network Cgw Ip Addresses Valid And Owned
  scope: compute.vpn_tunnel.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Validate and Own VPN Tunnel Network CGW IP Addresses
  rationale: Ensuring the VPN tunnel's network Customer Gateway (CGW) IP addresses are valid and owned by your organization mitigates risks of unauthorized access and data breaches. It prevents misconfigurations that could lead to traffic interception or redirection by malicious actors. This is crucial for maintaining secure and compliant network architecture as required by standards such as PCI-DSS and HIPAA.
  description: This rule checks that the IP addresses configured for the VPN tunnel's CGW are valid and owned by your organization. Verify that these IPs are part of your allocated address space and not mistakenly using external or public IPs that could expose your network to unauthorized access. Remediation involves auditing your VPN configurations, updating any incorrect IPs, and ensuring they are registered and controlled by your organization.
  references:
  - https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-vpns
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-77.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.compute.vpn_tunnel.network_cgw_no_public_management_interfaces_exposed
  service: compute
  resource: vpn_tunnel
  requirement: Network Cgw No Public Management Interfaces Exposed
  scope: compute.vpn_tunnel.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Prevent Public Exposure of VPN Tunnel Management Interfaces
  rationale: Exposing VPN tunnel management interfaces to the public internet increases the risk of unauthorized access, data breaches, and potential service disruptions. Such exposure can lead to compliance violations with frameworks like NIST and PCI-DSS, which require secure data transmission channels. Ensuring that management interfaces are only accessible from trusted networks mitigates these risks and protects sensitive data in transit.
  description: This rule checks that VPN tunnel management interfaces are not publicly accessible on Google Cloud Platform. It ensures that these interfaces are restricted to known and trusted networks by configuring appropriate firewall rules. Verification involves checking the inbound rules for the relevant VPN tunnel and confirming that no public IP addresses are allowed. Remediation includes updating firewall configurations to restrict access to internal or specific external IP ranges only.
  references:
  - https://cloud.google.com/vpn/docs/concepts/topologies#best_practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices/identity-access
- rule_id: gcp.compute.vpn_tunnel.network_health_monitoring_enabled
  service: compute
  resource: vpn_tunnel
  requirement: Network Health Monitoring Enabled
  scope: compute.vpn_tunnel.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Network Health Monitoring for VPN Tunnels
  rationale: Enabling network health monitoring for VPN tunnels is crucial for ensuring the reliability and performance of your network connections. Without it, potential issues such as packet loss or latency might go unnoticed, leading to disrupted services and potential downtime. This monitoring is also important for meeting compliance requirements that mandate continuous network oversight to ensure data integrity and availability.
  description: This rule checks whether network health monitoring is enabled for Google Cloud VPN tunnels. To verify, ensure that the VPN tunnel configuration includes health check settings that actively monitor tunnel status and performance metrics. Remediation involves configuring VPN tunnels with appropriate health check parameters via the GCP Console or using the gcloud command-line tool. This proactive approach helps in maintaining optimal performance and quickly identifying any issues that may arise.
  references:
  - https://cloud.google.com/network-connectivity/docs/vpn/how-to/monitor-vpn
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/monitoring
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.compute.vpn_tunnel.network_vpn_ike_phase_encryption_strong
  service: compute
  resource: vpn_tunnel
  requirement: Network Vpn Ike Phase Encryption Strong
  scope: compute.vpn_tunnel.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Strong Encryption for VPN IKE Phase
  rationale: Using strong encryption algorithms for IKE phases ensures data integrity and confidentiality during VPN communication. Weak encryption can expose data to interception and unauthorized access, leading to potential data breaches and non-compliance with regulations such as GDPR and HIPAA. Strong encryption is vital for protecting sensitive business data and maintaining trust with customers and partners.
  description: This rule checks that the VPN tunnel uses strong encryption algorithms, such as AES-256, for IKE phase 1 and phase 2. Proper configuration mitigates risks of data interception and unauthorized access. Verify configurations in the GCP Console under VPN configurations or through the gcloud command-line tool. Remediation involves updating the IKE phase settings to use strong encryption, ensuring both phases employ recommended algorithms.
  references:
  - https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-vpns
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-77r1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.compute.vpn_tunnel.network_vpn_pre_shared_keys_rotation_policy_defined
  service: compute
  resource: vpn_tunnel
  requirement: Network Vpn Pre Shared Keys Rotation Policy Defined
  scope: compute.vpn_tunnel.network_security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure VPN Tunnel Pre-Shared Keys Rotation Policy is Defined
  rationale: Regular rotation of VPN tunnel pre-shared keys mitigates the risk of unauthorized access due to key compromise. This practice is essential for maintaining the confidentiality and integrity of data transmitted over VPN connections and is often a requirement for compliance with security standards such as PCI-DSS and NIST. Failing to rotate keys can lead to prolonged exposure and increased susceptibility to brute force attacks.
  description: This rule checks if a key rotation policy for VPN tunnel pre-shared keys is defined and enforced within GCP. To verify, ensure that the VPN tunnel configurations include a documented schedule for key rotation, ideally automating the process where possible. Implementing a policy involves setting a rotation frequency that aligns with your organization's security posture and regulatory requirements. Remediation includes configuring key rotation intervals in Google Cloud Console and using automation tools like Terraform for consistent enforcement.
  references:
  - https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-vpns
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-77.pdf
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.container.addon.containers_kubernetes_from_trusted_channel
  service: container
  resource: addon
  requirement: Containers Kubernetes From Trusted Channel
  scope: container.addon.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Runs From GCP Trusted Channels
  rationale: Running Kubernetes from trusted channels ensures that clusters receive timely security patches and updates, minimizing the risk of exploiting known vulnerabilities. This practice is crucial for maintaining the integrity and availability of containerized applications and aligns with regulatory requirements for maintaining secure infrastructure.
  description: This rule checks if GKE clusters are configured to use Kubernetes versions from GCP's trusted release channels, such as Rapid, Regular, or Stable. Configuring clusters to follow these channels ensures automatic updates and patches. To verify, inspect the GKE cluster settings in the Google Cloud Console or via gcloud CLI to confirm the channel configuration. Remediate by updating cluster settings to align with a trusted release channel.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/release-channels
  - https://cloud.google.com/kubernetes-engine/docs/how-to/creating-managing-release-channels
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.addon.containers_kubernetes_no_privileged_permissions
  service: container
  resource: addon
  requirement: Containers Kubernetes No Privileged Permissions
  scope: container.addon.least_privilege
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Prohibit Privileged Permissions for Kubernetes Add-ons
  rationale: Using privileged permissions in Kubernetes add-ons can expose the cluster to security risks, including unauthorized access to sensitive resources and potential privilege escalation attacks. Ensuring that add-ons operate with the least privilege necessary helps mitigate vulnerabilities and align with compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that Kubernetes add-ons deployed in GCP do not run with privileged permissions. It verifies that the securityContext.privileged field is set to false in the Pod specifications of add-ons. Remediation involves auditing add-on configurations for privileged access and modifying them to adhere to the principle of least privilege by updating security settings to non-privileged modes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
- rule_id: gcp.container.addon.containers_kubernetes_version_pinned_and_supported
  service: container
  resource: addon
  requirement: Containers Kubernetes Version Pinned And Supported
  scope: container.addon.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Pin and Support Kubernetes Versions for Container Addons
  rationale: Pinning and supporting specific Kubernetes versions reduces the risk of vulnerabilities by ensuring that clusters run on versions that are actively maintained and patched. Unpinned or unsupported versions may expose clusters to security flaws, impacting business operations and leading to potential data breaches. This practice also helps in meeting compliance requirements by aligning with standards that mandate regular updates and patches.
  description: This rule verifies that Kubernetes versions for GCP container addons are pinned and supported. It checks if the clusters are running on a specific, supported version to ensure they receive security updates. Administrators should configure their GKE clusters to use supported Kubernetes versions by selecting a specific version and regularly updating to newer supported versions. This can be done via the GCP Console or gcloud CLI. Remediation involves reviewing the current version, comparing it against the supported list, and upgrading as necessary.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/release-notes
  - https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
- rule_id: gcp.container.cluster.abac_status_configured
  service: container
  resource: cluster
  requirement: Abac Status Configured
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure ABAC is Disabled for GKE Clusters
  rationale: Enabling ABAC (Attribute-Based Access Control) on GKE clusters can lead to security vulnerabilities as it allows overly permissive access to Kubernetes resources. Disabling ABAC and using RBAC (Role-Based Access Control) helps in minimizing access permissions, aligning with the principle of least privilege, and meeting compliance requirements like PCI-DSS and SOC2 by reducing the risk of unauthorized access.
  description: This rule checks if ABAC is disabled for Google Kubernetes Engine (GKE) clusters. By default, GKE uses RBAC, which is more secure and manageable. To verify, navigate to the GKE cluster settings in the Google Cloud Console and ensure that ABAC is turned off. Remediation involves disabling ABAC and configuring RBAC policies to control access to Kubernetes resources effectively.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#disable_abac
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.alpha_features_disabled_for_production
  service: container
  resource: cluster
  requirement: Alpha Features Disabled For Production
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Alpha Features in Production GKE Clusters
  rationale: Alpha features in Google Kubernetes Engine (GKE) are experimental and may not be stable or secure enough for production environments. Allowing these features increases the risk of potential vulnerabilities, instability, and non-compliance with certain security standards, which could lead to unauthorized access or data breaches.
  description: This rule checks if any GKE cluster in the environment is using alpha features, which are not recommended for production use due to their experimental nature. To ensure a stable and secure production environment, disable alpha features in all production clusters. Verify by reviewing the cluster configurations and ensure that no alpha APIs are enabled. If alpha features are present, consider migrating to stable or beta features where possible.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/alpha-features
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/standards_overview
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.and_nodepool_workload_identity_enforced
  service: container
  resource: cluster
  requirement: And Nodepool Workload Identity Enforced
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Workload Identity is Enforced for GKE Clusters and Nodepools
  rationale: Enforcing Workload Identity in GKE clusters and nodepools mitigates the risk of credential theft by ensuring that Google Kubernetes Engine (GKE) workloads run with specific and minimal IAM permissions. This approach reduces the attack surface by eliminating the need for long-lived credentials and helps in meeting compliance requirements by adhering to the principle of least privilege.
  description: This rule checks whether Workload Identity is enforced for GKE clusters and their associated nodepools. Workload Identity allows Kubernetes service accounts to act as IAM service accounts, providing secure access to Google Cloud services. To verify, ensure that the 'workloadIdentityConfig' field is configured with a valid 'identityNamespace'. Remediation involves updating the GKE cluster configuration to enable Workload Identity, using the command 'gcloud container clusters update CLUSTER_NAME --workload-pool=PROJECT_ID.svc.id.goog'.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview#workload-identity
  - CIS Google Kubernetes Engine Benchmark v1.0.0 - Section 6.10
  - NIST SP 800-53 Rev. 5 - AC-3 Access Enforcement
  - 'PCI DSS v3.2.1 - Requirement 7: Restrict access to cardholder data by business need to know'
  - ISO/IEC 27001:2013 - A.9.4.1 Information access restriction
- rule_id: gcp.container.cluster.autoscaling_enabled
  service: container
  resource: cluster
  requirement: Autoscaling Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Cluster Autoscaling is Enabled
  rationale: Enabling autoscaling for Kubernetes clusters ensures that resources are efficiently managed, automatically adjusting to load demands. This prevents over-provisioning and reduces costs while maintaining application performance. It also mitigates potential denial-of-service scenarios by ensuring sufficient resources during traffic surges, aligning with best practices for operational resilience and compliance with scalability requirements.
  description: This rule checks if autoscaling is enabled on GCP Kubernetes clusters. Autoscaling adjusts the number of nodes in a cluster based on workload and resource utilization metrics. To verify, ensure the `autoscaling` configuration is set in the cluster's node pool settings. Remediation involves enabling autoscaling by configuring the minimum and maximum number of nodes in the Google Cloud Console or via the `gcloud` command-line tool for existing clusters.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning
  - https://cloud.google.com/architecture/best-practices-for-scaling-your-app-on-gke
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.container.cluster.binary_authorization_enforced
  service: container
  resource: cluster
  requirement: Binary Authorization Enforced
  scope: container.cluster.authorization
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Binary Authorization is Enforced on GKE Clusters
  rationale: Binary Authorization in Google Kubernetes Engine (GKE) enhances security by ensuring only trusted container images are deployed. This mitigates the risk of deploying vulnerable or malicious images, protecting the integrity of the application. Enforcing Binary Authorization supports compliance with industry standards like PCI-DSS and SOC 2, which require strict control over software deployment processes.
  description: This rule checks whether Binary Authorization is enforced on GKE clusters. To verify, ensure that the `binaryAuthorization` setting is enabled in the cluster configuration. Enforcing Binary Authorization requires configuring attestors to validate container images against defined policies. Remediation involves updating the cluster settings to enable Binary Authorization and configuring policies that align with your security requirements.
  references:
  - https://cloud.google.com/binary-authorization/docs/enforce
  - https://cloud.google.com/kubernetes-engine/docs/how-to/binary-authorization
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.container.cluster.client_certificate_authentication_disabled
  service: container
  resource: cluster
  requirement: Client Certificate Authentication Disabled
  scope: container.cluster.authentication
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Disable Client Certificate Authentication on GKE Clusters
  rationale: Disabling client certificate authentication in Google Kubernetes Engine (GKE) is critical to mitigating risks associated with certificate-based attacks, including man-in-the-middle and unauthorized access. By relying on alternative authentication methods like IAM, organizations reduce complexity and adhere to security best practices, aligning with compliance frameworks such as NIST SP 800-53 and PCI-DSS 3.2.1.
  description: This rule checks if client certificate authentication is disabled for GKE clusters, a setting that prevents unauthorized access via compromised client certificates. To verify, inspect the cluster's authentication settings and ensure the 'clientCertificateConfig.issueClientCertificate' is set to 'false'. Remediation involves modifying the cluster configuration through the GCP Console or using 'gcloud' CLI to update the cluster settings, ensuring stronger security posture by leveraging IAM-based authentication.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.container.cluster.containers_kubernetes_admission_host_namespace_usage_denied
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Host Namespace Usage Denied
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Restrict Host Namespace Usage in GKE Clusters
  rationale: Allowing containers to share the host's namespace can lead to privilege escalation and potential exposure of sensitive host-level data. This configuration can be exploited by attackers to gain unauthorized access to the host system, posing a significant security risk. Ensuring that containers do not use the host's namespace helps maintain isolation and aligns with security best practices and compliance requirements.
  description: This rule checks that Kubernetes clusters in GCP do not allow containers to use the host's namespace, which is a potential security risk. Verify that the PodSecurityPolicy or similar admission controls are configured to deny the usage of 'hostPID', 'hostIPC', and 'hostNetwork'. To remediate, update your cluster's security policies to restrict these settings, ensuring that all container deployments adhere to these constraints.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#admission_controllers
  - https://cloud.google.com/kubernetes-engine/docs/concepts/pod-security-policies
  - CIS Google Kubernetes Engine (GKE) Benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
- rule_id: gcp.container.cluster.containers_kubernetes_admission_image_registry_allo_enforced
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Image Registry Allo Enforced
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Image Registry Admission Control in Kubernetes Clusters
  rationale: Enforcing image registry admission policies in Kubernetes clusters is crucial for ensuring that only trusted container images are deployed. This reduces the risk of introducing vulnerabilities from unverified sources, thereby protecting sensitive data and maintaining service integrity. Additionally, it helps in meeting compliance requirements related to software supply chain security.
  description: This rule checks whether Kubernetes clusters enforce admission controls to allow only images from approved registries. It involves setting up admission controllers or policies within the cluster to verify image sources during the deployment process. To implement, configure the Kubernetes admission controller to whitelist trusted registries and continuously monitor for policy compliance. This can be verified by reviewing the cluster's admission controller settings and ensuring they are configured correctly.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/pod-security-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://pci-dss.org/pci_security_standards/
  - https://cloud.google.com/architecture/best-practices-for-building-secure-gke-clusters
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
- rule_id: gcp.container.cluster.containers_kubernetes_admission_image_signature_veri_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Image Signature Veri Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Admission Image Signature Verification
  rationale: Enabling image signature verification in GCP Kubernetes clusters mitigates risks of deploying untrusted or malicious container images, which can lead to data breaches, service disruptions, and compliance violations. It ensures only signed and trusted images are deployed, aligning with security best practices and regulatory frameworks.
  description: This rule checks if Kubernetes clusters in GCP have image signature verification enabled, which is crucial for ensuring that only verified images are allowed to run. To verify, ensure that the policy controller is configured with the appropriate admission webhook to enforce signature checks. Remediation involves configuring the Binary Authorization to enforce image signature policies, thereby preventing unauthorized images from being deployed.
  references:
  - https://cloud.google.com/binary-authorization/docs/overview
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_admission_pod_security_admissi_default
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Pod Security Admissi Default
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Pod Security Admission for Clusters
  rationale: Enabling Kubernetes Pod Security Admission helps mitigate risks associated with unauthorized or unsafe pod configurations, reducing potential attack surfaces such as privilege escalation and resource exhaustion. It aligns with regulatory demands for secure container operations and aids in maintaining a compliant and secure Kubernetes environment, crucial for safeguarding sensitive data and business operations.
  description: This rule checks if Kubernetes Pod Security Admission is enabled in GCP Kubernetes clusters. Proper configuration helps enforce security policies at the pod level, preventing the deployment of pods that do not meet security requirements. To verify, ensure that the Pod Security Admission controller is set up in the GKE cluster configurations. Remediation involves configuring the PodSecurityPolicy in your Kubernetes Engine settings to restrict pod deployments based on specified security standards.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://snyk.io/blog/kubernetes-security-best-practices-cheat-sheet/
- rule_id: gcp.container.cluster.containers_kubernetes_admission_privilege_escalation_denied
  service: container
  resource: cluster
  requirement: Containers Kubernetes Admission Privilege Escalation Denied
  scope: container.cluster.least_privilege
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Deny Privilege Escalation in Kubernetes Admission
  rationale: Preventing privilege escalation in Kubernetes containers is crucial to maintain the security boundaries of your applications. If a container can escalate its privileges, it could potentially gain access to sensitive data or control over other resources, leading to data breaches or service disruptions. This practice is important for meeting compliance requirements such as PCI-DSS and HIPAA, which mandate strict access controls.
  description: This rule checks if Kubernetes admission controllers deny privilege escalation in GKE clusters. It ensures that containers cannot gain additional privileges beyond their initial configuration, reducing the risk of unauthorized access. To verify, check that the PodSecurityPolicy or equivalent configurations are set to prevent privilege escalation. Remediation involves updating your Kubernetes admission policies to deny any containers attempting to request escalated privileges.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#admission_controllers
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_anonymous_auth_disabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Anonymous Auth Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Anonymous Auth on Kubernetes API Server
  rationale: Anonymous authentication allows unauthenticated access to the Kubernetes API server, posing a security risk by potentially exposing sensitive operations to unauthorized users. This can lead to unauthorized access, data breaches, and compromise of cluster integrity. Disabling anonymous authentication helps in maintaining strict access control, complying with security best practices and regulatory standards like CIS and NIST.
  description: This rule checks if the Kubernetes API server on GCP Kubernetes Engine clusters has anonymous authentication disabled. Anonymous access should be turned off to ensure that all requests to the API server are authenticated. Verification involves examining the 'enableAnonymousAuth' field in the 'kube-apiserver' configuration and ensuring it is set to 'false'. To remediate, modify the cluster configuration to disable anonymous authentication, typically through the GCP Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#anonymous-requests
  - https://cloud.google.com/kubernetes-engine/docs/concepts/using-rbac
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_audit_logging_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Audit Logging Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Kubernetes API Server Audit Logging
  rationale: Enabling Kubernetes API server audit logging is crucial for tracking access and activity within your GKE clusters, which can help identify potential security breaches and unauthorized access attempts. This measure supports compliance with industry standards like PCI-DSS and HIPAA by ensuring that detailed logs are available for auditing and incident investigation. It also helps in maintaining the integrity and security of applications running on GKE.
  description: This check verifies that audit logging is enabled for the Kubernetes API server in Google Kubernetes Engine (GKE) clusters. Audit logs provide a record of actions taken within your cluster, including access to the Kubernetes API. To enable audit logging, navigate to the Google Cloud Console, select Kubernetes Engine, and configure the cluster's logging settings to include audit logs. Ensure that logs are stored securely and monitored regularly for suspicious activity.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging
  - https://cloud.google.com/security/compliance/cis-kubernetes-benchmark
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/sites/default/files/hipaa-privacy-rule.pdf
  - https://cloud.google.com/security/best-practices
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_authorization_mode_rbac
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Authorization Mode RBAC
  scope: container.cluster.authorization
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable RBAC for Kubernetes API Server Authorization
  rationale: Implementing Role-Based Access Control (RBAC) for the Kubernetes API server is critical for minimizing unauthorized access and controlling who can perform specific actions within the cluster. This enhances the security posture by ensuring that permissions are granted based on the principle of least privilege, reducing the risk of privilege escalation and potential data breaches. It also aligns with compliance requirements for regulated industries.
  description: This rule checks if the Kubernetes API server in your GCP container clusters is configured to use RBAC for authorization. RBAC allows for fine-grained access control by defining roles and binding them to users or service accounts. To verify, ensure that the 'authorization-mode' parameter includes 'RBAC' in the API server configuration. Remediation involves updating your cluster's API server settings to include 'RBAC' in the authorization modes, which can be done via the GCP console or command line interface.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_etcd_connection_encrypted
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Etcd Connection Encrypted
  scope: container.cluster.encryption
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes API Server to Etcd Connections are Encrypted
  rationale: Encrypting connections between the Kubernetes API server and Etcd is crucial to protect sensitive data, such as cluster configurations and secrets, from interception and unauthorized access. Unencrypted connections can expose the cluster to man-in-the-middle attacks, increasing the risk of data breaches and compliance violations with standards like PCI-DSS and HIPAA.
  description: This rule checks whether the communication between the Kubernetes API server and Etcd is encrypted with Transport Layer Security (TLS). To verify, ensure that the '--etcd-certfile' and '--etcd-keyfile' flags are specified in the API server configuration. If not, update the API server configuration to include these flags with the correct certificate and key files. This ensures data integrity and confidentiality during transmission.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/hipaa
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_insecure_port_disabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver Insecure Port Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Insecure Port on Kubernetes API Server
  rationale: Disabling the insecure port for the Kubernetes API server is crucial as it prevents unauthenticated access to the cluster, reducing the risk of unauthorized control or data exposure. This practice supports compliance with security frameworks requiring secure communication channels and protects against potential attack vectors targeting open, insecure ports.
  description: This rule checks if the Kubernetes API server on GCP container clusters has the insecure port (default 8080) disabled. It ensures that all API server communications occur over secure channels (HTTPS). To verify, review cluster configurations in GCP Console or use gcloud CLI to ensure the '--insecure-port' is set to '0'. Remediation involves updating the cluster configuration to set '--insecure-port=0' and ensuring all communications use the secure port (default 443).
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
- rule_id: gcp.container.cluster.containers_kubernetes_apiserver_tls_min_1_2_enforced
  service: container
  resource: cluster
  requirement: Containers Kubernetes Apiserver TLS Min 1 2 Enforced
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce TLS 1.2+ for Kubernetes API Server in GKE
  rationale: Enforcing TLS 1.2 or higher for the Kubernetes API server in GKE protects data-in-transit from interception and tampering, reducing the risk of man-in-the-middle attacks. This is critical for maintaining the confidentiality and integrity of the communication between clients and the API server, aligning with industry standards and regulatory requirements, such as PCI-DSS and NIST SP 800-52 for secure communications.
  description: This rule checks if the Kubernetes API server in a GKE cluster is configured to enforce a minimum of TLS 1.2 for secure communications. To verify, ensure the API server configuration in your GKE cluster specifies TLS 1.2 as the minimum version. Remediation involves updating the GKE cluster's API server settings to reject connections below TLS 1.2, which can be done via the `gcloud` command-line tool or the GCP Console by setting the appropriate security policies.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-clusters#using-tls
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_audit_logging_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Audit Logging Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Kubernetes Audit Logging for GKE Clusters
  rationale: Enabling Kubernetes audit logging is crucial for tracking important events and changes within your GKE clusters. It helps in identifying unauthorized access attempts, policy violations, and potential security threats, thus supporting compliance with regulatory standards such as PCI-DSS and HIPAA. Without audit logs, detecting and investigating security incidents becomes significantly more challenging, increasing the risk of undetected breaches.
  description: This rule checks whether Kubernetes audit logging is enabled for Google Kubernetes Engine (GKE) clusters. Audit logging should be configured to capture all API requests and responses, which can be achieved by setting the 'enableKubernetesAuditLogging' field to true in the cluster's logging configuration. To verify, use the Google Cloud Console or gcloud command-line tool to ensure that audit logging is activated. Remediation involves updating the cluster's logging settings to include Kubernetes audit logs.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/kubernetes-engine/docs/best-practices
  - https://cloud.google.com/kubernetes-engine/docs/concepts/audit-logging
- rule_id: gcp.container.cluster.containers_kubernetes_controller_manager_root_ca__configured
  service: container
  resource: cluster
  requirement: Containers Kubernetes Controller Manager Root Ca Configured
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Ensure Kubernetes Controller Manager Root CA is Configured
  rationale: Proper configuration of the Kubernetes Controller Manager Root CA is crucial to maintaining the security integrity of GKE clusters. Failing to configure this can lead to unauthorized access or data breaches, as the root CA is a critical component in the authentication and encryption infrastructure. Compliance with standards like PCI-DSS and ISO 27001 often requires robust certificate management practices.
  description: This rule checks that the Kubernetes Controller Manager in GCP's GKE clusters has been configured with a valid Root CA, which is essential for secure communication and authentication within the cluster. Verify this setting by inspecting the Controller Manager's configuration for a properly defined root CA file. Remediation involves updating the Controller Manager settings to include a valid root CA file path, ensuring all certificates are correctly signed and trusted by the cluster.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/certificates/
- rule_id: gcp.container.cluster.containers_kubernetes_controller_manager_secure_port_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Controller Manager Secure Port Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Secure Port for Kubernetes Controller Manager
  rationale: Enabling the secure port for the Kubernetes Controller Manager helps protect sensitive communication within the Kubernetes control plane. It mitigates the risk of unauthorized access and data interception, which are crucial for maintaining the integrity and confidentiality of cluster operations. This configuration supports compliance with security standards that mandate encryption of sensitive data in transit.
  description: This rule checks if the Kubernetes Controller Manager in your GCP cluster is configured to use a secure port for communication. By default, the secure port (443) should be enabled to ensure that all data exchanged between components is encrypted. To verify, check the `--secure-port` flag in the Controller Manager's configuration. If not set to 443, update the configuration to enable this secure port, ensuring that the control plane communications are encrypted.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_secure_communication_channels
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
- rule_id: gcp.container.cluster.containers_kubernetes_controller_manager_use_service_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Controller Manager Use Service Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Controller Manager Service in GKE
  rationale: Enabling the Kubernetes Controller Manager service in Google Kubernetes Engine (GKE) is crucial for maintaining control over the cluster's state and ensuring that the desired configurations are implemented. Without this service, automated management of node and pod lifecycle processes could be disrupted, leading to potential security vulnerabilities and downtime. Compliance with security standards like CIS benchmarks often requires such services to be enabled to ensure proper cluster management and monitoring.
  description: This rule checks whether the Kubernetes Controller Manager service is enabled in your GKE cluster. The Controller Manager is responsible for managing controllers that regulate node and pod lifecycle actions. To verify, access your GKE cluster settings and ensure that the Controller Manager is active. Remediation involves enabling the service through the GCP Console or using the `gcloud` CLI to update the cluster configuration, ensuring continuous management of cluster operations.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#control_plane
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://kubernetes.io/docs/concepts/overview/components/#kube-controller-manager
- rule_id: gcp.container.cluster.containers_kubernetes_default_service_account_autom_disabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Default Service Account Autom Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Kubernetes Default Service Account for GKE Clusters
  rationale: Using the default service account in Kubernetes clusters can lead to privilege escalation and unauthorized access within Google Kubernetes Engine (GKE). Misuse of default service accounts may result in potential exposure of sensitive data or unauthorized actions by workloads. Ensuring that workloads do not automatically use the default service account helps in maintaining least privilege and reducing attack surface.
  description: This rule checks if the default Kubernetes service account is disabled for automatic use within GKE clusters. By default, Kubernetes assigns the default service account to workloads, which can inadvertently grant excessive permissions. To verify, ensure that workloads specify custom service accounts with minimal permissions. Remediation involves explicitly specifying a custom service account with the required permissions in pod specifications, and disabling the automatic mounting of default service accounts by setting the 'automountServiceAccountToken' to false.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa
  - https://cloud.google.com/kubernetes-engine/docs/concepts/service-accounts
  - CIS Google Kubernetes Engine Benchmark
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - NIST SP 800-190 Application Container Security Guide
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.containers_kubernetes_etcd_auth_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Etcd Auth Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Etcd Authentication in GKE Clusters
  rationale: Enabling authentication for etcd in Kubernetes clusters is crucial to protect sensitive data stored in etcd, including configuration information and secrets. Unauthorized access to etcd can result in data breaches, service disruptions, and compliance violations, particularly those related to data protection regulations like GDPR and HIPAA.
  description: This rule checks whether authentication is enabled for etcd in Google Kubernetes Engine (GKE) clusters. Etcd serves as a key-value store for Kubernetes cluster data, and enabling authentication ensures that only authorized users and services can access it. To verify, ensure that the GKE cluster is configured with etcd authentication enabled. Remediation involves setting up secure communication channels and restricting access to etcd endpoints using IAM roles and policies.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
- rule_id: gcp.container.cluster.containers_kubernetes_etcd_client_tls_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Etcd Client TLS Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Etcd Client TLS is Enabled on Clusters
  rationale: Enabling TLS for Kubernetes etcd client communications is crucial to protect sensitive data from interception and unauthorized access. Without TLS, data transmitted between clients and etcd, which often includes configuration details and secrets, could be exposed to attackers. This is vital for maintaining data integrity and confidentiality, thus supporting compliance with security standards such as PCI-DSS and ISO 27001.
  description: This rule checks if TLS is enforced for client communications with the etcd service in Kubernetes clusters. Ensure that the etcd server is configured to use TLS encryption for client communications by specifying the '--client-cert-auth' and providing appropriate certificates. This can be verified by examining the etcd configuration files or through the Kubernetes API server settings. Remediate any configurations not using TLS by updating the etcd settings and redeploying the cluster with secured configurations.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-cluster
  - CIS Google Kubernetes Engine Benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.containers_kubernetes_etcd_encryption_at_rest_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Etcd Encryption At Rest Enabled
  scope: container.cluster.encryption
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Ensure Kubernetes ETCD Encryption at Rest is Enabled
  rationale: Enabling encryption at rest for Kubernetes ETCD data is critical to protect sensitive cluster metadata and secrets from unauthorized access. Unencrypted ETCD data can lead to serious security breaches, exposing configurations, secret keys, and user information. Compliance with standards such as PCI-DSS and HIPAA often requires encryption of sensitive data, ensuring protection against data theft and unauthorized disclosures.
  description: This rule checks if Kubernetes ETCD data is encrypted at rest within the GCP container cluster. Enabling this feature ensures that all ETCD data is stored in an encrypted format, safeguarding against data exposure in the event of a breach. To verify and remediate, navigate to the GCP Console, select Kubernetes Engine, choose the desired cluster, and ensure that 'Enable Kubernetes secrets encryption' is enabled under the Security settings. Follow the GCP documentation for detailed steps on configuring ETCD encryption.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-144.pdf
- rule_id: gcp.container.cluster.containers_kubernetes_etcd_peer_tls_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Etcd Peer TLS Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable TLS for Kubernetes Etcd Peer Communication
  rationale: Enabling TLS for Etcd peer communication in a Kubernetes cluster is critical to protect sensitive data exchanged between nodes. Unsecured peer communication can lead to data breaches, unauthorized access, and man-in-the-middle attacks. Compliance with security standards such as NIST and ISO 27001 often requires encryption of data in transit, making this a key aspect of regulatory adherence.
  description: This rule checks if Transport Layer Security (TLS) is enabled for communication between Etcd peers in a Kubernetes cluster. TLS encrypts the data exchanged, preventing eavesdropping and unauthorized access. To verify, ensure the Etcd configuration in your Kubernetes setup specifies TLS-enabled endpoints. Remediate by configuring Etcd to use TLS certificates for peer communication, which can be done through the GCP Console or by updating the Kubernetes API server settings.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-cluster-components
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_fargate_profile_execution_ro_privilege
  service: container
  resource: cluster
  requirement: Containers Kubernetes Fargate Profile Execution Ro Privilege
  scope: container.cluster.least_privilege
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Restrict Execution of Kubernetes Fargate Profiles with Ro Privilege
  rationale: Allowing read-only (Ro) privilege in Kubernetes Fargate profiles can lead to unauthorized access to container data, increasing the risk of data exfiltration and privilege escalation. This misconfiguration can impact compliance with data protection regulations such as GDPR and HIPAA, and can result in significant business and reputational damage.
  description: This rule checks for Kubernetes Fargate profiles configured with read-only (Ro) execution privileges within Google Kubernetes Engine (GKE). Ensuring that Fargate profiles do not have unnecessary privileges minimizes the attack surface and adheres to the principle of least privilege. To remediate, review the IAM roles associated with Fargate profiles and adjust permissions to the minimum required for functionality. Verify that service accounts used by Fargate profiles are limited to necessary operations only.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/fargate-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library?category=pcidss
- rule_id: gcp.container.cluster.containers_kubernetes_fargate_profile_logging_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Fargate Profile Logging Enabled
  scope: container.cluster.logging
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Enable Logging for GKE Autopilot Clusters
  rationale: Enabling logging for GKE Autopilot clusters ensures that all container activities are recorded, allowing for thorough monitoring and auditing. This visibility helps identify and respond to potential security incidents, compliance violations, and operational issues quickly. Without logging, detecting unauthorized access or misconfigurations becomes significantly more challenging, increasing the risk of data breaches and non-compliance with standards such as PCI-DSS and SOC2.
  description: This rule checks if logging is enabled for GKE Autopilot clusters, ensuring that all container activities are recorded. To verify, confirm that Cloud Operations is configured to capture logs for all workloads running in the cluster. If logging is not enabled, enable it in the cluster settings through the Google Cloud Console or via the gcloud CLI by setting the `enable-stackdriver-kubernetes` flag. This configuration is crucial for maintaining visibility into the cluster's operations and security posture.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/logging
  - https://cloud.google.com/security/compliance/cis#section-5
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/stackdriver/docs/solutions/gke
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.isaca.org/resources/cobit
- rule_id: gcp.container.cluster.containers_kubernetes_fargate_profile_private_subnets_only
  service: container
  resource: cluster
  requirement: Containers Kubernetes Fargate Profile Private Subnets Only
  scope: container.cluster.private_networking
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Fargate Profiles Use Private Subnets
  rationale: Using private subnets for Kubernetes Fargate profiles reduces the exposure of your container workloads to the internet, mitigating the risk of unauthorized access and potential attacks. This configuration supports compliance with data protection regulations by ensuring sensitive workloads remain within a controlled network environment, enhancing both security posture and operational integrity.
  description: This rule verifies that Kubernetes Fargate profiles within your GCP container clusters are configured to use only private subnets. To ensure compliance, review the network settings of your Fargate profiles and configure them to operate exclusively within private subnets. This setup limits the exposure of your workloads to the public internet, thereby reducing attack vectors. Remediation involves editing the cluster's networking settings to ensure Fargate profiles are associated with private subnets, which can be done through the GCP Console or using gcloud commands.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_networkpolicy_default_deny_e_namespace
  service: container
  resource: cluster
  requirement: Containers Kubernetes Networkpolicy Default Deny E Namespace
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Enforce Default Deny Network Policies in Kubernetes Namespaces
  rationale: Implementing a default deny network policy is crucial for minimizing the attack surface in Kubernetes environments. It helps prevent unauthorized access and lateral movement by ensuring that only explicitly allowed traffic is permitted. This approach is essential for maintaining data confidentiality and integrity, particularly in multi-tenant or sensitive environments, and aligns with compliance frameworks that require strict network segmentation.
  description: This check ensures that a Kubernetes NetworkPolicy is configured with a default deny rule for ingress and egress traffic in each namespace. By default, Kubernetes allows all traffic, which could lead to potential security breaches. To verify, review the NetworkPolicy resources in each namespace and ensure a policy exists that denies all traffic unless explicitly allowed. Remediation involves creating or updating NetworkPolicy resources to include a default deny rule, thus enforcing strict traffic controls.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://cloud.google.com/architecture/best-practices-for-using-kubernetes-network-policies
- rule_id: gcp.container.cluster.containers_kubernetes_networkpolicy_default_deny_i_namespace
  service: container
  resource: cluster
  requirement: Containers Kubernetes Networkpolicy Default Deny I Namespace
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Implement Default Deny Policy for Kubernetes Namespaces
  rationale: Applying a default deny network policy in Kubernetes namespaces is crucial for minimizing the attack surface by restricting both ingress and egress traffic to only what is necessary. This control reduces the risk of unauthorized access and potential data exfiltration, thereby safeguarding sensitive applications and data. It also aligns with compliance requirements for data protection and network security, such as PCI-DSS and NIST.
  description: This rule checks that all Kubernetes namespaces within a GCP container cluster have a default deny network policy implemented. A default deny policy ensures that no traffic is allowed by default, and only explicitly allowed traffic can pass through. To verify, review the network policies for each namespace and ensure they start with a deny-all rule. Remediation involves creating or updating network policies to include a 'deny all ingress and egress' rule, then selectively allowing necessary traffic.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.containers_kubernetes_networkpolicy_required_allowl_services
  service: container
  resource: cluster
  requirement: Containers Kubernetes Networkpolicy Required Allowl Services
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Ensure Kubernetes NetworkPolicies Allow Only Essential Services
  rationale: Implementing Kubernetes NetworkPolicies is crucial to control and restrict traffic between pods within a GKE cluster, minimizing the attack surface and reducing the risk of unauthorized access. Without these policies, clusters could be vulnerable to lateral movement by an attacker who gains access to a single pod, potentially violating compliance requirements like PCI-DSS and HIPAA, which mandate strict network segmentation and data protection.
  description: This rule checks if Kubernetes NetworkPolicies are applied to all services in the GKE cluster, ensuring that only allowed traffic can communicate with the pods. NetworkPolicies should be configured to permit only required ingress and egress traffic, effectively isolating workloads and protecting sensitive data. To verify compliance, inspect the NetworkPolicy resources in your cluster using 'kubectl get networkpolicies --all-namespaces'. Remediation involves creating and applying NetworkPolicies for each service, defining allowed ingress and egress traffic according to your security policy.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_private_control_plane_endpoint_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Private Control Plane Endpoint Enabled
  scope: container.cluster.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enable Private Endpoint for Kubernetes Control Plane
  rationale: Enabling a private endpoint for the Kubernetes control plane minimizes exposure to the internet, reducing the risk of unauthorized access and potential attacks on the cluster's management interfaces. This configuration aligns with the principle of least privilege by restricting access to the control plane to only those resources and users within the specified private network, thus enhancing the overall security posture and compliance with data protection regulations.
  description: This rule checks if the Kubernetes control plane endpoint is configured to use a private IP address accessible only within the VPC network. To verify, ensure that the cluster's private endpoint is enabled in the Google Cloud Console under Kubernetes Engine settings, or via the gcloud command-line tool by setting the 'privateEndpoint' attribute. Remediation involves updating the cluster configuration to enable private endpoint access, which can be done during cluster creation or by updating an existing cluster.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/containers-kubernetes/3-ways-to-secure-your-kubernetes-engine-cluster
  - https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
- rule_id: gcp.container.cluster.containers_kubernetes_scheduler_authentication_ku_configured
  service: container
  resource: cluster
  requirement: Containers Kubernetes Scheduler Authentication Ku Configured
  scope: container.cluster.authentication
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Ensure Kubernetes Scheduler Authentication Configured
  rationale: Configuring authentication for the Kubernetes scheduler in GCP ensures that only authorized users and services can interact with the scheduling component of your container orchestration. This minimizes the risk of unauthorized access, which could lead to workload disruptions, resource misallocation, or unauthorized data exposure, impacting business operations and compliance with regulations such as PCI-DSS and HIPAA.
  description: This rule checks if the Kubernetes scheduler in a GCP container cluster has authentication mechanisms properly configured. It involves verifying settings that enforce identity verification for scheduler API interactions. Remediation includes enabling authentication in the Kubernetes API server configuration, ensuring all communications use secure credentials and adhere to least privilege principles. This can be verified by checking the GCP Console or using gcloud CLI commands to review and update cluster settings.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - CIS Google Kubernetes Engine (GKE) Benchmark
  - NIST SP 800-190 Application Container Security Guide
  - 'PCI-DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.containers_kubernetes_scheduler_authorization_kub_configured
  service: container
  resource: cluster
  requirement: Containers Kubernetes Scheduler Authorization Kub Configured
  scope: container.cluster.authorization
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Scheduler RBAC is Properly Configured
  rationale: Proper configuration of Kubernetes Scheduler Authorization is crucial for preventing unauthorized access to cluster resources. Misconfigured RBAC can lead to privilege escalation, allowing attackers to execute malicious workloads, potentially leading to data breaches or service disruptions. Ensuring correct permissions align with compliance frameworks like PCI-DSS and HIPAA, minimizing business and legal risks.
  description: This rule checks whether the Kubernetes Scheduler RBAC policies are properly configured within GCP Kubernetes Engine clusters. Specifically, it verifies that the scheduler has the least privilege necessary to perform its functions. To verify, inspect the RBAC role bindings and ensure they follow the principle of least privilege. Remediation involves reviewing and updating the RBAC settings to restrict permissions to only what is necessary for the scheduler's operation.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
- rule_id: gcp.container.cluster.containers_kubernetes_scheduler_secure_port_tls_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Scheduler Secure Port TLS Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Scheduler Secure Port uses TLS
  rationale: Enabling TLS for the Kubernetes Scheduler Secure Port ensures encrypted communication, mitigating risks of data interception and unauthorized access. This is crucial for maintaining the confidentiality and integrity of sensitive cluster operations, supporting compliance with security standards like NIST and SOC2.
  description: This rule checks if the Kubernetes Scheduler Secure Port has TLS enabled. To verify, ensure the cluster configuration includes `--secure-port` and `--tls-cert-file` parameters set appropriately. Remediate by updating the scheduler configuration to include these parameters, ensuring communications are encrypted.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.container.cluster.containers_kubernetes_secrets_encryption_kms_enabled
  service: container
  resource: cluster
  requirement: Containers Kubernetes Secrets Encryption KMS Enabled
  scope: container.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS for Kubernetes Secrets Encryption in GKE
  rationale: Encrypting Kubernetes secrets using Customer-Managed Encryption Keys (CMEK) enhances security by ensuring sensitive information is protected with a robust encryption mechanism. It mitigates the risk of unauthorized access and data breaches, which can lead to significant financial loss and damage to brand reputation. Additionally, it helps meet compliance requirements for data protection standards such as PCI-DSS and HIPAA, which mandate strong encryption practices.
  description: This rule checks whether Google Kubernetes Engine (GKE) clusters have Kubernetes secrets encrypted with CMEK through Google Cloud Key Management Service (KMS). To verify, ensure clusters are configured to use KMS keys for encrypting secrets by setting the '--kms-key' flag during cluster creation. Remediation involves updating existing clusters to enable CMEK by modifying the encryption settings via the GCP console or gcloud CLI, ensuring that the KMS key is properly managed and rotated regularly.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/security/encryption-at-rest/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
- rule_id: gcp.container.cluster.containers_kubernetes_service_dns_and_metrics_from_a_sources
  service: container
  resource: cluster
  requirement: Containers Kubernetes Service Dns And Metrics From A Sources
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Monitor DNS and Metrics Access in GKE Clusters
  rationale: Monitoring DNS and metrics access in GKE clusters is essential to detect unusual behavior, prevent data exfiltration, and maintain visibility into cluster operations. Unauthorized access can lead to sensitive data exposure and compromise cluster integrity, impacting business continuity and compliance with standards such as ISO 27001 and SOC2.
  description: This rule checks for configurations that allow monitoring and logging of DNS requests and metrics access in Google Kubernetes Engine (GKE) clusters. Ensure that only authorized sources can access Kubernetes services to maintain a secure and observable environment. Configure Network Policies and enable logging to track access to sensitive endpoints. Remediate by reviewing access controls and ensuring that all access is logged and monitored.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/logs-dashboards
  - https://cloud.google.com/monitoring
- rule_id: gcp.container.cluster.containers_kubernetes_service_kube_system_services_no_public
  service: container
  resource: cluster
  requirement: Containers Kubernetes Service Kube System Services No Public
  scope: container.cluster.public_access
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Restrict Public Access to Kubernetes System Services
  rationale: Exposing Kubernetes system services to the public internet can lead to unauthorized access and potential compromise of the cluster's control plane. This exposure increases the risk of data breaches, denial-of-service attacks, and compliance violations, particularly for standards like PCI-DSS and HIPAA, which require strict access controls.
  description: This check ensures that Kubernetes system services within the kube-system namespace are not publicly accessible. It verifies the network configurations and firewall rules associated with the Kubernetes Engine cluster to restrict access to internal IPs only. To remediate, configure private clusters and update firewall rules to allow access only from specific IP ranges or VPCs, ensuring that no public IP addresses can reach the kube-system services.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://kubernetes.io/docs/concepts/security/overview/
- rule_id: gcp.container.cluster.control_plane_authorized_networks_configured
  service: container
  resource: cluster
  requirement: Control Plane Authorized Networks Configured
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Configure Control Plane Authorized Networks in GKE Clusters
  rationale: Configuring Control Plane Authorized Networks in Google Kubernetes Engine (GKE) clusters is vital for restricting access to the Kubernetes API server, reducing the risk of unauthorized access and potential data breaches. This configuration enhances security by ensuring that only trusted IP addresses can interact with the control plane, aligning with regulatory requirements such as NIST SP 800-53 and ISO 27001.
  description: This rule checks that Control Plane Authorized Networks are configured for GKE clusters, which involves specifying IP ranges that are allowed to access the Kubernetes API server. Verification can be done by reviewing the cluster settings in the Google Cloud Console or using the `gcloud` command-line tool. To remediate, update the cluster's settings to include authorized networks, ensuring that all entries are well-managed and documented. This can be done by navigating to the GKE cluster's 'Networking' settings and adding the required IP ranges.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview
- rule_id: gcp.container.cluster.google_managed_ssl_certificates_compliance
  service: container
  resource: cluster
  requirement: Google Managed SSL Certificates Compliance
  scope: container.cluster.compliance
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Use of Google Managed SSL Certificates in GKE Clusters
  rationale: Using Google Managed SSL Certificates in GKE clusters ensures that your Kubernetes workloads are protected with certificates that are automatically managed and renewed by Google Cloud. This minimizes the risk of expired certificates leading to insecure communication and potential data breaches, which can have significant business and legal repercussions, especially in industries governed by data protection regulations such as PCI-DSS and HIPAA.
  description: This rule checks if Google Kubernetes Engine (GKE) clusters are configured to use Google Managed SSL Certificates for securing communication. By default, GKE offers the option to use managed certificates, which are automatically maintained and renewed, reducing manual overhead and the risk of human error. To verify compliance, ensure that Google Managed SSL Certificates are enabled and properly configured in your GKE clusters. If not compliant, configure your GKE Ingress to use managed certificates by setting up the appropriate `ManagedCertificate` resource and associating it with your Ingress.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/ingress-xlb#managed-certs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.container.cluster.google_managed_ssl_certificates_enabled
  service: container
  resource: cluster
  requirement: Google Managed SSL Certificates Enabled
  scope: container.cluster.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Google Managed SSL Certificates for GKE Clusters
  rationale: Using Google Managed SSL Certificates ensures that SSL/TLS encryption is consistently applied to all traffic to your Google Kubernetes Engine (GKE) clusters, reducing the risk of data interception and ensuring compliance with standards requiring encryption in transit. This approach also simplifies certificate management, reducing the risk of human error and expired certificates, which can lead to service disruption and potential data breaches.
  description: This rule checks if Google Managed SSL Certificates are enabled for GKE clusters, ensuring that all ingress traffic is automatically secured with managed certificates. To verify, navigate to the GKE cluster settings in the Google Cloud Console and ensure that your ingress configurations utilize Google Managed SSL Certificates. Remediation involves configuring your ingress resources to use 'managedCertificate' with a 'spec.tls' field, thereby automating certificate provisioning and renewal.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs
  - https://cloud.google.com/security/compliance/cis-gke-benchmark
  - https://www.nist.gov/publications/nist-special-publication-800-57-part-1-revision-5
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.container.cluster.intranode_visibility_and_vpc_flow_logs_enabled
  service: container
  resource: cluster
  requirement: Intranode Visibility And VPC Flow Logs Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Intranode Visibility and VPC Flow Logs for Clusters
  rationale: Enabling intranode visibility and VPC flow logs enhances network monitoring and security by providing detailed insights into internal cluster communications and network traffic. This aids in the detection of anomalies, prevents potential data breaches, and supports compliance with regulations requiring detailed network activity logging, such as PCI-DSS and HIPAA.
  description: This rule checks if GKE clusters have both intranode visibility and VPC flow logs enabled. Intranode visibility allows for monitoring traffic between nodes within a cluster, while VPC flow logs capture information about network flows sent from and received by VM instances. To verify and enable these settings, navigate to the GKE cluster settings in the Google Cloud Console, enable intranode visibility under networking, and ensure VPC flow logs are activated in the VPC network settings. Remediation involves configuring these settings for each cluster to ensure comprehensive network monitoring.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/intranode-visibility
  - https://cloud.google.com/vpc/docs/using-flow-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.container.cluster.k8s_apiserver_admission_psa_enforce_mode
  service: container
  resource: cluster
  requirement: K8s Apiserver Admission Psa Enforce Mode
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce PSA Mode in K8s Apiserver Admission
  rationale: Enforcing Pod Security Admission (PSA) modes helps mitigate risks associated with unauthorized or insecure workloads running in your Kubernetes clusters. Without enforcement, malicious or poorly configured pods could compromise cluster security, leading to potential data breaches or service disruptions. Compliance frameworks often require strict access controls, making PSA enforcement necessary for regulatory adherence.
  description: This rule checks whether the Kubernetes API server's admission controller is configured to enforce Pod Security Admission (PSA) modes. PSA modes control the security context of pods, ensuring they adhere to defined security policies. Administrators should verify that the 'PodSecurityPolicy' admission controller is enabled and properly configured. To remediate, update the API server configuration to enable PSA with enforce mode by setting the appropriate flags and policy definitions.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/pod-security-admission
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/concepts/security/pod-security-admission/
- rule_id: gcp.container.cluster.k8s_apiserver_anonymous_auth_disabled
  service: container
  resource: cluster
  requirement: K8s Apiserver Anonymous Auth Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Anonymous Auth on Kubernetes API Server
  rationale: Disabling anonymous authentication on the Kubernetes API server is crucial to prevent unauthorized access to the cluster's control plane. Allowing anonymous requests can lead to potential exploitation by malicious actors, which may result in unauthorized data access, cluster disruption, or compromise of sensitive workloads. This configuration aligns with compliance requirements such as PCI-DSS and ISO 27001, ensuring that only authenticated and authorized users can interact with the Kubernetes API.
  description: This rule checks if the Kubernetes API server on GKE clusters has anonymous authentication disabled. Anonymous authentication allows requests to the API server without credentials, posing a security risk. To verify, ensure the 'anonymous-auth' flag is set to 'false' in the API server configuration. Remediation involves updating the GKE cluster configuration to explicitly disable anonymous authentication, which can typically be managed via the Google Cloud Console or using gcloud CLI commands.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_methods
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#configuring-authentication
- rule_id: gcp.container.cluster.k8s_apiserver_audit_logging_enabled
  service: container
  resource: cluster
  requirement: K8s Apiserver Audit Logging Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Kubernetes API Server Audit Logging
  rationale: Enabling Kubernetes API Server audit logging is crucial for tracking and analyzing requests made to the Kubernetes API. This capability helps detect unauthorized access and potential security breaches, while also supporting compliance with regulations that mandate detailed activity logging. It enhances visibility into operations, aiding in the forensic investigation of security incidents.
  description: This rule checks whether the Kubernetes API Server audit logging is enabled for GCP Kubernetes Engine clusters. Audit logging records all requests to the Kubernetes API server, providing a comprehensive trail of activity. To verify and enable this setting, navigate to the Google Cloud Console, select Kubernetes Engine, choose the desired cluster, and ensure that 'Enable Kubernetes API Server audit logging' is checked under the logging section. This ensures that all interactions with the Kubernetes API are logged for security and compliance purposes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/framework/security/design-for-security#logging_and_monitoring
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.k8s_apiserver_tls_min_version_1_2
  service: container
  resource: cluster
  requirement: K8s Apiserver TLS Min Version 1 2
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure K8s API Server Uses TLS v1.2 or Higher
  rationale: Enforcing a minimum TLS version of 1.2 for Kubernetes API server communication helps protect against vulnerabilities associated with older versions of the protocol. This ensures a higher standard of data encryption and integrity, reducing the risk of data breaches and man-in-the-middle attacks. Compliance with regulations such as PCI-DSS and HIPAA often mandates the use of strong encryption protocols, which includes TLS 1.2 or higher.
  description: This rule verifies that the Kubernetes API server within a Google Kubernetes Engine (GKE) cluster is configured to use TLS version 1.2 or higher. It is essential to configure the API server with the `--tls-min-version` flag set to '1.2' or higher to ensure secure communication. To check this setting, inspect the cluster's API server configuration. Remediation involves updating the cluster's API server settings via the GCP console or GCloud CLI to enforce the minimum TLS version requirement.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-your-cluster#api-server-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.k8s_controllermanager_secure_port_enabled
  service: container
  resource: cluster
  requirement: K8s Controllermanager Secure Port Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Secure Port for Kubernetes Controller Manager
  rationale: Enabling the secure port for the Kubernetes Controller Manager is crucial for ensuring that communication with the API server is encrypted and authenticated. This security measure mitigates risks such as man-in-the-middle attacks and unauthorized access, which can compromise cluster operations and sensitive data. Aligning with security standards like CIS GCP Benchmarks also helps meet regulatory compliance requirements.
  description: This rule checks if the Kubernetes Controller Manager is configured to use a secure port for communication. To verify, ensure that the `--secure-port` flag is set to a non-zero value in the Controller Manager's configuration. Remediation involves updating the cluster configuration to specify a secure port, typically 10257, in accordance with best practices. This action helps secure the control plane's communication channels.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/cis-benchmarks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
- rule_id: gcp.container.cluster.k8s_controllermanager_service_account_token_signing_enabled
  service: container
  resource: cluster
  requirement: K8s Controllermanager Service Account Token Signing Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure K8s Service Account Token Signing is Enabled
  rationale: Enabling service account token signing in Kubernetes ensures that tokens are cryptographically secure, reducing the risk of unauthorized access and token forgery. This feature is critical for maintaining the integrity and confidentiality of service account tokens, thus protecting sensitive workloads and data in the cluster. Compliance with security standards such as NIST and CIS often requires such measures to safeguard cloud environments.
  description: This check verifies whether the Kubernetes Controller Manager has service account token signing enabled in GCP clusters. To ensure this, check the cluster's configuration for the --service-account-signing-key-file and --root-ca-file flags. If these flags are not set, it is crucial to configure them to enable token signing. Remediation involves updating the Kubernetes Controller Manager configuration to include these flags, ensuring that all service account tokens are signed and verified.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restricting_service_account_usage
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
- rule_id: gcp.container.cluster.k8s_etcd_client_cert_auth_enabled
  service: container
  resource: cluster
  requirement: K8s Etcd Client Cert Auth Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable K8s Etcd Client Certificate Authentication
  rationale: Enabling client certificate authentication for etcd in Kubernetes clusters enhances security by ensuring that only authenticated clients can access the etcd server. This reduces the risk of unauthorized access and potential data exfiltration, supporting compliance with data protection regulations like GDPR and CCPA, which mandate robust access controls to sensitive data.
  description: This rule checks whether client certificate authentication is enabled for etcd in Kubernetes clusters. Etcd is a critical component in Kubernetes, storing all cluster data, and securing it with client certificates ensures that only verified entities can communicate with it. To verify, check the cluster's etcd configuration for the '--client-cert-auth' flag. Remediation involves updating your etcd configuration to include client certificate authentication, thus enhancing the security posture of the cluster.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-cluster
  - https://www.cisecurity.org/benchmark/kubernetes
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview
- rule_id: gcp.container.cluster.k8s_etcd_encryption_at_rest_enabled
  service: container
  resource: cluster
  requirement: K8s Etcd Encryption At Rest Enabled
  scope: container.cluster.encryption
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Enable Kubernetes Etcd Encryption at Rest in GKE Clusters
  rationale: Encrypting etcd data at rest in Kubernetes clusters is critical to protect sensitive configuration data, such as secrets and configuration maps. Without encryption, this data is vulnerable to unauthorized access in the event of a data breach or insider threat. Enabling encryption helps meet compliance requirements for data protection standards like PCI-DSS and HIPAA, reducing the risk of data exposure.
  description: This rule checks if etcd, the key-value store used by Kubernetes for configuration data, is encrypted at rest in GCP Kubernetes Engine (GKE) clusters. To verify, ensure the encryptionConfig field is enabled in the cluster's configuration. Remediation involves configuring the cluster with a Customer Managed Encryption Key (CMEK) to enable etcd encryption, providing an additional layer of security for sensitive data stored in clusters.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
  - https://cloud.google.com/architecture/best-practices-for-running-production-workloads-on-gke
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - 'NIST SP 800-53: System and Communications Protection'
  - 'PCI DSS Requirement 3: Protect Stored Cardholder Data'
- rule_id: gcp.container.cluster.k8s_scheduler_leader_election_enabled
  service: container
  resource: cluster
  requirement: K8s Scheduler Leader Election Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Scheduler Leader Election
  rationale: Enabling leader election for the Kubernetes scheduler ensures high availability and fault tolerance by allowing multiple schedulers to be deployed with only one acting as the leader at any time. This reduces the risk of a single point of failure in cluster management, which can impact application availability and lead to compliance violations with standards requiring fault-tolerant architecture.
  description: This rule checks if the Kubernetes scheduler in a GCP container cluster is configured with leader election enabled. Leader election allows a backup scheduler to take over seamlessly if the current leader fails, ensuring continuous cluster operations. To verify, ensure that the scheduler manifest includes the '--leader-elect=true' flag. Remediation involves updating the scheduler configuration to enable this flag if it is not set.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-architectures
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.container.cluster.k8s_scheduler_profiling_disabled
  service: container
  resource: cluster
  requirement: K8s Scheduler Profiling Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Kubernetes Scheduler Profiling in GCP Clusters
  rationale: Disabling Kubernetes Scheduler profiling reduces exposure to potential unauthorized access and information disclosure risks. Profiling data can be exploited by attackers to gain insights into cluster performance and behavior, potentially leading to targeted attacks. Ensuring profiling is disabled aligns with security best practices and helps maintain compliance with regulatory frameworks requiring minimal exposure of sensitive data.
  description: This rule checks whether Kubernetes Scheduler profiling is disabled in GCP clusters. Profiling should be turned off to prevent unnecessary exposure of internal data that could aid an attacker. To verify, examine the cluster configuration to ensure the '--profiling' flag is set to 'false' for the kube-scheduler. Remediation involves updating the cluster configuration to disable profiling and redeploying the scheduler component if needed.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/scheduler
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
- rule_id: gcp.container.cluster.k8s_scheduler_secure_port_enabled
  service: container
  resource: cluster
  requirement: K8s Scheduler Secure Port Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Secure Port on Kubernetes Scheduler
  rationale: Enabling the secure port for the Kubernetes scheduler is crucial to ensure encrypted communications between the scheduler and other cluster components. This mitigates the risk of man-in-the-middle attacks and unauthorized data interception, enhancing compliance with security standards like NIST SP 800-53 and ISO 27001 that emphasize secure communication protocols.
  description: This rule checks whether the Kubernetes scheduler in a GCP container cluster is configured to use a secure port for communications. The secure port should be enabled to ensure all communications are encrypted using TLS. Verify this setting by checking the scheduler's configuration file or parameters. Remediation involves configuring the scheduler to use the secure port, typically 10259, by updating the cluster's API server settings.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/securing-cluster-components
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
- rule_id: gcp.container.cluster.kubernetes_dashboard_disabled
  service: container
  resource: cluster
  requirement: Kubernetes Dashboard Disabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Kubernetes Dashboard in GKE Clusters
  rationale: The Kubernetes Dashboard can expose sensitive cluster information and administrative capabilities if not secured properly, leading to potential unauthorized access and control over cluster resources. Disabling it mitigates risks of exploitation by attackers who may leverage vulnerabilities in the dashboard to compromise the cluster. This precaution aligns with compliance requirements to minimize attack surfaces and protect data integrity.
  description: This rule checks that the Kubernetes Dashboard is disabled in Google Kubernetes Engine (GKE) clusters. The dashboard can be a vector for attacks if exposed to the internet without proper authentication and authorization. To verify, ensure that the Kubernetes Dashboard add-on is not installed on your GKE clusters. If installed, remove it using the GCP Console or gcloud CLI. Disabling the dashboard enhances security by reducing potential entry points for attackers.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/dashboards#disabling_the_kubernetes_dashboard
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://cloud.google.com/security/best-practices
  - https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
- rule_id: gcp.container.cluster.logging_monitoring_service_enabled
  service: container
  resource: cluster
  requirement: Logging Monitoring Service Enabled
  scope: container.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure GKE Cluster Logging and Monitoring is Enabled
  rationale: Enabling logging and monitoring for GKE clusters is crucial for detecting unauthorized access attempts, diagnosing operational issues, and maintaining audit trails. Without this, organizations may face increased risks of undetected security breaches and non-compliance with standards like PCI-DSS and SOC2, which require robust logging and monitoring mechanisms.
  description: This rule checks if Google Kubernetes Engine (GKE) clusters have both logging and monitoring services enabled. These services should be configured to collect system and application logs and metrics. To verify, inspect the cluster settings in the GCP Console or use gcloud CLI to ensure that 'loggingService' and 'monitoringService' are not set to 'none'. Remediation involves enabling these services through the GCP Console or using gcloud commands to update the cluster.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/logging-and-monitoring
  - https://cloud.google.com/kubernetes-engine/docs/concepts/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/logging
- rule_id: gcp.container.cluster.master_authorized_networks_enabled
  service: container
  resource: cluster
  requirement: Master Authorized Networks Enabled
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Enable Master Authorized Networks for GKE Clusters
  rationale: Enabling Master Authorized Networks in GKE clusters restricts access to the Kubernetes API server, minimizing the risk of unauthorized access. This control helps prevent potential attacks such as credential stuffing or brute force attempts from unauthorized IP ranges, which is crucial for maintaining the integrity and confidentiality of cluster workloads and data.
  description: This rule checks if Master Authorized Networks are enabled for Google Kubernetes Engine (GKE) clusters. When enabled, it allows access to the Kubernetes API server only from specific IP addresses, enhancing security by limiting potential attack vectors. To verify, navigate to the GKE cluster settings in the GCP Console and ensure that Master Authorized Networks are configured with the intended IP ranges. If not enabled, update the cluster settings to specify the authorized networks.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.cluster.network_policy_support_configured
  service: container
  resource: cluster
  requirement: Network Policy Support Configured
  scope: container.cluster.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Ensure Network Policy is Enabled for GKE Clusters
  rationale: Enabling network policy in Google Kubernetes Engine (GKE) is crucial for controlling traffic flow between pods, reducing the risk of lateral movement in the event of a breach. This enhances the security posture by allowing restrictions based on IP addresses, ports, and protocols, which is essential for compliance with security standards and protecting sensitive data from unauthorized access.
  description: This rule checks that network policy support is configured for GKE clusters, ensuring that network policies can be applied to control traffic. To verify, check if the network policy is enabled in your GKE cluster settings. Remediation involves enabling network policy by configuring the cluster with the `--enable-network-policy` flag during creation or updating existing clusters via GCP Console or `gcloud` CLI. This helps in isolating workloads and enforcing security controls efficiently.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.private_endpoint_and_public_access_disabled
  service: container
  resource: cluster
  requirement: Private Endpoint And Public Access Disabled
  scope: container.cluster.public_access
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Ensure Private Endpoint and Disable Public Access for GKE Clusters
  rationale: Disabling public access and configuring private endpoints for Google Kubernetes Engine (GKE) clusters reduces the attack surface by preventing unauthorized access from the internet. This setup mitigates risks such as unauthorized data exposure and potential DDoS attacks, which can lead to service disruptions and data breaches. Ensuring private-only access aligns with compliance requirements like PCI-DSS and ISO 27001, which mandate strict network access controls.
  description: This rule checks if GKE clusters have public access disabled and are configured with a private endpoint. To verify, inspect the cluster's networking configuration to ensure 'privateClusterConfig.enablePrivateEndpoint' is true and 'privateClusterConfig.enablePrivateNodes' is true, while ensuring 'publicClusterConfig' is not set. Remediation involves using the GCP Console or gcloud command to update the cluster settings to enable private endpoints and disable public access.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://cloud.google.com/kubernetes-engine/docs/best-practices-for-private-clusters
  - CIS Google Cloud Platform Foundation Benchmark
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.private_nodes_configured
  service: container
  resource: cluster
  requirement: Private Nodes Configured
  scope: container.cluster.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure GKE Clusters Have Private Nodes Configured
  rationale: Configuring private nodes in GKE clusters is crucial for reducing exposure to the public internet, minimizing the attack surface, and adhering to best practices for data protection and compliance with regulations such as HIPAA and PCI-DSS. This configuration helps protect sensitive workloads from unauthorized access and potential data breaches by ensuring that nodes communicate over private IP addresses.
  description: This rule checks whether Google Kubernetes Engine (GKE) clusters are configured to use private nodes. Private nodes must be enabled to ensure that all node-to-node and node-to-master communications occur over a private network, enhancing security and reducing public exposure. To verify, check the cluster's privateClusterConfig settings via the GCP Console or gcloud CLI. Remediation involves enabling private clusters through the GCP Console or updating the cluster with the `--enable-private-nodes` flag using the gcloud CLI.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/itl/applied-cybersecurity/nistcyberframework
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.container.cluster.private_nodes_enabled
  service: container
  resource: cluster
  requirement: Private Nodes Enabled
  scope: container.cluster.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enable Private Nodes in GKE Clusters
  rationale: Enabling private nodes in Google Kubernetes Engine (GKE) clusters ensures that the nodes have internal IP addresses only, reducing exposure to the public internet and minimizing the attack surface. This configuration helps protect sensitive workloads and complies with regulatory frameworks that mandate network segmentation and access control, such as PCI DSS and HIPAA. It also mitigates risks associated with data breaches and unauthorized access by limiting the network pathways available to potential attackers.
  description: This rule verifies that private nodes are enabled in GKE clusters, ensuring that nodes do not have external IP addresses and communicate over a private network. To verify, check the cluster configuration for 'privateNodesConfig' and ensure 'enablePrivateNodes' is set to true. Remediation involves modifying the cluster to enable private nodes, which can be done via the GCP Console, gcloud CLI, or Terraform. This configuration enhances security by isolating nodes from the public internet while still allowing communication with other Google Cloud services through private IPs.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-private-gke-clusters
  - https://cloud.google.com/security/best-practices/networking
- rule_id: gcp.container.cluster.rbac_privilege_escalation_permissions
  service: container
  resource: cluster
  requirement: RBAC Privilege Escalation Permissions
  scope: container.cluster.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Prevent RBAC Privilege Escalation in Kubernetes Clusters
  rationale: Improperly configured RBAC permissions can lead to privilege escalation, allowing unauthorized users to gain elevated access within Kubernetes clusters. This poses a significant security risk, potentially leading to data breaches, service disruptions, and non-compliance with standards such as NIST and PCI-DSS. Ensuring least privilege access is critical for maintaining the integrity and security of containerized applications.
  description: This rule checks for Kubernetes Role-Based Access Control (RBAC) permissions that could enable privilege escalation within GCP container clusters. It identifies roles with broad 'bind' or 'impersonate' permissions that could be exploited. To remediate, review and restrict these permissions, ensuring they adhere to the principle of least privilege. Use the Google Cloud Console or gcloud CLI to audit and adjust IAM roles accordingly.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://cloud.google.com/blog/products/identity-security/enabling-least-privilege-administration-with-rbac-a-best-practice
- rule_id: gcp.container.cluster.release_channel_regular_or_stable
  service: container
  resource: cluster
  requirement: Release Channel Regular Or Stable
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure GKE Clusters Use Regular or Stable Release Channels
  rationale: Using Regular or Stable release channels for Google Kubernetes Engine (GKE) clusters reduces the risk of deploying untested or unstable software, minimizing the potential for security vulnerabilities and operational disruptions. This approach aligns with best practices for maintaining a secure and reliable Kubernetes environment, ensuring that clusters receive timely security patches and updates. Compliance with security frameworks often mandates using stable software versions to mitigate risks associated with experimental features.
  description: This rule checks if GKE clusters are configured to use either the Regular or Stable release channels, which are known for their balance of feature updates and stability. Regular release channels offer a compromise between receiving new features and maintaining stability, while Stable channels focus on delivering well-tested, reliable updates. To verify, navigate to the Google Cloud Console, go to Kubernetes Engine, and check the Release Channel setting for each cluster. If necessary, change the channel by editing the cluster settings to ensure compliance with security best practices.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/release-channels
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/how-to/upgrade-a-cluster
  - https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.container.cluster.role_cluster_admin_restrictions
  service: container
  resource: cluster
  requirement: Role Cluster Admin Restrictions
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Restrict Container Cluster Admin Role Assignment
  rationale: The Cluster Admin role grants extensive permissions that can lead to unauthorized access or modification of cluster configurations, posing significant security risks. Limiting this role's assignment mitigates the potential for privilege escalation, reduces the attack surface, and aligns with compliance requirements such as PCI-DSS and ISO 27001, enhancing overall security posture.
  description: This rule checks for the assignment of the 'roles/container.clusterAdmin' role to users and service accounts. To secure the environment, ensure this role is assigned only to trusted personnel by reviewing IAM policies and using Google Cloud IAM Conditions for more granular control. Remediation involves auditing current assignments and implementing least privilege principles by minimizing the number of users with this role.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/iam
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.cluster.secrets_encryption_kms_enabled
  service: container
  resource: cluster
  requirement: Secrets Encryption KMS Enabled
  scope: container.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS for Secrets Encryption in GKE Clusters
  rationale: Encrypting secrets with a Key Management Service (KMS) reduces the risk of unauthorized access and exposure of sensitive data, which can lead to data breaches and compliance violations. Using KMS enhances security by providing additional control over the encryption keys, which is crucial for meeting regulatory requirements such as PCI DSS, HIPAA, and GDPR.
  description: This rule checks if Google Kubernetes Engine (GKE) clusters have Secrets Encryption enabled using Google Cloud Key Management Service (KMS). To verify, ensure that the cluster encryption configuration is set to use a Customer Managed Encryption Key (CMEK). Remediation involves updating the cluster's encryption settings to integrate with KMS by specifying a valid KMS key in the cluster's configuration, enhancing the security of stored secrets.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/kms/docs/
- rule_id: gcp.container.cluster.secrets_rbac_minimal_access
  service: container
  resource: cluster
  requirement: Secrets RBAC Minimal Access
  scope: container.cluster.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Limit RBAC Access to Secrets in GKE Clusters
  rationale: Minimizing RBAC access to secrets in Google Kubernetes Engine (GKE) clusters reduces the risk of unauthorized access to sensitive information, such as database credentials and API keys. Excessive permissions can lead to data breaches or service disruptions if credentials are misused or leaked. This practice supports compliance with regulations such as GDPR and PCI-DSS, which mandate strict access controls to sensitive data.
  description: This rule checks that RBAC roles in GKE clusters do not grant excessive permissions to access Kubernetes secrets. It ensures that only necessary roles have read or write access to secrets, aligning with the principle of least privilege. To verify, review the RBAC role bindings in your GKE clusters and ensure only intended service accounts and users have access to secrets. Remediation involves updating role bindings to remove excessive permissions and creating custom roles as needed.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#rbac
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security
- rule_id: gcp.container.cluster.security_posture_config_mode
  service: container
  resource: cluster
  requirement: Security Posture Config Mode
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Security Posture Config Mode in GKE Clusters
  rationale: Configuring Security Posture Config Mode in Google Kubernetes Engine (GKE) clusters helps enterprises maintain a robust security posture by automatically enforcing security policies and monitoring for compliance. This reduces the risk of misconfigurations that could lead to unauthorized access or data breaches, and ensures alignment with regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule checks if the GKE cluster has Security Posture Config Mode enabled, which is crucial for proactive security management. To verify, navigate to the Google Cloud Console, select Kubernetes Engine, and check if the Security Posture Config Mode is set. To enable, update your GKE cluster settings to include security policy configurations. This helps maintain compliance and enhances the security monitoring of your container workloads.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/security-posture-config
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.cluster.shielded_nodes_enabled
  service: container
  resource: cluster
  requirement: Shielded Nodes Enabled
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Shielded Nodes are Enabled in GKE Clusters
  rationale: Enabling Shielded Nodes in Google Kubernetes Engine (GKE) clusters enhances node security by protecting against rootkit and bootkit attacks, which can compromise the integrity of the node operating system. This feature is crucial for maintaining the confidentiality, integrity, and availability of workloads, particularly in environments subject to compliance requirements like PCI-DSS or HIPAA, where data protection is paramount.
  description: This rule checks if Shielded Nodes are enabled in GKE clusters. Shielded Nodes provide verifiable integrity of node boot processes and protect against unauthorized changes to the guest operating system. To enable Shielded Nodes, update the cluster to set the 'enableShieldedNodes' field to 'true'. Verification can be done via the GCP Console under Kubernetes Engine settings or using the 'gcloud' CLI with 'gcloud container clusters describe'. If not enabled, modify the cluster configuration to activate Shielded Nodes and apply changes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes
  - https://cloud.google.com/kubernetes-engine/docs/concepts/shielded-nodes
  - CIS Google Kubernetes Engine Benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.cluster.system_masters_group_absence
  service: container
  resource: cluster
  requirement: System Masters Group Absence
  scope: container.cluster.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure System Masters Group is Absent from GKE Clusters
  rationale: The presence of the system masters group in GKE clusters poses a security risk by granting overly broad permissions that may lead to unauthorized access and privilege escalation. Removing this group mitigates the risk of insider threats and aligns with the principle of least privilege, which is critical for maintaining a secure and compliant Kubernetes environment.
  description: This rule checks for the absence of the system masters group in Google Kubernetes Engine (GKE) clusters. To verify, inspect cluster role bindings and ensure that the system:masters group is not bound to any roles. Remediation involves modifying or removing any existing role bindings that include this group, thereby preventing excessive permissions. This configuration enhances security by limiting access to only necessary personnel and services.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restricting_access
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 6.5
  - 'NIST SP 800-53 AC-6: Least Privilege'
  - https://kubernetes.io/docs/concepts/security/overview/#privilege-separation
- rule_id: gcp.container.cluster.vpc_native_alias_ip_enabled
  service: container
  resource: cluster
  requirement: VPC Native Alias Ip Enabled
  scope: container.cluster.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure VPC Native Alias IP is Enabled for GKE Clusters
  rationale: Enabling VPC Native Alias IPs on Google Kubernetes Engine (GKE) clusters allows for better network isolation, scalability, and IP address management. This configuration reduces the risk of IP exhaustion and ensures that pods have access to all VPC network services securely. It is essential for meeting compliance requirements related to network segmentation and access control, thereby minimizing the risk of unauthorized access to resources.
  description: This rule checks if GKE clusters are using VPC Native Alias IPs, which allow each pod to receive a unique IP address from a range assigned to the cluster. To verify, ensure that the cluster configuration includes 'ipAllocationPolicy' with 'useIpAliases' set to true. Remediation involves updating the cluster settings to enable 'VPC Native' alias IPs, which can be done via the Google Cloud Console or gcloud CLI, enhancing the network security posture.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/solutions/security-overview
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.namespace.administrative_boundary_enforced
  service: container
  resource: namespace
  requirement: Administrative Boundary Enforced
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Enforce Administrative Boundaries in Kubernetes Namespaces
  rationale: Enforcing administrative boundaries in Kubernetes namespaces is critical to prevent unauthorized access and potential lateral movement within a GCP Kubernetes Engine cluster. This helps in maintaining clear separation of duties, ensuring that different teams or applications do not interfere with each other, which reduces the risk of accidental or malicious changes. Proper namespace isolation supports compliance with regulations requiring data segregation and access control, such as PCI-DSS and HIPAA.
  description: This rule checks if Kubernetes namespaces within a GCP Container service have enforced administrative boundaries using Role-Based Access Control (RBAC). It ensures namespaces are configured with specific roles and bindings to restrict user and service account permissions, preventing unauthorized access to resources. To verify, inspect the RBAC policies associated with each namespace and ensure they align with least privilege principles. Remediation involves reviewing and adjusting role bindings in the Kubernetes cluster to ensure proper isolation and control.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
- rule_id: gcp.container.namespace.containers_kubernetes_default_service_account_autom_disabled
  service: container
  resource: namespace
  requirement: Containers Kubernetes Default Service Account Autom Disabled
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Automatic Default Service Account in Kubernetes
  rationale: Disabling the automatic assignment of the default service account for Kubernetes containers reduces the risk of privilege escalation and unauthorized access to resources. This practice helps maintain the principle of least privilege, ensuring that applications run with only the permissions they need, thus minimizing potential attack vectors and aligning with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks if the default service account is disabled for Kubernetes containers in a namespace. The default service account often has elevated permissions that could be misused if exploited. To verify, review the namespace configurations and ensure no pods are using the default service account. Remediation involves setting custom service accounts with tailored permissions and updating pod specifications to reference these accounts.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#default-service-account
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/blog/topics/developers-practitioners/best-practices-using-google-kubernetes-engine-gke
- rule_id: gcp.container.namespace.containers_kubernetes_network_policies_present
  service: container
  resource: namespace
  requirement: Containers Kubernetes Network Policies Present
  scope: container.namespace.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Ensure Kubernetes Network Policies are Configured in Namespaces
  rationale: Kubernetes Network Policies are essential for defining the allowed traffic between pods in a namespace and external endpoints. Without these policies, there is a risk of unauthorized access and data breaches due to unrestricted network communications. Implementing network policies helps organizations comply with regulatory requirements such as PCI-DSS and ISO 27001 by enforcing strict access control and segmentation.
  description: This rule checks whether Kubernetes Network Policies are present for every namespace in your GCP Kubernetes Engine clusters. Network Policies act as a security boundary, allowing administrators to specify which connections are permitted, thereby minimizing attack surfaces. To verify, ensure that each namespace has at least one network policy defined that specifies ingress and egress rules as per your security requirements. Remediation involves creating and applying a NetworkPolicy resource in each namespace using Kubernetes YAML configurations.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/kubernetes
  - https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2_1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.namespace.containers_kubernetes_pod_security_level_restricted
  service: container
  resource: namespace
  requirement: Containers Kubernetes Pod Security Level Restricted
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Pods Use Restricted Security Context
  rationale: Implementing restricted pod security contexts in Kubernetes namespaces minimizes security risks by preventing privilege escalation and reducing attack surfaces. This is crucial for protecting sensitive data and workloads from unauthorized access, which can lead to data breaches, service disruptions, and non-compliance with regulatory standards such as PCI-DSS and HIPAA.
  description: This rule checks if Kubernetes pods within a namespace are configured with a restricted security context, which includes setting runAsNonRoot, disallowing privileged mode, and restricting capabilities. To verify compliance, review the PodSecurityPolicy or Pod Security Standards in place. Remediation involves applying or updating a PodSecurityPolicy to enforce these restrictions and ensuring developers adhere to these policies when deploying applications.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies
  - https://cloud.google.com/security/compliance/cis-benchmark
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.container.namespace.default_service_account_usage_disabled
  service: container
  resource: namespace
  requirement: Default Service Account Usage Disabled
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Default Service Account Usage in Kubernetes Namespaces
  rationale: Using the default service account in Kubernetes namespaces can lead to elevated security risks. It might provide unnecessary permissions that could be exploited by attackers if a container is compromised. Disabling its usage reduces the attack surface and aligns with the principle of least privilege, thus protecting sensitive data and operations.
  description: This rule checks whether the default service account is disabled in all Kubernetes namespaces within GCP. To verify, inspect the service account configurations in each namespace and ensure no workloads are using the default account. Remediation involves creating custom service accounts with minimal permissions tailored to application needs and updating workloads to use these instead of the default account.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - NIST SP 800-53 AC-6 Least Privilege
  - 'PCI-DSS Requirement 7: Restrict access to cardholder data by business need to know'
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - ISO/IEC 27001:2013 A.9.1.2 Access control to networks and network services
- rule_id: gcp.container.namespace.network_policy_defined
  service: container
  resource: namespace
  requirement: Network Policy Defined
  scope: container.namespace.network_security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Network Policies Are Defined for Kubernetes Namespaces
  rationale: Defining network policies within Kubernetes namespaces is crucial to controlling traffic flow between pods and services. Without explicit network policies, namespaces are vulnerable to unrestricted communication, increasing the risk of lateral movement by attackers within the cluster. This can lead to data breaches, unauthorized access, and potential regulatory non-compliance, impacting business operations and reputation.
  description: This rule checks if Kubernetes namespaces in GCP have associated network policies to manage ingress and egress traffic. Network policies allow you to specify how groups of pods can communicate with each other and other network endpoints. To verify, ensure that a policy exists for each namespace, specifying allowed traffic sources and destinations. Remediation involves creating and applying network policies using Kubernetes manifests or via the GCP Console, tailored to your security requirements.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://www.pcisecuritystandards.org/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://cloud.google.com/security/compliance/
- rule_id: gcp.container.namespace.pod_security_baseline_enforcement_configured
  service: container
  resource: namespace
  requirement: Pod Security Baseline Enforcement Configured
  scope: container.namespace.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Configure Pod Security Baseline Enforcement in Namespaces
  rationale: Pod Security Baseline Enforcement is crucial for minimizing security risks within Kubernetes clusters by ensuring pods adhere to defined security standards. Without enforcement, pods may run with excessive permissions, exposing the cluster to privilege escalation attacks and non-compliance with security frameworks like NIST and PCI-DSS.
  description: This rule checks whether Pod Security Baseline Enforcement is configured for all namespaces in GCP Kubernetes clusters. To verify, ensure that PodSecurityPolicy objects are defined and associated with namespaces to control the security context of pods. Remediation involves creating or updating PodSecurityPolicy resources to align with baseline security requirements and binding them to the necessary namespaces via RoleBindings.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.container.node.pool_auto_repair_status_configured
  service: container
  resource: node
  requirement: Pool Auto Repair Status Configured
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Node Pool Auto-Repair is Enabled
  rationale: Enabling auto-repair for Kubernetes node pools ensures that any unhealthy nodes are automatically repaired, reducing downtime and maintaining application availability. This feature helps mitigate risks associated with node failures, which can lead to service disruptions and potential data loss, impacting business continuity and customer trust.
  description: This rule checks whether auto-repair is enabled for all node pools in a GCP Kubernetes Engine cluster. Auto-repair automatically fixes unhealthy nodes, which can occur due to hardware or software issues. To verify, navigate to the GCP Console, go to Kubernetes Engine, select your cluster, and check the settings for each node pool to ensure 'Auto-repair' is enabled. If not configured, enable it to ensure nodes are self-healing and minimize manual intervention.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
- rule_id: gcp.container.node.pool_auto_upgrade_status_configured
  service: container
  resource: node
  requirement: Pool Auto Upgrade Status Configured
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure GKE Node Pool Auto-Upgrades Are Enabled
  rationale: Enabling auto-upgrades for GKE node pools ensures that nodes receive the latest security patches and feature updates, reducing the risk of vulnerabilities being exploited. This is critical for maintaining a secure Kubernetes environment and aligning with compliance requirements that mandate up-to-date software. Failure to do so could lead to increased exposure to known vulnerabilities and potential breaches.
  description: This check verifies that the auto-upgrade feature is enabled for all Google Kubernetes Engine (GKE) node pools. Auto-upgrades help automatically keep the nodes updated with the latest Kubernetes versions, which include critical security patches and improvements. To verify, navigate to the Google Cloud Console, select Kubernetes Engine, then select Clusters, and review each node pool's settings to ensure auto-upgrade is enabled. Remediation involves updating the node pool settings to enable auto-upgrades through the console or using the gcloud command line tool.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-upgrades
  - https://cloud.google.com/security/compliance/cis-gke-benchmark
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.node.pool_boot_disk_cmek_enabled
  service: container
  resource: node
  requirement: Pool Boot Disk Cmek Enabled
  scope: container.node.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable CMEK for GKE Node Pool Boot Disks
  rationale: Using Customer-Managed Encryption Keys (CMEK) for GKE node pool boot disks enhances data security by providing greater control over encryption keys. This mitigates risks of unauthorized data access and supports compliance with regulations requiring customer-controlled encryption, such as GDPR and CCPA.
  description: This rule checks if GKE node pool boot disks are encrypted using CMEK. By default, GCP uses Google-managed keys, but enabling CMEK allows you to manage key lifecycle and access. To verify, inspect the node pool configuration for the 'diskEncryptionKmsKey' property. To enable CMEK, update the node pool with a specified KMS key using the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-storage
  - https://cloud.google.com/security/compliance/cis#gke
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.container.node.pool_gke_sandbox_enabled
  service: container
  resource: node
  requirement: Pool Gke Sandbox Enabled
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable GKE Sandbox for Node Pools
  rationale: Enabling GKE Sandbox provides an additional layer of security by isolating container workloads using gVisor, reducing the risk of container escape and potential damage to the host system. This is crucial for maintaining a secure Kubernetes environment, particularly for organizations processing sensitive or regulated data, and helps in mitigating risks associated with multi-tenancy.
  description: This rule checks whether GKE Sandbox is enabled for Kubernetes node pools. GKE Sandbox uses gVisor to provide a secure isolation boundary between container workloads and the host operating system. To verify, ensure that node pools are configured with the 'sandbox.gke.io/runtime' node label. If not enabled, apply this label to enhance security by preventing kernel-level attacks. Consult the GCP console or use the gcloud command-line tool to update your node pool configurations.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods
  - https://cloud.google.com/kubernetes-engine/docs/concepts/sandbox-pods
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.node.pool_image_type_cos_containerd_configured
  service: container
  resource: node
  requirement: Pool Image Type Cos Containerd Configured
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Node Pool Image Type is COS with Containerd
  rationale: Using COS with containerd as the node image type enhances security by providing a minimal, hardened OS that reduces the attack surface and ensures compatibility with GKE features. This configuration aligns with security best practices and can help meet compliance requirements by ensuring that nodes run a consistent, secure OS environment, minimizing risks associated with vulnerabilities in more complex OS setups.
  description: This check verifies that the node pool image type is set to Container-Optimized OS (COS) with containerd for Google Kubernetes Engine (GKE) clusters. To ensure this, inspect the node pool configurations in the GCP Console or using gcloud CLI. Remediation involves updating any node pools using non-COS images to use COS with containerd, which is handled via the GCP Console, gcloud CLI, or Terraform. This setup offers improved security and stability by leveraging the lightweight, secure nature of COS.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/node-images#cos
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/concepts/containers/runtime-class/#containerd
  - https://cloud.google.com/containers/security/container-optimized-os-overview
- rule_id: gcp.container.node.pool_service_account_privilege_configured
  service: container
  resource: node
  requirement: Pool Service Account Privilege Configured
  scope: container.node.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Node Pool Service Account Uses Least Privilege
  rationale: Configuring node pools with the principle of least privilege minimizes the risk of unauthorized access and potential security breaches. Overprivileged service accounts can lead to data exfiltration, unauthorized resource access, and compliance violations with frameworks such as NIST and PCI-DSS. This configuration is crucial for maintaining a secure and compliant cloud environment.
  description: This rule checks if the service account used by GCP Kubernetes Engine node pools is configured with only the necessary permissions. Verify that the service account associated with the node pool has roles that are strictly required for its operations. Remediate by assigning only specific roles essential for the node's functions and avoid using default or overly permissive service accounts. Regularly audit service account permissions to ensure adherence to the principle of least privilege.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.node.pool_shielded_secure_boot_enabled
  service: container
  resource: node
  requirement: Pool Shielded Secure Boot Enabled
  scope: container.node.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Shielded VM Secure Boot is Enabled for Node Pools
  rationale: Enabling Shielded VM Secure Boot for Kubernetes node pools protects against persistent, low-level malware threats and rootkits. This security feature ensures the integrity of the boot process, providing a trusted path for system startup, which is essential for maintaining the overall security posture of your containerized applications and meeting compliance frameworks like PCI-DSS and NIST.
  description: This rule checks if Shielded VM Secure Boot is enabled for all node pools in Google Kubernetes Engine (GKE). Secure Boot is a critical feature that helps prevent unauthorized software and rootkits from executing during the boot process. To verify, ensure that the Shielded VM options, specifically Secure Boot, are enabled in the node pool configuration. Remediation involves updating existing node pools or creating new node pools with Shielded VM Secure Boot enabled.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/shielded-gke-nodes
  - https://cloud.google.com/security/compliance/cis-coredns
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/specialpublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.node_pool.authorization_policy_compliance
  service: container
  resource: node_pool
  requirement: Authorization Policy Compliance
  scope: container.node_pool.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Authorization Policies on GKE Node Pools
  rationale: Implementing authorization policies on GKE node pools reduces unauthorized access risks, protects sensitive workloads, and ensures compliance with security standards. Unauthorized access to node pools can lead to data breaches and service disruptions, negatively impacting business operations and reputation.
  description: This rule verifies that Google Kubernetes Engine (GKE) node pools have proper authorization policies in place to control access and permissions. It checks for configurations that ensure only authorized identities can access and manage workloads. To verify compliance, review and configure IAM policies on node pools to limit access to essential users. Remediation involves setting up role-based access controls (RBAC) and using IAM policies to enforce the principle of least privilege.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
- rule_id: gcp.container.node_pool.containers_kubernetes_container_instance_env_no_plai_secrets
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Container Instance Env No Plai Secrets
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Avoid Plaintext Secrets in Kubernetes Container Environment Variables
  rationale: Exposing secrets as plaintext in environment variables can lead to unauthorized access and data breaches if a container is compromised. This practice poses significant security risks, as attackers can easily extract sensitive information such as API keys and credentials, leading to potential data loss, service disruptions, and non-compliance with regulations like PCI-DSS and HIPAA.
  description: This rule checks if Kubernetes container instances within a node pool have environment variables set with plaintext secrets. The recommended practice is to use Kubernetes Secrets to manage sensitive data securely. Verify configurations using tools like kubectl to inspect environment variables and ensure they are sourced from Secrets. To remediate, refactor workloads to use Kubernetes Secrets and update deployment configurations accordingly.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/secrets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kubernetes-engine/docs/best-practices
- rule_id: gcp.container.node_pool.containers_kubernetes_container_instance_no_privi_containers
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Container Instance No Privi Containers
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Containers Run Without Privileged Mode
  rationale: Running containers in privileged mode grants them elevated access to the host system, which can lead to potential security breaches if compromised. This poses risks such as unauthorized data access, privilege escalation, and compliance violations with standards like PCI-DSS and HIPAA, which mandate strict control over system privileges.
  description: This rule checks that Kubernetes containers in GCP's node pools are not running in privileged mode. Privileged containers have extensive permissions, potentially allowing them to escape the container environment and access the host system. To verify, inspect your Kubernetes pod specifications for the 'privileged' flag and ensure it is set to 'false'. Remediation involves updating your Kubernetes configurations to remove any privileged settings from your container specifications.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/
- rule_id: gcp.container.node_pool.containers_kubernetes_container_instance_read_onl_filesystem
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Container Instance Read Onl Filesystem
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Containers Use Read-Only Filesystems
  rationale: Enforcing read-only filesystems in Kubernetes containers reduces the risk of unauthorized modifications and mitigates the impact of potential security vulnerabilities. This approach minimizes the attack surface, making it harder for attackers to exploit writable filesystems to escalate privileges or execute malicious code. Additionally, it supports compliance with security frameworks that emphasize data integrity and protection.
  description: This rule checks if Kubernetes containers within a node pool have their filesystems set to read-only. A read-only filesystem ensures that application code and libraries cannot be tampered with at runtime, enhancing container security. To verify this, inspect the container spec and ensure the 'securityContext.readOnlyRootFilesystem' field is set to true. Remediation involves updating the container specifications in your deployment configurations to enforce this setting.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_read-only_filesystems_for_containers
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.node_pool.containers_kubernetes_disk_encryption_enabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Disk Encryption Enabled
  scope: container.node_pool.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Kubernetes Disk Encryption for Node Pools
  rationale: Enabling disk encryption for Kubernetes node pools ensures that sensitive data is protected at rest, mitigating unauthorized access risks in the event of a data breach. It addresses compliance requirements by safeguarding data according to regulatory standards, such as GDPR and HIPAA, which mandate encryption for data protection.
  description: This rule checks whether Kubernetes node pools in Google Kubernetes Engine (GKE) have disk encryption enabled. Disk encryption ensures that the data stored on the persistent disks of nodes is encrypted using encryption keys, providing an additional layer of security. To enable encryption, configure the node pool to use customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK) through the Google Cloud Console or gcloud command line tool. Regularly review and update encryption settings to adhere to security policies.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-disks
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/document-2498
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.node_pool.containers_kubernetes_kubelet_anonymous_auth_disabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Kubelet Anonymous Auth Disabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes Kubelet Anonymous Auth is Disabled
  rationale: Disabling anonymous authentication for Kubernetes Kubelet is critical to prevent unauthorized access to the kubelet API, which could lead to potential security breaches such as unauthorized data exposure or resource manipulation. This measure helps in maintaining the integrity and confidentiality of cluster operations, aligning with compliance requirements like PCI-DSS and SOC2 that emphasize access control and data protection.
  description: This rule checks that anonymous authentication is disabled on Kubernetes Kubelet nodes within a GCP container node pool. Anonymous auth allows requests to be made to the kubelet API without authentication, posing a security risk. To verify, ensure that the kubelet configuration does not include the `--anonymous-auth` flag set to true. Remediate by setting this flag to false in the node pool configuration to restrict kubelet API access to authenticated users only.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_methods
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.node_pool.containers_kubernetes_kubelet_authz_webhook_or_rbac_enabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Kubelet Authz Webhook Or RBAC Enabled
  scope: container.node_pool.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Kubelet Authorization Webhook or RBAC is Enabled
  rationale: Enabling Kubelet Authorization Webhook or RBAC is crucial for enforcing fine-grained access control on Kubernetes nodes. Without these mechanisms, unauthorized users or processes may gain access to sensitive workloads, leading to potential data breaches or system compromise. This configuration helps meet compliance requirements such as PCI-DSS and ISO 27001 by ensuring proper access controls are in place.
  description: This rule checks that the Kubelet on GKE node pools has either the authorization webhook or RBAC enabled, which are critical for controlling access to the Kubernetes API. To verify, inspect the Kubelet configuration for 'authorization-mode' set to 'Webhook' or 'RBAC'. If neither is enabled, configure your GKE cluster to use RBAC by default or set up a webhook authorization. This ensures only authenticated and authorized requests are processed by the Kubernetes nodes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/projects/risk-management
  - https://www.pcisecuritystandards.org/document_library
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.node_pool.containers_kubernetes_kubelet_read_only_port_disabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Kubelet Read Only Port Disabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable Kubernetes Kubelet Read-Only Port on GCP Node Pools
  rationale: Enabling the read-only port on Kubernetes Kubelet exposes the node's metrics and status information to unauthenticated users, which can lead to unauthorized access and potential exploitation of the node. This misconfiguration can be leveraged by attackers to conduct reconnaissance, leading to more serious attacks such as privilege escalation or denial of service. Disabling this port is crucial for maintaining the confidentiality and integrity of the cluster's operational data and is often required to meet compliance with security standards.
  description: This rule checks if the read-only port for Kubernetes Kubelet is disabled on all node pools within a Google Kubernetes Engine (GKE) cluster. By default, the read-only port is set to 10255, which can be accessed without authentication. To enhance security, it should be set to '0' to disable it. This can be verified and configured by checking the node pool's Kubelet settings in the GCP Console or using the gcloud command-line tool. Remediation involves updating the node configuration to ensure the read-only port is disabled, thus reducing potential attack vectors.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_methods
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster
  - https://cloud.google.com/security/best-practices
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.node_pool.containers_kubernetes_kubelet_tls_min_1_2_enforced
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Kubelet TLS Min 1 2 Enforced
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Kubelet TLS Min Version 1.2 in GKE Node Pools
  rationale: Ensuring that the Kubelet uses a minimum TLS version of 1.2 helps protect data in transit from being intercepted or altered by unauthorized parties. This setting mitigates risks associated with weak encryption protocols and is critical for maintaining compliance with industry standards such as PCI-DSS and SOC 2, which mandate strong encryption practices.
  description: This rule checks if Kubelet in Google Kubernetes Engine (GKE) node pools enforces a minimum TLS version of 1.2. The TLS settings for Kubelet should be configured to reject connections that do not support at least version 1.2. To verify, inspect the kubelet configuration on each node and ensure the `--tls-min-version` flag is set to `1.2` or higher. Remediate by updating the cluster configuration to enforce the required TLS version, which may involve adjusting the node pool settings or upgrading the GKE version if necessary.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_authz
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-communications
- rule_id: gcp.container.node_pool.containers_kubernetes_nodes_no_public_ip
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Nodes No Public Ip
  scope: container.node_pool.public_access
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Prevent Public IPs on GKE Node Pools
  rationale: Exposing Kubernetes nodes with public IP addresses increases the attack surface, allowing potential unauthorized access to your cluster. This can lead to data breaches, service disruptions, and non-compliance with industry standards like PCI-DSS and HIPAA, which mandate strict control over network access and data protection.
  description: This rule checks that Google Kubernetes Engine (GKE) node pools do not have public IP addresses assigned to their nodes. To ensure that nodes are not publicly accessible, configure your GKE cluster to use private IP addresses only. Verification involves reviewing the networking settings of your node pools and adjusting them via Google Cloud Console or gcloud CLI to disable public IP allocation. Remediation can be done by creating node pools with private IP addresses only, enhancing security and compliance.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.container.node_pool.containers_kubernetes_shielded_secure_boot_enabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Shielded Secure Boot Enabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Shielded Secure Boot for Node Pools
  rationale: Enabling Shielded Secure Boot on Kubernetes node pools reduces the risk of firmware attacks by ensuring that the system boots with only trusted software. This enhances the security posture by protecting against unauthorized changes to the boot environment, a critical defense in maintaining data integrity and compliance with standards like PCI-DSS and SOC2.
  description: 'This rule checks if Shielded Secure Boot is enabled on node pools within GCP Kubernetes Engine. Shielded Secure Boot ensures that each boot component is cryptographically verified, safeguarding against tampered boot firmware. To verify, ensure that the node pool configuration includes ''shieldedInstanceConfig: { enableSecureBoot: true }''. Remediation involves updating your node pool configurations through GCP Console or CLI to enable Shielded Secure Boot.'
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#shielded_nodes
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/shielded-cloud
- rule_id: gcp.container.node_pool.containers_kubernetes_workload_identity_enabled
  service: container
  resource: node_pool
  requirement: Containers Kubernetes Workload Identity Enabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Kubernetes Workload Identity on GKE Node Pools
  rationale: Enabling Kubernetes Workload Identity on GKE node pools enhances security by ensuring that workloads have minimal permissions necessary to access Google Cloud services, reducing the risk of privilege escalation and data breaches. This configuration aligns with the principle of least privilege, mitigating risks associated with credential management and unauthorized access to resources.
  description: This rule verifies that Kubernetes Workload Identity is enabled on Google Kubernetes Engine (GKE) node pools. Workload Identity allows Kubernetes service accounts to act as Google service accounts, providing secure and fine-grained access control. To enable it, configure the node pool with Workload Identity during creation or update by setting the `--workload-pool` flag. Ensure that Kubernetes service accounts are properly annotated to utilize this identity feature. Remediation involves updating existing node pools to enable this setting through the GCP Console or `gcloud` CLI.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity
  - https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/nodePools
  - 'CIS GCP Benchmark: Section 5.3.1 Enable Workload Identity for GKE Clusters'
  - 'NIST SP 800-53: AC-6 Least Privilege'
- rule_id: gcp.container.node_pool.disk_cmek_enforcement
  service: container
  resource: node_pool
  requirement: Disk Cmek Enforcement
  scope: container.node_pool.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Node Pool Disks Use Customer-Managed Encryption Keys
  rationale: Enforcing the use of Customer-Managed Encryption Keys (CMEK) for node pool disks enhances data security by allowing organizations to maintain control over encryption keys. This minimizes the risk of unauthorized access to sensitive data, aligns with data protection regulations, and reduces the impact of potential data breaches. Implementing CMEK also supports compliance with standards like PCI-DSS and HIPAA that require strong encryption practices.
  description: This rule checks if node pools in Google Kubernetes Engine (GKE) clusters are configured to use Customer-Managed Encryption Keys for their persistent disks. To verify, inspect the node pool's configuration for CMEK settings in the GCP Console or use the gcloud CLI. Remediation involves updating the node pool to specify a CMEK during creation or through a configuration update. This ensures that data at rest is encrypted using keys managed and controlled by the organization, rather than Google-managed keys.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-disks
  - https://cloud.google.com/security/compliance/cis#cis-gke
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.container.node_pool.k8s_kubelet_anonymous_auth_disabled
  service: container
  resource: node_pool
  requirement: K8s Kubelet Anonymous Auth Disabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubelet Anonymous Authentication is Disabled
  rationale: Disabling anonymous authentication for kubelet nodes reduces the risk of unauthorized access to Kubernetes nodes, which can prevent data breaches and unauthorized data manipulation. This is crucial for maintaining the confidentiality and integrity of workloads and aligns with compliance requirements such as PCI-DSS and ISO 27001 that mandate secure access controls.
  description: This rule checks whether anonymous authentication is disabled for kubelets in your GCP Kubernetes Engine node pools. Anonymous authentication allows access to the kubelet API without authentication, which can be exploited by attackers. To verify, review node pool configurations and ensure the `--anonymous-auth` flag is set to `false`. Remediate by updating the node pool configurations to disable anonymous authentication.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_kubelet_authorization
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/architecture/nodes/#kubelet
- rule_id: gcp.container.node_pool.k8s_kubelet_client_cert_rotation_enabled
  service: container
  resource: node_pool
  requirement: K8s Kubelet Client Cert Rotation Enabled
  scope: container.node_pool.key_rotation
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable K8s Kubelet Client Cert Rotation in Node Pools
  rationale: Enabling Kubelet client certificate rotation helps ensure that the certificates used by each node's Kubelet component are automatically and regularly renewed, reducing the risk of certificate expiration which can lead to service disruptions. It mitigates potential attack vectors where expired certificates could be exploited, thus maintaining continuous, secure communication in your Kubernetes environment.
  description: This rule checks if the automatic rotation of client certificates for Kubelets in GCP Kubernetes Engine node pools is enabled. Without this setting, node-level security is weakened as certificates can expire without notice, leading to potential denial of service or unauthorized access. Verify this setting via the GCP Console under Kubernetes Engine settings or use gcloud CLI commands to ensure that the --rotate-certificates flag is enabled. Remediate by configuring your node pool with certificate rotation enabled, ensuring Kubelets are set to automatically renew their client certificates.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
  - https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools
  - 'CIS GCP Benchmark: 5.2.3 - Ensure Kubelet client certificate rotation is enabled'
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://security.googleblog.com/2017/10/introducing-gke-110-security-improvements.html
- rule_id: gcp.container.node_pool.k8s_kubelet_protect_kernel_defaults_enabled
  service: container
  resource: node_pool
  requirement: K8s Kubelet Protect Kernel Defaults Enabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubelet Protect Kernel Defaults is Enabled
  rationale: Enabling Kubelet Protect Kernel Defaults is crucial for maintaining the integrity and security of the node's kernel parameters. It prevents unauthorized changes that could expose vulnerabilities or lead to misconfigurations. This setting helps in meeting compliance requirements by ensuring the node's kernel settings are consistent with security policies and frameworks, thereby reducing the risk of exploitation.
  description: This rule checks if the 'Protect Kernel Defaults' feature is enabled for Kubelet on GKE node pools. This setting ensures that the Kubernetes nodes adhere to secure kernel configurations, preventing potential security vulnerabilities arising from incorrect kernel parameters. Verify this setting through the GCP Console or the gcloud command-line tool. To remediate, configure the node pool with 'Protect Kernel Defaults' enabled, ensuring compliance with security best practices.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#protect-kernel-defaults
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.node_pool.k8s_kubelet_read_only_port_disabled
  service: container
  resource: node_pool
  requirement: K8s Kubelet Read Only Port Disabled
  scope: container.node_pool.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable K8s Kubelet Read Only Port in Node Pools
  rationale: Disabling the Kubelet read-only port prevents unauthorized access to sensitive metrics and data, reducing the risk of information disclosure and potential exploitation by attackers. This is crucial for maintaining the confidentiality and integrity of cluster operations and is aligned with best practices for securing Kubernetes environments.
  description: This rule checks whether the read-only port for the Kubelet is disabled on all nodes within a GCP Kubernetes Engine node pool. The Kubelet's read-only port, when enabled, can expose sensitive endpoint information without authentication, making it a target for attackers. Ensure the '--read-only-port=0' flag is set in the Kubelet configuration on each node. Remediation involves updating the node pool configuration to disable the read-only port and applying the changes to the cluster nodes.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.cisecurity.org/benchmark/kubernetes
- rule_id: gcp.container.node_pool.node_pool_integrity_monitoring_enabled
  service: container
  resource: node_pool
  requirement: Node Pool Integrity Monitoring Enabled
  scope: container.node_pool.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Integrity Monitoring on GKE Node Pools
  rationale: Enabling integrity monitoring on GKE node pools helps detect unauthorized changes to the operating system and kernel. This is crucial for maintaining the security of containerized applications, as it can prevent potential breaches that may lead to data loss or service disruption. Furthermore, it supports compliance with industry standards demanding regular system integrity checks.
  description: This rule checks if integrity monitoring is enabled for Google Kubernetes Engine (GKE) node pools. Integrity monitoring ensures that any unexpected changes to the node's kernel or system packages are detected. To verify, ensure that the 'enable-integrity-monitoring' flag is set when creating or updating a node pool. Remediation involves using the GCP Console or gcloud CLI to enable integrity monitoring on existing node pools, enhancing the overall security posture by protecting against unauthorized modifications.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#node_pool_security
  - https://cloud.google.com/security/compliance/cis-gke-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/security-foundations/compliance
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.nodepool.metadata_server_enabled
  service: container
  resource: nodepool
  requirement: Metadata Server Enabled
  scope: container.nodepool.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Metadata Server on GKE Node Pools
  rationale: Enabling the metadata server on GKE node pools is crucial for managing service accounts, accessing instance metadata securely, and ensuring proper authorization checks are in place. This configuration mitigates the risk of unauthorized access to sensitive metadata that could lead to data breaches or privilege escalation attacks, thereby supporting compliance with data protection regulations like GDPR and HIPAA.
  description: This rule checks if the metadata server is enabled on Google Kubernetes Engine (GKE) node pools. The metadata server provides a secure channel for workloads to access metadata and service account credentials necessary for operations. Verifying this setting involves ensuring that the 'workload-metadata' configuration is set to 'GKE_METADATA' or 'SECURE', which can be adjusted through the GCP Console or gcloud CLI. Remediation involves updating the node pool configuration to enable the metadata server if it is not already active.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/protecting-cluster-metadata
  - CIS Google Kubernetes Engine Benchmark v1.2.0, Section 6.3.1
  - NIST SP 800-53 Rev. 5, AC-3 Access Enforcement
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.container.pod.seccomp_profile_runtime_default_enabled
  service: container
  resource: pod
  requirement: Seccomp Profile Runtime Default Enabled
  scope: container.pod.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enable Seccomp Profile Runtime Default for GKE Pods
  rationale: Seccomp (Secure Computing Mode) provides a way to filter system calls that a container can make, enhancing security by reducing the attack surface. Enforcing the Runtime Default profile helps prevent privilege escalation and protects against kernel vulnerabilities. This mitigation is critical for meeting compliance with standards like NIST SP 800-190 and PCI-DSS that emphasize container isolation and protection.
  description: 'This rule checks that GKE pods are configured to use the Seccomp Runtime Default profile, which restricts system calls to a defined set. To verify, ensure the pod''s securityContext includes ''seccompProfile'' set to ''type: RuntimeDefault''. If not configured, update your Kubernetes pod specifications to include this setting. This can be done by editing the YAML file of the pod or deployment and redeploying it. Enabling this profile provides a baseline level of security against container escape vulnerabilities.'
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_syscalls_with_seccomp
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/tutorials/clusters/seccomp/
- rule_id: gcp.container.pod.service_account_token_mount_configured
  service: container
  resource: pod
  requirement: Service Account Token Mount Configured
  scope: container.pod.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Service Account Token Mount Configuration in Pods
  rationale: Proper configuration of service account token mounts in Kubernetes pods is crucial to prevent unauthorized access to the Kubernetes API, reducing the risk of privilege escalation and data breaches. Misconfigured tokens can expose sensitive information, leading to potential compliance violations with standards like NIST SP 800-53 and PCI-DSS.
  description: This rule checks if Kubernetes pods have their service account token volumes properly configured, ensuring that tokens are not automatically mounted unless explicitly needed. To verify, inspect the pod specifications for the 'automountServiceAccountToken' field and ensure it is set to 'false' for pods that do not require API access. Remediation involves updating the pod configuration to disable automatic token mounting unless required for specific workloads.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#token-volume-projection
  - https://www.cisecurity.org/benchmark/kubernetes
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.rbac.containers_kubernetes_authorization_mode_rbac_enabled
  service: container
  resource: rbac
  requirement: Containers Kubernetes Authorization Mode RBAC Enabled
  scope: container.rbac.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Kubernetes RBAC Authorization Mode is Enabled
  rationale: Enabling RBAC (Role-Based Access Control) in Kubernetes is crucial for managing permissions and access to resources within your cluster. Without RBAC, there is a risk of unauthorized access, which can lead to data breaches, unauthorized resource modifications, and potential downtime. This configuration supports compliance with frameworks like NIST and ISO 27001, which emphasize the importance of access control.
  description: This rule checks if the Kubernetes cluster is configured with RBAC enabled, which is essential for granular access management. To verify, inspect the Kubernetes API server startup parameters to ensure '--authorization-mode' includes 'RBAC'. Remediation involves updating the cluster configuration to include 'RBAC' in the authorization mode settings, ensuring that roles and role bindings are properly defined to control access.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
- rule_id: gcp.container.rbac.containers_kubernetes_no_cluster_admin_bindings_to_users
  service: container
  resource: rbac
  requirement: Containers Kubernetes No Cluster Admin Bindings To Users
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Prevent Cluster Admin Role Bindings to Individual Users
  rationale: Assigning the cluster-admin role to individual users increases the risk of privilege abuse, leading to potential unauthorized access and control over Kubernetes clusters. This practice can result in non-compliance with frameworks like NIST and ISO 27001, as it violates the principle of least privilege and can expose sensitive data to unauthorized users.
  description: This rule checks for any direct bindings of the cluster-admin role to individual users in Kubernetes clusters on GCP. Such bindings should be avoided to prevent excessive privilege allocation. Instead, users should be assigned specific roles that match their required permissions. Remediation involves reviewing the existing role bindings, identifying any users with the cluster-admin role, and modifying their permissions to adhere to least privilege principles.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
- rule_id: gcp.container.rbac.containers_kubernetes_no_wildcard_rules_in_clusterroles
  service: container
  resource: rbac
  requirement: Containers Kubernetes No Wildcard Rules In Clusterroles
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Avoid Wildcard Permissions in Kubernetes ClusterRoles
  rationale: Wildcard permissions in Kubernetes ClusterRoles can lead to unauthorized access to cluster resources, increasing the risk of privilege escalation and data breaches. By restricting permissions to only those necessary for specific tasks, organizations mitigate potential attack vectors and adhere to the principle of least privilege, aligning with compliance mandates such as PCI-DSS and ISO 27001.
  description: This rule checks for the presence of wildcard ('*') permissions in Kubernetes ClusterRoles within GCP. Wildcard permissions can grant excessive access, allowing actions beyond intended use. To verify, inspect ClusterRole configurations in your GCP Kubernetes Engine and ensure permissions are explicitly defined. Remediation involves updating ClusterRoles to specify exact resources and actions required for each role, avoiding the use of '*'.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
- rule_id: gcp.container.rbac.containers_kubernetes_subjects_scoped_to_namespaces
  service: container
  resource: rbac
  requirement: Containers Kubernetes Subjects Scoped To Namespaces
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure Kubernetes RBAC Subjects Are Namespace Scoped
  rationale: Limiting RBAC subjects to specific namespaces minimizes the risk of privilege escalation and unauthorized access to resources across the cluster. This practice helps organizations comply with the principle of least privilege, reducing the attack surface and potential impact of malicious activities. It also aligns with compliance requirements to enforce strict access controls.
  description: This rule checks if Kubernetes RBAC subjects are restricted to specific namespaces rather than having cluster-wide permissions. To verify, review the RBAC RoleBindings and ClusterRoleBindings to ensure roles are assigned within namespaces. Remediation involves auditing existing bindings and adjusting them to use namespace-specific roles and bindings. This ensures that users and services have access only to the necessary resources within their designated namespaces.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/role-based-access-control
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.container.rbac.k8s_default_sa_not_cluster_admin
  service: container
  resource: rbac
  requirement: K8s Default Sa Not Cluster Admin
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Prevent Default K8s Service Account from Cluster Admin Role
  rationale: Allowing the default Kubernetes service account to have cluster-admin role grants it excessive permissions, increasing the risk of privilege escalation and unauthorized access. This can lead to data breaches, unauthorized resource manipulations, and compliance violations with frameworks like NIST and PCI-DSS, thus compromising organizational security posture.
  description: This rule checks if the default Kubernetes service account is assigned the cluster-admin role within GCP Kubernetes Engine clusters. To mitigate security risks, ensure that the default service account does not have cluster-admin privileges by adjusting role bindings through GCP IAM. Review current role assignments and modify them as necessary to adhere to the principle of least privilege.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/iam
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.container.rbac.k8s_no_cluster_admin_to_authenticated
  service: container
  resource: rbac
  requirement: K8s No Cluster Admin To Authenticated
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Restrict Cluster Admin Role for Authenticated Users
  rationale: Assigning the cluster admin role to all authenticated users can lead to unauthorized access and potential compromise of the entire Kubernetes cluster. This poses a significant security risk, allowing users to deploy malicious workloads or exfiltrate sensitive data. Limiting cluster admin privileges is crucial for maintaining a secure and compliant environment, ensuring adherence to best practices and regulatory standards such as NIST and PCI-DSS.
  description: This rule checks that the Kubernetes cluster does not assign the cluster admin role to all authenticated users by reviewing RBAC configurations. Administrators should ensure that only specific, trusted identities are granted cluster admin privileges. To remediate, audit the existing role bindings and adjust permissions by creating specific roles with the least privilege principle in mind and assigning them to designated users or service accounts. Regularly monitor and update permissions to maintain a secure setup.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.container.rbac.k8s_wildcard_verbs_disallowed
  service: container
  resource: rbac
  requirement: K8s Wildcard Verbs Disallowed
  scope: container.rbac.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disallow Wildcard Verbs in Kubernetes RBAC Policies
  rationale: Allowing wildcard verbs such as '*' in Kubernetes RBAC policies can lead to over-permissioning, increasing the risk of unauthorized access and potential exploitation of the Kubernetes environment. This can result in data breaches, service disruptions, and non-compliance with security standards like NIST SP 800-53 and ISO 27001, which require least privilege access controls.
  description: This rule checks for Kubernetes RBAC policies that use wildcard verbs, which allow unrestricted actions on resources. To secure your environment, replace wildcard verbs with specific actions such as 'get', 'list', or 'watch', based on the intended use case. Verify current policies through the GCP Console or CLI, and update any policies using wildcard verbs to ensure compliance with the principle of least privilege.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/73906.html
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.rbac.no_system_anonymous_bindings
  service: container
  resource: rbac
  requirement: No System Anonymous Bindings
  scope: container.rbac.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent System Anonymous Role Bindings in GKE Clusters
  rationale: Allowing system anonymous bindings in Kubernetes Engine can expose your cluster to unauthorized access, leading to potential data breaches or service disruptions. This practice can violate compliance requirements such as PCI-DSS and SOC2, where strict access controls are mandated. Unauthorized users could exploit this access to perform malicious actions, compromising the integrity and availability of your workloads.
  description: This rule checks for the presence of role bindings in Google Kubernetes Engine (GKE) that grant permissions to the 'system:anonymous' user. To verify, review the RoleBindings and ClusterRoleBindings in your GKE clusters for any roles assigned to 'system:anonymous'. To remediate, remove or replace these bindings with specific user or group bindings that adhere to the principle of least privilege. This ensures only authenticated and authorized users have access to sensitive operations within your clusters.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
- rule_id: gcp.container.rbac.nondefault_unauthenticated_bindings_absent
  service: container
  resource: rbac
  requirement: Nondefault Unauthenticated Bindings Absent
  scope: container.rbac.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure No Unauthenticated RBAC Bindings in Kubernetes Clusters
  rationale: Allowing unauthenticated access to Kubernetes clusters can lead to unauthorized users exploiting services, accessing sensitive data, or causing service disruptions. This poses significant security risks and could result in financial loss, reputational damage, and compliance violations with standards like PCI-DSS and HIPAA.
  description: This check ensures that Kubernetes Role-Based Access Control (RBAC) configurations in GCP do not have unauthenticated bindings outside of default settings. Verify that no roles are bound to unauthenticated users by reviewing the RBAC policies in your GKE clusters. Remediation involves auditing and removing any nondefault bindings to the unauthenticated group, ensuring only authenticated and authorized users have access.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.container.service.containers_kubernetes_external_ips_not_used
  service: container
  resource: service
  requirement: Containers Kubernetes External Ips Not Used
  scope: container.service.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Disable External IPs for Kubernetes Services
  rationale: Exposing Kubernetes services with external IPs can lead to unauthorized access and potential data breaches. By restricting external IPs, you minimize the attack surface of your cluster, thus protecting sensitive workloads and meeting compliance with industry standards such as NIST and ISO 27001.
  description: This rule checks if any Kubernetes service within your GCP environment is configured with an external IP. To enhance security, it's recommended to use internal IPs and leverage ingress controllers for controlled external access. Review your Kubernetes service configurations in the GCP Console or via kubectl, and ensure external IPs are not specified. Transition to internal-only IPs to mitigate exposure risks.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps
  - https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/73906.html
  - https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
- rule_id: gcp.container.service.containers_kubernetes_nodeport_usage_restricted
  service: container
  resource: service
  requirement: Containers Kubernetes Nodeport Usage Restricted
  scope: container.service.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Restrict Kubernetes NodePort Service Usage
  rationale: Open NodePorts can expose Kubernetes services to the entire internet, increasing the risk of unauthorized access and potential attacks such as Denial of Service (DoS). Restricting NodePort usage helps mitigate these risks and aligns with best practices for minimizing exposure of internal services, which is crucial for maintaining a secure and compliant cloud environment.
  description: This rule checks if Kubernetes services are configured to use NodePort, which exposes an application on a static port on each node's IP. It is recommended to minimize the use of NodePorts due to their exposure to the internet. Verification involves inspecting service configurations for NodePort usage. Remediation includes using internal or ClusterIP services, or implementing ingress controllers for controlled external access.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps
  - https://cloud.google.com/security-command-center/docs/how-to-remediate-container-vulnerabilities
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.service.containers_kubernetes_type_loadbalancer_internal_on_required
  service: container
  resource: service
  requirement: Containers Kubernetes Type Loadbalancer Internal On Required
  scope: container.service.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Ensure LoadBalancer Services are Internal Only
  rationale: Requiring Kubernetes LoadBalancer services to be internal minimizes exposure to external networks, reducing the attack surface and protecting against unauthorized access. This is crucial for safeguarding sensitive data in compliance with regulations like GDPR and PCI-DSS, and for mitigating risks such as data breaches and DDoS attacks.
  description: This rule checks if Kubernetes LoadBalancer services in GCP are configured as internal-only, which restricts traffic to within the Google Cloud Virtual Private Cloud (VPC). Verify this setting by ensuring the spec.loadBalancerIP field is not specified or the spec.loadBalancerSourceRanges field is set to internal IP ranges. To remediate, update the service manifest to remove any external IP configurations and specify internal-only settings in the GCP Console or via kubectl.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing
  - https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview#external_load_balancing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer
- rule_id: gcp.container.workload.containers_kubernetes_drop_net_raw_and_reduce_caps
  service: container
  resource: workload
  requirement: Containers Kubernetes Drop Net Raw And Reduce Caps
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Least Privilege on Kubernetes Container Capabilities
  rationale: Restricting container capabilities, particularly dropping NET_RAW, minimizes the attack surface by preventing containers from performing potentially harmful network operations. This is crucial for maintaining a secure Kubernetes environment and protecting against privilege escalation attacks, which can lead to unauthorized access and data breaches. Complying with this helps meet security standards and reduces risks associated with misconfigured containers.
  description: This rule ensures that Kubernetes containers have the NET_RAW capability dropped and other unnecessary capabilities minimized. Containers should only be granted the capabilities they need to function, adhering to the principle of least privilege. Verify the security context of your Kubernetes Pod specifications and adjust them to drop NET_RAW and other superfluous capabilities. Remediation involves updating your Pod or Deployment configurations to include a securityContext section that explicitly drops these capabilities.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/pod-security-policies
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/security_and_compliance/using-scc
- rule_id: gcp.container.workload.containers_kubernetes_env_no_plaintext_secrets
  service: container
  resource: workload
  requirement: Containers Kubernetes Env No Plaintext Secrets
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Prevent Plaintext Secrets in Kubernetes Container Environments
  rationale: Storing secrets in plaintext within Kubernetes environments poses significant security risks, such as unauthorized access and data breaches. This practice violates compliance with standards like PCI-DSS and HIPAA, which require robust data protection measures. Ensuring secrets are encrypted protects against insider threats and external attacks, maintaining trust and compliance.
  description: This rule checks for the presence of plaintext secrets in environment variables in Kubernetes containers. Organizations should use Kubernetes Secrets to manage sensitive information securely. Remediation involves auditing container configurations, replacing plaintext secrets with references to encrypted Kubernetes Secrets, and ensuring all deployments adhere to this standard. Regular audits and automated checks can help maintain compliance.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/secret
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://cloud.google.com/architecture/best-practices-for-secret-management
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf
- rule_id: gcp.container.workload.containers_kubernetes_host_network_pid_ipc_disabled
  service: container
  resource: workload
  requirement: Containers Kubernetes Host Network Pid Ipc Disabled
  scope: container.workload.network_security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Disable Host Network, PID, and IPC in Kubernetes Workloads
  rationale: Allowing containers to use the host's network, PID, or IPC namespaces can expose sensitive host information and resources to potentially malicious workloads. This increases the attack surface and the risk of container escape, compromising the entire node. Adhering to this practice supports compliance with security frameworks such as NIST and ensures isolation between containerized applications.
  description: This rule checks whether Kubernetes workloads are configured to use their own network, PID, and IPC namespaces, rather than sharing the host's. To verify, inspect the Pod specifications to ensure 'hostNetwork', 'hostPID', and 'hostIPC' are set to 'false'. Remediation involves updating the Pod definitions to disable host namespace sharing, enhancing workload isolation and security.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_pod_and_container_capabilities
  - https://cloud.google.com/architecture/best-practices-for-running-containers
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.container.workload.containers_kubernetes_image_tag_not_latest
  service: container
  resource: workload
  requirement: Containers Kubernetes Image Tag Not Latest
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Avoid Using 'latest' Tag for Kubernetes Container Images
  rationale: Using the 'latest' tag for container images can lead to unpredictable deployments, as it may pull different versions over time. This increases the risk of introducing vulnerabilities and reduces the ability to audit or roll back changes effectively. It can also lead to non-compliance with standards requiring controlled software deployment processes.
  description: This rule checks whether Kubernetes workloads are configured with container image tags set to 'latest'. Instead, specify a specific version tag to ensure consistent and predictable deployments. Verify by reviewing Kubernetes manifests or deployment configurations for image tags. To remediate, update the container image tag in your Kubernetes configuration to a specific version number and redeploy the workload.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/container-image-security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/containers/images/#updating-images
- rule_id: gcp.container.workload.containers_kubernetes_no_hostpath_mounts
  service: container
  resource: workload
  requirement: Containers Kubernetes No Hostpath Mounts
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Avoid HostPath Mounts in Kubernetes Workloads
  rationale: HostPath mounts can expose sensitive parts of the host filesystem to containers, increasing the risk of privilege escalation and data leakage. This may lead to unauthorized access, potential data breaches, and non-compliance with security standards such as NIST and PCI-DSS which mandate the protection of sensitive data and system integrity.
  description: This rule checks Kubernetes workloads for the use of HostPath mounts, which should be avoided to prevent exposing the host's filesystem to containers. Verify workloads by scanning their configurations and removing any HostPath volumes. Instead, use persistent storage solutions like PersistentVolumeClaims (PVCs) to safely manage storage. Remediation involves updating workload configurations to eliminate HostPath usage and implementing secure storage practices.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-123.pdf
  - https://cloud.google.com/architecture/security-foundations/compliance
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.workload.containers_kubernetes_no_privileged_containers
  service: container
  resource: workload
  requirement: Containers Kubernetes No Privileged Containers
  scope: container.workload.least_privilege
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: high
  title: Ensure No Privileged Containers in Kubernetes Workloads
  rationale: Running containers in privileged mode grants them elevated permissions, potentially allowing attackers to gain root access to the host node, leading to data breaches or infrastructure compromise. This can result in severe business impacts, including financial losses, reputational damage, and non-compliance with regulations like PCI-DSS and SOC2.
  description: This rule checks Kubernetes workloads to ensure that no containers are running with privileged access. Privileged containers can bypass security controls and have unrestricted access to the host system. To verify, inspect the security context configurations in your Kubernetes deployment YAML files and ensure the 'privileged' flag is set to false. Remediate by updating configurations to disable privileged mode and redeploy the workloads.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-190.pdf
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://cloud.google.com/security/compliance
- rule_id: gcp.container.workload.containers_kubernetes_read_only_root_filesystem
  service: container
  resource: workload
  requirement: Containers Kubernetes Read Only Root Filesystem
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Ensure Kubernetes Containers Have Read-Only Root Filesystem
  rationale: Enforcing a read-only root filesystem for Kubernetes containers minimizes the risk of unauthorized changes to the container's filesystem, which can prevent persistent malware and limit the impact of attacks. This control is crucial for maintaining the integrity of applications running in containers and aligns with compliance frameworks that emphasize system integrity and data protection.
  description: This rule checks that Kubernetes containers in your GCP environment are configured with a read-only root filesystem. A read-only root filesystem helps mitigate the risk of filesystem tampering by attackers, thus reducing the attack surface. To verify compliance, ensure that the 'readOnlyRootFilesystem' attribute is set to 'true' in the container's security context. Remediation involves updating your Kubernetes deployment configuration to enforce this setting across all applicable containers.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#read-only-root-filesystem
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
- rule_id: gcp.container.workload.containers_kubernetes_run_as_non_root
  service: container
  resource: workload
  requirement: Containers Kubernetes Run As Non Root
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: critical
  title: Ensure Kubernetes Containers Run as Non-Root User
  rationale: Running containers as a non-root user reduces the risk of privilege escalation within the container, thereby mitigating potential impacts from vulnerabilities or malicious attacks. This practice is crucial for maintaining a secure environment, aligning with compliance requirements such as PCI-DSS and ISO 27001, which demand strict control over user permissions.
  description: This rule verifies that Kubernetes workloads are configured to run containers as non-root users. The check ensures the 'runAsNonRoot' field in the security context of a PodSpec is set to true. To remediate, update your Kubernetes deployment configurations to include a security context that specifies a non-root user ID, ensuring that no container within the workload can execute processes as the root user.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#prevent_root
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-190/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups
- rule_id: gcp.container.workload.containers_kubernetes_seccomp_profile_runtime_default
  service: container
  resource: workload
  requirement: Containers Kubernetes Seccomp Profile Runtime Default
  scope: container.workload.security
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: low
  title: Enforce Seccomp Profile Runtime Default in Kubernetes Containers
  rationale: Using the default seccomp profile in Kubernetes containers enhances security by minimizing the attack surface and mitigating potential exploitation of kernel vulnerabilities. It protects against unauthorized system calls that could lead to privilege escalation or container escape, thereby safeguarding sensitive workloads and maintaining compliance with security standards.
  description: 'This rule checks if Kubernetes containers in GCP workloads are using the seccomp profile set to ''RuntimeDefault'', which restricts system call capabilities to a safe subset. To verify, ensure that your pod specifications include ''securityContext.seccompProfile.type: RuntimeDefault''. If not configured, update your Kubernetes deployment manifests to include this setting. This practice helps in adhering to least privilege principles and enhances container security posture.'
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/seccomp
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf
  - https://kubernetes.io/docs/tutorials/clusters/seccomp/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.container.workload.secrets_as_env_var_detection
  service: container
  resource: workload
  requirement: Secrets As Env Var Detection
  scope: container.workload.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: high
  title: Detect and Mitigate Secrets in Env Vars for GKE Workloads
  rationale: Storing secrets such as API keys or passwords in environment variables can expose sensitive information if the pod is compromised, leading to unauthorized access and potential data breaches. This practice violates compliance requirements such as PCI-DSS and HIPAA, which mandate strict controls over sensitive data handling.
  description: This rule checks if Kubernetes workloads in Google Kubernetes Engine (GKE) clusters are configured with secrets exposed as environment variables, which is a risky practice. To verify, inspect the workload configurations for any environment variable that contains sensitive information. Remediation involves using Kubernetes Secrets to store and manage sensitive data securely, and referencing them in workloads without directly exposing them as environment variables.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/concepts/secrets
  - 'CIS GCP Benchmark: 5.4.1 Ensure that Secrets are not stored in the pod specification'
  - https://www.pcisecuritystandards.org/pci_security/maintaining_payment_security
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://kubernetes.io/docs/concepts/configuration/secret/
- rule_id: gcp.datacatalog.entry.data_catalog_crawler_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Crawler Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Crawler Logs for Enhanced Monitoring
  rationale: Enabling Data Catalog Crawler Logs is crucial for monitoring and auditing data access patterns, which helps in identifying unauthorized access and potential data breaches. It aids in compliance with regulations such as GDPR, HIPAA, and SOC 2 by maintaining a detailed audit trail. This logging capability supports proactive security measures by providing insights into data management activities.
  description: This rule checks if logging is enabled for Data Catalog crawlers to monitor their activity. Specifically, it ensures that crawler logs are captured and stored in Google Cloud Logging. To verify, navigate to the Data Catalog service in the Google Cloud Console and ensure logging is configured for crawler activities. Remediation involves setting up logging by configuring the appropriate IAM roles and enabling logging in the Data Catalog settings to capture all crawler-related activities.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/logging
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_crawler_network_private_only
  service: datacatalog
  resource: entry
  requirement: Data Catalog Crawler Network Private Only
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Data Catalog Crawler Operates in Private Networks
  rationale: Restricting Data Catalog crawlers to private networks minimizes exposure to unauthorized internet access, reducing the risk of data breaches and ensuring compliance with regulatory frameworks like GDPR and HIPAA. This control helps prevent unauthorized data scraping and potential exfiltration, safeguarding sensitive data assets.
  description: This rule verifies that Data Catalog crawlers operate solely within private networks, ensuring that data discovery processes do not interact with the public internet. To enforce this, configure Data Catalog entries to use private IP addresses and limit access through VPC Service Controls. Remediation involves adjusting network configurations to ensure private connectivity and reviewing IAM policies to enforce network restrictions.
  references:
  - https://cloud.google.com/data-catalog/docs/networking
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/publications/nist-special-publication-800-53-revision-5-security-and-privacy-controls-information
  - https://cloud.google.com/vpc-service-controls/docs
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.datacatalog.entry.data_catalog_crawler_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Crawler Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Data Catalog Crawler Role Enforces Least Privilege
  rationale: Enforcing least privilege for Data Catalog crawler roles minimizes the risk of unauthorized access and data exposure. Misconfigured roles can lead to privilege escalation, data breaches, and non-compliance with regulatory standards such as GDPR and HIPAA. By limiting permissions, organizations can reduce their attack surface and improve their security posture.
  description: This rule checks that the Data Catalog crawler role is configured to follow the principle of least privilege by granting only necessary permissions. Verify that roles assigned to crawlers do not exceed the minimum required permissions for their tasks. To remediate, audit the IAM policies associated with the Data Catalog and adjust permissions accordingly, ensuring compliance with least privilege principles.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/manage-iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_crawler_scope_restricted_to_allowlist
  service: datacatalog
  resource: entry
  requirement: Data Catalog Crawler Scope Restricted To Allowlist
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Data Catalog Crawler Scope to Allowlist
  rationale: Limiting the Data Catalog Crawler's scope to a predefined allowlist helps prevent unauthorized access and data leaks by ensuring that only specified data sources are indexed and managed. This controls access, minimizes exposure to sensitive information, and supports compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule verifies that the Data Catalog Crawler's scope is restricted to an allowlist of approved data sources. To ensure compliance, regularly review and update the allowlist to include only necessary and authorized data sets. Remediation involves configuring the Data Catalog settings to specify an allowlist, which can be managed through the Google Cloud Console or via the gcloud command-line tool.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/catalog-entry
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/data-catalog/docs/security
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.datacatalog.entry.data_catalog_credentials_in_secrets_manager
  service: datacatalog
  resource: entry
  requirement: Data Catalog Credentials In Secrets Manager
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: high
  title: Ensure Data Catalog Credentials are Stored in Secrets Manager
  rationale: Storing Data Catalog credentials in Secrets Manager prevents unauthorized access and reduces the risk of credential leakage. This practice aligns with security best practices and compliance requirements, protecting sensitive data from being exposed through insecure storage methods, thus mitigating potential data breaches and ensuring regulatory compliance.
  description: This rule checks if Data Catalog credentials are securely stored in Google Cloud Secrets Manager. Ensure that all credentials related to Data Catalog entries are not hardcoded or stored in plaintext but are instead managed by Secrets Manager. To verify, audit the storage locations of your Data Catalog credentials and migrate any found outside of Secrets Manager. Remediation involves using the Secrets Manager API or Console to create secret entries for these credentials and updating your applications to fetch them securely.
  references:
  - https://cloud.google.com/secret-manager/docs
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 4.1
  - 'NIST SP 800-53 Rev 5: AC-2, AC-3, AC-6'
  - https://cloud.google.com/datacatalog/docs/how-to
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_database_cross_account_sharing_restricted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Database Cross Account Sharing Restricted
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Data Catalog Database Cross Account Sharing
  rationale: Restricting cross-account sharing of Data Catalog entries minimizes unauthorized access risks, protects sensitive data from exposure, and helps maintain data sovereignty. It is crucial to control who can access and share data across accounts to prevent data breaches and comply with regulatory requirements such as GDPR and CCPA.
  description: This rule checks if there are policies allowing cross-account sharing of Data Catalog database entries. To verify, review IAM policies and ensure that permissions for sharing entries are limited to trusted accounts only. Remediation involves adjusting IAM policies to restrict sharing permissions and applying least privilege principles to Data Catalog entries.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/policies
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_database_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Database Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of Data Catalog Database Entries
  rationale: Encrypting Data Catalog entries protects sensitive metadata from unauthorized access, reducing the risk of data breaches. This is crucial for maintaining customer trust and adhering to regulatory requirements such as GDPR and CCPA, which mandate the protection of personal data. Encryption at rest ensures that even if physical drives are compromised, the data remains protected.
  description: This rule checks whether entries in the Data Catalog have encryption enabled. It requires configuring the Data Catalog to use Google-managed or customer-managed encryption keys (CMEK) to encrypt metadata at rest. To verify, inspect the encryption settings of Data Catalog entries in the GCP Console or via the gcloud command line. Remediation involves enabling encryption on existing entries or ensuring new entries are configured with encryption from the start.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_database_policy_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Database Policy Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Database Policies
  rationale: Implementing least privilege in Data Catalog helps minimize potential unauthorized access to sensitive data, thereby reducing the risk of data breaches and ensuring compliance with regulatory requirements like GDPR and CCPA. Misconfigured permissions can lead to data exposure, which could result in significant financial and reputational damage.
  description: This rule checks that policies applied to Data Catalog entries grant only the necessary permissions required for users to perform their job functions. It involves verifying IAM roles assigned to users and service accounts against the principle of least privilege. To remediate, audit existing IAM policies and adjust permissions to ensure they are strictly necessary, using predefined roles where possible. Regularly review and update policies to adapt to changes in user roles or organizational requirements.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.datacatalog.entry.data_catalog_dev_endpoint_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Dev Endpoint Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dev Endpoint Encryption in Data Catalog
  rationale: Encrypting data at rest in the Data Catalog is crucial to prevent unauthorized access and maintain data confidentiality, integrity, and compliance with standards such as NIST and ISO 27001. Unencrypted endpoints can lead to data breaches, resulting in financial loss, reputational damage, and legal consequences.
  description: This rule checks if the development endpoints in GCP Data Catalog entries are encrypted at rest. Encryption is achieved using Cloud KMS keys to protect sensitive metadata. To verify, ensure that all entries in the Data Catalog have encryption configured using customer-managed keys. Remediation involves setting up Cloud KMS and configuring the Data Catalog to use these keys for encryption.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypt-data
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datacatalog.entry.data_catalog_dev_endpoint_no_public_access
  service: datacatalog
  resource: entry
  requirement: Data Catalog Dev Endpoint No Public Access
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Restrict Public Access to Data Catalog Dev Endpoints
  rationale: Allowing public access to Data Catalog development endpoints can expose sensitive metadata and configuration details to unauthorized users, leading to potential data breaches and compliance violations. By restricting public access, organizations can reduce attack surfaces and meet regulatory requirements such as GDPR, HIPAA, and PCI-DSS that mandate controlled access to sensitive data.
  description: This rule checks for public access configurations on Data Catalog development endpoints to ensure they are not exposed to the internet. Verify that access policies for Data Catalog entries are set to deny public access by default. Remediation involves reviewing IAM roles and permissions to ensure that only authorized users within the organization can access these endpoints. Implement firewall rules and network policies to further enforce restricted access.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc/docs/firewalls
- rule_id: gcp.datacatalog.entry.data_catalog_dev_endpoint_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Dev Endpoint Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Dev Endpoint Role
  rationale: Enforcing least privilege for Data Catalog roles minimizes the risk of unauthorized access and potential data breaches. Over-privileged roles can lead to accidental or malicious data exposure, violating compliance standards such as GDPR and CCPA, and could result in significant financial and reputational damage.
  description: This rule checks that the Data Catalog Dev Endpoint role has permissions limited to only those necessary for development activities. Verify role assignments to ensure they do not include unnecessary permissions such as 'roles/datacatalog.admin'. Remediation involves auditing current roles and removing any excessive permissions, ensuring they align with the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/manage-iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry.data_catalog_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Entries are Encrypted at Rest
  rationale: Encrypting Data Catalog entries at rest is crucial for protecting sensitive metadata against unauthorized access and potential data breaches. This measure reduces the risk of data exposure, aligns with industry best practices, and helps meet compliance standards such as GDPR, HIPAA, and ISO 27001 that mandate encryption of sensitive data.
  description: This rule checks whether your Data Catalog entries are configured to be encrypted at rest using Google-managed encryption keys (GMEK) or customer-managed encryption keys (CMEK). To verify, ensure that encryption settings are enabled for all entries within the Data Catalog. Remediation involves configuring the Data Catalog service to use GMEK by default or setting up CMEK through Cloud KMS for enhanced control over encryption keys.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypting-entries
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.datacatalog.entry.data_catalog_job_logs_and_metrics_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Job Logs And Metrics Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Data Catalog Job Logs and Metrics are Enabled
  rationale: Enabling logs and metrics for Data Catalog jobs is crucial for monitoring data access and modifications, which helps identify and mitigate potential data breaches. It supports incident response and forensic investigations by providing a trail of activities. Additionally, it aids in meeting compliance requirements like GDPR and CCPA by ensuring accountability and transparency in data operations.
  description: This rule verifies that logging and metrics are enabled for Google Cloud Data Catalog entries, ensuring that all access and changes to data are tracked. To check this, confirm that audit logs are configured for Data Catalog within the Google Cloud Console under Logging > Logs Router. Remediation involves setting up appropriate sinks and enabling audit logging for 'DATA_READ', 'DATA_WRITE', and 'ADMIN_READ' activities for better visibility and control.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_job_network_private_only
  service: datacatalog
  resource: entry
  requirement: Data Catalog Job Network Private Only
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Network for Data Catalog Jobs
  rationale: Restricting Data Catalog jobs to private networks minimizes exposure to unauthorized access, reducing the risk of data breaches. This control is crucial for maintaining data integrity and confidentiality, especially when dealing with sensitive information. It aligns with regulatory requirements for protecting data in transit and supports compliance with frameworks like ISO 27001 and NIST.
  description: This check ensures that Data Catalog jobs are configured to run exclusively within private networks, preventing exposure to public internet threats. Verify that the service account executing Data Catalog jobs has network restrictions applied to ensure the jobs communicate only over private IPs. Remediation involves setting network tags and firewall rules to permit job execution solely on private networks, ensuring secure data handling.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/private-networks
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/architecture/best-practices-vpc-design
- rule_id: gcp.datacatalog.entry.data_catalog_job_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Job Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Job Roles
  rationale: Ensuring least privilege in Data Catalog roles minimizes the attack surface by restricting access to only necessary resources. This reduces potential damage from compromised accounts and aligns with compliance requirements such as GDPR and SOC 2, which mandate access controls and data protection. Implementing least privilege helps prevent unauthorized data exposure and promotes accountability by ensuring users operate within their defined boundaries.
  description: This rule checks for adherence to the principle of least privilege in Data Catalog job roles. It ensures that job roles assigned to users or service accounts only provide the minimum permissions necessary to perform their tasks. To verify, review IAM policies for Data Catalog entries and adjust them to remove unnecessary permissions. Remediation involves auditing current role assignments, identifying excessive permissions, and updating roles to align with least privilege principles.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/document-171
  - https://cloud.google.com/security/compliance/soc-2/
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_job_script_location_private_and_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Job Script Location Private And Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Job Script Locations Are Secure and Encrypted
  rationale: Securing and encrypting job script locations in Google Cloud's Data Catalog ensures that sensitive data scripts are protected from unauthorized access and potential data breaches. This is crucial for maintaining data confidentiality and integrity, especially when handling regulated data, and helps organizations comply with data protection laws such as GDPR and CCPA.
  description: This rule checks that all job script locations in GCP's Data Catalog are set to be private and encrypted. Scripts should be stored in Cloud Storage buckets configured with Bucket Policy Only, ensuring that only authorized users can access them. Encryption at rest should be enabled using Google-managed or customer-managed encryption keys (CMEK). Verification involves reviewing bucket IAM policies and encryption settings. Remediation includes updating bucket policies to restrict access and enabling CMEK as needed.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypt-data
  - https://cloud.google.com/security/encryption-at-rest/default-encryption/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/storage/docs/access-control/iam
- rule_id: gcp.datacatalog.entry.data_catalog_ml_transform_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Ml Transform Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable ML Transform Logs in Data Catalog
  rationale: Enabling logging for ML transforms in Data Catalog is vital for tracking access and changes to machine learning-related metadata, which can help prevent unauthorized access and modifications. It assists in forensic analysis and compliance with regulations such as GDPR and HIPAA, which mandate detailed logging of data access and transformations.
  description: This rule checks if logging is enabled for ML transforms within Google Cloud's Data Catalog service. To verify, ensure that audit logs for ML transformations are active by configuring the appropriate log sinks and filters in the Cloud Logging service. Remediation involves setting up or correcting log configuration to capture ML transformation activities, thereby ensuring that all relevant access and changes are recorded.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/logging/docs/audit
  - CIS GCP Foundation Benchmark v1.3.0, Section 7.3
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_ml_transform_network_private_only
  service: datacatalog
  resource: entry
  requirement: Data Catalog Ml Transform Network Private Only
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Data Catalog ML Transform Uses Private Network
  rationale: Enforcing private network usage for Data Catalog ML Transform reduces the risk of unauthorized access and data exfiltration by restricting access to internal networks only. This is crucial for maintaining data integrity and confidentiality, especially when handling sensitive data subject to regulatory compliance such as GDPR or HIPAA.
  description: This rule checks that all Data Catalog ML Transform operations are conducted over private networks. To verify, ensure that the Data Catalog service is configured to use VPC Service Controls and restricts public network access. Remediation involves updating network configurations to ensure that the service endpoints are only accessible within a private network, leveraging VPC peering or private service access.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/vpc-service-controls/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security
- rule_id: gcp.datacatalog.entry.data_catalog_ml_transform_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Ml Transform Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog ML Transform Role
  rationale: Implementing least privilege for Data Catalog roles mitigates the risk of unauthorized access and potential data breaches, which can lead to data loss, compliance violations, and reputational damage. Ensuring that ML Transform roles have only necessary permissions protects sensitive data and aligns with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks that the ML Transform role in Data Catalog is configured with the minimum permissions necessary for its operation. Verify that the role does not have excess permissions by reviewing IAM policies attached to it. Remediation involves adjusting IAM policies to ensure only required permissions are granted, removing any superfluous access that does not align with explicit job functions.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_partition_access_policies_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Partition Access Policies Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Partition Access
  rationale: Minimizing access to Data Catalog entries reduces the risk of unauthorized data exposure and potential misuse. Implementing least privilege principles helps protect sensitive data, maintain compliance with regulatory standards, and mitigate the impact of insider threats or compromised credentials.
  description: This rule checks for Data Catalog entries to ensure that access policies are configured with the principle of least privilege. It verifies that only necessary permissions are granted to users, groups, or service accounts for data partitions. To remediate, review and adjust IAM policies to align with the minimum access required for each role, removing unnecessary permissions and ensuring proper logging for audit purposes.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-manage-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_partition_catalog_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Partition Catalog Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Partition Catalog is Encrypted at Rest
  rationale: Encrypting data at rest in Data Catalog is crucial for protecting sensitive information from unauthorized access and data breaches. Unencrypted data can be vulnerable to theft or exposure, posing compliance risks with standards like GDPR, HIPAA, and PCI-DSS. Encryption at rest helps mitigate these risks by ensuring that data remains secure even if the storage media is compromised.
  description: This rule checks whether Data Catalog entries, specifically partition catalogs, are encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that encryption settings are configured in the Data Catalog API or via the Google Cloud Console. Remediation involves enabling encryption on existing entries and ensuring new entries are configured with encryption by default.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/encryption-at-rest/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_private_networking_enforced
  service: datacatalog
  resource: entry
  requirement: Data Catalog Private Networking Enforced
  scope: datacatalog.entry.logging
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for Data Catalog Entries
  rationale: Enforcing private networking for Data Catalog entries mitigates the risk of unauthorized access and data exposure by restricting access to a secure, internal network. This is crucial for protecting sensitive metadata managed within GCP's Data Catalog, ensuring that only authenticated internal traffic can access it. By aligning with best practices for network segmentation, organizations can reduce the attack surface and comply with security standards such as PCI-DSS and ISO 27001.
  description: This rule checks if private networking is enforced for Data Catalog entries, ensuring they are only accessible from internal IPs. To verify, inspect the network settings of your Data Catalog to confirm that private IPs are used and that public access is disabled. Remediation involves configuring VPC Service Controls and setting up private Google access, ensuring all access requests come from within the defined network perimeter.
  references:
  - https://cloud.google.com/data-catalog/docs/networking
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/vpc-service-controls/docs/overview
- rule_id: gcp.datacatalog.entry.data_catalog_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog RBAC Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Entry Access
  rationale: Implementing least privilege access in Data Catalog is critical to minimize the risk of unauthorized data exposure and potential data breaches. By restricting permissions to only what is necessary, organizations can reduce the attack surface and comply with regulatory requirements like GDPR and CCPA, which mandate stringent data access controls.
  description: This rule checks if the Data Catalog entry permissions adhere to the principle of least privilege. It verifies that roles assigned to users or service accounts are limited to essential access only. To remediate, review current IAM policies on Data Catalog entries and adjust permissions to ensure users have only the minimum necessary access. Audit logs should be enabled to monitor any unauthorized access attempts.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
- rule_id: gcp.datacatalog.entry.data_catalog_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Roles
  rationale: Implementing least privilege in Data Catalog roles minimizes the risk of unauthorized data access and potential data breaches. By restricting permissions to only what is necessary, organizations can reduce their attack surface and comply with regulatory standards such as GDPR and CCPA, which emphasize data protection and privacy.
  description: This rule checks that roles assigned to Data Catalog entries adhere to the principle of least privilege by ensuring that users and service accounts have only the permissions necessary for their tasks. To verify compliance, review IAM policies for Data Catalog entries and adjust role assignments to align with least privilege principles. Remediation involves auditing current role configurations, identifying excessive permissions, and reassigning roles with reduced privileges while ensuring necessary operations are not hindered.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/datacatalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.datacatalog.entry.data_catalog_table_access_policies_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Table Access Policies Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege on Data Catalog Access Policies
  rationale: Implementing least privilege on Data Catalog table access ensures that users and services have only the permissions necessary to perform their tasks, reducing the risk of data breaches and unauthorized access. Failure to apply this principle can lead to excessive permissions, increasing the attack surface and potentially violating compliance frameworks such as GDPR, HIPAA, and NIST SP 800-53.
  description: This rule verifies that access policies on Data Catalog entries are configured following the principle of least privilege. It checks for overly permissive roles or any misconfigurations that grant unnecessary access. To remediate, audit permissions regularly and use predefined roles that align with job functions. Remove any roles that exceed the necessary permissions and implement strict access reviews.
  references:
  - https://cloud.google.com/datacatalog/docs/how-to/granting-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices-for-enterprise-organizations
- rule_id: gcp.datacatalog.entry.data_catalog_table_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Table Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Table Entries are Encrypted at Rest
  rationale: Encrypting Data Catalog table entries at rest is crucial to protect sensitive information from unauthorized access and potential breaches. This measure reduces the risk of data exposure in case of a security incident and helps organizations comply with regulatory requirements such as GDPR and CCPA, which mandate the protection of personal data. Encryption at rest also safeguards intellectual property and maintains customer trust.
  description: This rule checks whether Data Catalog table entries are encrypted at rest using Google-managed encryption keys or customer-managed encryption keys (CMEK). To verify, examine the encryption settings of your entries in the Google Cloud Console under the Data Catalog service. If entries are not encrypted, enable encryption by configuring CMEK or relying on default Google-managed encryption. Remediation involves updating the Data Catalog configuration to ensure all entries are encrypted at rest.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.datacatalog.entry.data_catalog_table_public_sharing_disabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Table Public Sharing Disabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Prevent Public Sharing of Data Catalog Tables
  rationale: Disabling public sharing of Data Catalog tables is crucial to prevent unauthorized access to sensitive information, which could lead to data breaches and non-compliance with regulatory standards. Publicly accessible tables can expose critical business and customer data, increasing the risk of exploitation by malicious actors. Compliance with frameworks such as NIST, PCI-DSS, and ISO 27001 often requires strict access controls to protect data integrity and confidentiality.
  description: This rule checks for any Data Catalog tables that are publicly accessible and ensures public sharing is disabled. Verify that no tables have IAM policies granting 'allUsers' or 'allAuthenticatedUsers' roles. Remediation involves reviewing the IAM policies of each Data Catalog table and removing any permissions that allow public access. Use the Google Cloud Console or gcloud CLI to audit and adjust permissions as necessary.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://cloud.google.com/iam/docs/overview
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.data_catalog_tls_required
  service: datacatalog
  resource: entry
  requirement: Data Catalog TLS Required
  scope: datacatalog.entry.logging
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS is Required for Data Catalog Entries
  rationale: Requiring TLS for Data Catalog entries ensures that data in transit is encrypted, protecting sensitive metadata from interception and unauthorized access. This is crucial for maintaining data integrity and confidentiality, reducing the risk of data breaches, and complying with regulations such as GDPR and HIPAA that mandate encryption of data in transit.
  description: This rule checks if TLS is required for all interactions with Data Catalog entries to ensure secure communication. To verify, inspect the network configuration settings of your Data Catalog service to confirm that TLS is enforced. Remediation involves configuring your service to require TLS by enabling endpoints that support HTTPS and disabling any non-encrypted endpoints. This ensures that all data transmissions are encrypted.
  references:
  - https://cloud.google.com/data-catalog/docs/security
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.datacatalog.entry.data_catalog_trigger_event_sources_restricted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Trigger Event Sources Restricted
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Event Sources for Data Catalog Triggers
  rationale: Restricting event sources for Data Catalog triggers is crucial to prevent unauthorized data access and manipulation, which could lead to data breaches and non-compliance with regulations like GDPR and HIPAA. Unrestricted event sources increase the risk of accidental or malicious data exposure, impacting business operations and damaging trust.
  description: This rule checks that only approved event sources are allowed to trigger actions within the Google Cloud Data Catalog. Ensure that event source restrictions are configured by setting appropriate IAM roles and permissions, limiting who can create and modify these triggers. Remediation involves auditing existing triggers, removing non-essential sources, and implementing least privilege access for those that remain.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/triggers
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS GCP Benchmark v1.3.0 - 6.2
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_trigger_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Trigger Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Entry Trigger Logs
  rationale: Enabling trigger logs for Data Catalog entries is crucial for tracking changes and access patterns, which helps in identifying unauthorized access or modifications. This enhances data governance and compliance with regulatory requirements such as GDPR and CCPA, reducing the risk of data breaches and ensuring accountability.
  description: This rule checks whether audit logging is enabled for Data Catalog entries, ensuring all interactions are logged. To verify, ensure the 'dataAccess' audit logs are enabled in the Cloud Audit Logs for the Data Catalog service. Remediation involves navigating to the IAM & Admin section of the GCP Console, accessing the Audit Logs settings, and enabling 'dataAccess' logs for Data Catalog.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logs
  - https://cloud.google.com/logging/docs/audit/configure-data-access#config-console
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.data_catalog_trigger_targets_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Trigger Targets Least Privilege
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: high
  title: Ensure Data Catalog Trigger Targets Use Least Privilege
  rationale: Applying the principle of least privilege to Data Catalog triggers minimizes the risk of unauthorized access and potential data breaches. This approach is crucial for protecting sensitive information and avoiding compliance violations with standards such as PCI-DSS and HIPAA, which mandate strict access controls to maintain data integrity and confidentiality.
  description: This rule checks if Data Catalog triggers have permissions exceeding their functional requirements. Specifically, it examines IAM roles assigned to triggers and ensures they only have the minimum necessary permissions. To verify, review IAM policies associated with Data Catalog entries and adjust roles to align with least privilege principles. Remediation involves auditing roles and permissions, and restricting access to essential operations only.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.datacatalog.entry.data_catalog_version_immutability_enforced
  service: datacatalog
  resource: entry
  requirement: Data Catalog Version Immutability Enforced
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Immutability of Data Catalog Entry Versions
  rationale: Enforcing immutability on Data Catalog entries ensures that once data is cataloged, it cannot be altered, mitigating risks of data tampering and unauthorized changes. This enhances the integrity and reliability of metadata, which is crucial for compliance with regulations like GDPR and CCPA, and supports audit trails for forensic analysis.
  description: This rule checks if Data Catalog entries have version immutability enforced, ensuring that once a version is created, it cannot be modified or deleted. To verify, review the Data Catalog settings within the GCP Console to confirm that immutability policies are configured. Remediation involves setting up a policy that enforces version immutability, which can be done through the IAM policy bindings or using gcloud CLI commands to lock entry versions.
  references:
  - https://cloud.google.com/data-catalog/docs/
  - https://cloud.google.com/security/compliance/gdpr/
  - https://cloud.google.com/security/compliance/ccpa/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.datacatalog.entry.data_catalog_workflow_cross_account_trusts_restricted
  service: datacatalog
  resource: entry
  requirement: Data Catalog Workflow Cross Account Trusts Restricted
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Cross Account Trusts in Data Catalog Workflows
  rationale: Cross-account trusts in Data Catalog workflows can lead to unauthorized access and data leakage if not properly managed. Restricting these trusts reduces the risk of data exposure and ensures that data processing workflows are only accessed by authorized accounts, aligning with compliance obligations and minimizing potential attack vectors.
  description: This rule checks for configurations in Data Catalog entries where cross-account trusts are allowed in workflows. Ensure that only necessary accounts have trust relationships and that all accounts involved are vetted and monitored. To verify, review the IAM policies associated with Data Catalog entries and ensure that cross-account permissions are minimized. Remediation involves editing IAM policies to limit trust relationships and implementing audit logging to monitor access patterns.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.datacatalog.entry.data_catalog_workflow_execution_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Catalog Workflow Execution Role Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Workflow Roles
  rationale: Configuring least privilege for workflow execution roles in Data Catalog reduces the risk of unauthorized access and potential data breaches. Overly permissive roles can lead to accidental or malicious data exposure, impacting compliance with regulations like GDPR and CCPA, and causing financial and reputational damage.
  description: This rule checks if Data Catalog workflow execution roles are configured with excessive privileges. It ensures roles only have permissions necessary for their intended tasks, adhering to the principle of least privilege. To verify, review IAM policies associated with Data Catalog entries and adjust them to minimize permissions. Remediation involves auditing current roles, identifying excessive permissions, and refining roles to align with operational requirements while maintaining security.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/workflows
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.datacatalog.entry.data_catalog_workflow_kms_encryption_enabled
  service: datacatalog
  resource: entry
  requirement: Data Catalog Workflow KMS Encryption Enabled
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Data Catalog Workflows
  rationale: Enabling KMS encryption for Data Catalog workflows is crucial to protect sensitive data and metadata from unauthorized access and potential data breaches. This measure helps mitigate the risk of data exposure and supports compliance with data protection regulations such as GDPR and HIPAA, which require robust encryption for stored data.
  description: This rule verifies that Google Cloud's Data Catalog workflows use Customer-Managed Encryption Keys (CMEK) to encrypt data at rest. To enable this, configure your Data Catalog entries to utilize a KMS key from Google Cloud Key Management Service (KMS). Verify the encryption settings in the Google Cloud console under the Data Catalog section and ensure that the 'Encryption' field indicates a KMS key. If not, update the configuration to attach a KMS key to the Data Catalog entry.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/overview
- rule_id: gcp.datacatalog.entry.data_governance_access_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Data Governance Access RBAC Least Privilege
  scope: datacatalog.entry.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Entry Access
  rationale: Implementing least privilege access for Data Catalog entries minimizes the risk of unauthorized data exposure and helps prevent potential data breaches. This is crucial for protecting sensitive information and maintaining compliance with data protection regulations such as GDPR and CCPA. A least privilege model limits access to only what is necessary for users to perform their job functions, reducing the attack surface and potential for insider threats.
  description: This rule checks that IAM roles and permissions granted to users for Data Catalog entries adhere to the principle of least privilege. It ensures that users have access only to the necessary resources and operations required for their role. To verify compliance, review IAM policies on Data Catalog entries and remove any excessive permissions. Remediation may involve adjusting IAM roles, customizing policies, or using predefined roles that align with job responsibilities.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.datacatalog.entry.data_governance_capture_enabled
  service: datacatalog
  resource: entry
  requirement: Data Governance Capture Enabled
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Governance Capture is Enabled for Data Catalog Entries
  rationale: Enabling Data Governance Capture in GCP Data Catalog entries ensures that sensitive data is adequately monitored and protected through lifecycle management. This is crucial for mitigating risks such as unauthorized access and data breaches, while also fulfilling compliance requirements for data privacy regulations like GDPR and CCPA.
  description: This check verifies that Data Governance Capture is activated for entries within the GCP Data Catalog. This setting helps in maintaining audit trails and managing data access policies effectively. To verify, ensure the Data Catalog service is configured to log access and changes to entries. Remediation involves enabling logging and governance features in the Data Catalog settings to track data usage and modifications comprehensively.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/privacy/gdpr
- rule_id: gcp.datacatalog.entry.data_governance_export_destination_encrypted
  service: datacatalog
  resource: entry
  requirement: Data Governance Export Destination Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Governance Export Destinations are Encrypted
  rationale: Encrypting export destinations in Google Data Catalog mitigates risks of unauthorized data access and potential data breaches, ensuring compliance with data protection regulations such as GDPR and CCPA. Encrypting data at rest is crucial for maintaining data confidentiality and integrity in the event of unauthorized access or system compromise.
  description: This rule checks whether export destinations for data governance within Google Data Catalog are encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). To verify, ensure that the export destination storage buckets or databases are configured with encryption at rest. Remediation involves enabling or enforcing encryption settings on these resources to secure sensitive data.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.datacatalog.entry.datalake_access_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Datalake Access RBAC Least Privilege
  scope: datacatalog.entry.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Datalake Access Uses RBAC with Least Privilege
  rationale: Implementing RBAC with least privilege for datalake access in GCP's Data Catalog reduces the risk of unauthorized data manipulation and exposure, ensuring sensitive information is accessed only by necessary personnel. This aligns with compliance mandates like GDPR and HIPAA, which require strict access controls to protect personal and sensitive data, thereby mitigating the risk of data breaches and financial penalties.
  description: This rule checks that access to datalake entries in GCP Data Catalog is configured using Role-Based Access Control (RBAC) with the principle of least privilege. Verify that only necessary roles are assigned to users or service accounts, and regularly audit permissions to ensure relevance. Remediation involves reviewing IAM policies within the Data Catalog, removing excessive permissions, and assigning roles that align with user responsibilities.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.datacatalog.entry.datalake_capture_enabled
  service: datacatalog
  resource: entry
  requirement: Datalake Capture Enabled
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Datalake Capture is Enabled
  rationale: Enabling Datalake Capture in Google Cloud Data Catalog ensures that all metadata and data lineage information is securely captured and stored. This feature helps in data governance, compliance with data protection regulations, and reduces the risk of data breaches by maintaining an auditable trail of data access and modifications.
  description: This rule verifies that Datalake Capture is enabled for entries in Google Cloud Data Catalog. Enabling this feature ensures that metadata changes, data lineage, and access patterns are logged and preserved. To verify, check the Data Catalog settings under the Datalake Capture configuration and ensure it is activated. If not enabled, configure the Data Catalog entry settings to activate Datalake Capture, enhancing traceability and compliance with data privacy standards.
  references:
  - https://cloud.google.com/data-catalog/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.datalake_catalog_audit_logging_enabled
  service: datacatalog
  resource: entry
  requirement: Datalake Catalog Audit Logging Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Datalake Catalog Audit Logging is Enabled
  rationale: Enabling audit logging for Datalake Catalog entries is crucial for maintaining visibility over data access and modifications. This helps identify unauthorized access, mitigate data exfiltration risks, and supports compliance with regulatory requirements such as GDPR and CCPA. By monitoring access and changes, organizations can quickly respond to potential security incidents and ensure data integrity.
  description: This rule verifies that audit logging is enabled for all entries in the Datalake Catalog within Google Cloud's Data Catalog service. Audit logs should be configured to capture 'ADMIN_READ', 'DATA_READ', and 'DATA_WRITE' activities to provide comprehensive visibility. To enable logging, configure the appropriate IAM roles and permissions, and ensure that Cloud Audit Logs are set up in the Google Cloud Console under 'IAM & Admin' -> 'Audit Logs'. Regularly review the logs to ensure compliance and address any anomalies.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logs
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - NIST SP 800-53 Rev. 5
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.datacatalog.entry.datalake_catalog_cross_account_sharing_restricted
  service: datacatalog
  resource: entry
  requirement: Datalake Catalog Cross Account Sharing Restricted
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Cross-Account Sharing of Data Catalog Entries
  rationale: Cross-account sharing of Data Catalog entries can lead to unauthorized access and potential data breaches, impacting business integrity and compliance with data protection regulations such as GDPR and CCPA. Limiting this access ensures that sensitive metadata and data lake information are only available to approved accounts, reducing the risk of data exfiltration.
  description: This rule checks for configurations that allow cross-account sharing of Data Catalog entries in GCP, which should be restricted to prevent unauthorized access. Verify that IAM policies do not grant cross-account access to Data Catalog entries. Remediation involves reviewing and updating IAM roles and policies to ensure they align with least privilege principles, and auditing logs to detect unauthorized access attempts.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.datacatalog.entry.datalake_catalog_metadata_encryption_enabled
  service: datacatalog
  resource: entry
  requirement: Datalake Catalog Metadata Encryption Enabled
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Metadata Encryption for Data Catalog Entries
  rationale: Enabling encryption for Data Catalog metadata protects sensitive data from unauthorized access and potential breaches. This is critical in maintaining data confidentiality and integrity, especially in industries with strict regulatory requirements such as finance and healthcare. Encryption at rest safeguards against threats like data theft or unauthorized disclosure, aligning with compliance mandates like GDPR and HIPAA.
  description: This rule verifies that encryption is enabled for metadata stored in Google Cloud Data Catalog entries, ensuring data is encrypted at rest. To verify, check the encryption configuration of your Data Catalog entries to confirm they use Google-managed or customer-managed keys. Remediation involves configuring the Data Catalog to use encryption keys, preferably customer-managed keys, to enhance control over the encryption processes.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.datacatalog.entry.datalake_catalog_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Datalake Catalog RBAC Least Privilege
  scope: datacatalog.entry.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Datalake Catalog Entry RBAC Follows Least Privilege
  rationale: Implementing the principle of least privilege for Datalake Catalog entries mitigates the risk of unauthorized data access, reducing potential data breaches and maintaining compliance with regulations such as GDPR and HIPAA. Overly permissive roles can lead to inadvertent exposure of sensitive data, impacting business reputation and incurring regulatory fines.
  description: This rule checks that access permissions for Datalake Catalog entries are granted based on the principle of least privilege. It ensures that roles assigned to users or service accounts are scoped appropriately, avoiding overly broad permissions. Verification requires auditing IAM policies associated with data catalog entries and adjusting them to limit access to what is strictly necessary. Remediation involves reviewing and modifying IAM policies to remove excessive permissions and ensure only necessary roles are granted.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS GCP Benchmark 1.0.0 - Section 4.3
  - NIST SP 800-53 Rev. 5 - AC-6 Least Privilege
  - ISO 27001:2013 - A.9.1.2 Access Control
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.datalake_crawler_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Datalake Crawler Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Datalake Crawler Logs in Data Catalog Entries
  rationale: Enabling Datalake Crawler Logs is crucial for maintaining visibility into data access and modification activities within GCP's Data Catalog. Without logging, organizations cannot effectively monitor or audit data interactions, leading to potential compliance violations and an increased risk of undetected data breaches.
  description: This rule checks whether logging is enabled for datalake crawler entries in the GCP Data Catalog. To verify, ensure that audit logs for Data Catalog are activated, capturing all reads, writes, and administrative actions. Remediation involves configuring the appropriate IAM permissions and enabling audit logging in the Cloud Console under the Logging section of the Data Catalog settings.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logs
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.datalake_crawler_network_private_only
  service: datacatalog
  resource: entry
  requirement: Datalake Crawler Network Private Only
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Datalake Crawler Uses Private Network Only
  rationale: Using private networks for Datalake Crawlers minimizes exposure to potential threats by restricting access to authorized internal networks only. This reduces the risk of data breaches and unauthorized access, which is critical for maintaining the confidentiality and integrity of sensitive data. Compliance with regulations like GDPR and HIPAA often requires ensuring data processing activities are secure and access is limited to trusted environments.
  description: This rule checks that Datalake Crawlers within Google Cloud's Data Catalog are configured to operate solely on private networks. It involves verifying that network interfaces are not exposed to the public internet and ensuring that private IPs are used. Remediation involves updating network configurations to restrict Datalake Crawler access to internal IP addresses only, potentially using VPCs or other network segmentation methods.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/document-800-53
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.datalake_crawler_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Datalake Crawler Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Datalake Crawler Role Uses Least Privilege
  rationale: Limiting permissions to the minimum necessary reduces the attack surface and potential for data breaches. Over-privileged roles can lead to unauthorized data access, which may result in regulatory non-compliance and financial penalties. Implementing least privilege is crucial for protecting sensitive data and maintaining trust with users and stakeholders.
  description: This rule verifies that the roles assigned to Datalake crawlers in Google Cloud's Data Catalog service are configured with the principle of least privilege. Specifically, it checks whether the crawler role has only the necessary permissions to perform its function without excess capabilities. To remediate, review and adjust the IAM policy to ensure the crawler role only has essential permissions. Use the 'iam.roles.get' and 'iam.roles.update' commands for this purpose, and regularly audit these roles to maintain compliance.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-special-publication-800-53
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.datalake_crawler_scope_restricted_to_allowlist
  service: datacatalog
  resource: entry
  requirement: Datalake Crawler Scope Restricted To Allowlist
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Datalake Crawler Scope to Allowlist in Data Catalog
  rationale: Restricting the scope of datalake crawlers to a defined allowlist minimizes the risk of unauthorized data exposure and ensures that only approved data sets are processed. This is critical for maintaining data privacy, meeting compliance obligations such as GDPR, and preventing data breaches by limiting access to sensitive information.
  description: This check ensures that the Datalake Crawler in Google Cloud Data Catalog is configured to only access resources specified in an allowlist. Verify that the crawler's configuration includes an allowlist of approved data sources and no wildcards or unrestricted patterns. Remediation involves reviewing the crawler's current access scope, identifying necessary resources, and updating the configuration to restrict access accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.datalake_export_destination_encrypted
  service: datacatalog
  resource: entry
  requirement: Datalake Export Destination Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Datalake Export Destinations are Encrypted
  rationale: Encrypting datalake export destinations mitigates the risk of unauthorized access and data breaches. Unencrypted data can be exploited by attackers, leading to potential financial loss, reputational damage, and non-compliance with regulations such as GDPR, CCPA, and HIPAA which mandate encryption of sensitive data at rest.
  description: This rule checks whether the export destination of datalake entries in Google Cloud Data Catalog is encrypted. Verify that the destination uses Google-managed or customer-managed encryption keys (CMEK) to protect data at rest. Remediate by configuring the export destination bucket with CMEK through the Google Cloud Console or gcloud CLI, ensuring compliance with security policies and regulations.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Control 7.1
  - https://cloud.google.com/kms/docs/cmek
  - NIST SP 800-53 Rev. 5, SC-28 Protection of Information at Rest
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_crawler_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Crawler Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Lineage Crawler Logs in Data Catalog
  rationale: Enabling Lineage Crawler Logs in Data Catalog is crucial for maintaining an audit trail of data access and modifications. This logging feature helps in detecting unauthorized access, meeting compliance requirements such as GDPR and CCPA, and enhancing data governance by providing visibility into data lineage and usage patterns.
  description: This rule checks whether Lineage Crawler Logs are enabled for Data Catalog entries. Ensuring these logs are active involves verifying that audit logging is configured to capture lineage crawler activities. To enable logging, navigate to the GCP Console, access the Data Catalog settings, and ensure the audit logs for lineage crawlers are turned on. This setup aids in monitoring data flow and identifying potential security incidents.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_crawler_network_private_only
  service: datacatalog
  resource: entry
  requirement: Lineage Crawler Network Private Only
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Lineage Crawlers Use Private Networks Only
  rationale: Restricting Data Catalog lineage crawlers to private networks minimizes exposure to unauthorized access and potential data exfiltration. This practice is crucial for protecting sensitive metadata from being accessed by malicious actors over the internet. Compliance with network security standards like PCI-DSS and SOC2 requires strict access controls to maintain data integrity and confidentiality.
  description: This rule checks if Data Catalog lineage crawlers operate solely within private networks, ensuring they do not have public internet access. Verify that the network configuration for lineage crawlers is set to use only private IP addresses. To remediate, configure the network settings in the GCP console to restrict the crawler's communication to VPCs and disable any public IP allocation.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/network-connectivity/docs/vpc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry.lineage_crawler_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Crawler Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Lineage Crawler Role
  rationale: Limiting the privileges of the Lineage Crawler Role minimizes the risk of unauthorized access and potential data breaches. Excessive permissions can lead to exposure of sensitive information and non-compliance with data protection regulations such as GDPR and HIPAA, thereby increasing the organization's liability and risk profile.
  description: This rule verifies that the Lineage Crawler Role is assigned only the necessary permissions to perform its function within the Data Catalog service. The role should not have permissions beyond what is required for data lineage tracking. To assess compliance, review the IAM policy bindings for the role and ensure it does not include permissions unrelated to lineage crawling. Remediation involves adjusting the IAM policy to remove unnecessary permissions, aligning with the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_crawler_scope_restricted_to_allowlist
  service: datacatalog
  resource: entry
  requirement: Lineage Crawler Scope Restricted To Allowlist
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Lineage Crawler Scope to Allowlist
  rationale: Restricting the Lineage Crawler to an allowlist minimizes the risk of unauthorized data exposure and ensures adherence to data protection regulations. This limitation is crucial for maintaining data integrity and confidentiality, reducing the risk of data breaches, and aligning with compliance requirements such as GDPR and HIPAA.
  description: This rule verifies that the Data Catalog Lineage Crawler's scope is restricted to a defined allowlist, preventing the crawler from accessing unauthorized data entries. To ensure compliance, configure the Lineage Crawler to only scan entries listed in a pre-approved allowlist. This can be verified by reviewing the crawler's configuration settings in the GCP Console or via API. Remediation includes updating the crawler configuration to specify an allowlist of trusted entries.
  references:
  - https://cloud.google.com/data-catalog/docs/reference/rest
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_credentials_in_secrets_manager
  service: datacatalog
  resource: entry
  requirement: Lineage Credentials In Secrets Manager
  scope: datacatalog.entry.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Store Data Catalog Lineage Credentials in Secrets Manager
  rationale: Storing lineage credentials in Secrets Manager helps prevent unauthorized access to sensitive data and limits the risk of credential exposure. Failure to securely manage credentials can lead to data breaches, compliance violations with frameworks like PCI-DSS and HIPAA, and potential financial and reputational damage.
  description: This rule checks if lineage credentials used in Data Catalog entries are securely stored in Google Cloud Secret Manager. Ensure that all lineage credentials are not hardcoded in the application or stored in plaintext, but instead managed through Secret Manager with appropriate IAM policies limiting access. Remediation involves migrating credentials to Secret Manager and updating applications to retrieve them securely.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/datacatalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_database_cross_account_sharing_restricted
  service: datacatalog
  resource: entry
  requirement: Lineage Database Cross Account Sharing Restricted
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross Account Data Catalog Lineage Database Sharing
  rationale: Restricting cross-account sharing of Data Catalog lineage databases is crucial to prevent unauthorized access and data breaches. Exposing sensitive metadata across accounts can lead to data leakage and compliance violations, impacting business trust and risking penalties under regulations such as GDPR and CCPA.
  description: This rule checks for any Data Catalog lineage databases shared across accounts and ensures that sharing is restricted to only necessary entities. Verify configurations by reviewing IAM policies associated with Data Catalog entries and adjust permissions to minimize exposure. Remediate by restricting access to specific roles or identities within the same account or trusted domains, following the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/entries
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_database_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Database Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption of Lineage Database in Data Catalog
  rationale: Encrypting the lineage database in GCP Data Catalog is crucial to protect sensitive metadata from unauthorized access and to comply with data protection regulations. Unencrypted metadata can expose critical lineage information, risking data leaks and non-compliance with standards like GDPR and HIPAA. Encrypting data at rest mitigates these risks and ensures that sensitive information is protected even if the storage is compromised.
  description: This rule checks if the lineage database associated with GCP Data Catalog entries is encrypted at rest. Ensure that encryption is enabled to protect metadata integrity and confidentiality. Verification involves checking the encryption configuration settings in your Data Catalog setup. To remediate, configure the Data Catalog to use Customer-Managed Encryption Keys (CMEK) or Google-managed encryption for all stored metadata.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.datacatalog.entry.lineage_database_policy_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Database Policy Least Privilege
  scope: datacatalog.entry.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Database Policies
  rationale: Implementing least privilege access in Data Catalog lineage databases reduces the risk of unauthorized data exposure or modification, which can lead to data breaches, non-compliance with data protection regulations, and potential financial and reputational damage. This principle helps ensure that users and services have only the permissions necessary to perform their intended functions, mitigating the impact of compromised accounts or insider threats.
  description: This rule checks that Data Catalog entry policies for lineage databases are configured with the least privilege principle. Specifically, it verifies that permissions granted to users and services are restricted to only those necessary for their roles. To remediate, review and adjust IAM policies to remove excessive permissions and ensure alignment with organizational access control policies. Regularly audit and update these policies to reflect changes in user roles or data access requirements.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/policy
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/privacy/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry.lineage_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Lineage is Encrypted at Rest
  rationale: Encrypting lineage data at rest protects sensitive metadata from unauthorized access and potential breaches. This is crucial for maintaining data integrity and confidentiality, especially in regulated industries such as healthcare and finance where compliance with standards like HIPAA and GDPR is required. Failure to encrypt data can lead to data exposure, violating compliance requirements and resulting in significant legal and financial repercussions.
  description: This rule checks that all Data Catalog entry lineage information is encrypted at rest using Customer-Managed Encryption Keys (CMEK). To verify, ensure that the Data Catalog service is configured to use CMEK by setting the appropriate encryption key on the resources handling sensitive lineage data. Remediation involves updating the Data Catalog configuration to select a suitable CMEK for encrypting the data at rest, thus enhancing the security posture and meeting compliance mandates.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/entry-overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.datacatalog.entry.lineage_job_logs_and_metrics_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Job Logs And Metrics Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Lineage Job Logs and Metrics in Data Catalog
  rationale: Enabling lineage job logs and metrics provides visibility into data processing activities within Google Cloud's Data Catalog. This is crucial for identifying unauthorized data access or anomalies, ensuring data integrity, and maintaining audit trails for compliance with standards like SOC 2 and ISO 27001. Failure to log these activities could result in undetected data breaches or compliance violations.
  description: This rule checks if lineage job logs and metrics are enabled in the Data Catalog entries. To verify, ensure that logging and metrics collection are configured in the Google Cloud Console under Data Catalog settings. Remediation involves enabling the necessary logging options to capture detailed information about data lineage processes, which can be done via the GCP Console or by using Terraform with the appropriate logging configuration for Data Catalog.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/monitoring
- rule_id: gcp.datacatalog.entry.lineage_job_network_private_only
  service: datacatalog
  resource: entry
  requirement: Lineage Job Network Private Only
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Lineage Jobs Use Private Networks Only
  rationale: Using private network connectivity for lineage jobs in Google Cloud Data Catalog minimizes exposure to public internet threats, reducing the risk of data breaches and unauthorized access. This practice supports compliance with data protection regulations like GDPR and enhances the security posture by limiting network access to authorized entities only.
  description: This rule checks that lineage jobs within Data Catalog entries are configured to use private networks exclusively. To verify, ensure that the network settings for lineage jobs are set to private IPs, avoiding public IP exposure. Remediation involves updating the network configuration of these jobs to use VPC Service Controls or Private Google Access, thereby restricting access to internal networks only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.datacatalog.entry.lineage_job_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Job Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Jobs
  rationale: Applying the principle of least privilege to Data Catalog lineage jobs minimizes the risk of unauthorized data access and potential data breaches. This is critical for maintaining data integrity and confidentiality, especially in environments subject to regulatory compliance such as GDPR or HIPAA. Misconfigured permissions can lead to over-privileged roles that expose sensitive data or disrupt operations.
  description: This rule checks that IAM roles associated with Data Catalog lineage jobs are configured with the minimal set of permissions required for their function. Verify that roles do not have excessive permissions by auditing IAM policies linked to Data Catalog entries for lineage jobs. Remediate by adjusting IAM policies in Google Cloud Console or via gcloud CLI to limit access to necessary resources only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.datacatalog.entry.lineage_job_script_location_private_and_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Job Script Location Private And Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Job Script Locations Are Private & Encrypted
  rationale: Lineage job scripts in Data Catalog contain sensitive metadata that can expose data lineage and dependencies. If not properly secured, unauthorized users could access or tamper with these scripts, leading to potential data breaches and compliance violations. Ensuring scripts are stored in private and encrypted locations mitigates these risks and aligns with data protection standards.
  description: This rule checks if the locations where lineage job scripts are stored are both private and encrypted. Specifically, it verifies that these scripts are in Google Cloud Storage buckets with appropriate IAM policies that restrict access and have default encryption enabled. To verify, review bucket permissions and ensure default encryption keys are set. Remediation involves adjusting IAM policies for least privilege and enabling default bucket encryption with customer-managed keys (CMEK) if necessary.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.datacatalog.entry.lineage_ml_transform_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Ml Transform Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Lineage ML Transform Logs Are Enabled for Data Catalog Entries
  rationale: Enabling lineage ML transform logs in Data Catalog is crucial for maintaining visibility into data transformations, which helps in tracking data provenance and ensuring data integrity. This is important for identifying unauthorized data manipulations and detecting potential data breaches. Moreover, it supports compliance with regulatory standards that require detailed data handling records.
  description: This rule verifies that logging is enabled for machine learning data transformations within Data Catalog entries. To ensure compliance, ensure that the Data Catalog service has audit logs activated to capture all ML transformation events. Remediation involves configuring the appropriate logging settings in the Google Cloud Console or via gcloud commands to ensure that lineage logs capture all necessary transformation details. This setup assists in monitoring and auditing data workflows effectively.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_ml_transform_network_private_only
  service: datacatalog
  resource: entry
  requirement: Lineage Ml Transform Network Private Only
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Lineage ML Transform Uses Private Network Only
  rationale: Restricting Lineage ML Transform operations to private networks mitigates risks associated with unauthorized access and data exfiltration. This control is crucial for protecting sensitive data and maintaining compliance with privacy regulations such as GDPR and HIPAA, which require stringent data security measures. It also aligns with best practices for minimizing exposure to the public internet, reducing potential attack vectors.
  description: This rule verifies that all Lineage ML Transform operations within Google Cloud Data Catalog are configured to use private network endpoints exclusively. To achieve this, ensure that the network configuration for the Data Catalog entries associated with ML transforms does not include public IP addresses. Verify by checking the network settings in the GCP Console for each relevant entry and update configurations to restrict access to private IP ranges. Remediation involves modifying network settings to disable public access and enable private Google Access for VPCs.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 4.5
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
- rule_id: gcp.datacatalog.entry.lineage_ml_transform_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Ml Transform Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Lineage Ml Transform Role in Data Catalog
  rationale: Implementing the principle of least privilege for the Lineage ML Transform Role in GCP Data Catalog minimizes the risk of unauthorized access and potential data breaches. This is crucial for maintaining data integrity and confidentiality, especially when handling sensitive information that could be exposed or altered by overly permissive permissions. Adhering to this principle also supports compliance with regulatory standards such as GDPR and CCPA, reducing legal and financial risks.
  description: This rule verifies that the Lineage ML Transform Role in GCP Data Catalog is configured with the minimum permissions necessary to perform its intended functions, ensuring it does not have excessive access rights. To verify, review the IAM policies associated with Data Catalog entries and ensure roles are appropriately scoped. If a role has more permissions than needed, modify the policy to restrict access. Use the GCP Console or gcloud CLI to audit and update IAM policies, ensuring they align with the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_partition_access_policies_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Partition Access Policies Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege on Data Catalog Lineage Partition Access
  rationale: Implementing least privilege access policies for Data Catalog lineage partitions minimizes the risk of unauthorized data exposure and potential data breaches. It helps safeguard sensitive information from insider threats and reduces the attack surface by limiting user permissions to only what is necessary. Compliance with regulatory standards like GDPR and HIPAA often mandates strict access controls to protect sensitive data.
  description: This rule checks that Data Catalog lineage partitions have access policies configured to adhere to the principle of least privilege. It verifies that only users and service accounts with a legitimate need can access specific partitions. To remediate, review the IAM policies associated with Data Catalog entries and adjust permissions to ensure they are restricted to the minimum required roles. Use GCP IAM policy management tools to audit and modify access levels accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/fine-grained-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_partition_catalog_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Partition Catalog Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Partition Catalog is Encrypted at Rest
  rationale: Encrypting the Lineage Partition Catalog data at rest mitigates risks associated with unauthorized data access and breaches. This practice supports compliance with data protection regulations like GDPR and HIPAA by ensuring sensitive metadata remains confidential. Failure to encrypt can expose organizations to potential data theft and legal penalties.
  description: This rule verifies that Lineage Partition Catalog within Google Cloud Data Catalog is encrypted using Customer-Managed Encryption Keys (CMEK). To check compliance, ensure that all Data Catalog entries have encryption configured in their metadata settings. Remediation involves configuring the Data Catalog service to use CMEK for encrypting data at rest, which can be done via the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.datacatalog.entry.lineage_private_networking_enforced
  service: datacatalog
  resource: entry
  requirement: Lineage Private Networking Enforced
  scope: datacatalog.entry.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Lineage Private Networking is Enforced for Data Catalog
  rationale: Enforcing private networking for data catalog lineage ensures that metadata is not exposed to the public internet, reducing the risk of unauthorized access and potential data breaches. This control aligns with best practices for network security by minimizing attack surfaces and is crucial for meeting compliance requirements related to data protection and privacy, such as GDPR and HIPAA.
  description: This rule verifies that Data Catalog lineage entries are configured to use private networking, which restricts access to resources via Google Cloud's internal network. To achieve this, ensure that the entry's network settings are configured to use VPC Service Controls or Private Google Access. Verification involves checking the network configurations in the Google Cloud Console or via the gcloud CLI. Remediation includes updating the network settings to enforce private networking for all lineage data.
  references:
  - https://cloud.google.com/data-catalog/docs/networking
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://cloud.google.com/vpc-service-controls/docs/concepts
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry.lineage_rbac_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage RBAC Least Privilege
  scope: datacatalog.entry.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Lineage RBAC is Configured for Least Privilege
  rationale: Implementing least privilege access in Data Catalog lineage RBAC minimizes the risk of unauthorized data access and potential data breaches, thereby protecting sensitive information and ensuring compliance with data protection regulations. Over-permissioned access can lead to privilege escalation and data exfiltration, posing significant security and compliance risks.
  description: This rule checks that Data Catalog lineage RBAC settings are configured to grant only the necessary permissions required for users to perform their job functions. Verify that IAM policies associated with Data Catalog entries restrict access to the minimum privileges needed. Remediation involves reviewing and updating IAM roles and permissions to align with the principle of least privilege.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/docs/security/iam#least-privilege
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Data Catalog Lineage Role Follows Least Privilege
  rationale: Applying the principle of least privilege to Data Catalog lineage roles minimizes the risk of unauthorized access and potential data breaches. Over-permissioned roles can lead to inadvertent data exposure and non-compliance with regulations such as GDPR and HIPAA, which require stringent access controls to protect sensitive data.
  description: This rule checks that the roles assigned to users or service accounts for Data Catalog entries are limited to the minimum necessary permissions. Verify that roles such as 'roles/datacatalog.viewer' or custom roles with specific permissions are used instead of broad roles like 'owner' or 'editor'. To remediate, audit current role assignments and adjust them to ensure they grant only required permissions, using IAM policies and Google Cloud Console.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.datacatalog.entry.lineage_table_access_policies_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Table Access Policies Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Lineage Table Access Policies Follow Least Privilege Principle
  rationale: Implementing least privilege access control for lineage tables in Data Catalog helps minimize the risk of unauthorized data exposure and potential data breaches. This is crucial for protecting sensitive data assets and maintaining compliance with regulatory frameworks such as GDPR and CCPA, which mandate strict data access controls. By limiting access to only those who absolutely need it, organizations can reduce their attack surface and prevent insider threats.
  description: This rule checks whether access policies for lineage tables in Google Cloud Data Catalog are configured to adhere to the principle of least privilege. It ensures that only authorized users with a legitimate need have access. To verify, review IAM roles and permissions assigned to Data Catalog resources and adjust them to limit access to the minimum necessary. Remediation involves auditing current access permissions, identifying unnecessary privileges, and revoking or restricting access accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry.lineage_table_encrypted
  service: datacatalog
  resource: entry
  requirement: Lineage Table Encrypted
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Table Encryption in GCP Data Catalog
  rationale: Encrypting lineage tables in GCP Data Catalog is crucial to protect sensitive metadata from unauthorized access and potential data breaches. This practice mitigates risks associated with data exposure and helps organizations comply with data privacy regulations such as GDPR and CCPA, ensuring that sensitive information remains confidential and secure.
  description: This rule checks if lineage tables in GCP Data Catalog entries are encrypted using Google-managed or customer-managed encryption keys. Verify the encryption status by reviewing the Data Catalog entry settings in the GCP Console or using the gcloud CLI. To remediate non-compliance, enable encryption at rest by configuring the appropriate encryption key settings for the Data Catalog entries.
  references:
  - https://cloud.google.com/data-catalog/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest
  - https://cloud.google.com/docs/security-overview
- rule_id: gcp.datacatalog.entry.lineage_table_public_sharing_disabled
  service: datacatalog
  resource: entry
  requirement: Lineage Table Public Sharing Disabled
  scope: datacatalog.entry.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Sharing for Data Catalog Lineage Tables
  rationale: Publicly shared lineage tables in GCP's Data Catalog can expose sensitive metadata, leading to data breaches and unauthorized access. This poses a significant risk to data integrity and confidentiality, potentially violating compliance requirements such as GDPR, HIPAA, and PCI-DSS. Ensuring lineage tables are not publicly accessible helps maintain strict access controls and reduces the attack surface.
  description: This check ensures that lineage tables within GCP's Data Catalog are not shared with the public. It verifies that the 'allAuthenticatedUsers' and 'allUsers' members are not granted roles on Data Catalog entries. To remediate, review the IAM policy bindings for Data Catalog entries and remove any public access, ensuring only authorized users have the necessary permissions. This can be done via the GCP Console or using the gcloud CLI by inspecting and modifying the IAM policies.
  references:
  - https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.entryGroups.entries
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_tls_required
  service: datacatalog
  resource: entry
  requirement: Lineage TLS Required
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS for Data Catalog Entry Lineage
  rationale: TLS encryption is critical for protecting data in transit, ensuring that lineage information in Data Catalog entries is not exposed to unauthorized access. Without TLS, data could be intercepted, leading to potential data breaches and non-compliance with standards such as GDPR and HIPAA.
  description: This rule checks whether TLS is enforced for network connections when accessing lineage data within Data Catalog entries. To verify, inspect the network settings of your Data Catalog service to ensure TLS/SSL is configured and enforced. Remediation involves configuring the Data Catalog to only accept TLS connections, which can typically be done through the Google Cloud Console by setting the appropriate security policies.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.datacatalog.entry.lineage_trigger_event_sources_restricted
  service: datacatalog
  resource: entry
  requirement: Lineage Trigger Event Sources Restricted
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Lineage Trigger Event Sources in Data Catalog
  rationale: Restricting lineage trigger event sources is crucial to prevent unauthorized data modifications and lineage tracking from untrusted sources. This enhances data integrity and confidentiality, reducing the risk of exposure to data breaches and ensuring compliance with regulatory requirements such as GDPR and CCPA. It helps organizations maintain trust with stakeholders by safeguarding sensitive data.
  description: This rule checks if Data Catalog entries have restricted lineage trigger event sources. Ensure that only trusted and necessary services are allowed to trigger lineage events to prevent unauthorized data access. To verify, review the Data Catalog entry settings and configure access policies to limit event sources. Remediation involves setting IAM policies that explicitly grant permissions to trusted services and users only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.datacatalog.entry.lineage_trigger_logs_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Trigger Logs Enabled
  scope: datacatalog.entry.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Lineage Trigger Logs
  rationale: Enabling lineage trigger logs in GCP Data Catalog provides detailed insights into data movement and transformations, allowing for enhanced visibility and accountability. This is crucial for identifying unauthorized access or anomalies, thus mitigating risks of data breaches. Compliance with regulations such as GDPR and CCPA often requires detailed audit trails, making this a key component in meeting legal obligations.
  description: This rule checks whether lineage trigger logs are enabled for GCP Data Catalog entries. To verify, ensure that the logging configuration for each Data Catalog entry includes lineage triggers. Remediation involves configuring the Data Catalog service to log all lineage events through the Google Cloud Console or via the CLI. This ensures all data lineage activities are captured and can be audited for security and compliance purposes.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance/resources
- rule_id: gcp.datacatalog.entry.lineage_trigger_targets_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Trigger Targets Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Implement Least Privilege for Data Catalog Lineage Triggers
  rationale: Ensuring that lineage triggers in GCP Data Catalog are configured with the least privilege principle minimizes the risk of unauthorized access and potential data breaches. Misconfigured permissions can lead to exposure of sensitive metadata, impacting regulatory compliance under frameworks such as GDPR, HIPAA, and SOC2. By adhering to least privilege, organizations can reduce the attack surface and fulfill compliance obligations.
  description: This rule checks whether lineage trigger targets in GCP Data Catalog entries are granted only the necessary permissions to perform their functions. Users must review and configure IAM policies to ensure that roles are limited to the essential permissions required for specific tasks. Remediation involves auditing current permissions, removing unnecessary roles, and applying more restrictive IAM roles to limit access. Verification can be done through the GCP Console or gcloud CLI by inspecting IAM settings on Data Catalog entries.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_version_immutability_enforced
  service: datacatalog
  resource: entry
  requirement: Lineage Version Immutability Enforced
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce Lineage Version Immutability in Data Catalog
  rationale: Enforcing version immutability for data lineage in GCP's Data Catalog is crucial as it ensures the integrity and authenticity of data history. This mitigates risks associated with unauthorized changes that could lead to data breaches or compliance violations, such as those outlined in GDPR and CCPA. Immutable versions also help in maintaining accurate audit trails and support forensic investigations, thereby enhancing trust in data governance practices.
  description: This rule checks whether lineage version immutability is enforced for entries in the GCP Data Catalog. Configuring immutable versions prevents any alterations to recorded data lineage, preserving a reliable history of data transformations and movements. Verification involves reviewing Data Catalog settings to ensure that versioning policies are properly configured and enforced. To remediate, enable versioning policies that specify immutability for lineage records, and audit configurations regularly to ensure ongoing compliance.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry.lineage_workflow_cross_account_trusts_restricted
  service: datacatalog
  resource: entry
  requirement: Lineage Workflow Cross Account Trusts Restricted
  scope: datacatalog.entry.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Lineage Workflow Trusts in Data Catalog
  rationale: Restricting cross-account trusts in lineage workflows is crucial to prevent unauthorized access and data leaks across different GCP accounts. Such restrictions help mitigate risks associated with data sovereignty and ensure compliance with data protection regulations such as GDPR and CCPA. Organizations can avoid potential breaches and maintain control over sensitive data by enforcing strict access controls.
  description: This rule ensures that cross-account trusts in Data Catalog lineage workflows are restricted to prevent unauthorized access to metadata entries. Verify that IAM policies for Data Catalog entries do not allow broad access to external accounts. Remediation involves reviewing IAM policies and limiting permissions to only necessary GCP accounts, ensuring that trust relationships are explicitly defined and scoped appropriately.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/workflows
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/data-catalog/docs/concepts
- rule_id: gcp.datacatalog.entry.lineage_workflow_execution_role_least_privilege
  service: datacatalog
  resource: entry
  requirement: Lineage Workflow Execution Role Least Privilege
  scope: datacatalog.entry.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Role
  rationale: Granting excessive permissions to the Lineage Workflow Execution Role in Data Catalog can lead to unauthorized data access, modification, or deletion. This increases the risk of data breaches and non-compliance with regulations such as GDPR and CCPA, which mandate stringent data access controls. Minimizing permissions aligns with the principle of least privilege, reducing potential attack vectors and safeguarding sensitive data.
  description: This rule checks that the Lineage Workflow Execution Role in Data Catalog is configured with the minimum necessary permissions. Review the IAM policies associated with this role to ensure they are restrictive yet sufficient for operational needs. Remediation involves auditing current permissions, removing any superfluous roles, and implementing more granular IAM policies to align with least privilege principles.
  references:
  - https://cloud.google.com/datacatalog/docs/how-to-manage-iam
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.datacatalog.entry.lineage_workflow_kms_encryption_enabled
  service: datacatalog
  resource: entry
  requirement: Lineage Workflow KMS Encryption Enabled
  scope: datacatalog.entry.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Data Catalog Lineage Workflows
  rationale: Enabling KMS encryption for Data Catalog lineage workflows helps protect sensitive metadata and lineage information from unauthorized access, reducing the risk of data breaches. This is critical for compliance with data protection regulations such as GDPR and CCPA, which mandate strong encryption practices to safeguard personal and sensitive data. Failing to encrypt this information could expose organizations to significant legal and reputational risks.
  description: This rule checks if Cloud KMS encryption is enabled for Data Catalog lineage workflows, ensuring that metadata and lineage data are encrypted at rest using customer-managed keys. To verify, confirm that the 'encryptionSpec' field is configured with a valid KMS key in the Data Catalog entry settings. Remediation involves assigning a Cloud KMS key to the respective Data Catalog entries by updating the 'encryptionSpec' to include the 'kmsKeyName' parameter pointing to a Cloud KMS key resource.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/lineage
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.datacatalog.entry_group.data_catalog_audit_logging_enabled
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Audit Logging Enabled
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Data Catalog Audit Logging is Enabled
  rationale: Enabling audit logging for Data Catalog entry groups is crucial for maintaining visibility into data access and modifications. It helps identify unauthorized access attempts, supports forensic analysis in the event of a data breach, and ensures compliance with regulations such as GDPR and CCPA that require detailed logging of data access activities.
  description: This rule checks if audit logging is enabled for Data Catalog entry groups on GCP. Audit logs capture all read and write operations, providing an audit trail of interactions with data assets. To verify, ensure that audit logs for the Data Catalog service are configured in the GCP Console under 'Audit Logs' settings. Remediation involves enabling 'Admin Read', 'Data Read', and 'Data Write' logs for Data Catalog in the Cloud Console.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry_group.data_catalog_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Cross Account Sharing Restricted
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Cross-Account Sharing in Data Catalog Entry Groups
  rationale: Restricting cross-account sharing in Data Catalog minimizes the risk of unauthorized data access and potential data breaches. It ensures that sensitive metadata is only accessible to trusted accounts, aligning with data governance policies and compliance requirements such as GDPR and CCPA.
  description: This rule checks for configurations in Google Cloud Data Catalog to ensure entry groups are not shared across accounts unless explicitly required. To verify, inspect IAM policies on entry groups to confirm that cross-account principals are not granted access. Remediation involves removing unnecessary cross-account permissions and configuring IAM roles to limit access to necessary internal accounts only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.datacatalog.entry_group.data_catalog_metadata_encryption_enabled
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Metadata Encryption Enabled
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Data Catalog Metadata Encryption is Enabled
  rationale: Encrypting metadata in Data Catalog protects sensitive information from unauthorized access, reducing the risk of data breaches and ensuring compliance with regulations such as GDPR and HIPAA. Metadata often contains critical business insights and intellectual property, making its protection essential to maintain organizational privacy and data integrity.
  description: This rule checks that metadata within GCP Data Catalog entry groups is encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK) to safeguard data at rest. To verify, ensure that the entry group settings specify an encryption key. Remediation involves configuring the Data Catalog to use CMEK or GMEK by accessing the 'entry group' configuration in the GCP Console or using the gcloud CLI to set the necessary encryption options.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance
  - CIS GCP Foundation Benchmark v1.3.0, Control 5.1
  - NIST SP 800-53 Rev. 5, Control SC-28
  - https://cloud.google.com/kms/docs
- rule_id: gcp.datacatalog.entry_group.data_catalog_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog RBAC Least Privilege
  scope: datacatalog.entry_group.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege on Data Catalog Entry Groups
  rationale: Implementing least privilege principles for Data Catalog Entry Groups reduces the risk of unauthorized data access and potential data breaches. This control helps prevent privilege escalation and limits the impact of compromised accounts, aligning with compliance mandates such as GDPR and CCPA which emphasize data protection and privacy.
  description: This rule verifies that RBAC (Role-Based Access Control) policies for Data Catalog Entry Groups are configured to ensure users and service accounts have only the permissions necessary to perform their roles. It checks for overly permissive roles and provides guidance on assigning predefined roles with the minimum required permissions. To remediate, review and adjust IAM policies for Entry Groups, removing excessive permissions and applying least privilege principles.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.datacatalog.entry_group.data_catalog_registry_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Registry Cross Account Sharing Restricted
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Cross-Account Sharing in Data Catalog
  rationale: Restricting cross-account sharing in Data Catalog is crucial to prevent unauthorized access to sensitive metadata. Cross-account sharing can lead to accidental data exposure and increase the risk of data breaches. It helps in maintaining compliance with data protection regulations by ensuring that only authorized accounts have access to specific metadata entries.
  description: This rule checks that Data Catalog Entry Groups are not shared across different GCP accounts unless explicitly required. To verify, review the IAM policies attached to the Entry Groups and ensure they do not grant permissions to external accounts. Remediation involves auditing current sharing settings and updating IAM policies to restrict access to trusted accounts only. Implement least privilege access and regularly monitor access logs to detect any unauthorized sharing activities.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/data-catalog/docs/how-to
- rule_id: gcp.datacatalog.entry_group.data_catalog_registry_metadata_encryption_enabled
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Registry Metadata Encryption Enabled
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Data Catalog Registry Metadata is Encrypted at Rest
  rationale: Encrypting metadata in Google Cloud Data Catalog safeguards sensitive information against unauthorized access and data breaches, which can lead to compliance violations with regulations such as GDPR, HIPAA, and PCI-DSS. Failure to encrypt metadata at rest increases the risk of exposure of critical data assets and can result in financial penalties and reputational damage.
  description: This rule verifies that Google Cloud Data Catalog entry groups have encryption enabled for registry metadata at rest. Specifically, it checks if the Data Catalog entry group utilizes customer-managed encryption keys (CMEK) or Google-managed keys to protect metadata. To ensure compliance, configure your Data Catalog entry groups to use CMEK by setting the appropriate encryption settings in the GCP Console or via the gcloud command-line tool. Regularly audit your configuration to maintain security posture.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/privacy/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/pci-dss
- rule_id: gcp.datacatalog.entry_group.data_catalog_registry_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Data Catalog Registry RBAC Least Privilege
  scope: datacatalog.entry_group.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Data Catalog Entry Group Follows Least Privilege Model
  rationale: Implementing least privilege access in GCP Data Catalog Entry Groups mitigates the risk of unauthorized data exposure and manipulation. It reduces the attack surface by ensuring users and services have access only to the resources they need, aligning with compliance requirements such as GDPR and ISO 27001, thereby protecting sensitive business data from breaches.
  description: This rule checks that IAM roles granted to Data Catalog Entry Groups adhere to the principle of least privilege. Verify that only necessary permissions are assigned by reviewing the IAM policy bindings associated with each entry group. Remediation involves auditing the roles and permissions and modifying IAM policies to remove excessive permissions, ensuring users and services have the minimum necessary access.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/overview/whitepapers
- rule_id: gcp.datacatalog.entry_group.data_governance_change_audit_logging_enabled
  service: datacatalog
  resource: entry_group
  requirement: Data Governance Change Audit Logging Enabled
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Data Governance Change Audit Logging for Entry Groups
  rationale: Enabling audit logging for data governance changes in Google Cloud's Data Catalog helps track the modification history of entry groups. This is crucial for identifying unauthorized changes, ensuring accountability, and meeting compliance requirements such as SOC2 and ISO 27001. Without audit logs, detecting potential security breaches or misconfigurations becomes challenging, increasing organizational risk.
  description: This rule checks whether audit logging is enabled for data governance changes in Data Catalog entry groups. Audit logs should capture 'admin read' and 'admin write' operations to ensure all modifications are tracked. Verify this setting by reviewing the audit log configuration in the Cloud Console under the Logging section or using the gcloud command-line tool. To enable, configure the appropriate logging settings in the IAM & Admin section, specifying the 'dataAccess' log type for entry group resources.
  references:
  - https://cloud.google.com/data-catalog/docs/audit-logs
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.datacatalog.entry_group.data_governance_cross_account_sharing_review_required
  service: datacatalog
  resource: entry_group
  requirement: Data Governance Cross Account Sharing Review Required
  scope: datacatalog.entry_group.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Review Cross-Account Sharing in Data Catalog Entry Groups
  rationale: Cross-account sharing of data catalog entry groups without proper governance can lead to unauthorized data access, resulting in potential data breaches and compliance violations. This is particularly critical when considering sensitive data governed by regulations such as GDPR or HIPAA, where data sharing must be scrutinized and controlled to prevent unauthorized disclosures.
  description: This rule checks for entry groups in GCP Data Catalog that are shared across accounts, ensuring a governance review is in place for such configurations. It examines IAM policies associated with entry groups to identify cross-account access and recommends a review process to validate necessity and compliance with data governance policies. Remediation involves auditing the IAM policies, consulting stakeholders for data-sharing requirements, and removing unnecessary cross-account permissions.
  references:
  - https://cloud.google.com/datacatalog/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.datacatalog.entry_group.data_governance_update_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Data Governance Update RBAC Least Privilege
  scope: datacatalog.entry_group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Entry Group Updates
  rationale: Ensuring least privilege access to Data Catalog entry groups is critical to prevent unauthorized modifications which can lead to data exposure or integrity issues. Misconfigured permissions can increase the risk of data breaches, violate compliance regulations, and ultimately result in financial and reputational damage. Implementing precise RBAC policies helps in aligning with compliance frameworks and securing sensitive data effectively.
  description: This rule verifies that only necessary roles are granted permissions to update Data Catalog entry groups, aligning with the principle of least privilege. Check IAM policies to ensure that roles like Viewer and Metadata Viewer do not have unnecessary update permissions. Remediate by auditing current role assignments and adjusting permissions using `gcloud` commands or the GCP Console to restrict update capabilities to roles like Editor or Owner only when justified.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-roles#data-catalog-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices-for-roles-and-permissions
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.datacatalog.entry_group.datalake_registry_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Datalake Registry Cross Account Sharing Restricted
  scope: datacatalog.entry_group.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Sharing of Data Catalog Entry Groups
  rationale: Restricting cross-account sharing of Data Catalog entry groups is crucial to prevent unauthorized access to sensitive data within your datalake registry. Unauthorized cross-account sharing can lead to data breaches, non-compliance with data protection regulations (such as GDPR or CCPA), and potential financial and reputational damage to your organization.
  description: This rule checks for configurations in Google Cloud's Data Catalog to ensure that entry groups are not shared across different accounts without explicit authorization. Verify that IAM policies attached to Data Catalog entry groups do not include principals from external accounts. Remediation involves auditing IAM policies and removing any unauthorized external account permissions, ensuring only intended and trusted accounts have access.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/entry-group-overview
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry_group.datalake_registry_metadata_encryption_enabled
  service: datacatalog
  resource: entry_group
  requirement: Datalake Registry Metadata Encryption Enabled
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Encryption for Datalake Registry Metadata
  rationale: Ensuring encryption for datalake registry metadata protects sensitive information from unauthorized access and potential breaches. This is crucial for mitigating risks such as data leaks and unauthorized data manipulation, while also fulfilling compliance requirements like GDPR, which mandate data protection and privacy. Encryption at rest is a key control for defending against insider threats and external attacks.
  description: This rule checks whether encryption is enabled for datalake registry metadata within Google Cloud's Data Catalog. To verify, ensure that the entry group's metadata is encrypted using Cloud KMS keys. This can be configured by setting the 'kmsKeyName' field in the entry group settings. Remediation involves using Google Cloud Console or gcloud command-line tool to update the entry group configuration to include a valid KMS key for encryption.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypting
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.datacatalog.entry_group.datalake_registry_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Datalake Registry RBAC Least Privilege
  scope: datacatalog.entry_group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Datalake Registry Entry Group
  rationale: Implementing least privilege in RBAC for Data Catalog entry groups helps minimize potential data breaches by restricting access to necessary personnel only. This reduces the attack surface and aids in compliance with regulations like GDPR and HIPAA, which mandate controlled access to sensitive data. It also mitigates insider threats and limits the damage from compromised accounts.
  description: This rule verifies that RBAC policies for Data Catalog entry groups are configured to adhere to the principle of least privilege. It checks that only roles necessary for specific tasks are granted to users, groups, or service accounts. To remediate, review IAM policies for entry groups and remove excessive permissions, ensuring that each role aligns with its required data access. Use GCP IAM policy analysis tools to audit permissions effectively.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/manage-iam
  - https://cloud.google.com/security/compliance/cis#cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry_group.lineage_audit_logging_enabled
  service: datacatalog
  resource: entry_group
  requirement: Lineage Audit Logging Enabled
  scope: datacatalog.entry_group.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Lineage Audit Logging for Data Catalog Entry Groups
  rationale: Enabling Lineage Audit Logging for Data Catalog Entry Groups helps track data access and modifications, which is crucial for identifying unauthorized activities and maintaining data integrity. It supports compliance with regulations such as GDPR and HIPAA by providing audit trails necessary for data governance and incident response. Without this logging, organizations may face increased risks of data breaches and non-compliance fines.
  description: This rule checks whether Lineage Audit Logging is enabled for entry groups within Google Cloud's Data Catalog service. To ensure compliance, configure audit logging by navigating to the Google Cloud Console, selecting Data Catalog, and verifying that the 'Audit Logs' settings include both 'Admin Read' and 'Data Access'. If not enabled, update the IAM policies to support logging. Proper configuration ensures that all access and changes to data entries are logged for monitoring and forensic purposes.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.entry_group.lineage_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Lineage Cross Account Sharing Restricted
  scope: datacatalog.entry_group.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Lineage Cross Account Sharing in Data Catalog
  rationale: Restricting cross-account sharing of lineage data in GCP Data Catalog is crucial to prevent unauthorized access to sensitive metadata, which can lead to data leaks and compliance violations. By ensuring that lineage data is not shared across accounts without proper controls, organizations can mitigate risks associated with data privacy breaches and align with regulatory requirements such as GDPR and CCPA.
  description: This rule checks for configurations in GCP Data Catalog where entry group lineage data is shared across accounts. To verify, review IAM policies associated with entry groups and ensure that only authorized accounts have access. Remediation involves refining IAM roles and permissions to limit access to lineage data, ensuring it is only available to trusted accounts within the organization.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/entry-groups
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/privacy/gdpr
- rule_id: gcp.datacatalog.entry_group.lineage_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Lineage RBAC Least Privilege
  scope: datacatalog.entry_group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Data Catalog Entry Group RBAC Follows Least Privilege
  rationale: Implementing least privilege for Data Catalog entry group lineage ensures that users and services have only the permissions essential for their roles, reducing the risk of unauthorized access and data breaches. This approach mitigates potential insider threats and limits the blast radius in case of credential compromise, aligning with compliance mandates such as NIST and ISO 27001.
  description: This rule checks that IAM roles assigned to Data Catalog entry group lineage follow the principle of least privilege. Verify that only necessary roles with limited permissions are granted to users or service accounts. Remediation involves reviewing IAM policies and adjusting roles to ensure that overly permissive roles are replaced with roles that strictly adhere to the minimum permissions needed for functionality.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.entry_group.lineage_registry_cross_account_sharing_restricted
  service: datacatalog
  resource: entry_group
  requirement: Lineage Registry Cross Account Sharing Restricted
  scope: datacatalog.entry_group.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Lineage Registry Sharing
  rationale: Restricting cross-account sharing of lineage registries in GCP Data Catalog minimizes the risk of unauthorized access to sensitive metadata, which could lead to data leaks or breaches. It is crucial for maintaining data sovereignty and complying with data privacy regulations such as GDPR and CCPA. Unrestricted sharing could expose critical lineage information to external accounts, posing a significant security risk.
  description: This rule checks if lineage registries within GCP Data Catalog entry groups are shared across accounts, which should be restricted to prevent unauthorized access. Verify that IAM policies for entry groups do not grant access to external accounts. Remediation involves reviewing and refining IAM policies to limit access to trusted internal accounts only, ensuring compliance with organizational data protection policies. Use the GCP Console or CLI to audit and manage access control settings.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-guide
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.entry_group.lineage_registry_metadata_encryption_enabled
  service: datacatalog
  resource: entry_group
  requirement: Lineage Registry Metadata Encryption Enabled
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Lineage Registry Metadata Encryption is Enabled
  rationale: Enabling encryption for Lineage Registry Metadata in GCP Data Catalog is crucial to protect sensitive data from unauthorized access and potential breaches. This measure mitigates risks associated with data exposure and helps meet compliance with regulations such as GDPR and HIPAA, which mandate data protection standards. Encrypting metadata at rest reduces the attack surface, safeguarding data integrity and confidentiality.
  description: This rule checks whether encryption is enabled for Lineage Registry Metadata in GCP Data Catalog's entry groups. It verifies that the metadata is encrypted using Google-managed keys or customer-managed keys, ensuring that data is protected at rest. To enable encryption, configure the entry group's settings to use the appropriate key management service. Verification involves reviewing the Data Catalog settings to confirm the encryption status and the key type in use. Remediation requires updating the settings to enforce encryption if not already enabled.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encrypting-data
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datacatalog.entry_group.lineage_registry_rbac_least_privilege
  service: datacatalog
  resource: entry_group
  requirement: Lineage Registry RBAC Least Privilege
  scope: datacatalog.entry_group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Registry
  rationale: Implementing least privilege access for the Data Catalog Lineage Registry is crucial to minimize potential data breaches and unauthorized access. By restricting permissions to only what is necessary, organizations can reduce the risk of data leaks, maintain the integrity of lineage data, and comply with regulatory requirements such as GDPR and CCPA.
  description: This rule checks that roles assigned to users or service accounts on Data Catalog entry groups related to the Lineage Registry adhere to the principle of least privilege. Specifically, it verifies that entities are not granted more permissions than necessary to perform their job functions. To remediate, review IAM policies and remove unnecessary roles or privileges from users and service accounts. Utilize IAM Recommender for identifying and revoking excessive permissions.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/iam/docs/recommender-overview
  - https://www.nist.gov/privacy-framework
- rule_id: gcp.datacatalog.entry_group.lineage_store_encrypted
  service: datacatalog
  resource: entry_group
  requirement: Lineage Store Encrypted
  scope: datacatalog.entry_group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Store Encryption in Data Catalog Entry Groups
  rationale: Encrypting lineage data in GCP Data Catalog is crucial for protecting sensitive information from unauthorized access and potential data breaches. Encryption at rest helps mitigate risks associated with data exfiltration and ensures compliance with data protection regulations such as GDPR and HIPAA, which mandate safeguarding personal and sensitive data.
  description: This rule checks that all lineage store data within GCP Data Catalog entry groups are encrypted at rest. By default, GCP encrypts data using Google-managed keys, but organizations can enhance security by using customer-managed encryption keys (CMEK). To verify and configure this setting, navigate to the Data Catalog settings and ensure that the 'lineage store encryption' option is enabled. Remediation involves setting up CMEK through Cloud Key Management Service (KMS) and associating it with your Data Catalog entry group.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/privacy/gdpr
- rule_id: gcp.datacatalog.policy_tag.data_governance_policy_admin_change_audit_logging_enabled
  service: datacatalog
  resource: policy_tag
  requirement: Data Governance Policy Admin Change Audit Logging Enabled
  scope: datacatalog.policy_tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Enable Audit Logging for Policy Tag Admin Changes
  rationale: Enabling audit logging for policy tag admin changes in Google Cloud Data Catalog is crucial for maintaining a secure data governance framework. Without logging, unauthorized changes to policy tags could go undetected, leading to potential data exposure, loss of data integrity, and non-compliance with industry regulations such as GDPR or HIPAA. Audit logs help in tracking changes, facilitating forensic investigations, and ensuring accountability.
  description: This rule checks that audit logging is enabled for all administrative changes to policy tags within Google Cloud Data Catalog. To verify, ensure that the Data Access audit logs for Data Catalog are configured to log admin write actions on policy tags. Remediation involves enabling audit logging in the Google Cloud Console under the IAM & Admin section, specifically within the audit logs settings for each project where Data Catalog is used.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nice/resources/nist-cybersecurity-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.policy_tag.data_governance_policy_admin_mfa_required
  service: datacatalog
  resource: policy_tag
  requirement: Data Governance Policy Admin MFA Required
  scope: datacatalog.policy_tag.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce MFA for Data Governance Policy Admins
  rationale: Requiring Multi-Factor Authentication (MFA) for Data Governance Policy Admins in Google Cloud's Data Catalog reduces the risk of unauthorized access to sensitive data classification and tagging policies. This measure protects against credential theft and insider threats, which can lead to data breaches, regulatory non-compliance, and significant financial and reputational damage.
  description: This rule checks whether MFA is enforced for users with administrative privileges on Data Catalog policy tags. Administrators must enable MFA in their Identity and Access Management (IAM) settings, ensuring an additional layer of security. To verify compliance, review IAM policies for admin roles and confirm that MFA is enabled. Remediation involves configuring IAM settings to enforce MFA for all users with 'roles/datacatalog.policyTagAdmin' permissions.
  references:
  - https://cloud.google.com/iam/docs/using-mfa
  - https://cloud.google.com/security-command-center/docs/security-health-analytics-best-practices
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 1.12
  - NIST SP 800-63B Digital Identity Guidelines
  - ISO/IEC 27001:2013 - A.9.2.1 User Access Management
  - https://cloud.google.com/datacatalog/docs/how-to/policy-tags
- rule_id: gcp.datacatalog.policy_tag.data_governance_policy_admin_rbac_least_privilege
  service: datacatalog
  resource: policy_tag
  requirement: Data Governance Policy Admin RBAC Least Privilege
  scope: datacatalog.policy_tag.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Least Privilege for Data Governance Policy Admin in Data Catalog
  rationale: Implementing least privilege for Data Governance Policy Admin roles minimizes the risk of unauthorized access and potential data breaches, safeguarding sensitive data. This is crucial to comply with data protection regulations like GDPR and CCPA, which mandate strict access controls to protect personal data and mitigate security threats.
  description: This rule checks whether users assigned the 'Data Governance Policy Admin' role in Data Catalog have permissions strictly necessary for their duties. It verifies that no excessive permissions are granted that could lead to privilege escalation or data exposure. Remediation involves reviewing existing role assignments, removing unnecessary permissions, and ensuring roles align with the principle of least privilege. This can be done via the Google Cloud Console or gcloud CLI by inspecting IAM policies and revising them accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/policy-tag-manager
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag.data_catalog_data_quality_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Data Catalog Data Quality Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Quality Logs in GCP Data Catalog
  rationale: Enabling data quality logs in Google Cloud's Data Catalog is crucial for maintaining visibility into data integrity and compliance. Without these logs, organizations may face challenges in identifying data anomalies, auditing data-related activities, and meeting regulatory requirements such as GDPR and HIPAA. This logging capability helps mitigate risks associated with data breaches and ensures accountability in data management processes.
  description: This rule verifies that data quality logs are enabled for tags in Google Cloud's Data Catalog. To ensure this setting, navigate to the Data Catalog settings in the GCP Console and enable logging for data quality. This allows for comprehensive monitoring and auditing of data quality issues. Remediation involves adjusting the configuration settings to activate logging, ensuring that all data interactions are recorded for future analysis and compliance audits.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/set-up-logging
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.tag.data_catalog_data_quality_output_location_private__encrypted
  service: datacatalog
  resource: tag
  requirement: Data Catalog Data Quality Output Location Private Encrypted
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Data Quality Output is Private and Encrypted
  rationale: Encrypting Data Catalog output locations ensures sensitive data is protected from unauthorized access and potential breaches, which could lead to financial loss, reputational damage, and non-compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks that Data Catalog data quality output locations are both private and encrypted using Google-managed or customer-managed encryption keys. Verify by ensuring the output location is in a Google Cloud Storage bucket that is not publicly accessible and has encryption enabled. To remediate, configure the bucket to disable public access and enable encryption settings.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datacatalog.tag.data_catalog_data_quality_role_least_privilege
  service: datacatalog
  resource: tag
  requirement: Data Catalog Data Quality Role Least Privilege
  scope: datacatalog.tag.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Data Quality Role
  rationale: Applying the principle of least privilege in GCP's Data Catalog service mitigates the risk of unauthorized access and potential data breaches. It reduces the attack surface by limiting permissions to only those necessary for users to perform their roles. This practice aligns with compliance standards like ISO 27001 and NIST SP 800-53, which emphasize minimizing access to sensitive data to prevent insider threats and accidental data exposures.
  description: This rule checks if roles assigned for managing Data Catalog tags adhere to the principle of least privilege. It verifies that only users or service accounts with a justified business need have access to create, update, or delete tags. To remediate, audit all current role assignments to ensure they are scoped to the minimum necessary permissions. Use predefined roles where possible, and regularly review and update access controls to reflect changes in job responsibilities.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/tag-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices/identity-access-management
- rule_id: gcp.datacatalog.tag.data_governance_quality_jobs_role_least_privilege
  service: datacatalog
  resource: tag
  requirement: Data Governance Quality Jobs Role Least Privilege
  scope: datacatalog.tag.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Governance Quality Jobs Role
  rationale: Implementing least privilege for roles managing Data Catalog tags minimizes the risk of unauthorized data access or modification, protecting sensitive metadata from internal and external threats. This approach supports compliance with regulations such as GDPR and HIPAA, which mandate strict access controls to safeguard data privacy and integrity.
  description: This rule checks that roles assigned to manage Data Catalog tags for data governance quality jobs adhere to the principle of least privilege. Ensure that only essential permissions are granted, such as 'datacatalog.tags.create' and 'datacatalog.tags.update', and avoid broader permissions that are unnecessary for the task. Regularly audit permissions using IAM policy analysis and adjust roles to remove excessive privileges. Remediation involves using the Google Cloud Console, IAM Policy Analyzer, or gcloud CLI to refine role permissions.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/governance
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.datacatalog.tag.data_governance_quality_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Data Governance Quality Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Tag Governance Logging
  rationale: Enabling data governance quality logs in Google Cloud Data Catalog helps organizations maintain visibility over tag management and usage. This is crucial for ensuring data integrity, preventing unauthorized access, and meeting compliance requirements such as GDPR and CCPA. Without proper logging, misconfigurations or unauthorized changes could go unnoticed, leading to potential data breaches or loss of compliance.
  description: This rule checks whether logging is enabled for Data Catalog tags to monitor the governance quality of data. To verify, ensure that audit logging is configured for Data Catalog in the Google Cloud Console under the 'IAM & Admin' section, then 'Audit Logs'. Remediation involves enabling Data Catalog audit logs by selecting the appropriate log type (Admin Activity, Data Access, etc.) and specifying the necessary log sinks for long-term storage and analysis.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.tag.data_governance_quality_outputs_encrypted_and_private
  service: datacatalog
  resource: tag
  requirement: Data Governance Quality Outputs Encrypted And Private
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Tags Are Encrypted and Private
  rationale: Encrypting data governance quality outputs ensures that sensitive data is protected against unauthorized access, mitigating the risk of data breaches. It supports compliance with regulatory requirements such as GDPR and HIPAA, which mandate the protection of personal and sensitive information. By maintaining data privacy and confidentiality, organizations reduce legal liabilities and preserve their reputation.
  description: This rule checks that all data governance quality outputs in Google Cloud Data Catalog are encrypted at rest and are configured to be private. Ensure that the tagging system in Data Catalog is set up to use customer-managed encryption keys (CMEK) for robust security control. Verification involves reviewing the Data Catalog settings in the GCP Console under the 'Security' section or using the gcloud CLI to ensure encryption settings use CMEK. Remediation requires configuring the Data Catalog to use CMEK and setting access controls to restrict visibility to authorized users only.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/encryption-at-rest/
  - https://www.cisecurity.org/benchmark/google_cloud_computing/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_access_rbac_least_privilege
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Access RBAC Least Privilege
  scope: datacatalog.tag.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for DataCatalog Tag Access
  rationale: Implementing least privilege access for Data Catalog tags minimizes the risk of unauthorized access and potential data breaches. This practice is crucial in preventing exposure of sensitive metadata and ensuring compliance with regulatory frameworks such as GDPR and CCPA. Unauthorized access to data quality recommendations can lead to data integrity issues and undermine trust in data-driven decisions.
  description: This rule checks if access to Data Catalog tags related to data quality recommendations is granted based on the principle of least privilege. Specifically, it verifies that only users or service accounts with a legitimate need have the necessary roles assigned. To remediate, review IAM policies and adjust roles to ensure minimal access is granted. Use the Google Cloud Console or gcloud CLI to audit and modify roles associated with Data Catalog, focusing on removing excessive permissions.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/grant-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Computing Foundations Benchmark v1.1.0, section 4.1
  - NIST SP 800-53 Rev. 5, AC-6 Least Privilege
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_outputs_encrypted_and_private
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Outputs Encrypted And Private
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Tags Are Encrypted and Private
  rationale: Data quality recommendation outputs in Google Cloud's Data Catalog can contain sensitive metadata that, if exposed, may lead to data breaches or unauthorized access. Encrypting these outputs and ensuring their privacy helps in mitigating risks such as data tampering, unauthorized exposure, and compliance violations with standards like GDPR and HIPAA.
  description: This rule verifies that data quality recommendation outputs in Data Catalog are encrypted using a customer-managed encryption key (CMEK) and are not publicly accessible. To ensure compliance, configure Data Catalog to use CMEK for all tags and verify IAM policies to restrict access to authorized users only. Remediation involves updating encryption settings and auditing permissions through the Google Cloud Console or CLI.
  references:
  - https://cloud.google.com/datacatalog/docs/how-to/encrypt-data
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/kms/docs
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_run_execution_role_lea_privilege
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Run Execution Role Lea Privilege
  scope: datacatalog.tag.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Quality Execution Role
  rationale: Assigning excessive privileges to roles can lead to unauthorized access and potential data breaches. Ensuring that the Data Quality Recommendation Run Execution Role adheres to the principle of least privilege is crucial to minimize risk exposure and meet compliance requirements such as PCI-DSS and ISO 27001. This practice not only protects sensitive data but also helps in maintaining audit trails and preventing privilege escalation attacks.
  description: This check verifies that the Data Quality Recommendation Run Execution Role in Google Cloud Data Catalog is configured with the least privilege necessary to perform its tasks. Specifically, it examines permissions assigned to the role to ensure they are limited to only what is required for executing data quality recommendations. To remediate, review and adjust the role's permissions in the IAM policy to remove any unnecessary access rights. Regular audits and updates to permissions are recommended to maintain security posture.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/datacatalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_run_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Run Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Quality Recommendation Logs in Data Catalog
  rationale: Enabling logs for Data Quality Recommendations in Data Catalog is crucial for ensuring auditability and traceability of data quality assessments. It aids in identifying potential misconfigurations or errors in data tagging processes, which can lead to inaccurate data insights. This practice supports compliance with data governance frameworks by providing evidence of data accuracy and integrity assessments.
  description: This rule verifies if logging is enabled for data quality recommendation runs in Google Cloud's Data Catalog. To ensure this, users should configure the Data Catalog to log all recommendation activities by enabling the appropriate logging settings in Stackdriver (or Cloud Logging). This can be done by setting up an audit log that captures 'Admin Read' and 'Data Write' actions. Failure to do so may result in the inability to trace data quality recommendations and could hinder troubleshooting and compliance efforts.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/logging
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/architecture/data-governance/data-quality
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_run_metadata_logging_enabled
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Run Metadata Logging Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Metadata Logging for Data Quality Recommendations
  rationale: Enabling metadata logging for data quality recommendations in Google Cloud Data Catalog ensures that all actions related to data quality are tracked and auditable. This is crucial for identifying data quality issues promptly, preventing data misuse, and ensuring compliance with regulatory requirements such as GDPR and CCPA. Failure to log metadata can lead to undetected data anomalies, impacting data integrity and potentially resulting in financial penalties.
  description: This rule checks whether metadata logging is enabled for data quality recommendation runs in GCP Data Catalog. To verify, ensure that audit logs are configured to capture all data quality recommendation events. This involves setting up a logging sink to export audit logs to a centralized logging solution. Remediation involves navigating to the Data Catalog in GCP Console, enabling audit logging, and configuring log sinks to capture necessary metadata.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/audit-logging
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/solutions/security-overview
- rule_id: gcp.datacatalog.tag.data_quality_recommendation_run_output_encrypted_private
  service: datacatalog
  resource: tag
  requirement: Data Quality Recommendation Run Output Encrypted Private
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Quality Outputs are Encrypted and Private
  rationale: Encrypting Data Quality Recommendation outputs in Google Cloud's Data Catalog ensures that sensitive information is protected against unauthorized access and potential data breaches. This mitigates the risk of data exposure and aligns with compliance requirements such as GDPR, PCI-DSS, and HIPAA. Proper encryption also helps in maintaining customer trust and safeguarding intellectual property.
  description: This rule checks that outputs generated from Data Quality Recommendations in Google Cloud's Data Catalog are encrypted and kept private. It ensures that the data is secured using Google-managed or customer-managed encryption keys. To verify compliance, review the Data Catalog settings to confirm that encryption is enabled and configured correctly. Remediation involves setting up and applying the appropriate encryption keys to the Data Catalog outputs to ensure they are encrypted at rest.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/encryptions
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.datacatalog.tag.data_quality_rule_definition_version_pinned
  service: datacatalog
  resource: tag
  requirement: Data Quality Rule Definition Version Pinned
  scope: datacatalog.tag.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Quality Rule Definition Version is Pinned
  rationale: Pinning data quality rule definitions is crucial to maintain consistency and reliability in data quality assessments. Without version control, changes could inadvertently affect data integrity, leading to potential data mishandling, and impact compliance with standards like GDPR or HIPAA. This control helps mitigate the risk of unauthorized or accidental changes that could compromise data protection and privacy.
  description: This rule checks whether the data quality rule definitions in Data Catalog tags are pinned to a specific version. To verify, inspect the Data Catalog configurations for tag templates and ensure each rule definition is associated with a fixed version. Remediate by updating rule definitions to specify a version, preventing unintended impacts from future updates.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-guides
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/data-catalog/docs/concepts
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.tag.data_quality_rule_parameters_no_plaintext_secrets
  service: datacatalog
  resource: tag
  requirement: Data Quality Rule Parameters No Plaintext Secrets
  scope: datacatalog.tag.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Prevent Plaintext Secrets in Data Quality Rule Parameters
  rationale: Storing secrets in plaintext within Data Catalog tags can expose sensitive information to unauthorized users, leading to data breaches and compliance violations. Protecting secrets is crucial for maintaining data confidentiality and integrity, especially in highly regulated environments where data privacy laws like GDPR and HIPAA apply.
  description: This rule checks that Data Quality Rule Parameters in Data Catalog tags do not contain plaintext secrets, such as API keys or passwords. It ensures that any sensitive information is encrypted or stored securely using Google Cloud's Secret Manager. To remediate, verify the tags do not include plaintext secrets and migrate any such data to a secure storage solution, updating references in your applications accordingly.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-guides
  - https://cloud.google.com/secret-manager/docs
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://gdpr-info.eu/
  - https://www.hipaajournal.com/hipaa-security-rule/
- rule_id: gcp.datacatalog.tag.data_quality_rule_source_trusted
  service: datacatalog
  resource: tag
  requirement: Data Quality Rule Source Trusted
  scope: datacatalog.tag.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Trusted Sources for Data Quality Rules in Data Catalog
  rationale: Ensuring that data quality rules in Data Catalog are sourced from trusted entities is crucial to maintain data integrity and prevent unauthorized data manipulation. Untrusted sources can introduce erroneous data quality rules, leading to data misinterpretation and potential compliance breaches with standards like GDPR or CCPA. By validating the source of these rules, organizations can mitigate risks associated with data breaches and maintain trust with their stakeholders.
  description: This rule checks that all data quality rules in Google Cloud's Data Catalog are sourced from verified, trusted sources. It involves verifying the source identity and ensuring their permissions align with the least privilege principle. To remediate, review and update IAM policies to restrict rule creation to trusted service accounts and regularly audit rule sources. Use Cloud Audit Logs to track changes and identify unauthorized modifications.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/overview/whitepaper
- rule_id: gcp.datacatalog.tag.datalake_data_quality_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Datalake Data Quality Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Quality Logs for Datalake in Data Catalog
  rationale: Enabling data quality logging in a datalake environment helps identify inconsistencies, errors, and non-compliance with data standards, reducing the risk of poor data quality impacting business intelligence and decision-making processes. Additionally, it aids in compliance with data governance policies by providing an audit trail of data quality checks.
  description: This rule checks whether data quality logging is enabled for datalake tags in Google Cloud Data Catalog. To verify, ensure that the Data Catalog service is configured to log data quality metrics for all tags associated with datalake resources. Remediation involves accessing the Data Catalog settings in the GCP Console and enabling logging for data quality metrics under the specific tags used by the datalake. This enhances visibility into data integrity and supports data governance and compliance efforts.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag.datalake_data_quality_output_location_private_and_encrypted
  service: datacatalog
  resource: tag
  requirement: Datalake Data Quality Output Location Private And Encrypted
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Datalake Outputs are Private & Encrypted at Rest
  rationale: Securing datalake outputs with encryption at rest and private access mitigates risks of unauthorized data exposure and data breaches. This practice ensures that sensitive data is protected against internal and external threats, aligns with compliance mandates like GDPR and HIPAA, and maintains customer trust by safeguarding personal information.
  description: This rule checks that all data quality outputs from datalakes in Data Catalog are stored in locations that enforce encryption at rest and are not publicly accessible. Verify that the storage buckets used have uniform access configured to private and utilize Cloud KMS for encryption. If a bucket is public or lacks encryption, update its IAM policy to restrict access and apply a valid KMS key for data encryption.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/architecture/encryption-and-key-management
- rule_id: gcp.datacatalog.tag.datalake_data_quality_role_least_privilege
  service: datacatalog
  resource: tag
  requirement: Datalake Data Quality Role Least Privilege
  scope: datacatalog.tag.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Datalake Data Quality Roles in Data Catalog
  rationale: Implementing least privilege for datalake roles mitigates the risk of unauthorized access and data breaches, protecting sensitive data and maintaining data integrity. Over-privileged roles can lead to inadvertent data exposure, non-compliance with regulations such as GDPR and CCPA, and potential financial and reputational damage.
  description: This rule checks if roles assigned to manage datalake data quality in Data Catalog adhere to the principle of least privilege. Specifically, it verifies that roles are configured to only allow necessary permissions for data quality tasks without granting excessive access. To remediate, review and update IAM policies to ensure roles are strictly aligned with job functions, using predefined roles where possible and custom roles when necessary. Regular audits and adjustments should be conducted to maintain compliance with this principle.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.datacatalog.tag.lineage_data_quality_logs_enabled
  service: datacatalog
  resource: tag
  requirement: Lineage Data Quality Logs Enabled
  scope: datacatalog.tag.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Data Catalog Lineage Data Quality Logs
  rationale: Enabling lineage data quality logs in GCP's Data Catalog helps organizations maintain visibility into data asset changes and ensure data integrity. This is crucial for identifying potential data quality issues and unauthorized access attempts, thereby reducing the risk of data breaches and non-compliance with regulatory standards such as GDPR and HIPAA.
  description: This rule checks if lineage data quality logging is enabled for Data Catalog tags in GCP. When enabled, it captures detailed logs about data lineage and quality, which can be used for auditing and debugging purposes. Administrators can verify this setting through the GCP Console by navigating to the Data Catalog settings and ensuring 'Lineage Data Quality Logs' is turned on. To enable, go to the specific Data Catalog entry, access the settings, and activate logging for lineage data quality.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.datacatalog.tag.lineage_data_quality_output_location_private_and_encrypted
  service: datacatalog
  resource: tag
  requirement: Lineage Data Quality Output Location Private And Encrypted
  scope: datacatalog.tag.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Data Quality Output is Private and Encrypted
  rationale: Storing lineage data quality outputs insecurely can expose sensitive information, leading to data breaches and non-compliance with regulations like GDPR and CCPA. Encrypting this data at rest and ensuring its storage location is private mitigates risks of unauthorized access and data leakage, protecting organizational and customer data integrity.
  description: This rule checks that the storage location for lineage data quality outputs in Google Cloud's Data Catalog is both private and encrypted. It ensures that Cloud Storage buckets used for this purpose have appropriate IAM policies restricting public access and that bucket-level encryption is enabled. Verification involves reviewing bucket settings in the Cloud Console or using command-line tools. Remediation steps include configuring IAM policies to restrict access and enabling encryption either by using Google-managed keys or customer-managed keys.
  references:
  - https://cloud.google.com/datacatalog/docs
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.datacatalog.tag.lineage_data_quality_role_least_privilege
  service: datacatalog
  resource: tag
  requirement: Lineage Data Quality Role Least Privilege
  scope: datacatalog.tag.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Lineage Data Quality Role
  rationale: Implementing least privilege for roles in GCP Data Catalog is crucial to minimize the risk of unauthorized access and potential data breaches. Over-permissioned roles can lead to data leaks, non-compliance with regulations like GDPR and HIPAA, and increased attack surface for malicious insiders. Ensuring roles are tightly scoped helps protect sensitive data and maintains trust with customers and partners.
  description: This rule checks that the roles assigned to users or service accounts for managing lineage data quality in Data Catalog adhere to the principle of least privilege. It ensures that permissions are limited to only those necessary for their job functions. Administrators should regularly audit IAM policies associated with Data Catalog tags and adjust them to prevent excessive permissions, thus reducing the risk of unauthorized actions. Remediation involves modifying IAM policies to remove unnecessary permissions, using predefined roles where possible, and employing custom roles only when absolutely necessary.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/granting-access
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 5
  - https://cloud.google.com/iam/docs/best-practices
- rule_id: gcp.datacatalog.tag_template.data_catalog_classifier_definition_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Classifier Definition Version Pinned
  scope: datacatalog.tag_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Data Catalog Classifier Version is Pinned
  rationale: Pinning the Data Catalog classifier definition version is crucial to maintain consistent and predictable metadata tagging across data assets. It helps mitigate risks associated with unexpected changes that could result in misclassification of data, potentially leading to unauthorized data exposure or compliance violations. This control supports maintaining data integrity and compliance with regulations that require strict data classification protocols.
  description: This rule checks if the Data Catalog classifier definition version is pinned in tag templates. A pinned version ensures that the metadata tagging behavior remains consistent, preventing potential breakdowns in data governance strategies. To verify, review the tag template configurations in the Data Catalog and ensure the classifier version is explicitly specified. If not pinned, update the tag template to a specific version, documented and tested, to prevent discrepancies during metadata tagging operations.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/tag-templates
  - https://cloud.google.com/security/compliance/cis#section-5
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/data-catalog/docs/concepts
  - https://cloud.google.com/architecture/framework/security
- rule_id: gcp.datacatalog.tag_template.data_catalog_classifier_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Classifier RBAC Least Privilege
  scope: datacatalog.tag_template.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Tag Templates
  rationale: Implementing least privilege access for Data Catalog Tag Templates minimizes the risk of unauthorized access and data breaches. It helps protect sensitive metadata by ensuring only necessary permissions are granted, reducing the attack surface and aiding compliance with standards like ISO 27001 and PCI-DSS, which mandate strict access controls.
  description: This rule checks that permissions on Data Catalog Tag Templates are configured to follow the principle of least privilege. Verify that roles assigned to users or service accounts have only the minimum necessary permissions. Regularly review IAM policies to ensure they do not grant excessive access. To remediate, audit current role bindings and adjust IAM policies to align with least privilege principles, removing unnecessary permissions.
  references:
  - https://cloud.google.com/datacatalog/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.datacatalog.tag_template.data_catalog_classifier_source_trusted
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Classifier Source Trusted
  scope: datacatalog.tag_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Data Catalog Classifier Sources Are Trusted
  rationale: Trusting only verified data sources in Data Catalog prevents unauthorized or malicious data from being classified and used by your organization, reducing the risk of data breaches and ensuring data integrity. This is vital for meeting compliance requirements such as GDPR and CCPA, which mandate secure handling of sensitive data.
  description: This rule checks whether the sources of data classified within Google Cloud Data Catalog are from trusted origins. Ensure that tag templates are configured to only include data from authenticated and verified sources. Verification can be done by setting up appropriate IAM permissions and using Google Cloud's identity and access management to restrict access to trusted users and services. Regularly audit and update your IAM policies to reflect changes in your data landscape.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.datacatalog.tag_template.data_catalog_ruleset_encrypted_at_rest
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Ruleset Encrypted At Rest
  scope: datacatalog.tag_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Ruleset Encryption at Rest
  rationale: Encrypting Data Catalog rulesets at rest protects sensitive metadata from unauthorized access and helps mitigate the risk of data breaches. This security measure is critical for maintaining data confidentiality and integrity, especially in sectors subject to regulatory compliance such as finance, healthcare, and government. Failure to apply encryption could lead to significant financial penalties and reputational damage.
  description: This rule verifies that Data Catalog tag templates are encrypted at rest using Google-managed or customer-managed encryption keys. To confirm compliance, check that the Data Catalog API is configured to use encryption for tag templates. Remediation involves ensuring that all tag templates are stored in a project where encryption at rest is enforced, utilizing Google Cloud Key Management Service (KMS) if necessary.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.tag_template.data_catalog_ruleset_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Ruleset RBAC Least Privilege
  scope: datacatalog.tag_template.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Tag Template Roles
  rationale: Implementing least privilege in Data Catalog tag templates minimizes the risk of unauthorized access, data breaches, and non-compliance with regulations such as GDPR and HIPAA. By restricting permissions to only those necessary for specific roles, organizations reduce their attack surface and maintain better control over sensitive metadata.
  description: This rule checks that roles assigned to Data Catalog tag templates adhere to the principle of least privilege. Review permissions granted to users and service accounts to ensure they are limited to necessary actions such as viewing or editing tags. Remediate by adjusting IAM policies to restrict roles to essential operations only, using predefined roles where possible. Verification can be done through the GCP Console or gcloud CLI by examining IAM policy bindings on tag templates.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag_template.data_catalog_ruleset_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Data Catalog Ruleset Version Pinned
  scope: datacatalog.tag_template.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Data Catalog Tag Template Ruleset Version is Pinned
  rationale: Pinning the version of a Data Catalog ruleset ensures consistency and reliability in data classification and governance processes. Unpinned versions may lead to unexpected behavior and potential security risks as changes in newer versions could introduce vulnerabilities or non-compliance with data governance policies.
  description: This rule checks whether the Data Catalog tag templates have their ruleset versions explicitly pinned. To verify, review the tag templates in the Data Catalog to ensure each has a specific ruleset version defined. Remediation involves updating the tag templates to specify a version number, thereby preventing automatic upgrades that could disrupt data governance operations.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/tag-templates
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/data-catalog/docs/how-to/policy-tag-manager#manage_policies
- rule_id: gcp.datacatalog.tag_template.data_quality_ruleset_encrypted_at_rest
  service: datacatalog
  resource: tag_template
  requirement: Data Quality Ruleset Encrypted At Rest
  scope: datacatalog.tag_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Quality Ruleset is Encrypted at Rest
  rationale: Encrypting data at rest is crucial for protecting sensitive information from unauthorized access and potential data breaches. This aligns with compliance requirements such as GDPR and HIPAA, which mandate the protection of personal and sensitive data. Failure to encrypt data at rest can expose the organization to legal liabilities and reputational damage.
  description: This rule checks whether all Data Quality Rulesets within Google Cloud's Data Catalog are encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that encryption settings are configured in the tag template properties of the Data Catalog. Remediation involves applying appropriate encryption policies, such as enabling Google Cloud KMS to manage encryption keys, to secure data at rest.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/data-catalog/docs/how-to
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 5.3
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.hipaajournal.com/hipaa-encryption-requirements
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag_template.data_quality_ruleset_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Data Quality Ruleset RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Data Catalog Tag Templates
  rationale: Implementing least privilege for Data Catalog tag templates minimizes potential unauthorized access and data exposure risks. This is crucial for maintaining data integrity and confidentiality, especially in regulated environments where compliance with standards like HIPAA or PCI-DSS is required. Misconfigured permissions could lead to data breaches or compliance violations, impacting both financial and reputational aspects of the business.
  description: This rule checks that IAM roles granted on Data Catalog tag templates adhere to the principle of least privilege. It ensures that only necessary permissions are delegated, reducing the risk of unauthorized access. Verify that roles assigned have the minimal set of permissions needed for their function by auditing IAM policies associated with tag templates. To remediate, adjust the roles to limit permissions to only those that are essential for the task, and regularly review IAM policy assignments.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/tag-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.datacatalog.tag_template.data_quality_ruleset_version_immutable
  service: datacatalog
  resource: tag_template
  requirement: Data Quality Ruleset Version Immutable
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Quality Ruleset Version is Immutable
  rationale: Immutability of data quality ruleset versions prevents unauthorized changes that could compromise data integrity and privacy. It ensures that once a ruleset is applied, it remains consistent, reducing the risk of accidental or malicious alterations. This integrity is crucial for maintaining trust with stakeholders and complying with regulations such as GDPR and HIPAA.
  description: This rule checks that the data quality ruleset version within Google Cloud Data Catalog tag templates is immutable. To verify, ensure that once a ruleset version is created, it cannot be edited or deleted. Remediation involves configuring the tag template settings to lock the ruleset version upon creation. This can be done through the GCP Console or using the gcloud CLI, ensuring only authorized personnel can manage these settings.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to/tag-templates
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.datacatalog.tag_template.datalake_classifier_definition_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Datalake Classifier Definition Version Pinned
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Datalake Classifier Definition Version is Pinned
  rationale: Pinning the version of Datalake classifier definitions is crucial to maintain consistent data classification and prevent unauthorized or unintended changes that could lead to data misclassification. This helps in mitigating risks of data breaches and ensures compliance with data protection regulations such as GDPR and CCPA, which require stringent data management controls.
  description: This rule checks whether the Datalake classifier definitions within Google Cloud's Data Catalog have their versions pinned to avoid unexpected updates that might compromise data classification integrity. To verify this, check the tag template configurations in the Data Catalog and ensure that the classifier version is set to a specific version rather than 'latest'. Remediation involves explicitly setting the classifier definition version in the tag template configuration.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-guides
  - https://cloud.google.com/security/compliance/cis-google-cloud-computing-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/privacy
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datacatalog.tag_template.datalake_classifier_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Datalake Classifier RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Datalake Classifier Tag Templates
  rationale: Implementing least privilege access for the Datalake Classifier in Data Catalog helps minimize the risk of unauthorized data access and potential data breaches. Excessive permissions can lead to accidental or intentional misuse of sensitive data, impacting business operations and violating compliance mandates such as GDPR and CCPA.
  description: This rule checks that only necessary permissions are granted to users or groups for accessing tag templates associated with the Datalake Classifier in the Data Catalog. It verifies that permissions align with least privilege principles, specifically focusing on reducing over-privileged accounts. Remediation involves reviewing and adjusting IAM policies to ensure that only essential roles are assigned, utilizing predefined roles where possible instead of overly permissive custom roles.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/policy-tags
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag_template.datalake_classifier_source_trusted
  service: datacatalog
  resource: tag_template
  requirement: Datalake Classifier Source Trusted
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Trusted Source for Datalake Classifier Tags
  rationale: Ensuring that only trusted sources can apply tags in Data Catalog helps prevent unauthorized data classification, which can lead to data leakage, misclassification, and potential breaches. Mismanagement of data classification tags can result in non-compliance with regulations like GDPR and CCPA, exposing the organization to legal and financial penalties.
  description: This rule verifies that only trusted entities are allowed to create or modify tag templates in Google Cloud Data Catalog, specifically for Datalake classifiers. Check that IAM policies for tag templates strictly limit permissions to verified sources. Remediation involves auditing current IAM roles and permissions, then revoking access from untrusted entities and ensuring that only secured service accounts or authenticated users have the necessary roles to apply tags.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/tag-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0 - 6.7
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.datacatalog.tag_template.lineage_classifier_definition_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Lineage Classifier Definition Version Pinned
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Classifier Definition Version is Pinned
  rationale: Pinning the lineage classifier definition version in Data Catalog ensures consistency and integrity of data classification processes. Unpinned versions may lead to unintentional updates, potentially exposing sensitive data or causing compliance violations. This practice supports data governance and mitigates risks associated with unauthorized data access.
  description: This rule verifies that the lineage classifier definition version in Data Catalog's tag templates is explicitly pinned. To check, review the tag template configurations to ensure a fixed version number is specified. If not pinned, update the tag template settings in the Google Cloud Console or via gcloud command-line to include a version identifier. This prevents automatic updates to newer, untested versions which could introduce vulnerabilities.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/data-catalog/docs/reference/rest/v1/projects.locations.tagTemplates
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/data-catalog/docs/how-to
- rule_id: gcp.datacatalog.tag_template.lineage_classifier_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Lineage Classifier RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Tag Template Access
  rationale: Implementing least privilege access for Data Catalog tag templates is crucial to minimize the risk of unauthorized data modification or exposure. Improper access configurations can lead to accidental data leaks or insider threats, impacting compliance with regulations like GDPR and CCPA and potentially causing significant business reputational and financial damage.
  description: This rule checks if IAM roles assigned to Data Catalog tag templates adhere to the principle of least privilege. Ensure that only necessary permissions are granted to users and service accounts, limiting access to those required for specific tasks. To verify compliance, review IAM policies associated with each tag template and adjust roles to restrict permissions. Remediation involves using the Cloud Console or gcloud CLI to update IAM policies, ensuring only essential roles like 'roles/datacatalog.viewer' or 'roles/datacatalog.editor' are assigned as per operational needs.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/overview/whitepapers
- rule_id: gcp.datacatalog.tag_template.lineage_classifier_source_trusted
  service: datacatalog
  resource: tag_template
  requirement: Lineage Classifier Source Trusted
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Trusted Sources for Lineage Classifier in Data Catalog
  rationale: Using untrusted sources for lineage classification can lead to inaccurate data governance, impacting decision-making and increasing the risk of data breaches. Trustworthy sources are essential to maintain data integrity and comply with regulations such as GDPR and CCPA, which mandate stringent data protection and privacy controls.
  description: This rule verifies that all sources used for lineage classification in Google Cloud's Data Catalog are from trusted origins. It checks the configuration of tag templates to ensure that only approved and verified sources are used. To remediate, review the list of sources configured in the tag template and update them to include only those that are verified and compliant with your organization's data governance policies.
  references:
  - https://cloud.google.com/data-catalog/docs/how-to-entries
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-compliance-requirements/
- rule_id: gcp.datacatalog.tag_template.lineage_rule_definition_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Lineage Rule Definition Version Pinned
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Rule Version in Data Catalog is Pinned
  rationale: Pinning the version of a lineage rule definition is crucial as it ensures that the rule's behavior remains consistent over time, preventing unexpected changes that could lead to data misclassification or unauthorized data access. This practice mitigates the risk of compliance violations by maintaining data integrity and alignment with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks if the version of lineage rule definitions in Google Cloud Data Catalog tag templates is pinned. A pinned version prevents unintended updates which could compromise data classification accuracy. To verify, review your tag template settings within Data Catalog and ensure each lineage rule has a set version. Remediation involves specifying a fixed version number for each lineage rule definition in tag templates to maintain consistent data processing logic.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/tag-templates
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.datacatalog.tag_template.lineage_rule_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Lineage Rule RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege on Data Catalog Tag Template Lineage
  rationale: Applying the principle of least privilege to Data Catalog tag templates minimizes the risk of unauthorized data access or modification, protecting sensitive metadata associated with Google Cloud resources. This is critical for maintaining data integrity and confidentiality, especially in compliance with regulations such as GDPR and HIPAA, which mandate strict access controls.
  description: This rule checks whether IAM roles assigned to Data Catalog tag templates adhere to the principle of least privilege, ensuring only necessary permissions are granted. Verify that users and service accounts have the minimum required roles, such as 'roles/datacatalog.tagTemplateUser' instead of broader roles like 'roles/datacatalog.admin'. Remediation involves auditing current IAM policies, removing excessive permissions, and implementing least privilege access through role adjustments or custom roles as needed.
  references:
  - https://cloud.google.com/datacatalog/docs/how-to/tag-templates
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.datacatalog.tag_template.lineage_rule_source_trusted
  service: datacatalog
  resource: tag_template
  requirement: Lineage Rule Source Trusted
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Rule Source in Data Catalog is Trusted
  rationale: Ensuring the source of lineage rules within Data Catalog is trusted helps protect against unauthorized data access and manipulation. This is crucial for maintaining data integrity and privacy, as well as for compliance with regulations such as GDPR and CCPA that require stringent data protection measures. Untrusted sources may introduce security vulnerabilities that could lead to data breaches and financial penalties.
  description: This rule checks whether the sources of lineage rules in Data Catalog's tag templates are from trusted origins. To verify, review the lineage rules associated with your tag templates and confirm they originate from known, secure sources. Remediation involves configuring access controls to restrict the creation of lineage rules to trusted entities only, and auditing existing rules for compliance with security policies.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.datacatalog.tag_template.lineage_ruleset_encrypted_at_rest
  service: datacatalog
  resource: tag_template
  requirement: Lineage Ruleset Encrypted At Rest
  scope: datacatalog.tag_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Catalog Lineage Ruleset is Encrypted at Rest
  rationale: Encrypting data at rest is critical to protect sensitive information from unauthorized access and ensure compliance with data protection regulations. Without encryption, data stored in the cloud is vulnerable to breaches, potentially leading to financial loss, reputational damage, and regulatory penalties.
  description: This rule checks that all Lineage Rulesets associated with Data Catalog Tag Templates are encrypted at rest using Google-managed or customer-managed keys. To verify, ensure that the Encryption Configuration in the Data Catalog settings specifies a valid key. Remediation involves configuring the Data Catalog to use Cloud KMS keys for encrypting data at rest, providing an additional layer of security.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/security/encryption-at-rest/
- rule_id: gcp.datacatalog.tag_template.lineage_ruleset_rbac_least_privilege
  service: datacatalog
  resource: tag_template
  requirement: Lineage Ruleset RBAC Least Privilege
  scope: datacatalog.tag_template.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Catalog Tag Template Lineage
  rationale: Implementing least privilege for lineage rulesets in GCP Data Catalog minimizes the risk of unauthorized access and data breaches. Misconfigured permissions can lead to data leaks, violating compliance requirements such as GDPR and CCPA, and may result in financial and reputational damage. Proper access control is critical for maintaining data integrity and confidentiality.
  description: This rule checks that access to Data Catalog tag templates with lineage rulesets is restricted to only those identities that require it. Verify that IAM roles assigned to users or service accounts have the least privileges necessary. Remediate by auditing current permissions, removing unnecessary roles, and using predefined roles over primitive roles whenever possible. Regularly review and adjust access rights to adapt to organizational changes.
  references:
  - https://cloud.google.com/data-catalog/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance
- rule_id: gcp.datacatalog.tag_template.lineage_ruleset_version_pinned
  service: datacatalog
  resource: tag_template
  requirement: Lineage Ruleset Version Pinned
  scope: datacatalog.tag_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Lineage Ruleset Version is Pinned in Data Catalog
  rationale: Pinning the lineage ruleset version in Google Cloud Data Catalog ensures that the data classification and lineage tracking use a consistent version of rulesets. This prevents unexpected changes that could impact data security and compliance, reducing the risk of unauthorized data alterations and ensuring adherence to data governance policies.
  description: This rule checks if the lineage ruleset version in the Data Catalog tag template is pinned. A pinned version ensures that lineage tracking and data classification remain consistent and predictable. To verify, review the Data Catalog tag templates for lineage ruleset configurations and ensure a specific version is set. If not pinned, update the tag template configuration to specify a version number for the lineage ruleset, ensuring stability and compliance with data governance practices.
  references:
  - https://cloud.google.com/data-catalog/docs/concepts
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dataflow.job.data_pipeline_allowed_values_defined_where_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Allowed Values Defined Where Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Uses Allowed Parameter Values
  rationale: Defining allowed parameter values for Dataflow pipelines is crucial for maintaining data integrity and preventing unauthorized data processing activities. Without clear restrictions, there's an increased risk of data breaches or non-compliance with regulatory standards such as GDPR or HIPAA, especially when dealing with sensitive data. This practice helps organizations maintain control over data processing operations and safeguards against inadvertent data exposure.
  description: This rule checks if Dataflow jobs have clearly defined allowed values for parameters, ensuring that only approved configurations are used. To verify compliance, review the Dataflow job configurations and confirm that parameter values are restricted to pre-approved ranges or sets. If discrepancies are found, update the job configurations to enforce allowed values, using GCP's parameter validation options. This ensures consistent and secure data processing operations.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations#data-management
  - https://cloud.google.com/dataflow/docs/guides/setting-pipeline-options
- rule_id: gcp.dataflow.job.data_pipeline_attributes_no_plaintext_secrets
  service: dataflow
  resource: job
  requirement: Data Pipeline Attributes No Plaintext Secrets
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Prevent Plaintext Secrets in Dataflow Job Attributes
  rationale: Storing secrets in plaintext within Dataflow job attributes poses significant security risks, including unauthorized access and data breaches. This is critical for protecting sensitive data and maintaining compliance with regulations like GDPR and HIPAA, which mandate strict data protection measures. Ensuring secrets are encrypted mitigates the risk of exposure and potential financial and reputational damage.
  description: This rule checks that no secrets or sensitive information are stored in plaintext within the attributes of a Google Cloud Dataflow job. Verify that secrets are not included directly in job configurations or command-line arguments. Instead, use Google Cloud Secret Manager to securely reference secrets. Remediation involves updating job configurations to remove any plaintext secrets and replacing them with secure references.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs
  - CIS GCP Benchmark v1.3.0 - 5.1 Ensure that Cloud Dataflow jobs do not contain sensitive information
  - 'NIST SP 800-53: AC-3 Access Enforcement'
  - 'PCI-DSS Requirement 3: Protect stored cardholder data'
  - ISO/IEC 27001:2013 A.8.2 Information Classification
- rule_id: gcp.dataflow.job.data_pipeline_binding_only_expected_params_bound
  service: dataflow
  resource: job
  requirement: Data Pipeline Binding Only Expected Params Bound
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Parameters Are Properly Bound
  rationale: Ensuring that only expected parameters are bound in Dataflow jobs is crucial to prevent unauthorized data access and manipulation. Improper parameter binding can lead to data leakage, unauthorized data manipulation, and compliance breaches, affecting the organization's data integrity and confidentiality. This is particularly important for meeting regulatory requirements such as GDPR and HIPAA that mandate strict data protection controls.
  description: This rule checks that Dataflow jobs only bind expected parameters, which helps prevent injection attacks and unauthorized data access. Verify that Dataflow job configurations do not include extraneous parameters and that all parameters are explicitly defined in job templates. Remediation involves auditing the job configurations and removing any unexpected or unnecessary parameters, ensuring that the job adheres to the security policies defined by your organization.
  references:
  - https://cloud.google.com/dataflow/docs/guides/dataflow-security-best-practices
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.dataflow.job.data_pipeline_binding_secret_refs_resolved_at_runtime
  service: dataflow
  resource: job
  requirement: Data Pipeline Binding Secret Refs Resolved At Runtime
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Dataflow Secret Refs Resolved at Runtime for Security
  rationale: Resolving secret references at runtime minimizes the risk of exposing sensitive information, thereby protecting data integrity and confidentiality. This practice is crucial in mitigating unauthorized access and ensuring compliance with data protection regulations such as GDPR and HIPAA, which mandate rigorous control over sensitive data handling.
  description: This rule checks that Dataflow jobs are configured to resolve secret references at runtime, ensuring that secrets are not hardcoded or stored insecurely. Verify that your Dataflow pipeline uses secret managers or other secure methods to retrieve secrets dynamically during execution. To remediate, configure your Dataflow job to use the GCP Secret Manager API or equivalent to fetch secrets securely at runtime, ensuring these configurations are not exposed in code.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.dataflow.job.data_pipeline_binding_sensitive_params_not_logged
  service: dataflow
  resource: job
  requirement: Data Pipeline Binding Sensitive Params Not Logged
  scope: dataflow.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Prevent Logging of Sensitive Params in Dataflow Jobs
  rationale: Logging sensitive parameters in Dataflow jobs can expose confidential information, leading to potential data breaches and non-compliance with regulations such as GDPR and HIPAA. This rule helps mitigate the risk of unauthorized access to sensitive data, protecting organizational assets and maintaining customer trust.
  description: This rule checks that Dataflow jobs do not log sensitive parameters, which can occur if logging levels are misconfigured. To verify, ensure that logging configurations exclude sensitive data by checking job settings in the GCP Console or using the gcloud command-line tool. Remediation involves adjusting logging settings to mask or exclude sensitive information from being logged.
  references:
  - https://cloud.google.com/dataflow/docs/guides/logging
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/logging/docs/audit
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_constraint_allowlist_defined_when_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Constraint Allowlist Defined When Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipeline Constraints Are Allowlisted
  rationale: Defining an allowlist for data pipeline constraints in GCP Dataflow jobs helps mitigate risks of unauthorized data processing and potential data breaches. It ensures that only approved pipelines can be executed, aligning with security best practices and compliance requirements such as GDPR and HIPAA, which mandate strict data processing controls.
  description: This rule checks whether a Dataflow job has an allowlist defined for pipeline constraints. Without this, there is a risk of executing unauthorized or potentially harmful data processing tasks. Verify the allowlist configuration in the job settings and update it to include only trusted data pipelines. This can be done through the GCP Console or by using gcloud commands to update the job's parameter settings.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://www.iso.org/standard/73906.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/dataflow/docs/guides/using-gcloud
- rule_id: gcp.dataflow.job.data_pipeline_constraint_max_length_defined_when_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Constraint Max Length Defined When Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipeline Max Length is Defined
  rationale: Defining a maximum length for data pipelines in Dataflow jobs helps prevent resource exhaustion and potential service disruptions. This is crucial for maintaining the availability and reliability of services, as well as safeguarding sensitive data from unintentional exposure due to overly complex or long-running processes. Compliance with data protection regulations often requires such constraints to manage and mitigate operational risks.
  description: This rule checks if the maximum length for data pipelines in Dataflow jobs is specified where applicable. Configuring this setting involves defining constraints in the pipeline's configuration files or using the Dataflow API. To verify, review the pipeline configuration in the Dataflow console or via the GCP CLI, ensuring a max length is set. Remediation involves updating the pipeline configuration to include a max length constraint, thereby optimizing resource usage and enhancing security posture.
  references:
  - https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/deployment-best-practices
- rule_id: gcp.dataflow.job.data_pipeline_constraint_min_length_defined_when_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Constraint Min Length Defined When Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Min Length Constraint Is Defined
  rationale: Defining a minimum length constraint for data pipelines in Dataflow is crucial to prevent data breaches by ensuring that data fields meet necessary security and integrity requirements. This minimizes the risk of processing incomplete or malformed data, which can lead to data leakage or processing errors that compromise data privacy. Compliance with industry standards such as GDPR and PCI-DSS often demands stringent data integrity checks.
  description: This rule verifies whether a minimum length constraint is defined for applicable data pipelines within Dataflow jobs. To check compliance, inspect the pipeline configurations to ensure that all relevant data fields have an explicitly set minimum length constraint. If not configured, modify the Dataflow job settings to include these constraints, ensuring that sensitive data is processed with appropriate length validations, enhancing data integrity and security.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/dataflow/docs/guides/dataflow-security
- rule_id: gcp.dataflow.job.data_pipeline_constraint_pattern_defined_when_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Constraint Pattern Defined When Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Constraint Patterns for Data Pipelines
  rationale: Defining constraint patterns for Dataflow jobs is crucial for maintaining data integrity and security. Without these constraints, data pipelines may inadvertently process or expose sensitive data, increasing the risk of data breaches and non-compliance with regulations such as GDPR and HIPAA. By enforcing constraint patterns, organizations can ensure that their data processing activities are within the defined security boundaries, reducing the risk of unauthorized data access and potential legal penalties.
  description: This check verifies that Dataflow jobs have defined constraint patterns when applicable, ensuring that data pipelines adhere to organizational security policies. To define these constraints, configure job parameters to include security specifications, such as encryption at rest using Cloud KMS. Remediation involves modifying the Dataflow job configuration to include these constraints and ensuring they are enforced during job execution. Verify compliance by reviewing job settings in the GCP Console or using gcloud commands to inspect job configurations.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.dataflow.job.data_pipeline_default_not_sensitive
  service: dataflow
  resource: job
  requirement: Data Pipeline Default Not Sensitive
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipelines Are Not Using Default Encryption
  rationale: Using default encryption for Dataflow jobs can expose sensitive data to unauthorized access and potential breaches if the default keys are compromised. Encrypting data with customer-managed keys enhances control over data privacy and compliance with regulations such as GDPR and CCPA, which require stringent data protection measures.
  description: This rule checks if Dataflow jobs use customer-managed encryption keys instead of the default Google-managed keys for encrypting data at rest. To verify, review the Dataflow job configurations for the `kmsKeyName` property. Remediation involves configuring Dataflow jobs to use a specific customer-managed key by specifying its `kmsKeyName`, ensuring enhanced data security and compliance with regulatory requirements.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/security/compliance/gdpr/
  - CIS GCP Foundation Benchmark v1.3.0 - Section 7.10
  - NIST SP 800-57 Part 1
  - ISO/IEC 27001:2013
- rule_id: gcp.dataflow.job.data_pipeline_definition_signed_and_verified
  service: dataflow
  resource: job
  requirement: Data Pipeline Definition Signed And Verified
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Definitions are Signed and Verified
  rationale: Signing and verifying Dataflow pipeline definitions ensure that the jobs are not tampered with and originate from a trusted source. This practice mitigates risks such as unauthorized data manipulation, phishing attacks, and ensures compliance with data protection regulations by maintaining data integrity and authenticity.
  description: This rule checks that all Dataflow job definitions are digitally signed and verified before execution. Organizations should implement a process where pipeline definitions are signed using a GCP-managed key and verified to prevent the execution of malicious code. Remediation involves enabling signing and verification in the Dataflow job submission process, and using Cloud KMS to manage and store cryptographic keys securely.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/kms/docs/best-practices
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_definition_storage_encrypted
  service: dataflow
  resource: job
  requirement: Data Pipeline Definition Storage Encrypted
  scope: dataflow.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipeline Definitions Are Encrypted at Rest
  rationale: Encrypting Dataflow job pipeline definitions at rest safeguards sensitive configuration data from unauthorized access and potential data breaches. This is critical to maintain data confidentiality and integrity, especially when handling sensitive information that may be subject to regulatory compliance such as PCI-DSS or HIPAA. Inadequate encryption can lead to exposure of operational details that could be exploited by malicious actors.
  description: This check verifies that all Dataflow job pipeline definitions are stored with encryption at rest enabled. By default, Google Cloud encrypts data at rest, but it is essential to confirm that customer-managed encryption keys (CMEK) are used for enhanced security. To verify, inspect the Dataflow job settings to ensure that the 'kmsKeyName' parameter is set appropriately. Remediation involves configuring Dataflow jobs to use CMEK by specifying a valid Cloud KMS key, ensuring the key is managed securely and access is restricted to authorized personnel.
  references:
  - https://cloud.google.com/dataflow/docs/guides/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dataflow.job.data_pipeline_definition_storage_private
  service: dataflow
  resource: job
  requirement: Data Pipeline Definition Storage Private
  scope: dataflow.job.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Dataflow Pipeline Definitions are Stored Privately
  rationale: Storing Dataflow pipeline definitions in publicly accessible locations can expose sensitive configuration details and business logic, increasing the risk of data breaches and unauthorized access. Keeping these definitions private helps mitigate potential attacks and aligns with compliance requirements for data protection such as GDPR and HIPAA.
  description: This rule verifies that Dataflow job pipeline definitions are stored in private locations, preventing unauthorized access. To ensure privacy, configure Google Cloud Storage buckets containing pipeline definitions with IAM policies that restrict access to only necessary service accounts and users. Regularly audit bucket permissions to maintain security posture. Remediation involves adjusting bucket permissions via the Google Cloud Console or using gcloud commands to enforce private access.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dataflow.job.data_pipeline_definition_version_immutability_enforced
  service: dataflow
  resource: job
  requirement: Data Pipeline Definition Version Immutability Enforced
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Definition Version Immutability
  rationale: Enforcing immutability of data pipeline definitions in Dataflow reduces the risk of unauthorized changes, ensuring consistent and repeatable data processing. This is crucial for maintaining data integrity and meeting compliance requirements such as SOC2 and PCI-DSS, which mandate strict controls over data processing environments.
  description: This rule checks if the Dataflow pipeline definitions are immutable, preventing modifications to deployed versions. Verify that the Dataflow jobs use settings that lock the pipeline definition upon deployment. To enforce this, configure your CI/CD pipeline to deploy new versions instead of updating existing ones. This ensures auditability and traceability of data processing logic changes.
  references:
  - https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
  - https://cloud.google.com/dataflow/docs/concepts/security-and-compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.dataflow.job.data_pipeline_max_length_defined_where_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Max Length Defined Where Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Job Pipeline Max Length is Defined
  rationale: Defining a maximum length for Dataflow job pipelines helps prevent excessive data processing durations which can lead to increased costs, potential data exposure, and compliance violations. By setting clear limits, organizations can mitigate risks associated with long-running data processes that may inadvertently handle sensitive data longer than necessary, aligning with data protection regulations.
  description: This rule checks whether a maximum length is configured for Dataflow job pipelines to ensure they do not run indefinitely. Without these limits, pipelines might process data longer than intended, increasing the likelihood of data retention beyond policy stipulations. To verify, review Dataflow job configurations to ensure a max length is defined and adjust the settings via the GCP Console or gcloud command-line tool. This helps maintain control over data processing durations and aligns with best practices for data lifecycle management.
  references:
  - https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dataflow.job.data_pipeline_metadata_disallowed_attribute_keys_blocked
  service: dataflow
  resource: job
  requirement: Data Pipeline Metadata Disallowed Attribute Keys Blocked
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Block Disallowed Metadata Keys in Dataflow Jobs
  rationale: Blocking disallowed metadata keys in Dataflow jobs is crucial to prevent unauthorized access to sensitive information. Metadata can inadvertently expose information that could be exploited by attackers, leading to data breaches or non-compliance with data protection regulations. Ensuring that only approved metadata keys are used mitigates the risk of data leakage and supports organizational compliance with frameworks like GDPR and HIPAA.
  description: This rule checks that Dataflow jobs do not use metadata keys that are disallowed by policy. Administrators must review and configure the Dataflow job settings to ensure only authorized metadata keys are utilized. Verification involves auditing the job configurations and employing IAM policies to restrict the use of sensitive metadata keys. Remediation requires updating job configurations and applying IAM roles that enforce these restrictions.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_metadata_sensitive_keys_require_secret_type
  service: dataflow
  resource: job
  requirement: Data Pipeline Metadata Sensitive Keys Require Secret Type
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Secret Type for Sensitive Keys in Dataflow Metadata
  rationale: It is crucial to protect sensitive information in data pipeline metadata to prevent unauthorized access and potential data breaches. Ensuring that sensitive keys are stored as secrets reduces the risk of exposure, aligns with best practices for data protection, and helps maintain compliance with regulatory requirements such as GDPR and HIPAA.
  description: This rule checks that sensitive keys within Google Cloud Dataflow job metadata are configured as secret types rather than plain text. Verify that any configuration parameter or metadata key that contains sensitive information, such as API keys or passwords, is stored using the Secret Manager service. Remediation involves identifying these keys and migrating them to Secret Manager, updating your Dataflow jobs to reference the secrets securely.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_metadata_values_not_plaintext_secret
  service: dataflow
  resource: job
  requirement: Data Pipeline Metadata Values Not Plaintext Secret
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Dataflow Metadata Values Are Not Plaintext Secrets
  rationale: Storing plaintext secrets in Dataflow metadata increases the risk of unauthorized access to sensitive information, potentially leading to data breaches. This practice can also violate compliance requirements such as GDPR, HIPAA, and PCI-DSS, where protecting sensitive data is mandated. Ensuring metadata does not contain plaintext secrets helps maintain data confidentiality and integrity, reducing the organization's risk exposure.
  description: This rule checks that no plaintext secrets, such as passwords or API keys, are stored within the metadata of Dataflow jobs. To verify, inspect Dataflow job configurations for any sensitive information stored in plaintext. Remediation involves using secret management services like Google Cloud Secret Manager to store sensitive data securely and referring to them in the configurations via environment variables or other secure methods.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/standard/54534.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.dataflow.job.data_pipeline_min_length_defined_where_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Min Length Defined Where Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Data Pipeline Min Length is Defined for Dataflow Jobs
  rationale: Defining a minimum data pipeline length in Dataflow jobs is crucial for maintaining data integrity and consistency, especially in complex data processing environments. This requirement helps mitigate risks of data loss or incomplete data processing, which can lead to inaccurate analytics and business decisions. Additionally, compliance with data protection standards such as GDPR and HIPAA may mandate specific data handling processes, which this rule supports by ensuring pipelines are properly configured.
  description: This rule checks that Dataflow jobs have a minimum pipeline length defined where applicable, ensuring that data processing tasks are not prematurely terminated. Verification involves reviewing job configurations to ensure that pipeline length parameters are set appropriately. Remediation requires configuring the Dataflow job settings through the GCP Console or using IaC tools like Terraform to specify a minimum pipeline length that aligns with business and compliance requirements.
  references:
  - https://cloud.google.com/dataflow/docs/guides/specifying-exec-params
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/dataflow/docs/resources/faq
  - https://cloud.google.com/dataflow/docs/guides/setting-pipeline-options
- rule_id: gcp.dataflow.job.data_pipeline_object_environment_no_plaintext_secrets
  service: dataflow
  resource: job
  requirement: Data Pipeline Object Environment No Plaintext Secrets
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Avoid Plaintext Secrets in Dataflow Pipeline Environments
  rationale: Storing plaintext secrets in Dataflow pipeline environments poses significant security risks, including unauthorized access to sensitive data and potential data breaches. Compliance frameworks such as PCI-DSS and HIPAA require strict management of sensitive information, and failing to encrypt secrets can lead to non-compliance and financial penalties.
  description: This rule checks for the presence of plaintext secrets within Dataflow pipeline environment variables. It is essential to encrypt secrets using Google Cloud's Secret Manager or similar services, ensuring they are not exposed in plaintext. Verification involves inspecting Dataflow job configurations for unsecured environment variables and updating them to reference encrypted secrets. Remediation includes using Secret Manager API calls in your pipeline code to securely retrieve sensitive information.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/secret-manager/docs/overview
  - https://cloud.google.com/dataflow/docs/guides/using-public-datasets
- rule_id: gcp.dataflow.job.data_pipeline_object_execution_role_least_privilege
  service: dataflow
  resource: job
  requirement: Data Pipeline Object Execution Role Least Privilege
  scope: dataflow.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataflow Job Roles Follow Least Privilege Principle
  rationale: Applying the principle of least privilege to Dataflow job roles reduces the risk of unauthorized access and potential data breaches. This practice minimizes the impact of compromised credentials and aligns with compliance mandates such as PCI-DSS and ISO 27001, which require strict access controls to protect sensitive data and maintain trust with stakeholders.
  description: This rule checks that the roles assigned to Dataflow jobs only have the permissions necessary to perform their intended tasks. Verify that service accounts and users with access to Dataflow jobs are granted the minimal set of permissions required for their function. Remediation involves auditing current role assignments and tightening permissions using custom roles or predefined roles with the least privilege necessary to execute data pipeline tasks.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/iam/docs/understanding-roles#dataflow-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_object_logging_enabled
  service: dataflow
  resource: job
  requirement: Data Pipeline Object Logging Enabled
  scope: dataflow.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Dataflow Job Data Pipeline Object Logging
  rationale: Enabling logging for Dataflow jobs provides visibility into data processing activities, crucial for detecting anomalies and unauthorized access. This enhances the ability to respond to security incidents and supports compliance with data protection regulations like GDPR and SOC 2, which require audit trails for data processing activities.
  description: This rule checks if Dataflow jobs have object logging enabled to record interactions with data pipeline components. Verify logging settings in the Dataflow console under the job's configuration or via the Google Cloud CLI. To enable logging, adjust job configurations to include appropriate logging flags. Ensure that logs are stored securely and monitored for suspicious activities.
  references:
  - https://cloud.google.com/dataflow/docs/guides/logging
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmarks
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.dataflow.job.data_pipeline_object_network_private_only
  service: dataflow
  resource: job
  requirement: Data Pipeline Object Network Private Only
  scope: dataflow.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Dataflow Jobs Use Private Network for Data Pipeline
  rationale: Utilizing private networks for Dataflow jobs minimizes exposure to the public internet, thereby reducing the risk of unauthorized access and data breaches. This practice is crucial for maintaining data confidentiality and integrity, particularly for organizations handling sensitive information or adhering to strict compliance standards like PCI-DSS or HIPAA.
  description: This rule checks that Dataflow jobs are configured to use private IPs for data pipeline operations, ensuring network traffic remains within Google Cloud's secure network. To verify, inspect Dataflow job configurations for the use of 'enablePrivateIps' within the 'network' settings. Remediation involves setting 'enablePrivateIps' to true in the Dataflow job configuration to route traffic through private IPs only, thereby enhancing network security.
  references:
  - https://cloud.google.com/dataflow/docs/guides/specifying-networks#configuring_private_ip
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.dataflow.job.data_pipeline_pattern_defined_where_applicable
  service: dataflow
  resource: job
  requirement: Data Pipeline Pattern Defined Where Applicable
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Jobs Define Data Pipeline Patterns
  rationale: Defining data pipeline patterns in Dataflow jobs is critical for maintaining data integrity and security. Without defined patterns, there is an increased risk of data mismanagement, unauthorized access, and non-compliance with data protection regulations such as GDPR and HIPAA. This practice also helps in standardizing processes, which reduces errors and enhances auditability.
  description: This rule checks whether Dataflow jobs in GCP have defined data pipeline patterns applicable to their operations. To verify, review the job configurations to ensure patterns are explicitly stated and align with organizational security policies. If patterns are undefined, update the Dataflow job configurations to include appropriate patterns that enforce encryption at rest and control data access. This can be done by using predefined templates or custom JSON files that specify the necessary parameters.
  references:
  - https://cloud.google.com/dataflow/docs/guides/templates/provided-templates
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/privacy/
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_artifacts_private_and_encrypted
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline Artifacts Private And Encrypted
  scope: dataflow.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Artifacts Are Private and Encrypted
  rationale: Encrypting Dataflow pipeline artifacts at rest ensures that sensitive data remains protected from unauthorized access and potential breaches. This is crucial for maintaining data confidentiality, meeting regulatory compliance such as GDPR and HIPAA, and safeguarding intellectual property and customer data from malicious actors.
  description: This rule verifies that Dataflow pipeline artifacts are stored in a private and encrypted manner. Ensure that data storage buckets used for Dataflow jobs are configured with 'Uniform bucket-level access' and use customer-managed encryption keys (CMEK) for encryption at rest. To verify, check the bucket settings in the Google Cloud Console and configure the necessary IAM permissions and CMEK settings to restrict access and enable encryption.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - CIS GCP Benchmark v1.1.0 - 4.1 Ensure that Cloud Storage bucket is not anonymously or publicly accessible
  - NIST SP 800-53 Rev. 5 SC-13 Cryptographic Protection
  - 'PCI-DSS v3.2.1 Requirement 3: Protect Stored Cardholder Data'
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_execution_roles_least_privilege
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline Execution Roles Least Privilege
  scope: dataflow.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataflow Pipeline Roles Use Least Privilege
  rationale: Ensuring least privilege for Dataflow pipeline roles reduces the risk of unauthorized access and potential data leaks. Misconfigured roles can lead to excessive permissions that may be exploited by malicious actors, impacting data integrity and confidentiality. Compliance with least privilege principles is crucial for meeting regulatory standards such as GDPR and HIPAA.
  description: This rule checks that Dataflow pipeline roles are configured with the least privilege necessary for their function. It verifies that roles assigned to Dataflow jobs do not include permissions beyond what is needed for pipeline execution. To remediate, review and update IAM policies to ensure roles have only the necessary permissions, removing any excessive privileges.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_kms_encryption_enabled
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline KMS Encryption Enabled
  scope: dataflow.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Dataflow Job Uses KMS for Pipeline Encryption
  rationale: Enabling KMS encryption for Dataflow pipelines helps protect sensitive data from unauthorized access and meets regulatory requirements such as PCI-DSS and HIPAA. Without encryption, data at rest is vulnerable to breaches, potentially leading to data loss, reputational damage, and financial penalties.
  description: This rule checks whether Google Cloud Dataflow jobs have Key Management Service (KMS) encryption enabled for pipelines. To verify, ensure that a KMS key is specified in the pipeline's configuration settings. Remediation involves setting up a KMS key in Google Cloud and configuring the Dataflow job to use this key for encrypting pipeline data during job initialization.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_logs_and_metrics_enabled
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline Logs And Metrics Enabled
  scope: dataflow.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Dataflow Pipeline Logs and Metrics Collection
  rationale: Enabling logs and metrics for Dataflow jobs is crucial for monitoring and troubleshooting data pipelines. Without these logs, identifying performance bottlenecks, errors, and unusual activity becomes challenging, which can lead to data integrity issues and potential breaches. Additionally, many compliance frameworks require logging and monitoring to ensure data processing activities are auditable.
  description: This check ensures that logging and monitoring are enabled for Dataflow jobs to capture detailed pipeline execution data. Verify that Stackdriver Logging and Monitoring are configured to collect logs and metrics for Dataflow pipelines. Remediation involves enabling these settings in the Dataflow job configuration to facilitate real-time insights and historical analysis of pipeline behavior.
  references:
  - https://cloud.google.com/dataflow/docs/guides/logging-and-monitoring
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/logging/docs/audit
  - CIS Google Cloud Platform Foundation Benchmark
  - NIST SP 800-53
  - ISO/IEC 27001
- rule_id: gcp.dataflow.job.data_pipeline_pipeline_private_networking_enforced
  service: dataflow
  resource: job
  requirement: Data Pipeline Pipeline Private Networking Enforced
  scope: dataflow.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for Dataflow Pipelines
  rationale: Enforcing private networking in Dataflow pipelines reduces the risk of data exposure by limiting network access to internal IPs only. This mitigates threats such as unauthorized access and data breaches, crucial for maintaining data confidentiality and integrity. Compliance with network security policies and standards such as ISO 27001 and SOC 2 reinforces trust and ensures regulatory adherence.
  description: This rule ensures that Dataflow jobs are configured to use private IP addresses. Verify that the 'network' setting in the Dataflow job configuration specifies a VPC with private Google access enabled. To remediate, update the Dataflow job settings to utilize a private VPC network, ensuring no public IPs are assigned. This enhances security by confining network traffic within the Google Cloud internal network.
  references:
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://cloud.google.com/dataflow/docs/guides/specifying-networks
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Section 7.8
  - ISO/IEC 27001:2013
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.dataflow.job.data_pipeline_type_set
  service: dataflow
  resource: job
  requirement: Data Pipeline Type Set
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataflow Pipeline Type is Correctly Configured
  rationale: Misconfiguration of Dataflow pipeline types can lead to ineffective data processing and potential data exposure. By ensuring the correct pipeline type, organizations reduce the risk of processing sensitive data in an unsecured manner, which is crucial for maintaining data privacy and meeting compliance requirements such as GDPR and CCPA.
  description: This rule checks that Dataflow jobs have the appropriate pipeline type set, ensuring data is processed securely and efficiently. Verify that the pipeline type is set to 'streaming' or 'batch' based on the intended use case and data sensitivity. Remediation involves reviewing the Dataflow job configurations and updating the pipeline type to match the data processing requirements, ensuring encryption at rest is enabled for all data processed by the pipeline.
  references:
  - https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/dataflow/docs/resources/faq
- rule_id: gcp.dataflow.job.data_pipeline_value_from_secrets_manager_when_sensitive
  service: dataflow
  resource: job
  requirement: Data Pipeline Value From Secrets Manager When Sensitive
  scope: dataflow.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Sensitive Data in Dataflow Uses Secrets Manager
  rationale: Using Secrets Manager for sensitive data in Dataflow jobs mitigates the risk of data leaks and unauthorized access. This practice aligns with data protection regulations such as GDPR and CCPA, reducing legal and financial risks. It also enhances security by ensuring that sensitive information is encrypted and managed securely, reducing exposure to threats like insider attacks and data breaches.
  description: This rule checks that any sensitive parameters used in Dataflow jobs are retrieved securely from Google Cloud Secrets Manager. Verify that configurations in Dataflow pipelines reference secrets stored in Secrets Manager rather than hardcoding sensitive values. To remediate, update Dataflow job configurations to use the Secret Manager API for sensitive data retrieval, ensuring data is encrypted at rest and access is controlled via IAM roles.
  references:
  - https://cloud.google.com/dataflow/docs/guides/security
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.dataproc.cluster.data_analytics_admin_access_least_privilege
  service: dataproc
  resource: cluster
  requirement: Data Analytics Admin Access Least Privilege
  scope: dataproc.cluster.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Enforce Least Privilege for Dataproc Cluster Admin Access
  rationale: Ensuring least privilege in Dataproc clusters minimizes the risk of unauthorized data access and potential data breaches. Improperly assigned admin rights can lead to privilege escalation and unauthorized access to sensitive data, posing significant business and regulatory compliance risks, including violation of GDPR or HIPAA standards.
  description: This rule checks that the Data Analytics Admin role in Dataproc clusters is assigned only to users who absolutely need it. Verify that IAM policies on the cluster restrict admin access to essential personnel by reviewing IAM roles and permissions. If admin access is overly permissive, adjust the IAM policy to remove unnecessary permissions and apply the principle of least privilege by assigning only essential roles. Regular audits should be conducted to ensure compliance.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices
  - NIST SP 800-53, Rev. 4 - Access Control
- rule_id: gcp.dataproc.cluster.data_analytics_audit_logging_enabled
  service: dataproc
  resource: cluster
  requirement: Data Analytics Audit Logging Enabled
  scope: dataproc.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Data Analytics Audit Logging in Dataproc Clusters
  rationale: Enabling audit logging for Dataproc clusters is crucial for tracking data access and administrative actions, helping to detect unauthorized activities and support forensic investigations. This practice supports compliance with regulations such as GDPR and CCPA, which mandate detailed logging of data processing activities. Without adequate logging, organizations face increased risks of undetected data breaches and non-compliance penalties.
  description: This rule ensures that audit logging is enabled for Dataproc clusters, capturing details of data access and administrative operations. To verify, check the cluster's IAM policy to ensure the 'roles/logging.logWriter' role is attached to the service account running the cluster. Remediation involves updating the IAM policy to include this role, thereby enabling audit logs to be sent to Cloud Logging for review and alerting.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/logging
  - https://cloud.google.com/logging/docs/audit
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://cloud.google.com/security/compliance
  - 'NIST SP 800-53: AU-2, AU-3'
  - ISO/IEC 27001:2013 - A.12.4
- rule_id: gcp.dataproc.cluster.data_analytics_encryption_at_rest_cmek
  service: dataproc
  resource: cluster
  requirement: Data Analytics Encryption At Rest Cmek
  scope: dataproc.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Dataproc Clusters Use CMEK for Encryption at Rest
  rationale: Using Customer-Managed Encryption Keys (CMEK) for encrypting data at rest in Dataproc clusters provides control over encryption keys, enhancing data protection and compliance with regulatory standards. Without CMEK, you risk unauthorized data access if Google-managed keys are compromised, which can lead to data breaches and non-compliance with frameworks like PCI-DSS and HIPAA.
  description: This rule checks if Dataproc clusters are configured to use Customer-Managed Encryption Keys (CMEK) for encrypting data at rest. To verify, inspect the Dataproc cluster settings in the Google Cloud Console or use the gcloud CLI to ensure CMEK is enabled. Remediation involves configuring CMEK by creating a Cloud KMS key and specifying it in the Dataproc cluster's encryption settings during cluster creation or updating existing clusters.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.dataproc.cluster.data_analytics_private_networking_enforced
  service: dataproc
  resource: cluster
  requirement: Data Analytics Private Networking Enforced
  scope: dataproc.cluster.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Networking for Dataproc Clusters
  rationale: Enforcing private networking for Dataproc clusters mitigates the risk of exposing sensitive data and processing operations to public networks. This configuration helps prevent unauthorized access, reduces attack surface, and ensures compliance with regulations such as GDPR and HIPAA, which mandate secure data processing environments.
  description: This rule checks if Dataproc clusters are configured to use private IP addresses instead of public IPs, ensuring that communication occurs within a secure VPC. To verify, ensure the 'enablePrivateEndpoint' and 'enablePrivateNodes' fields are set to true in the cluster configuration. Remediation involves updating the cluster settings to disable public IPs and enabling private networking options.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/architecture/best-practices-vpc-design
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/gdpr/
- rule_id: gcp.dataproc.cluster.data_analytics_subnet_group_private_subnets_only
  service: dataproc
  resource: cluster
  requirement: Data Analytics Subnet Group Private Subnets Only
  scope: dataproc.cluster.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Enforce Private Subnets for Dataproc Cluster Networking
  rationale: Ensuring that Dataproc clusters operate within private subnets minimizes exposure to public internet threats, thereby reducing the risk of unauthorized access and data breaches. This control helps meet privacy and security requirements crucial for compliance with regulations such as PCI-DSS and GDPR, protecting sensitive data processed and analyzed by Dataproc clusters.
  description: This rule checks if Dataproc clusters are configured to use subnets that are restricted to private IP addresses only. To verify, ensure that the 'enablePrivateEndpoint' setting is true within the cluster's network configuration. This setting prevents the cluster's nodes from obtaining public IP addresses, thereby restricting inbound and outbound traffic to internal network paths only. Remediation involves updating the cluster configuration to enable private subnet usage, which can be accomplished through the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.dataproc.cluster.data_analytics_tls_min_1_2_enforced
  service: dataproc
  resource: cluster
  requirement: Data Analytics TLS Min 1 2 Enforced
  scope: dataproc.cluster.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enforce TLS 1.2 for Dataproc Cluster Data Analytics
  rationale: Enforcing a minimum of TLS 1.2 for Dataproc clusters ensures that data in transit is protected against interception and tampering. This is crucial for maintaining data integrity and confidentiality, especially when handling sensitive data. It reduces the risk of data breaches through exploitation of older, less secure protocols and aligns with compliance requirements like PCI-DSS and HIPAA, which mandate strong encryption standards.
  description: This rule verifies that all Dataproc clusters are configured to enforce a minimum of TLS 1.2 for data analytics, ensuring secure communication channels. To check compliance, verify the cluster's configuration in the Google Cloud Console or via the gcloud CLI, ensuring that the cluster's properties enforce TLS 1.2. Remediation involves updating cluster configurations to disable older protocols and enforce TLS 1.2 or higher, which can be done by modifying the cluster's security settings.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2_SA_Quick_Reference_Guide.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.dataproc.cluster.datalake_dev_endpoint_encrypted
  service: dataproc
  resource: cluster
  requirement: Datalake Dev Endpoint Encrypted
  scope: dataproc.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataproc Cluster Datalake Endpoint is Encrypted
  rationale: Encryption of data at rest is crucial for protecting sensitive information stored in Dataproc clusters. Without encryption, data is vulnerable to unauthorized access, which could lead to data breaches, financial loss, and non-compliance with regulations like GDPR and HIPAA. Encrypted endpoints help mitigate risks related to data exposure and enhance the overall security posture of your cloud environment.
  description: This rule checks whether Dataproc clusters have encryption enabled for Datalake endpoints. To ensure data protection, configure your clusters to use customer-managed encryption keys (CMEK) or Google-managed keys for all data at rest. Verify this setting via the Google Cloud Console under the 'Clusters' section by ensuring the 'Encryption' field is properly set. If not configured, update the cluster settings to enable encryption, thereby securing the data against unauthorized access.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.dataproc.cluster.datalake_dev_endpoint_no_public_access
  service: dataproc
  resource: cluster
  requirement: Datalake Dev Endpoint No Public Access
  scope: dataproc.cluster.public_access
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: critical
  title: Prevent Public Access to Dataproc Dev Endpoints
  rationale: Publicly accessible Dataproc dev endpoints can lead to unauthorized data access and potential data breaches. This configuration exposes sensitive information to external threats, increasing the risk of exploitation and non-compliance with regulations such as GDPR and HIPAA. Ensuring private access helps protect business assets and maintains customer trust.
  description: This rule checks if Dataproc clusters in development environments are configured without public IP addresses. Public IPs should be replaced with private IPs to restrict access to trusted networks only. Verification involves reviewing the cluster's network configuration settings in the GCP Console or via the gcloud command-line tool. To remediate, update the cluster's network settings to use private IPs and configure firewall rules to limit access to specific IP ranges.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.dataproc.cluster.datalake_dev_endpoint_role_least_privilege
  service: dataproc
  resource: cluster
  requirement: Datalake Dev Endpoint Role Least Privilege
  scope: dataproc.cluster.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataproc Cluster Dev Endpoints Use Least Privilege Roles
  rationale: Assigning excessive permissions to Dataproc clusters can lead to unauthorized access and data exfiltration, compromising sensitive data and violating compliance frameworks such as PCI-DSS and HIPAA. By adhering to the principle of least privilege, organizations minimize potential attack vectors and reduce the risk of privilege escalation.
  description: This rule checks that Dataproc clusters used for development purposes in a data lake environment have the least privilege roles assigned. Verify that IAM roles associated with the clusters do not grant permissions beyond what is necessary for their specific tasks. Remediation involves auditing current IAM policies, removing superfluous permissions, and assigning roles that follow the principle of least privilege. Regularly review and update IAM policies to ensure compliance with security best practices.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security
- rule_id: gcp.dataproc.cluster.encrypted_with_cmks_disabled
  service: dataproc
  resource: cluster
  requirement: Encrypted With Cmks Disabled
  scope: dataproc.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataproc Clusters Use CMKs for Encryption
  rationale: Using Customer-Managed Encryption Keys (CMKs) for Dataproc clusters enhances data protection by giving organizations full control over the encryption keys, which can mitigate risks related to unauthorized data access and satisfy regulatory compliance requirements. Failure to use CMKs can expose sensitive data to unauthorized access and may lead to non-compliance with data protection laws such as GDPR and industry standards like PCI-DSS.
  description: This rule checks if Google Cloud Dataproc clusters are using Customer-Managed Encryption Keys (CMKs) for encryption at rest. To comply with this rule, configure the Dataproc clusters to use a CMK from Cloud Key Management Service (Cloud KMS) during cluster creation. Verifying compliance involves checking the cluster's encryption configuration in the Google Cloud Console or using the gcloud command-line tool. Remediation involves updating the cluster configuration to specify a CMK from Cloud KMS.
  references:
  - https://cloud.google.com/dataproc/docs/guides/encryption
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/kms/docs/key-rotation
- rule_id: gcp.dataproc.cluster.master_nodes_no_public_ip
  service: dataproc
  resource: cluster
  requirement: Master Nodes No Public Ip
  scope: dataproc.cluster.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Dataproc Master Nodes Don't Have Public IPs
  rationale: Public IPs on Dataproc master nodes expose critical infrastructure to the internet, increasing the risk of unauthorized access and potential data breaches. This can lead to significant business disruptions and non-compliance with regulatory mandates such as PCI-DSS and HIPAA, which require strict controls on network access.
  description: This rule checks that Google Cloud Dataproc clusters are configured such that master nodes do not have publicly accessible IP addresses. Ensuring only private IPs are used limits exposure to potential threats. Verify by checking the network configuration of Dataproc clusters to confirm that master nodes are only accessible within a VPC. Remediate by modifying the cluster's network settings to use private IPs, or by setting up firewall rules to restrict access.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/dataproc/docs/guides/dataproc-security
- rule_id: gcp.dataproc.cluster.pd_encryption_cmek_enabled
  service: dataproc
  resource: cluster
  requirement: Pd Encryption Cmek Enabled
  scope: dataproc.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure CMEK is Enabled for Dataproc Clusters
  rationale: Enabling Customer-Managed Encryption Keys (CMEK) for Dataproc clusters ensures that the encryption keys are managed by the organization, reducing the risk of unauthorized access and enhancing control over data encryption. This is crucial for complying with regulatory requirements such as GDPR and HIPAA, which mandate strict control over encryption keys to protect sensitive data.
  description: This rule checks if Dataproc clusters are configured with Customer-Managed Encryption Keys (CMEK) for persistent disk (PD) encryption. To verify, ensure that the 'gceClusterConfig.diskConfig.bootDiskKmsKeyName' field is specified in the cluster's configuration. If not enabled, configure CMEK by specifying a valid KMS key during the cluster creation process. This provides enhanced security by allowing organizations to manage encryption keys, ensuring data at rest is protected by keys under their control.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/customer-managed-encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.dataproc.job.datalake_logs_and_metrics_enabled
  service: dataproc
  resource: job
  requirement: Datalake Logs And Metrics Enabled
  scope: dataproc.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging and Metrics for Dataproc Jobs
  rationale: Enabling logs and metrics for Dataproc jobs is crucial for monitoring and auditing data processing activities. It helps detect anomalies, troubleshoot issues, and maintain compliance with regulations like PCI-DSS and HIPAA, which require detailed logging of data access and processing activities. Without these logs, organizations risk undetected data breaches and non-compliance penalties.
  description: This rule checks if logging and metrics are enabled for Dataproc jobs. To verify compliance, ensure that Cloud Logging and Cloud Monitoring are configured for each Dataproc job. This involves enabling the 'Google Cloud's operations suite' in your project settings and ensuring that job submissions include logging configurations. If not enabled, update your Dataproc job configurations to include the necessary logging and monitoring settings for comprehensive audit trails and performance insights.
  references:
  - https://cloud.google.com/dataproc/docs/guides/logging
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud
  - https://cloud.google.com/dataproc/docs/concepts/configurations
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.dataproc.job.datalake_ml_transform_logs_enabled
  service: dataproc
  resource: job
  requirement: Datalake Ml Transform Logs Enabled
  scope: dataproc.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Dataproc Job Datalake ML Transform Logs
  rationale: Enabling logs for Datalake ML Transform jobs in Dataproc allows for auditability and traceability of data transformations, which is crucial for identifying unauthorized access or modifications. This enhances the security posture by providing visibility into operations, aiding in compliance with data protection regulations such as GDPR and HIPAA, and ensuring data integrity.
  description: This rule checks if logging is enabled for Datalake ML Transform jobs in Google Cloud Dataproc. To verify, ensure that the jobs have logging configurations set to capture logs in a designated storage bucket. Remediation involves configuring the Dataproc jobs to enable logging by setting the `driver-log-levels` parameter to an appropriate level and specifying a logging destination, such as Google Cloud Storage or Stackdriver Logging.
  references:
  - https://cloud.google.com/dataproc/docs/guides/logging
  - https://cloud.google.com/security/compliance/gdpr/
  - https://cloud.google.com/hipaa/
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/architecture/best-practices-for-policy-compliance
- rule_id: gcp.dataproc.job.datalake_ml_transform_network_private_only
  service: dataproc
  resource: job
  requirement: Datalake Ml Transform Network Private Only
  scope: dataproc.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Dataproc Jobs Use Private Networks Only
  rationale: Using private networks for Dataproc jobs minimizes exposure to unauthorized access, reduces the risk of data breaches, and complies with regulatory requirements for data protection. By restricting Dataproc jobs to private networks, organizations can mitigate risks associated with public internet exposure, such as man-in-the-middle attacks and unauthorized data interception.
  description: This rule checks that Dataproc jobs are configured to run on private IP networks only. To verify, ensure that the Dataproc cluster's network configuration specifies a subnet that does not have 'Private Google Access' enabled. Remediation involves updating the cluster's network settings to restrict traffic to internal IP addresses, thus preventing public internet exposure.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc/docs/private-access-options
- rule_id: gcp.dataproc.job.datalake_ml_transform_role_least_privilege
  service: dataproc
  resource: job
  requirement: Datalake Ml Transform Role Least Privilege
  scope: dataproc.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Dataproc Datalake ML Transform Role
  rationale: Implementing least privilege access for Dataproc jobs minimizes the risk of unauthorized data access and potential data breaches. By restricting permissions to the minimum necessary, organizations reduce their exposure to threats, meet compliance mandates such as GDPR, and protect sensitive data from internal and external threats.
  description: This rule checks that Dataproc jobs using the Datalake ML Transform Role do not have excessive permissions beyond what is necessary for their function. Verify that IAM policies grant only essential permissions. Remediation involves auditing current role assignments, using predefined roles where possible, and customizing roles to remove unnecessary permissions.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS GCP Benchmark v1.3.0, Section 4.6
  - NIST SP 800-53 Rev. 5 AC-6 Least Privilege
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.dataproc.job.datalake_network_private_only
  service: dataproc
  resource: job
  requirement: Datalake Network Private Only
  scope: dataproc.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Dataproc Jobs Use Private Networks Only
  rationale: Restricting Dataproc jobs to private networks mitigates the risk of unauthorized access and data exfiltration by ensuring that jobs cannot be accessed from the public internet. This is crucial for maintaining the confidentiality and integrity of sensitive data processed in Dataproc, and helps organizations comply with regulatory standards such as GDPR and HIPAA by enforcing network access control.
  description: This rule verifies that Dataproc jobs are configured to use only private networks by checking the network settings of each job. To ensure compliance, configure the Dataproc cluster's network to be private and specify the same network for all jobs. Remediate by reviewing Dataproc network configurations and updating jobs to run on private IP-only subnets, thus preventing exposure to public networks.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
  - https://cloud.google.com/network-connectivity/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.dataproc.job.datalake_role_least_privilege
  service: dataproc
  resource: job
  requirement: Datalake Role Least Privilege
  scope: dataproc.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataproc Job Uses Least Privilege Role
  rationale: Applying the principle of least privilege minimizes the potential attack surface by ensuring that Dataproc jobs only have the permissions necessary to perform their tasks. This reduces the risk of accidental or malicious activity that can lead to data breaches or unauthorized access, aligning with compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks if Dataproc jobs are assigned roles with the minimum necessary permissions in Google Cloud IAM. To verify, audit the roles assigned to Dataproc jobs for excessive permissions. Remediation involves reviewing and updating IAM roles to match the specific needs of the job while removing unnecessary permissions. This can be achieved by customizing predefined roles or creating custom roles tailored to job requirements.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.dataproc.job.datalake_script_location_private_and_encrypted
  service: dataproc
  resource: job
  requirement: Datalake Script Location Private And Encrypted
  scope: dataproc.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Dataproc Script Locations are Private and Encrypted
  rationale: Storing Dataproc job scripts in non-private or unencrypted locations exposes sensitive data and intellectual property to unauthorized access. This can lead to data breaches, financial loss, and non-compliance with regulatory standards such as PCI-DSS and HIPAA. Ensuring scripts are private and encrypted mitigates these risks by protecting data at rest and maintaining confidentiality.
  description: This rule checks that Dataproc job scripts are stored in Google Cloud Storage buckets configured with access controls that restrict public access and enforce encryption. Verify that the bucket's 'Public access prevention' setting is enabled and that 'Default encryption' uses a customer-managed encryption key (CMEK). Remediate by updating bucket permissions to remove public access and configure encryption using CMEK to enhance data protection.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security#encryption
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/controls/cis-google-cloud-computing-foundations-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption/
- rule_id: gcp.dataproc.job.datalake_trigger_event_sources_restricted
  service: dataproc
  resource: job
  requirement: Datalake Trigger Event Sources Restricted
  scope: dataproc.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Datalake Trigger Event Sources in Dataproc Jobs
  rationale: Restricting event sources for Dataproc jobs helps mitigate unauthorized data processing and access, reducing the risk of data leaks and ensuring compliance with data protection regulations such as GDPR and CCPA. By controlling event sources, organizations can prevent accidental or malicious triggers that could exploit sensitive data within their data lakes.
  description: This rule checks whether Dataproc jobs have restricted trigger event sources to prevent unauthorized or unintended job executions. Ensure that Dataproc jobs are configured to only accept events from trusted and verified sources, such as specific Cloud Pub/Sub topics or GCS buckets. Review and configure event sources in the Dataproc job settings to align with organizational security policies. Remediation involves updating access permissions and configurations to limit event triggers to authorized sources only.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/dataproc/docs/concepts/jobs/overview
- rule_id: gcp.dataproc.job.datalake_trigger_logs_enabled
  service: dataproc
  resource: job
  requirement: Datalake Trigger Logs Enabled
  scope: dataproc.job.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Dataproc Job Trigger Logs for Audit Monitoring
  rationale: Enabling logs for Dataproc job triggers is crucial for auditing and monitoring data access patterns, which helps identify unauthorized access attempts and misconfigurations. This enhances the security posture by ensuring traceability of data operations, essential for meeting compliance requirements such as PCI-DSS and SOC2.
  description: This rule checks that logging is enabled for all Dataproc jobs to capture trigger events. To verify, ensure that logging is activated in the Dataproc cluster configuration settings under 'Job History'. Remediation involves updating the cluster configuration to enable logging, thus providing a detailed audit trail for data processing activities.
  references:
  - https://cloud.google.com/dataproc/docs/guides/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/monitoring
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.dataproc.job.datalake_trigger_targets_least_privilege
  service: dataproc
  resource: job
  requirement: Datalake Trigger Targets Least Privilege
  scope: dataproc.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Dataproc Jobs Use Least Privilege Principle for Datalake Access
  rationale: Applying the least privilege principle to Dataproc jobs minimizes the risk of unauthorized access to sensitive data within a datalake, reducing potential data breaches and ensuring compliance with data protection regulations such as GDPR and CCPA. Ensuring that jobs have only the permissions necessary to perform their tasks mitigates the risk of privilege escalation and unauthorized data manipulation or exfiltration.
  description: This rule checks that Dataproc jobs accessing a datalake are configured with the minimum required permissions. It involves verifying IAM roles associated with service accounts used by these jobs to ensure they do not exceed necessary access levels. Remediation includes auditing the roles assigned to these service accounts and adjusting permissions to align with the least privilege model. This can be done by reviewing and modifying IAM policy bindings for Dataproc job service accounts to restrict over-privileged permissions.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r3.pdf
  - https://cloud.google.com/security/overview/whitepaper
  - https://pages.nist.gov/800-63-3/
- rule_id: gcp.dataproc.workflow_template.datalake_cross_account_trusts_restricted
  service: dataproc
  resource: workflow_template
  requirement: Datalake Cross Account Trusts Restricted
  scope: dataproc.workflow_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Trusts in Dataproc Workflow Templates
  rationale: Restricting cross-account trusts in Dataproc workflow templates is critical to prevent unauthorized access and potential data breaches. Cross-account access can expose sensitive data and resources to entities outside your organization, increasing the risk of data exfiltration and non-compliance with regulations such as GDPR and CCPA. Properly managing these trusts ensures that only authorized accounts can access and operate on your data lakes, maintaining data integrity and privacy.
  description: This rule verifies that Dataproc workflow templates do not permit cross-account trusts unless explicitly required and securely managed. It checks the IAM policies associated with the workflow templates to ensure that no external accounts have been granted unnecessary permissions. To remediate, review and update IAM policies to limit access to internal accounts only, and establish cross-account roles with the least privilege principle if external access is necessary. Regular audits and monitoring should be conducted to maintain compliance.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.dataproc.workflow_template.datalake_execution_role_least_privilege
  service: dataproc
  resource: workflow_template
  requirement: Datalake Execution Role Least Privilege
  scope: dataproc.workflow_template.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Dataproc Execution Role
  rationale: Limiting privileges for Dataproc execution roles reduces the risk of unauthorized data access and potential data breaches. Over-privileged roles can lead to accidental or malicious operations affecting data integrity and confidentiality, posing significant business risks and non-compliance with standards like PCI-DSS and SOC2.
  description: This rule checks whether Dataproc workflow templates are utilizing roles with the least privilege necessary for execution. It ensures that roles are not over-privileged by auditing IAM policies associated with the workflow templates. Remediation involves reviewing the roles and permissions assigned, ensuring they align strictly with the operational requirements of the data lake tasks, and removing excess permissions.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/iam
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
  - https://www.nist.gov/cyberframework
- rule_id: gcp.dataproc.workflow_template.datalake_kms_encryption_enabled
  service: dataproc
  resource: workflow_template
  requirement: Datalake KMS Encryption Enabled
  scope: dataproc.workflow_template.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Dataproc Workflow Templates
  rationale: Enabling KMS encryption for Dataproc workflow templates is crucial for protecting sensitive data processed in data lakes. Without encryption, data at rest is vulnerable to unauthorized access, potentially leading to data breaches and non-compliance with regulations such as GDPR and HIPAA. Encrypting data at rest helps mitigate risks associated with insider threats and external attacks.
  description: This rule checks if Google Cloud Key Management Service (KMS) encryption is enabled for Dataproc workflow templates, ensuring data at rest within the data lake is encrypted. To verify, check the 'encryptionConfig' field in the workflow template configuration. If not set, configure the workflow templates to use a KMS key by specifying it in the 'encryptionConfig' section. This configuration ensures that all data processed by the workflow templates is encrypted at rest using a customer-managed key.
  references:
  - https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/kms
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.datastudio.report.data_analytics_export_controls_enabled
  service: datastudio
  resource: report
  requirement: Data Analytics Export Controls Enabled
  scope: datastudio.report.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Data Analytics Export Controls in Data Studio Reports
  rationale: Ensuring that Data Analytics Export Controls are enabled in Data Studio reports helps prevent unauthorized access and exfiltration of sensitive data. This is crucial for maintaining data privacy and complying with industry regulations such as GDPR and CCPA, which require strict controls over data handling and export. Failure to do so could lead to data breaches, reputational damage, and significant financial penalties.
  description: This rule checks whether the Data Analytics Export Controls are enabled in Google Data Studio reports. These controls restrict the ability to export data, helping to ensure that only authorized users have access to sensitive information. To verify, review the settings in each Data Studio report and enable export controls under the report settings. Remediation involves configuring these settings to limit data export capabilities to authorized personnel only.
  references:
  - https://cloud.google.com/datastudio/docs/security-and-permissions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security
- rule_id: gcp.datastudio.report.data_analytics_public_embeds_disabled
  service: datastudio
  resource: report
  requirement: Data Analytics Public Embeds Disabled
  scope: datastudio.report.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Public Embeds for Data Studio Reports
  rationale: Public embeds of Data Studio reports can expose sensitive analytics data to unauthorized users, leading to potential data breaches and non-compliance with data protection regulations such as GDPR and CCPA. Disabling public embeds mitigates the risk of data exfiltration and helps ensure that only authorized personnel can access critical business insights.
  description: This rule verifies that public embeds are disabled for Data Studio reports to prevent unauthorized access. Administrators should ensure that the 'Public Embed' option is turned off in the sharing settings of each report. Remediation involves reviewing Data Studio report permissions and settings, and configuring the reports to restrict access to authenticated users only.
  references:
  - https://support.google.com/datastudio/answer/9053467?hl=en
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.iso.org/standard/54534.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.datastudio.report.data_analytics_sharing_restricted_to_org
  service: datastudio
  resource: report
  requirement: Data Analytics Sharing Restricted To Org
  scope: datastudio.report.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Data Studio Report Sharing to Organization
  rationale: Limiting Data Studio report sharing to within the organization minimizes the risk of data leaks and unauthorized access. Exposing analytics data to external entities without proper controls could lead to business intelligence compromises and regulatory non-compliance, particularly concerning data privacy laws like GDPR and CCPA.
  description: This rule checks that Data Studio reports are only shared with users within the organization, preventing external sharing. Verify the sharing settings by accessing the Data Studio report permissions and ensure no external email addresses are granted access. To remediate, adjust the sharing settings to 'Anyone within your organization' and remove any external sharing permissions.
  references:
  - https://support.google.com/datastudio/answer/9053467?hl=en
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/privacy/
- rule_id: gcp.datastudio.report.data_analytics_sso_required
  service: datastudio
  resource: report
  requirement: Data Analytics Sso Required
  scope: datastudio.report.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Single Sign-On for Data Studio Reports
  rationale: Requiring Single Sign-On (SSO) for accessing Data Studio reports enhances security by centralizing authentication and reducing the risk of unauthorized access. It helps prevent data breaches by leveraging strong authentication mechanisms, and it simplifies user management, contributing to compliance with data protection regulations like GDPR and HIPAA.
  description: This check ensures that Single Sign-On (SSO) is enforced for accessing Google Data Studio reports. To verify, administrators should ensure that the Data Studio is integrated with an identity provider (IdP) supporting SSO, and all users are required to authenticate via this system. Remediation involves configuring the IdP settings in the GCP Console to enforce SSO across Data Studio. Ensure that the identity provider is securely set up with multi-factor authentication (MFA) for added security.
  references:
  - https://cloud.google.com/solutions/centralized-identity-access-management
  - https://cloud.google.com/datastudio/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.dlp.inspect_template.data_governance_classification_auto_classification_supported
  service: dlp
  resource: inspect_template
  requirement: Data Governance Classification Auto Classification Supported
  scope: dlp.inspect_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable Auto Classification in DLP Inspect Templates
  rationale: Automatic data classification enhances data governance by ensuring sensitive information is consistently identified and protected, reducing the risk of data breaches and aiding in regulatory compliance such as GDPR and CCPA. Failure to utilize automated classification can result in unintentional exposure of sensitive data, financial penalties, and reputational damage.
  description: This rule checks if Google Cloud DLP inspect templates are configured to support automatic data classification, which classifies data using predefined or custom info types. To verify, ensure the inspect template has auto classification enabled. Remediation involves updating the template configuration to include auto classification settings, aligning with your organizational data protection policies.
  references:
  - https://cloud.google.com/dlp/docs/creating-templates
  - CIS GCP Benchmark v1.3.0, Section 10.2
  - https://www.nist.gov/privacy-framework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.dlp.inspect_template.data_governance_classification_policy_blocks_publi_sensitive
  service: dlp
  resource: inspect_template
  requirement: Data Governance Classification Policy Blocks Publi Sensitive
  scope: dlp.inspect_template.policy_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DLP Templates Enforce Publi Sensitive Data Classification
  rationale: Blocking the exposure of sensitive data classified under data governance policies is crucial to avoid data breaches, potential financial losses, and compliance violations. This helps organizations maintain trust with customers by ensuring sensitive data is protected and adheres to privacy regulations such as GDPR and CCPA. Mismanagement of such data can lead to severe penalties and reputational damage.
  description: This rule checks whether Data Loss Prevention (DLP) inspect templates in GCP are configured to enforce data governance classification policies for public sensitive data. It verifies that inspect templates block or redact sensitive data, mitigating unauthorized access risks. To remediate, update the inspect templates to include classification rules aligned with your data governance policies, ensuring sensitive information is not inadvertently exposed.
  references:
  - https://cloud.google.com/dlp/docs/creating-inspect-templates
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-122.pdf
  - https://www.iso.org/standard/54534.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.dlp.inspect_template.data_governance_classification_required_sensitivity__present
  service: dlp
  resource: inspect_template
  requirement: Data Governance Classification Required Sensitivity Present
  scope: dlp.inspect_template.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DLP Inspector Templates Classify Sensitive Data
  rationale: Proper classification of sensitive data is crucial for maintaining data privacy and compliance with regulations such as GDPR and HIPAA. Without appropriate sensitivity levels in DLP inspector templates, sensitive data might be inadequately protected, leading to potential data breaches and financial penalties. Ensuring data governance classifications are present helps mitigate the risk of unauthorized data access and misuse.
  description: This rule checks that all DLP inspector templates in use have defined data governance classifications with the required sensitivity levels. Verify that each template includes sensitivity annotations to classify data types such as PII or financial information. To remediate, review and update DLP inspector templates to include accurate sensitivity labels, ensuring they align with your organization's data protection policies and regulatory requirements.
  references:
  - https://cloud.google.com/dlp/docs/creating-templates
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.dlp.job.data_governance_compliance_access_rbac_least_privilege
  service: dlp
  resource: job
  requirement: Data Governance Compliance Access RBAC Least Privilege
  scope: dlp.job.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure DLP Job RBAC Follows Least Privilege Principle
  rationale: Implementing least privilege access for DLP jobs is critical to minimize unauthorized data exposure and potential data breaches. Inadequate access controls can lead to misuse of sensitive information, regulatory non-compliance, and potential financial and reputational damage. Least privilege access ensures that users and services have only the permissions necessary for their functions, reducing the attack surface.
  description: This rule checks if DLP jobs in GCP adhere to the principle of least privilege by ensuring that Role-Based Access Control (RBAC) configurations are properly set. Specifically, it verifies that only necessary permissions are assigned to users and service accounts interacting with DLP jobs. To remediate, review IAM policies and restrict permissions to only those required for job execution. Use the Google Cloud Console or gcloud CLI to audit and adjust IAM roles, ensuring compliance with data governance policies.
  references:
  - https://cloud.google.com/dlp/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 1.1
  - https://www.nist.gov/privacy-framework
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dlp.job.data_governance_compliance_export_destinations_private
  service: dlp
  resource: job
  requirement: Data Governance Compliance Export Destinations Private
  scope: dlp.job.private_networking
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure DLP Job Export Destinations are Private
  rationale: Ensuring that Data Loss Prevention (DLP) job export destinations are private mitigates the risk of unauthorized data access and leakage, protecting sensitive information from exposure. This practice supports compliance with data protection regulations such as GDPR and CCPA, preventing potential breaches and financial penalties.
  description: This rule verifies that DLP job export destinations are configured to use private networking, ensuring data is not exposed to public internet access. Specifically, it checks if the export destinations, such as Cloud Storage buckets, are set with private IP addresses or reside within a VPC. To remediate, configure the export destination to use private IPs or set appropriate network policies to restrict public access. Validate through the GCP Console by checking network settings associated with the DLP job.
  references:
  - https://cloud.google.com/dlp/docs/concepts-job-triggers
  - https://cloud.google.com/vpc/docs/private-access-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.dlp.job.data_governance_compliance_reports_storage_encrypted
  service: dlp
  resource: job
  requirement: Data Governance Compliance Reports Storage Encrypted
  scope: dlp.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DLP Job Reports are Encrypted at Rest
  rationale: Encrypting data at rest for DLP job reports is crucial to protect sensitive information from unauthorized access and potential data breaches. This practice supports compliance with data protection regulations such as GDPR and CCPA, mitigating risks of financial penalties and reputational damage. Encryption at rest also helps safeguard against insider threats and physical theft of storage media.
  description: This rule checks whether Data Loss Prevention (DLP) job reports are encrypted when stored in Google Cloud Storage. To verify compliance, ensure that Cloud Storage buckets used for storing DLP job outputs have default encryption enabled, either using Google-managed keys or customer-managed keys (CMEK). If encryption is not enabled, configure the bucket settings to apply encryption for all stored objects. This can be done through the GCP Console, gcloud CLI, or Cloud Storage JSON API.
  references:
  - https://cloud.google.com/dlp/docs
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-111.pdf
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.dlp.job.privacy_anonymization_jobs_private_networking
  service: dlp
  resource: job
  requirement: Privacy Anonymization Jobs Private Networking
  scope: dlp.job.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure DLP Anonymization Jobs Use Private Networking
  rationale: Configuring DLP anonymization jobs to use private networking minimizes exposure to public internet threats, reducing the risk of data breaches and unauthorized access. It supports compliance with data protection regulations like GDPR and HIPAA by ensuring sensitive data is processed in a secure environment. This setup enhances the security posture by restricting network access to authorized entities only.
  description: This rule checks that Google Cloud Data Loss Prevention (DLP) anonymization jobs are configured to use private networking, such as VPC Service Controls or Private Google Access. Verify that the DLP jobs have network policies enforcing private IP usage to ensure that data does not transit over the public internet. To remediate, configure VPC Service Controls and ensure that all DLP jobs are set to use private IPs, limiting network exposure and enhancing security.
  references:
  - https://cloud.google.com/dlp/docs/concepts-job-triggers
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://cloud.google.com/vpc/docs/configure-private-google-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dlp.job.privacy_anonymization_outputs_encrypted
  service: dlp
  resource: job
  requirement: Privacy Anonymization Outputs Encrypted
  scope: dlp.job.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DLP Job Outputs are Encrypted for Privacy
  rationale: Encrypting outputs of Data Loss Prevention (DLP) jobs is crucial to protect sensitive information from unauthorized access, ensuring compliance with data protection regulations such as GDPR and CCPA. Anonymization outputs, if left unencrypted, can be vulnerable to breaches, potentially leading to data leaks and significant financial and reputational damage.
  description: This rule verifies that all privacy anonymization outputs from DLP jobs within Google Cloud are encrypted. To ensure compliance, configure DLP jobs to use Cloud KMS for encryption. This can be checked through the DLP settings in the GCP Console or by using gcloud commands. If outputs are not encrypted, adjust the DLP job configuration to specify an appropriate encryption key.
  references:
  - https://cloud.google.com/dlp/docs/concepts-crypto
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/kms/docs
- rule_id: gcp.dlp.job.privacy_masking_execution_roles_least_privilege
  service: dlp
  resource: job
  requirement: Privacy Masking Execution Roles Least Privilege
  scope: dlp.job.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for DLP Privacy Masking Roles
  rationale: Implementing least privilege for DLP privacy masking roles is crucial to minimize the risk of unauthorized data access and potential data breaches. Over-permissioned roles can lead to exposure of sensitive information, which can result in regulatory non-compliance and damage to the organization's reputation. Ensuring that roles have only the necessary permissions helps mitigate insider threats and meets compliance requirements such as GDPR and HIPAA.
  description: This rule checks for adherence to the principle of least privilege in assigning roles for DLP privacy masking jobs. Ensure that only necessary permissions are granted to roles involved in executing DLP jobs by reviewing IAM policies and removing any excessive permissions. To verify compliance, audit current IAM roles and permissions associated with DLP jobs, and adjust them to meet the minimum required for task execution. Remediate by using predefined roles with the least privilege or custom roles tailored to specific job needs.
  references:
  - https://cloud.google.com/dlp/docs/quickstart
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.dlp.job.privacy_masking_policies_present_for_sensitive_fields
  service: dlp
  resource: job
  requirement: Privacy Masking Policies Present For Sensitive Fields
  scope: dlp.job.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Privacy Masking for Sensitive Data in DLP Jobs
  rationale: Implementing privacy masking policies for sensitive fields in DLP jobs is crucial to protect against unauthorized data exposure and ensure compliance with data protection regulations such as GDPR and HIPAA. Without appropriate masking, sensitive information may be inadvertently exposed, leading to data breaches, financial penalties, and reputational damage.
  description: This rule checks that all DLP jobs have privacy masking policies configured for sensitive fields. To verify, review the DLP job configurations and ensure that masking transformations, such as redaction or tokenization, are applied to sensitive data types like PII or PHI. Remediation involves configuring DLP jobs to include transformation configurations that mask sensitive fields according to organizational policies.
  references:
  - https://cloud.google.com/dlp/docs/creating-job-triggers
  - https://cloud.google.com/dlp/docs/transformations-reference
  - CIS GCP Benchmark v1.3.0 - Section 3.11
  - NIST SP 800-53 Rev. 5 - Security and Privacy Controls for Information Systems
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.dns.managed_zone.dnssec_disabled
  service: dns
  resource: managed_zone
  requirement: Dnssec Disabled
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNSSEC is Enabled for Managed Zones
  rationale: Disabling DNSSEC can expose your domain to DNS spoofing and cache poisoning attacks, which can redirect users to malicious sites without their knowledge. Enabling DNSSEC enhances data integrity and authenticity, protecting your business's reputation and ensuring compliance with regulatory standards such as NIST SP 800-53 and ISO 27001, which mandate the use of secure communications.
  description: This rule checks if DNS Security Extensions (DNSSEC) is disabled for Cloud DNS managed zones. DNSSEC provides cryptographic authentication of DNS data, preventing data tampering and ensuring the integrity of DNS responses. To verify, navigate to the Cloud DNS page in the GCP Console and check the 'DNSSEC' settings for your managed zones. If DNSSEC is disabled, enable it by selecting 'On' under the DNSSEC configuration to secure your DNS infrastructure.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.dns.managed_zone.rsasha1_in_use_to_zone_sign_in_dnssec_configured
  service: dns
  resource: managed_zone
  requirement: Rsasha1 In Use To Zone Sign In Dnssec Configured
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNSSEC Configured Without RSASHA1 for Managed Zones
  rationale: Using RSASHA1 for DNSSEC signing is considered weak due to its vulnerability to cryptographic attacks, potentially leading to DNS spoofing or cache poisoning. This can result in unauthorized data interception or redirection, undermining business operations and violating compliance requirements such as PCI-DSS and NIST SP 800-53.
  description: This rule checks if RSASHA1 is used in DNSSEC configurations for GCP DNS managed zones. RSASHA1 is deprecated and should be replaced with stronger algorithms like RSASHA256 or RSASHA512. To verify, review the DNSSEC settings in the Cloud Console or via gcloud CLI. Remediate by updating the DNSSEC configuration to utilize stronger signing algorithms to enhance security against cryptographic vulnerabilities.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://datatracker.ietf.org/doc/html/rfc6781
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dns.managed_zone.sign_in_dnssec
  service: dns
  resource: managed_zone
  requirement: Sign In Dnssec
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNSSEC is Enabled for Managed Zones
  rationale: Enabling DNS Security Extensions (DNSSEC) on DNS managed zones is crucial for protecting DNS data integrity and authenticity. Without DNSSEC, DNS responses can be spoofed or tampered with, leading to potential man-in-the-middle attacks and data breaches. Compliance with standards like NIST and PCI-DSS often require DNSSEC to prevent such vulnerabilities.
  description: This rule checks whether DNSSEC is enabled for all DNS managed zones in GCP. DNSSEC adds a layer of security by digitally signing DNS data, ensuring its authenticity and integrity. To verify, navigate to the Cloud DNS section in the GCP Console and ensure 'DNSSEC' is set to 'On' for each managed zone. To enable DNSSEC, configure the zone settings to use either 'Transfer' or 'Automatic' signing modes as per your security requirements.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/projects/dnssec
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.dns.managed_zone.zone_cross_account_sharing_restricted
  service: dns
  resource: managed_zone
  requirement: Zone Cross Account Sharing Restricted
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Cross-Account Sharing for DNS Managed Zones
  rationale: Restricting cross-account sharing of DNS managed zones minimizes the risk of unauthorized access and potential data exposure. It enhances the security boundary by ensuring that only designated accounts can manage DNS settings, reducing the likelihood of misconfigurations and unauthorized changes that could lead to service disruptions or data breaches. Compliance with standards like ISO 27001 requires controlled access to sensitive infrastructure components.
  description: This rule checks if DNS managed zones are configured to prevent cross-account sharing, ensuring that only authorized Google Cloud projects or accounts can access and modify DNS settings. Verification involves reviewing IAM policies associated with the managed zones to confirm that no external accounts have unnecessary permissions. Remediation steps include auditing current access permissions, removing any unintended external account permissions, and establishing a process for regular reviews of access controls.
  references:
  - https://cloud.google.com/dns/docs/security-overview
  - https://cloud.google.com/iam/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dns.managed_zone.zone_dnssec_enabled
  service: dns
  resource: managed_zone
  requirement: Zone Dnssec Enabled
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNSSEC is Enabled for GCP Managed DNS Zones
  rationale: Enabling DNSSEC for DNS zones significantly mitigates the risk of DNS spoofing and cache poisoning attacks, which can lead to unauthorized access and data breaches. This feature is critical for maintaining data integrity and trust in DNS responses, supporting compliance with security frameworks that require data protection and integrity measures.
  description: This rule checks whether DNSSEC is enabled for all managed DNS zones in GCP. DNSSEC provides an additional layer of security by digitally signing DNS information, which helps verify the authenticity of DNS data. To verify this setting, navigate to the Cloud DNS section in the GCP Console, select the managed zone, and ensure DNSSEC is enabled in the settings. Remediate by enabling DNSSEC on zones where it is not currently activated, thus ensuring cryptographic protection for DNS queries.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://tools.ietf.org/html/rfc4033
- rule_id: gcp.dns.managed_zone.zone_dnssec_enabled_where_supported
  service: dns
  resource: managed_zone
  requirement: Zone Dnssec Enabled Where Supported
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable DNSSEC for Managed Zones Where Supported
  rationale: Enabling DNSSEC for managed zones ensures integrity and authenticity of DNS data by preventing DNS spoofing attacks. This is crucial for maintaining trust in internet communications and is often required for compliance with standards like NIST and PCI-DSS. Without DNSSEC, domains are vulnerable to cache poisoning, which can lead to unauthorized data interception and service redirection.
  description: This rule checks if DNS Security Extensions (DNSSEC) are enabled for all managed zones within GCP where it is supported. DNSSEC adds a layer of security by digitally signing records, which helps prevent attackers from forging DNS responses. To verify, ensure that the DNSSEC status for each managed zone is set to 'on' in the Google Cloud Console or via gcloud CLI. Remediation involves accessing the managed zone settings and enabling DNSSEC through the 'DNS security settings' tab.
  references:
  - https://cloud.google.com/dns/docs/dnssec
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.dns.managed_zone.zone_dnssec_no_rsa_sha1_key
  service: dns
  resource: managed_zone
  requirement: Zone Dnssec No Rsa Sha1 Key
  scope: dns.managed_zone.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Disable RSA-SHA1 Keys for DNSSEC in Managed Zones
  rationale: Using RSA-SHA1 for DNSSEC can expose DNS zones to cryptographic vulnerabilities, compromising data integrity and privacy. It is crucial to use stronger algorithms to ensure the authenticity of DNS data, prevent cache poisoning attacks, and comply with modern security standards.
  description: This rule checks if DNSSEC is configured using the RSA-SHA1 algorithm in Google Cloud DNS managed zones. RSA-SHA1 is considered weak due to advancements in cryptanalysis. To mitigate risks, configure DNSSEC with stronger algorithms like RSA-SHA256 or RSA-SHA512. Verify DNSSEC settings in the Cloud Console under Cloud DNS > Managed Zones, and update the DNSSEC configuration as necessary to use supported algorithms.
  references:
  - https://cloud.google.com/dns/docs/dnssec-config
  - https://cloud.google.com/security/compliance/cis#cis_google_cloud_computing_foundations_benchmark
  - https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf
  - https://datatracker.ietf.org/doc/rfc6781/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dns.managed_zone.zone_dnssec_zone_signing_algorithm_not_rsa_sha1
  service: dns
  resource: managed_zone
  requirement: Zone Dnssec Zone Signing Algorithm Not Rsa Sha1
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Avoid RSA-SHA1 for DNSSEC Zone Signing in Managed Zones
  rationale: Using RSA-SHA1 for DNSSEC zone signing is deprecated due to its vulnerability to cryptographic attacks, which can lead to domain spoofing and data interception. Adopting stronger algorithms like RSA-SHA256 ensures the integrity and authenticity of DNS data, aligning with security best practices and compliance mandates such as NIST SP 800-57.
  description: This rule checks that DNSSEC zone signing algorithms configured for GCP managed DNS zones do not use the insecure RSA-SHA1 method. To verify, inspect DNSSEC settings in Cloud DNS and ensure the algorithm is set to a stronger option like RSA-SHA256. To remediate, update the DNSSEC configuration in the Cloud Console or via the gcloud CLI to use a secure algorithm, ensuring protection against cryptographic vulnerabilities.
  references:
  - https://cloud.google.com/dns/docs/dnssec-config
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.cloudflare.com/learning/dns/dnssec/dnssec-algorithms/
  - https://www.ietf.org/rfc/rfc6781.txt
- rule_id: gcp.dns.managed_zone.zone_query_logging_enabled
  service: dns
  resource: managed_zone
  requirement: Zone Query Logging Enabled
  scope: dns.managed_zone.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DNS Managed Zone Query Logging is Enabled
  rationale: Enabling query logging in DNS managed zones is critical for maintaining visibility into DNS queries, which helps detect and respond to potential security incidents such as DNS spoofing and cache poisoning. It provides audit trails necessary for compliance with regulatory standards like PCI-DSS and assists in forensic investigations by tracking DNS query activities.
  description: This rule checks if DNS Managed Zones in GCP have query logging enabled. Query logging captures detailed information about DNS queries made against a zone, which can be verified through the Google Cloud Console or using the gcloud command-line tool. To enable logging, access the Cloud DNS settings for the managed zone and activate query logging. This action ensures auditability and enhances monitoring capabilities, facilitating timely detection of suspicious activities.
  references:
  - https://cloud.google.com/dns/docs/query-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.dns.managed_zone.zone_zone_transfer_restricted_or_tsig_required
  service: dns
  resource: managed_zone
  requirement: Zone Zone Transfer Restricted Or Tsig Required
  scope: dns.managed_zone.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNS Zone Transfers Are Restricted or Use TSIG
  rationale: Unrestricted DNS zone transfers can expose sensitive information about the internal network structure, making it easier for attackers to plan targeted attacks. Using TSIG (Transaction Signature) ensures that zone transfers are authenticated and restricted to trusted sources, reducing the risk of data leakage and improving compliance with security frameworks such as NIST and ISO 27001.
  description: This rule checks if DNS managed zones have zone transfers restricted or if Transaction Signature (TSIG) is required. Zone transfers should only be allowed to specific IP addresses or require TSIG keys to authenticate requests, mitigating unauthorized data access. To ensure security, review your DNS managed zone settings in GCP, configure the 'allow_transfer' parameter to include only trusted IPs, or enable TSIG for authentication. This can be verified and configured through the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/dns/docs/tsig
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.dns.policy.logging_enabled
  service: dns
  resource: policy
  requirement: Logging Enabled
  scope: dns.policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DNS Policies Have Logging Enabled
  rationale: Enabling logging for DNS policies is crucial for monitoring and detecting unauthorized access or changes to DNS configurations, which could lead to misconfigurations or data leakage. Logging supports incident response and forensic investigations by providing a detailed record of DNS policy activities. It also helps in meeting compliance requirements for data protection regulations and industry standards.
  description: This rule checks whether logging is enabled for Google Cloud DNS policies to capture detailed records of policy-related activities. To verify, ensure that DNS policies are configured to log to Cloud Logging where audit logs can be monitored. If logging is not enabled, configure the DNS policy to send logs to an appropriate Cloud Logging sink. This can be done via the GCP Console or gcloud CLI by setting the 'enableLogging' field to true in the DNS policy configuration.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.dns.policy.traffic_health_checks_required_for_failover
  service: dns
  resource: policy
  requirement: Traffic Health Checks Required For Failover
  scope: dns.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure DNS Traffic Health Checks for Failover Policies
  rationale: Enforcing traffic health checks in DNS failover policies is crucial for maintaining service availability and preventing unauthorized access. Without health checks, failover mechanisms may not detect outages or performance issues, leading to potential data loss, service disruptions, or security vulnerabilities. This practice is essential for compliance with business continuity and data protection regulations.
  description: This rule verifies that DNS policies configured for failover have traffic health checks enabled. Traffic health checks continuously monitor the availability and performance of DNS endpoints, ensuring that failover is only initiated when a primary endpoint is truly unreachable. To implement this, configure health checks for each endpoint in your DNS policy settings. Remediation involves accessing the DNS policy settings in the Google Cloud Console, enabling health checks, and specifying the appropriate criteria to match your service level agreements (SLAs).
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://cloud.google.com/dns/docs/health-checks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.dns.policy.traffic_policy_documents_encrypted_and_private
  service: dns
  resource: policy
  requirement: Traffic Policy Documents Encrypted And Private
  scope: dns.policy.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DNS Policy Traffic Documents Are Encrypted and Private
  rationale: Encrypting DNS traffic policy documents ensures that sensitive configuration data remains secure and inaccessible to unauthorized entities. This is crucial for preventing data leaks and maintaining privacy, particularly in compliance with regulations such as GDPR and HIPAA, which mandate the protection of sensitive information. Failure to encrypt can lead to data breaches, resulting in reputational damage and potential legal penalties.
  description: This rule checks if DNS policy traffic documents within GCP are encrypted both in transit and at rest. It verifies that the policies are protected using Google Cloud's customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK). To comply, ensure that all DNS traffic policy documents are configured to use encryption by default. Remediation involves setting up encryption keys, associating them with the DNS policies, and confirming that all data transactions are encrypted.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/dns/docs/security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.dns.policy.traffic_targets_approved_domains_only
  service: dns
  resource: policy
  requirement: Traffic Targets Approved Domains Only
  scope: dns.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict DNS Traffic to Approved Domains Only
  rationale: Restricting DNS traffic to approved domains reduces the risk of data exfiltration and prevents communication with malicious or unauthorized domains. This control is critical for maintaining the integrity and confidentiality of sensitive data and ensuring that DNS queries are only directed to trusted destinations, aligning with regulatory frameworks like PCI-DSS and ISO 27001.
  description: This rule ensures that DNS policies within your GCP environment allow traffic only to pre-approved domains. This involves configuring DNS policies to include a list of permitted domains and regularly auditing these settings to ensure compliance. Remediation involves reviewing the DNS policy settings in the Google Cloud Console or via CLI, updating the permitted domains list, and conducting regular policy audits.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.dns.resource_record_set.record_set_alias_targets_in_approved_services
  service: dns
  resource: resource_record_set
  requirement: Record Set Alias Targets In Approved Services
  scope: dns.resource_record_set.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DNS Alias Targets Are From Approved GCP Services
  rationale: Restricting DNS alias targets to approved services helps prevent unauthorized access and data exfiltration by ensuring that only trusted and verified services can be targeted. This mitigates the risk of DNS spoofing attacks, which could lead to users being redirected to malicious sites, compromising sensitive data and potentially violating compliance mandates such as GDPR or HIPAA.
  description: This rule checks DNS resource record sets to ensure that alias targets are limited to a predefined list of approved GCP services. This is verified by reviewing the DNS configuration settings for alias records and validating that targets are within the allowed services. To remediate, configure the DNS alias records to only point to services that are included in the approved list, which can be managed through your organization's GCP IAM policies and DNS settings.
  references:
  - https://cloud.google.com/dns/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.dns.resource_record_set.record_set_caa_records_present_for_root_and_wildcard
  service: dns
  resource: resource_record_set
  requirement: Record Set Caa Records Present For Root And Wildcard
  scope: dns.resource_record_set.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: critical
  title: Ensure CAA Records for Root/Wildcard in DNS Resource Sets
  rationale: CAA records specify which certificate authorities are allowed to issue certificates for a domain, reducing the risk of certificate mis-issuance. Without CAA records for both root and wildcard domains, unauthorized certificates could be issued, potentially leading to man-in-the-middle attacks and data breaches. This is crucial for compliance with security standards such as PCI-DSS and NIST, which emphasize strong authentication and encryption practices.
  description: This rule checks for the presence of CAA records in DNS resource record sets for both root and wildcard domains. To verify, inspect the DNS settings in Google Cloud DNS and ensure that CAA records are configured correctly. Remediation involves adding CAA records that list authorized certificate authorities, using the Google Cloud Console or gcloud command-line tool. This helps prevent unauthorized certificate issuance and strengthens domain security.
  references:
  - https://cloud.google.com/dns/docs/records/caa-records
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library
  - https://tools.ietf.org/html/rfc8659
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.dns.resource_record_set.record_set_dmarc_record_present_when_mx_present
  service: dns
  resource: resource_record_set
  requirement: Record Set Dmarc Record Present When Mx Present
  scope: dns.resource_record_set.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure DMARC Record Exists with MX Record
  rationale: Implementing a DMARC (Domain-based Message Authentication, Reporting & Conformance) record is crucial when an MX (Mail Exchange) record is present to prevent email spoofing and phishing attacks. Without DMARC, organizations risk unauthorized use of their domain in fraudulent emails, potentially leading to brand damage, loss of customer trust, and non-compliance with email security standards.
  description: This rule checks for the presence of a DMARC record in the DNS configuration whenever an MX record is detected. A DMARC record provides email authentication protocols to ensure that legitimate emails are properly authenticated against established SPF and DKIM standards. To verify, inspect the DNS zone file for a TXT record named '_dmarc' that specifies DMARC policies. To remediate, add a proper DMARC record to your DNS configuration with policies aligned to your organization's email security requirements.
  references:
  - https://cloud.google.com/dns/docs/records
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-177r1.pdf
  - https://www.icann.org/resources/pages/dmarc-2018-01-17-en
  - https://tools.ietf.org/html/rfc7489
- rule_id: gcp.dns.resource_record_set.record_set_no_overly_broad_wildcard_records_in_sensiti_zones
  service: dns
  resource: resource_record_set
  requirement: Record Set No Overly Broad Wildcard Records In Sensiti Zones
  scope: dns.resource_record_set.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Wildcard Records in Sensitive DNS Zones
  rationale: Overly broad wildcard DNS records in sensitive zones can lead to security vulnerabilities by allowing unintended subdomains to resolve, potentially facilitating domain hijacking or phishing attacks. These records can also expose the organization to compliance risks by violating best practices for DNS management, impacting confidentiality and integrity of critical services.
  description: This rule checks for the presence of overly broad wildcard DNS records (e.g., '*.example.com') in sensitive DNS zones within GCP. Such configurations should be avoided because they could unintentionally expose internal services or allow unauthorized access to subdomains. To verify, review DNS configurations for wildcard entries in sensitive zones and replace them with specific records where possible. Remediation involves carefully assessing DNS needs and limiting wildcard usage to non-sensitive contexts.
  references:
  - https://cloud.google.com/dns/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://owasp.org/www-project-top-ten/2017/A6-Security-Misconfiguration
- rule_id: gcp.dns.resource_record_set.record_set_no_rfc1918_in_public_zones
  service: dns
  resource: resource_record_set
  requirement: Record Set No Rfc1918 In Public Zones
  scope: dns.resource_record_set.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disallow RFC1918 IPs in Public DNS Zones
  rationale: Exposing RFC1918 IP addresses in public DNS zones can lead to information leakage, allowing potential attackers to gather internal network topology and plan attacks. This can result in unauthorized access, data breaches, and non-compliance with regulations mandating the protection of internal infrastructure details.
  description: This rule checks that no DNS records in public DNS zones contain RFC1918 private IP addresses. RFC1918 IPs are intended for internal use only and should not be exposed to the public internet. Verification involves auditing DNS records to confirm the absence of these private IPs. Remediation includes removing or replacing these records with appropriate public IPs or moving them to private DNS zones.
  references:
  - https://cloud.google.com/dns/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-compliance-guide
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.elasticsearch.cluster.domains_access_control_enabled
  service: elasticsearch
  resource: cluster
  requirement: Domains Access Control Enabled
  scope: elasticsearch.cluster.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Domains Access Control for Elasticsearch Clusters
  rationale: Enabling domains access control for Elasticsearch clusters is critical to prevent unauthorized access and data breaches. By restricting access to specific domains, organizations can mitigate risks associated with exposed sensitive data and ensure compliance with regulatory requirements such as GDPR and CCPA. This measure also protects against unauthorized data manipulation and exfiltration, which can lead to significant financial and reputational damage.
  description: This rule checks whether domains access control is enabled for Elasticsearch clusters within Google Cloud Platform. To verify, ensure that the Elasticsearch clusters are configured to only allow traffic from trusted domains using access policies. Remediation involves setting up Identity and Access Management (IAM) policies that limit access to specific domains or IP ranges, ensuring that only authorized users and services can interact with the cluster.
  references:
  - https://cloud.google.com/elasticsearch/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.elasticsearch.cluster.domains_encryption_at_rest_enabled
  service: elasticsearch
  resource: cluster
  requirement: Domains Encryption At Rest Enabled
  scope: elasticsearch.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption at Rest for Elasticsearch Domains
  rationale: Enabling encryption at rest for Elasticsearch domains protects sensitive data from unauthorized access and potential breaches. This is crucial for maintaining data confidentiality, especially in regulated industries where compliance with standards such as PCI-DSS, HIPAA, and ISO 27001 is mandatory. Encrypting data at rest mitigates risks associated with data theft and unauthorized data exposure.
  description: This rule checks if Elasticsearch clusters in Google Cloud have encryption at rest enabled. Specifically, it verifies that the 'enableEncryptionAtRest' setting is active, ensuring that all data stored within the cluster is encrypted. To verify, check the cluster settings in the GCP Console or use the gcloud CLI to confirm the encryption status. To remediate, update the cluster configuration to enable encryption at rest, ensuring compliance with data protection policies.
  references:
  - https://cloud.google.com/elasticsearch/docs/encryption-at-rest
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://www.hhs.gov/hipaa/index.html
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.elasticsearch.cluster.domains_logging_enabled
  service: elasticsearch
  resource: cluster
  requirement: Domains Logging Enabled
  scope: elasticsearch.cluster.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Domains Logging for Elasticsearch Clusters
  rationale: Enabling domains logging for Elasticsearch clusters is crucial for monitoring and auditing access and changes to your data. Without logging, unauthorized access or misconfigurations may go undetected, posing risks to data integrity and compliance with regulations like GDPR and HIPAA. Proper logging helps in forensic analysis and incident response, ensuring that potential threats are quickly identified and mitigated.
  description: This rule checks if Elasticsearch clusters have domains logging enabled, which records all access and administrative actions. To verify, ensure that logging is activated in the Elasticsearch cluster settings within your GCP console. Remediation involves navigating to the cluster's configuration and enabling logging, ensuring all actions on the cluster are logged for audit purposes. This enhances visibility and accountability, crucial for maintaining a secure environment.
  references:
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/solutions/elasticsearch-on-gcp
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.elasticsearch.cluster.domains_node_to_node_encryption_enabled
  service: elasticsearch
  resource: cluster
  requirement: Domains Node To Node Encryption Enabled
  scope: elasticsearch.cluster.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable Node-to-Node Encryption for Elasticsearch Clusters
  rationale: Enabling node-to-node encryption in Elasticsearch clusters is crucial for protecting sensitive data in transit from unauthorized access and potential eavesdropping. This security measure helps mitigate risks such as data breaches and ensures compliance with industry standards and regulations like GDPR and HIPAA, which mandate robust data protection practices.
  description: This rule checks if node-to-node encryption is enabled for Elasticsearch clusters on GCP. Node-to-node encryption ensures that data transmitted between nodes within the Elasticsearch cluster is encrypted. To verify, check the Elasticsearch cluster settings for 'xpack.security.transport.ssl.enabled' set to 'true'. Remediation involves configuring the Elasticsearch service to enable this setting, which can typically be done via the service's configuration file or API.
  references:
  - https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-tls.html
  - https://cloud.google.com/solutions/elasticsearch-on-gcp
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_access_log_fields_identity_present
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Access Log Fields Identity Present
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Access Logs Include Identity Information
  rationale: Capturing identity information in API access logs is crucial for auditing and tracking access patterns, which helps in detecting unauthorized access and potential security breaches. This practice supports compliance with regulations such as NIST SP 800-53 and PCI-DSS, which require detailed audit logs for sensitive data access.
  description: This rule checks whether identity fields are present in the API Monitoring access logs for Google Cloud Endpoints. To verify, ensure that the `logConfig` settings for endpoints include identity-related fields. Remediation involves configuring the service to include identity information in its access logs, which can be accomplished by updating the service's logging configuration in the Google Cloud Console or using the `gcloud endpoints` command-line tool.
  references:
  - https://cloud.google.com/endpoints/docs/openapi/logging
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_access_log_sink_configured
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Access Log Sink Configured
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Access Logs are Configured for Endpoints
  rationale: Configuring API access logs for endpoints is crucial for monitoring and auditing API usage, which helps in identifying unauthorized access attempts and potential data breaches. This enhances the security posture by ensuring compliance with regulations such as NIST and SOC2, which mandate detailed logging and monitoring of access to sensitive data.
  description: This rule checks if an API access log sink is configured for GCP endpoints. Proper configuration involves setting up a log sink that captures API access logs to a centralized logging system like Google Cloud Logging. To verify, ensure the log sink exists and is collecting the relevant logs. Remediation involves configuring a log sink with the required permissions to capture and store these logs in Google Cloud Logging, enabling real-time monitoring and alerts for suspicious activities.
  references:
  - https://cloud.google.com/endpoints/docs/openapi/logging
  - https://cloud.google.com/logging/docs/export/configure_export
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_access_logging_enabled
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Access Logging Enabled
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Monitoring Access Logging is Enabled
  rationale: Enabling API access logging is crucial for auditing and monitoring access to your APIs, which helps in identifying unauthorized access attempts and analyzing usage patterns. This is important for compliance with security standards and regulations such as PCI-DSS and HIPAA, which mandate logging of access to sensitive data. Failure to enable logging can result in undetected security breaches and non-compliance with regulatory requirements.
  description: 'This rule checks if API Monitoring''s access logging is enabled for endpoints on Google Cloud Platform. To verify, ensure that logging is configured in the Endpoints configuration file (openapi.yaml or swagger.yaml) by setting ''x-google-endpoints'' field with appropriate logging settings. Remediation involves editing the configuration file to include ''logging: true'' under ''x-google-endpoints'' and redeploying the API configuration. This ensures that all access attempts are logged and monitored for security and compliance purposes.'
  references:
  - https://cloud.google.com/endpoints/docs/openapi/logging
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-security-rule/
  - https://cloud.google.com/architecture/best-practices-for-private-endpoints
  - https://cloud.google.com/logging/docs/audit/configure-data-access
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_execution_logging_level_minimum_error
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Execution Logging Level Minimum Error
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure API Execution Logging Level is Set to Minimum Error
  rationale: Proper logging of API execution events at a minimum level of error is crucial for identifying potential security threats and operational issues within your GCP environment. Logging errors helps in tracking failed API interactions, which could indicate misuse or misconfiguration, and supports compliance with audit requirements by maintaining a history of access and actions.
  description: This rule checks that the API execution logging level for GCP Endpoints is set to at least 'ERROR'. This ensures that all error-level events are logged, providing visibility into issues that could affect the reliability and security of your services. To verify, navigate to the Endpoints configuration and ensure the logging level is set to 'ERROR' or higher. Remediation involves updating the logging configuration to meet this minimum level.
  references:
  - https://cloud.google.com/endpoints/docs/openapi/logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.endpoints.endpoint.api_monitoring_api_logs_retention_days_minimum
  service: endpoints
  resource: endpoint
  requirement: API Monitoring API Logs Retention Days Minimum
  scope: endpoints.endpoint.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Minimum Retention for API Monitoring Logs
  rationale: Retaining API monitoring logs for a minimum period is critical for incident response, forensic investigations, and compliance with regulatory requirements. Insufficient log retention may result in loss of data necessary for detecting breaches or conducting audits, increasing the risk of undetected security incidents.
  description: This rule checks that API monitoring logs for GCP Endpoints are retained for a minimum specified number of days. To verify, access the Cloud Logging configuration for each endpoint and ensure the log retention policy is set to the organization's required minimum. Remediation involves adjusting the log retention settings in GCP Logging to meet or exceed the minimum retention period prescribed by your security policy.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/architecture/framework/security/logging-and-monitoring
  - CIS Google Cloud Platform Foundations Benchmark
  - NIST SP 800-53 Rev. 5
  - PCI-DSS v3.2.1 Requirement 10.7
  - ISO/IEC 27001:2013 Information Security Management
- rule_id: gcp.essentialcontacts.contact.organization_notification_categories_coverage
  service: essentialcontacts
  resource: contact
  requirement: Organization Notification Categories Coverage
  scope: essentialcontacts.contact.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Comprehensive Organization Notification Categories Coverage
  rationale: Ensuring that all relevant notification categories are covered by essential contacts is crucial for timely response to incidents and maintaining compliance with security policies. Inadequate notification coverage can lead to missed alerts about potential security threats, impacting the organization's ability to respond effectively and protect sensitive data.
  description: This rule checks whether all organization notification categories are adequately covered by essential contacts. It verifies that at least one contact is configured to receive notifications for each critical category such as security, technical issues, and billing. To remediate, ensure that essential contacts are assigned to all relevant notification categories. Use the GCP Console or CLI to review and update contact configurations as necessary.
  references:
  - https://cloud.google.com/resource-manager/docs/essential-contacts/overview
  - https://cloud.google.com/resource-manager/docs/essential-contacts/using-essential-contacts
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.filestore.instance.backup_enabled
  service: filestore
  resource: instance
  requirement: Backup Enabled
  scope: filestore.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Filestore Instance Backups Are Enabled
  rationale: Enabling backups for Filestore instances is crucial for business continuity and data recovery in the event of data loss, corruption, or system failure. Without regular backups, critical business data stored in Filestore could be permanently lost, leading to significant operational disruptions and financial loss. Additionally, maintaining backups aligns with compliance requirements such as ISO 27001 and SOC2, which mandate data protection and disaster recovery strategies.
  description: This rule verifies that Google Cloud Filestore instances have backups enabled to ensure data resilience. To check this, ensure that the Filestore instance's backup configuration is set to 'enabled' in the GCP Console or via the Cloud SDK. If backups are not enabled, configure them by accessing the Filestore instance settings and enabling the backup option, specifying the frequency and retention period according to your organizational policy. This proactive measure ensures that critical data is safeguarded against unforeseen events.
  references:
  - https://cloud.google.com/filestore/docs/backup
  - https://cloud.google.com/security/compliance/cis#section8
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/dr-scenarios
  - https://cloud.google.com/filestore/docs/creating-backups
- rule_id: gcp.filestore.instance.encryption_at_rest_enabled
  service: filestore
  resource: instance
  requirement: Encryption At Rest Enabled
  scope: filestore.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Filestore Instances Have Encryption at Rest Enabled
  rationale: Encrypting data at rest in Filestore instances is crucial for protecting sensitive information from unauthorized access and breaches in case of physical theft or compromise. Failing to enable encryption can lead to significant data exposure risks, non-compliance with regulatory standards like PCI-DSS, HIPAA, and GDPR, and potential financial and reputational damage.
  description: This rule checks whether encryption at rest is enabled for Google Cloud Filestore instances. Encryption at rest protects data by automatically encrypting it before it is written to disk and decrypting it when it is read. To verify, ensure that the Filestore instances are configured to use the Google-managed encryption or customer-managed encryption keys (CMEK). Remediation involves updating the Filestore instance settings to enable encryption by default during creation or configuring CMEK for existing instances.
  references:
  - https://cloud.google.com/filestore/docs/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.filestore.instance.have_backup_enabled
  service: filestore
  resource: instance
  requirement: Have Backup Enabled
  scope: filestore.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Ensure Filestore Instances Have Backups Enabled
  rationale: Enabling backups for Filestore instances is crucial for data recovery in case of accidental deletion, corruption, or ransomware attacks. It helps maintain business continuity and minimizes downtime by allowing restoration of data to a previous state. Additionally, certain compliance frameworks require regular data backups to ensure data integrity and availability.
  description: This rule checks if backups are enabled for Filestore instances. Backups should be configured to run automatically on a schedule that aligns with the organization's recovery point objectives (RPO). To verify, navigate to the Filestore instance in the GCP Console and check the backup configuration settings. If backups are not enabled, configure a backup schedule through the Cloud Console or use the gcloud command-line tool. Ensure that backup data is stored in a secure location and consider retention policies to manage storage costs.
  references:
  - https://cloud.google.com/filestore/docs/backups
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/best-practices-for-enterprise-organizations
- rule_id: gcp.firestore.collection.encryption_enabled_gcp_logging_kms_encryption_enable_enabled
  service: firestore
  resource: collection
  requirement: Encryption Enabled Gcp Logging KMS Encryption Enable Enabled
  scope: firestore.collection.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Collections Use KMS for At-Rest Encryption
  rationale: Encrypting Firestore data at rest using Cloud KMS protects sensitive information from unauthorized access and meets compliance requirements for data protection. Without encryption, data breaches can lead to significant financial loss, reputational damage, and regulatory penalties.
  description: This rule verifies that Firestore collections are encrypted using Google Cloud's Key Management Service (KMS) to provide enhanced security for data at rest. To ensure compliance, confirm that the 'encryptionConfig' field in Firestore settings is configured with a valid KMS key. Remediation involves updating Firestore collection settings to specify a KMS key for encryption via the GCP Console or the gcloud CLI.
  references:
  - https://cloud.google.com/firestore/docs/encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/kms/docs
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.firestore.collection.encryption_enabled_gcp_logging_kms_encryption_enable_logging
  service: firestore
  resource: collection
  requirement: Encryption Enabled Gcp Logging KMS Encryption Enable Logging
  scope: firestore.collection.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Collections Use KMS for Encryption and Logging
  rationale: Enabling KMS encryption and logging for Firestore collections protects sensitive data from unauthorized access and provides audit trails for compliance. This is critical for preventing data breaches and ensuring data integrity, especially for organizations handling regulated data under frameworks like HIPAA or PCI-DSS.
  description: This rule checks whether Firestore collections are encrypted using Google Cloud KMS and that encryption operations are logged. To verify, ensure that the Firestore collection's encryption settings are configured to use a customer-managed encryption key (CMEK) with logging enabled in Cloud Logging. Remediation involves assigning a KMS key to Firestore collections and configuring Cloud Logging to capture encryption events.
  references:
  - https://cloud.google.com/firestore/docs/encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/logging/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.hhs.gov/hipaa/index.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.firestore.database.backup_enabled
  service: firestore
  resource: database
  requirement: Backup Enabled
  scope: firestore.database.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: medium
  title: Ensure Firestore Database Backups Are Enabled
  rationale: Enabling backups for Firestore databases is crucial to protect against data loss due to accidental deletions, corruption, or malicious attacks. Without backups, restoring data to its prior state is impossible, which can result in significant operational disruptions, financial losses, and non-compliance with data protection regulations such as GDPR or HIPAA.
  description: This rule checks whether automatic backups are enabled for Firestore databases. To verify, ensure that the Firestore service is configured to schedule regular backups through the Google Cloud Console or via the gcloud command line tool. Remediation involves setting up automated backups by navigating to the Firestore settings in the Google Cloud Console, enabling the backup feature, and configuring the backup schedule according to business needs.
  references:
  - https://cloud.google.com/firestore/docs/backup-restore
  - https://cloud.google.com/security/compliance/cis
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/architecture/securing-firestore-data
- rule_id: gcp.firestore.database.encryption_enabled
  service: firestore
  resource: database
  requirement: Encryption Enabled
  scope: firestore.database.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Databases Have Encryption at Rest Enabled
  rationale: Enabling encryption at rest for Firestore databases is crucial to protect sensitive data from unauthorized access and breaches. Encrypting data helps meet compliance requirements such as GDPR, HIPAA, and PCI-DSS by safeguarding personal and financial information. Without encryption, organizations risk data exposure through potential insider threats or physical theft of storage devices.
  description: This rule checks if Firestore databases have encryption at rest enabled, ensuring data is encrypted using Google-managed keys by default. To verify, navigate to the GCP Console, select Firestore, and confirm that the database encryption settings indicate 'Google-managed encryption keys.' If not enabled, configure encryption by updating the Firestore settings to use customer-managed encryption keys (CMEK) for enhanced security control. Regular audits and monitoring of encryption status are recommended to maintain compliance.
  references:
  - https://cloud.google.com/firestore/docs/security/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.firestore.database.global_tables_enabled
  service: firestore
  resource: database
  requirement: Global Tables Enabled
  scope: firestore.database.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Firestore Global Tables Are Enabled for Data Redundancy
  rationale: Enabling global tables in Firestore enhances data redundancy and availability by distributing data across multiple regions, reducing the risk of data loss due to regional failures. This capability is critical for maintaining business continuity and meeting compliance requirements such as GDPR and CCPA, which mandate data availability and integrity. Without global tables, an organization may face increased risk of data unavailability and potential breaches of data protection regulations.
  description: This rule checks whether Firestore databases have global tables enabled, which allows for automatic data replication across multiple regions. To verify, navigate to the Firestore console and review the database configuration for multi-region support. To enable it, modify the database settings to use a multi-region location from the Firestore settings. This ensures improved data durability and meets compliance needs by leveraging GCP's multi-region infrastructure.
  references:
  - https://cloud.google.com/firestore/docs/locations
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.firestore.database.pitr_enabled
  service: firestore
  resource: database
  requirement: Pitr Enabled
  scope: firestore.database.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Firestore Database Point-In-Time Recovery (PITR) is Enabled
  rationale: Enabling Point-In-Time Recovery (PITR) for Firestore databases is crucial for mitigating the risk of data loss due to accidental deletions or malicious activities. It provides the ability to restore databases to a specific point in time, which is vital for business continuity and meets compliance requirements for data protection and privacy regulations such as GDPR and HIPAA.
  description: This rule checks whether Point-In-Time Recovery (PITR) is enabled for Firestore databases. To verify, navigate to the Firestore section in the GCP Console and check the backup and restore settings to ensure PITR is activated. If not enabled, configure PITR by setting up automated backups and restore capabilities through the Firestore settings. This step ensures that you can recover data to any point within the configured retention period, enhancing data resilience and reducing downtime.
  references:
  - https://cloud.google.com/firestore/docs/backups
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-iec-27001-information-security.html
- rule_id: gcp.firestore.database.tables_kms_cmk_encryption_enabled
  service: firestore
  resource: database
  requirement: Tables KMS CMK Encryption Enabled
  scope: firestore.database.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Tables Use KMS CMK for Encryption
  rationale: Encrypting Firestore tables with a Customer-Managed Key (CMK) ensures that sensitive data is protected with a key that you control, enhancing security and compliance with regulations such as GDPR and HIPAA. This reduces the risk of unauthorized data access, especially in scenarios of data breaches or insider threats, by providing an additional layer of encryption management.
  description: This rule checks if Firestore databases are encrypted using a Customer-Managed Key (CMK) via Google Cloud Key Management Service (KMS). To verify, ensure that the encryption configuration for Firestore specifies a KMS key rather than the default Google-managed encryption. Remediation involves updating Firestore database settings to utilize a specific KMS key under your control, which can be done through the Google Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/firestore/docs/server-side-encryption
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.firestore.database.tables_pitr_enabled
  service: firestore
  resource: database
  requirement: Tables Pitr Enabled
  scope: firestore.database.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Enable PITR for Firestore Databases
  rationale: Point-in-Time Recovery (PITR) enables you to restore your Firestore database to any point within a retention window, mitigating data loss risks due to accidental deletes or corruption. This is crucial for ensuring business continuity, protecting sensitive information from inadvertent exposures, and meeting data protection regulations such as GDPR and CCPA.
  description: This rule checks if Point-in-Time Recovery is enabled for Firestore databases. To verify, ensure that the Firestore settings include a retention policy that supports PITR. Remediation involves configuring database settings to enable PITR, providing the ability to recover data to a specific time, enhancing data resilience and privacy compliance.
  references:
  - https://cloud.google.com/firestore/docs/best-practices
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/firestore/docs/manage-data
- rule_id: gcp.firestore.document.data_protection_storage_table_encryption_at_rest_enabled
  service: firestore
  resource: document
  requirement: Data Protection Storage Table Encryption At Rest Enabled
  scope: firestore.document.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Firestore Documents Use Encryption At Rest
  rationale: Data stored in Firestore must be encrypted at rest to protect sensitive information from unauthorized access and comply with regulations such as GDPR and HIPAA. Without encryption, data breaches could expose critical business information and lead to significant financial and reputational damage. Encryption at rest mitigates risks by ensuring that data is unreadable without proper decryption keys.
  description: This rule checks whether Firestore documents are encrypted at rest using Google-managed encryption keys. Firestore automatically encrypts all data before it is written to disk and decrypts it when read by an authorized user. To verify, ensure that Firestore databases are configured to use default Google-managed encryption. No additional user configuration is needed unless customer-managed encryption keys are required for enhanced security. Remediation involves reviewing Firestore settings and enabling customer-managed keys if needed.
  references:
  - https://cloud.google.com/firestore/docs/server-side-encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - Section 7.5
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-encryption-requirements/
- rule_id: gcp.firestore.document.data_protection_storage_table_private_network_only_supported
  service: firestore
  resource: document
  requirement: Data Protection Storage Table Private Network Only Supported
  scope: firestore.document.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict Firestore Document Access to Private Network
  rationale: Restricting Firestore document access to a private network reduces the risk of unauthorized access from the public internet, thereby protecting sensitive data from potential breaches. This control is crucial for meeting regulatory requirements such as GDPR and HIPAA, which mandate strong data protection measures. It also mitigates risks associated with network-based attacks, including DDoS and data exfiltration.
  description: This rule checks if Firestore documents are configured to allow access only from a private network. To verify, ensure that Firestore's network settings are configured to use VPC Service Controls, which limit access to the internal IP range of your organization. Remediation involves setting up VPC Service Controls and configuring the access policy to restrict network access to specified internal IP ranges only.
  references:
  - https://cloud.google.com/firestore/docs/security/rules-conditions
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.firestore.document.data_protection_storage_table_rbac_least_privilege
  service: firestore
  resource: document
  requirement: Data Protection Storage Table RBAC Least Privilege
  scope: firestore.document.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Firestore Document RBAC Uses Least Privilege
  rationale: Implementing least privilege for Firestore documents minimizes unauthorized access risks and potential data breaches. This practice is crucial for protecting sensitive information against insider threats and external attacks, and it supports compliance with regulations such as PCI-DSS, HIPAA, and GDPR, which mandate strict access controls and data protection measures.
  description: This rule checks if Firestore document access permissions follow the principle of least privilege by ensuring roles are assigned with minimal necessary permissions. Verify that IAM roles like 'roles/firestore.viewer' or custom roles with restricted permissions are used instead of broad roles like 'roles/editor'. Remediation involves auditing and adjusting IAM policies to limit access based on job function, using predefined roles where possible and creating custom roles for specific needs.
  references:
  - https://cloud.google.com/firestore/docs/security/get-started
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
- rule_id: gcp.healthcare.consent_store.privacy_consent_access_rbac_least_privilege
  service: healthcare
  resource: consent_store
  requirement: Privacy Consent Access RBAC Least Privilege
  scope: healthcare.consent_store.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Consent Store Access
  rationale: Implementing least privilege access for healthcare consent stores is critical to protect sensitive patient information and comply with healthcare regulations such as HIPAA. Over-privileged access can lead to unauthorized data exposure, posing significant risks to patient privacy and the organization's compliance standing. Proper RBAC minimizes attack vectors and aligns with industry best practices for data security.
  description: This rule ensures that roles assigned to access healthcare consent stores in GCP are configured with the minimum necessary permissions. It checks for overly permissive roles and recommends adjusting IAM policies to limit access based on the principle of least privilege. Verification involves auditing IAM roles and bindings for services interacting with consent stores, and remediation includes modifying roles to remove unnecessary permissions and regularly reviewing access policies.
  references:
  - https://cloud.google.com/healthcare/docs/how-tos/permissions-healthcare-api
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/overview/whitepapers
- rule_id: gcp.healthcare.consent_store.privacy_encrypted
  service: healthcare
  resource: consent_store
  requirement: Privacy Encrypted
  scope: healthcare.consent_store.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Consent Store Data is Encrypted at Rest
  rationale: Encrypting data in the consent store is crucial to protect sensitive patient information from unauthorized access and data breaches. This practice is essential for maintaining patient trust, complying with healthcare regulations like HIPAA, and mitigating risks associated with data exposure. Encryption helps safeguard data integrity and confidentiality, reducing the potential for costly legal issues and reputational damage.
  description: This check verifies that all data in the Google Cloud Healthcare API's consent store is encrypted at rest using customer-managed encryption keys (CMEK). To ensure compliance, enable CMEK for your consent store by configuring a Cloud Key Management Service (KMS) key. Verify encryption settings in the Google Cloud Console under the consent store configuration or use the gcloud CLI to confirm CMEK is enabled. Remediation involves updating consent store settings to use CMEK and regularly rotating keys.
  references:
  - https://cloud.google.com/healthcare/docs/how-tos/consent-management
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 7.1
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/kms/docs/how-tos
  - https://cloud.google.com/security/compliance
- rule_id: gcp.healthcare.consent_store.privacy_rights_execution_roles_least_privilege
  service: healthcare
  resource: consent_store
  requirement: Privacy Rights Execution Roles Least Privilege
  scope: healthcare.consent_store.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Healthcare Consent Store Roles
  rationale: Adhering to the principle of least privilege for roles managing healthcare consent stores is crucial to minimize unauthorized access and potential data breaches. This measure helps protect sensitive patient information, reduces the risk of inadvertent data leaks, and ensures compliance with healthcare regulations like HIPAA, which mandate strict access controls over electronic personal health information (ePHI).
  description: This rule checks if roles assigned to users managing healthcare consent stores in GCP are configured with the least privilege necessary to execute privacy rights. Ensure that only essential permissions are granted by auditing current IAM roles and adjusting them to include only the necessary actions, such as 'consentStores.retrieve' and 'consentStores.evaluate'. Use GCP IAM policy review tools to identify any overly permissive roles, and update them in the IAM settings of the Healthcare API. Regular audits and role adjustments should be conducted to maintain compliance.
  references:
  - https://cloud.google.com/healthcare/docs/how-tos/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/iam/docs/creating-managing-roles
- rule_id: gcp.healthcare.consent_store.privacy_rights_workflow_storage_encrypted
  service: healthcare
  resource: consent_store
  requirement: Privacy Rights Workflow Storage Encrypted
  scope: healthcare.consent_store.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Encrypt Consent Store Data at Rest for Privacy Rights Compliance
  rationale: Encrypting data at rest in GCP Healthcare Consent Stores is crucial to protect sensitive health information from unauthorized access and potential data breaches. It helps meet compliance requirements such as HIPAA, which mandates the safeguarding of personal health information, and mitigates risks associated with data theft or exposure. This encryption ensures that even if data is accessed improperly, it remains unreadable without the proper decryption keys.
  description: This rule checks that all data stored in the GCP Healthcare Consent Store, specifically related to privacy rights workflows, is encrypted at rest. Verify that Google-managed encryption is enabled by default or that customer-managed encryption keys (CMEK) are utilized. Remediation involves configuring the consent store to use CMEK if additional control over encryption keys is required. Ensure that the encryption settings are in line with organizational policies and regulatory standards to maintain data integrity and confidentiality.
  references:
  - https://cloud.google.com/healthcare/docs/how-tos/consent-management
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/healthcare/docs/concepts/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.group.has_users
  service: iam
  resource: group
  requirement: Has Users
  scope: iam.group.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Groups Contain Active Users
  rationale: IAM Groups without users can lead to administrative overhead and security blind spots as they may be overlooked in access reviews. Groups with no members might indicate orphaned resources, potentially leading to policy misconfigurations or unauthorized access if old permissions are accidentally applied. Ensuring groups have active users aids in maintaining a clean and secure access management policy, aligning with compliance standards such as NIST and ISO 27001.
  description: This rule checks for IAM groups in your GCP environment that do not contain any active users. An IAM group should have at least one member to justify its existence and ensure proper access management. To verify, review the members of each group via the Google Cloud Console or use the 'gcloud iam groups list-memberships' command. If a group is found to be empty, evaluate its necessity and either populate it with appropriate users or remove it if it is redundant.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/manage-groups
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
- rule_id: gcp.iam.group.identity_access_rbac_attached_policies_not_admin_star
  service: iam
  resource: group
  requirement: Identity Access RBAC Attached Policies Not Admin Star
  scope: iam.group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Admin Role Assignments in IAM Group Policies
  rationale: Assigning overly permissive roles such as 'roles/*admin' to IAM groups can lead to unauthorized access and potential data breaches. This misconfiguration poses a significant security risk by allowing broad privilege escalation, violating the principle of least privilege and potentially breaching compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks for IAM groups with attached policies granting any 'roles/*admin' permissions, which should be avoided to maintain security best practices. The check ensures that IAM roles are assigned with the least privilege necessary. To remediate, review and modify IAM group policies to replace 'roles/*admin' with more granular permissions, and regularly audit group roles to ensure compliance and security.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/creating-managing-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.iam.group.identity_access_rbac_external_sharing_restricted_w_supported
  service: iam
  resource: group
  requirement: Identity Access RBAC External Sharing Restricted W Supported
  scope: iam.group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict External Sharing for IAM Groups in GCP
  rationale: Restricting external sharing of IAM groups is critical to prevent unauthorized access to resources. Uncontrolled external access can lead to data breaches, non-compliance with data protection regulations, and increased risk of insider threats. Adhering to this practice helps maintain organizational security posture and supports compliance with frameworks like ISO 27001 and SOC2.
  description: This rule checks for IAM groups in GCP that have been shared externally beyond the organization. It ensures that group membership and permissions are not granted to identities outside the trusted domain. Administrators should review group access policies and configure identity sharing settings to restrict external access, using conditional roles and Google Workspace integration. Remediation involves auditing group memberships and adjusting IAM policies to limit external sharing capabilities.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.iam.group.identity_access_rbac_no_inline_policies
  service: iam
  resource: group
  requirement: Identity Access RBAC No Inline Policies
  scope: iam.group.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Use of Inline Policies for IAM Group Access
  rationale: Inline policies directly attached to IAM groups can lead to complex permission management and potential security risks due to difficulties in tracking and auditing access rights. This can result in excessive privileges and violate compliance frameworks like NIST and ISO 27001, which require clear access control mechanisms and auditability.
  description: This rule checks for the presence of inline policies directly attached to IAM groups, which should be avoided in favor of managed policies to ensure clear and manageable access controls. To verify, review IAM group settings in the GCP Console or via gcloud CLI for inline policies and transition them to managed policies where necessary. Remediation involves detaching inline policies and developing equivalent managed policies for improved governance and auditability.
  references:
  - https://cloud.google.com/iam/docs/policies-overview#inline_policies
  - https://cloud.google.com/docs/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/using-managed-policies
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.iam.key.90_days
  service: iam
  resource: key
  requirement: 90 Days
  scope: iam.key.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Rotate IAM Keys Every 90 Days
  rationale: Regular rotation of IAM keys mitigates the risk of unauthorized access due to key compromise, which can lead to data breaches and unauthorized modifications. This practice aligns with security best practices and compliance requirements to ensure ongoing protection of sensitive resources and data in Google Cloud environments.
  description: This rule checks for IAM keys that have not been rotated in the last 90 days. Stale keys can pose a security risk by potentially being exposed to unauthorized users. To verify, inspect the last used date of each IAM key via the GCP Console or CLI. Remediate by creating a new key and removing the old key, ensuring minimal disruption to services relying on these keys.
  references:
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.iam.key.access_key_90_days
  service: iam
  resource: key
  requirement: Access Key 90 Days
  scope: iam.key.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Rotate IAM Access Keys Older Than 90 Days
  rationale: Stale IAM access keys pose a significant security risk as they may be exposed or compromised over time, leading to unauthorized access to resources. Regular rotation of these keys mitigates risks associated with key leakage and meets compliance requirements such as PCI-DSS and ISO 27001, ensuring that only authorized identities access critical resources.
  description: This rule checks for IAM access keys in GCP that are older than 90 days and prompts for their rotation. Regular rotation of access keys is a security best practice that helps prevent unauthorized access resulting from key exposure or compromise. Verify the age of keys by reviewing the IAM console under 'Service Accounts' and 'Keys'. Remediate by creating new keys and deleting the old ones, ensuring to update any systems or applications with the new key credentials.
  references:
  - https://cloud.google.com/iam/docs/using-iam-securely#best_practices_for_managing_service_account_keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.iam.key.key_configured
  service: iam
  resource: key
  requirement: Key Configured
  scope: iam.key.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure IAM Keys are Properly Configured and Managed
  rationale: Improperly configured IAM keys can lead to unauthorized access, data breaches, and loss of sensitive information. This poses significant risks to business continuity and compliance with regulations such as PCI-DSS and HIPAA, which require rigorous access controls to protect data integrity and confidentiality.
  description: This rule checks if IAM keys are configured according to security best practices, including key rotation policies and usage restrictions. It ensures keys are not left active indefinitely and are used only where necessary. To verify, review IAM policies in the Google Cloud Console or via gcloud commands, and implement automated key rotation and monitoring practices. Remediation involves setting up key policies that enforce expiration and logging access attempts.
  references:
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/index.html
  - https://cloud.google.com/security/best-practices
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.iam.policy.attached_only_to_group_or_roles
  service: iam
  resource: policy
  requirement: Attached Only To Group Or Roles
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Are Attached Only to Groups or Roles
  rationale: Attaching IAM policies directly to users can lead to permissions being overlooked when users change roles or leave the organization, increasing the risk of privilege misuse. Ensuring policies are attached to groups or roles facilitates better management of permissions and aligns with least privilege principles, reducing security vulnerabilities and aiding in compliance with frameworks like ISO 27001 and SOC 2.
  description: This rule checks that IAM policies are not directly attached to individual user accounts but are instead assigned to groups or roles. This practice helps maintain organized and manageable access control by ensuring permissions are uniformly applied and easily auditable. To verify, review IAM policies in the Google Cloud Console or via gcloud CLI to ensure they target groups or roles. Remediate by reassigning user-specific policies to appropriate groups or roles, leveraging predefined roles where possible.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/manage-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.policy.audit_logs_enabled
  service: iam
  resource: policy
  requirement: Audit Logs Enabled
  scope: iam.policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure IAM Policy Audit Logs Are Enabled
  rationale: Enabling audit logs for IAM policies is crucial to maintain a clear record of access changes and permission modifications, which helps in tracking unauthorized accesses, potential breaches, and non-compliance with regulatory standards. This aids in forensic analysis and accountability, reducing the risk of insider threats and ensuring alignment with compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks whether audit logs are enabled for IAM policies across your Google Cloud Platform environment. Audit logging should be configured to capture Admin Activity and Data Access logs, ensuring that all changes to IAM policies are recorded. To verify, navigate to the Logging section in your GCP console, ensure that logging is enabled for IAM policy changes. Remediation involves setting up Log Sinks to capture these logs in Cloud Logging. Regularly review the logs for anomalies and integrate them with alerting systems for proactive monitoring.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.policy.cloud_asset_inventory_enabled
  service: iam
  resource: policy
  requirement: Cloud Asset Inventory Enabled
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Cloud Asset Inventory is Enabled
  rationale: Enabling Cloud Asset Inventory is crucial for comprehensive visibility into GCP assets, aiding in effective security posture management. Without it, organizations may face increased risks of unauthorized access, data breaches, and non-compliance with industry standards like CIS and NIST. This can lead to financial loss, reputational damage, and regulatory penalties.
  description: This rule checks whether Cloud Asset Inventory is enabled in GCP. It ensures that all assets are continuously monitored and logged, providing a real-time inventory of resources and their IAM policies. To verify, navigate to the Cloud Asset Inventory page in the GCP Console and confirm that asset inventory is activated. Remediation involves enabling Cloud Asset Inventory via the console or gcloud CLI to maintain an up-to-date asset list for audit and compliance purposes.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.iam.policy.compliance
  service: iam
  resource: policy
  requirement: Compliance
  scope: iam.policy.compliance
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Meet Compliance Standards
  rationale: Non-compliant IAM policies can lead to unauthorized access, data breaches, and non-adherence to regulatory requirements, impacting an organization's reputation and resulting in financial liabilities. Ensuring policies comply with standards like CIS and NIST mitigates these risks by enforcing least privilege and secure authentication practices.
  description: This rule checks for IAM policies that do not align with compliance standards, focusing on ensuring roles and permissions adhere to the principle of least privilege. Verify that all IAM policies are reviewed regularly and updated to maintain compliance with frameworks like CIS and NIST. Remediate by auditing IAM roles and permissions, and eliminate any excessive or unnecessary access rights. Use IAM policy analyzer tools for assessment and adjustment.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
- rule_id: gcp.iam.policy.console_mfa_enabled
  service: iam
  resource: policy
  requirement: Console MFA Enabled
  scope: iam.policy.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Multi-Factor Authentication for GCP Console Access
  rationale: Enforcing multi-factor authentication (MFA) for console access reduces the risk of unauthorized access due to compromised credentials. Without MFA, attackers can exploit stolen passwords to access sensitive resources, potentially leading to data breaches or service disruptions. Compliance with regulations such as PCI-DSS and ISO 27001 often mandates MFA to protect access to critical systems.
  description: This rule checks if IAM policies require multi-factor authentication for accessing the Google Cloud Console. To verify, ensure that all users have 2-Step Verification enabled in their account settings. Remediation involves configuring an organizational policy that mandates MFA through the Google Admin Console. This ensures all users are prompted for a second verification step when signing into the console.
  references:
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations#multi-factor_authentication
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely#enforce-mfa
  - https://cloud.google.com/resource-manager/docs/managing-org-policies
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.policy.domain_compliance_configured
  service: iam
  resource: policy
  requirement: Domain Compliance Configured
  scope: iam.policy.compliance
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Domain Compliance is Configured for IAM Policies
  rationale: Configuring domain compliance for IAM policies reduces the risk of unauthorized access by ensuring that only identities from approved domains can access GCP resources. This minimizes the risk of data breaches and supports compliance with industry regulations like GDPR and HIPAA, which require strict access controls to protect sensitive information.
  description: This rule checks if IAM policies are configured to enforce domain compliance, ensuring that access is restricted to identities from specified domains. To verify, review the IAM policy bindings to confirm that they include a 'members' field with domain-specific identities. Remediation involves updating the IAM policy to include domain-restricted email addresses or groups to ensure compliance.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
- rule_id: gcp.iam.policy.identity_access_rbac_conditions_used_where_applicable
  service: iam
  resource: policy
  requirement: Identity Access RBAC Conditions Used Where Applicable
  scope: iam.policy.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Utilize RBAC Conditions in IAM Policies
  rationale: Using RBAC conditions in IAM policies enhances security by enforcing context-aware access controls, reducing the risk of unauthorized access and data breaches. This approach aligns with the principle of least privilege, ensuring users have access only when necessary, and supports compliance with regulations like GDPR and HIPAA by safeguarding sensitive data.
  description: This rule checks for the use of RBAC conditions in IAM policies to ensure that access permissions are contextually appropriate. Specifically, it verifies that conditions such as time-based access, IP address restrictions, or specific device requirements are implemented where applicable. To remediate, review IAM policies and incorporate relevant conditions by editing policy bindings through the GCP Console or using the gcloud CLI. Ensure conditions are regularly updated to reflect changes in access needs.
  references:
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://cloud.google.com/iam/docs/policies-best-practices
  - CIS Google Cloud Computing Foundations Benchmark v1.1.0
  - NIST SP 800-53 (AC-6)
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.policy.identity_access_rbac_no_action_star_on_resource_star
  service: iam
  resource: policy
  requirement: Identity Access RBAC No Action Star On Resource Star
  scope: iam.policy.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prohibit Wildcard Actions on Wildcard Resources in IAM Policies
  rationale: Allowing wildcard actions on wildcard resources in IAM policies may lead to excessive and unintended permissions, increasing the risk of privilege escalation and unauthorized access. This could result in data breaches, financial loss, and non-compliance with regulatory standards such as GDPR or HIPAA, where granular access control is mandated.
  description: 'This rule checks IAM policies to ensure that no identity has been granted permissions with both action and resource set to ''*''. Such configurations could allow overly broad access, violating the principle of least privilege. To verify, review IAM policies for entries with ''action: *'' and ''resource: *''. Remediate by specifying precise actions and resources required for each role, minimizing the scope of access to only what is necessary.'
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/docs/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/iam/docs/best-practices
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.iam.policy.identity_access_rbac_no_policy_allows_without_constraints
  service: iam
  resource: policy
  requirement: Identity Access RBAC No Policy Allows Without Constraints
  scope: iam.policy.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict IAM Policies with No Constraints
  rationale: Allowing IAM policies without constraints can lead to excessive permissions, increasing the risk of privilege escalation and unauthorized access. This can result in data breaches, compliance violations, and potential financial and reputational damage. Ensuring policies have constraints aligns with security best practices and regulatory requirements.
  description: This rule checks for IAM policies that grant permissions without any constraints, such as conditions or specific roles. Unconstrained policies may inadvertently allow broad access to sensitive resources. To verify, review IAM policies for any wildcard entries or lack of conditional logic. Remediation involves refining these policies to include specific constraints and conditions, thereby tightening access controls.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/docs/security/compliance
- rule_id: gcp.iam.policy.identity_access_rbac_resource_constraints_present
  service: iam
  resource: policy
  requirement: Identity Access RBAC Resource Constraints Present
  scope: iam.policy.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure RBAC Resource Constraints are Applied in IAM Policies
  rationale: Implementing RBAC resource constraints in IAM policies is crucial for minimizing unauthorized access and potential data breaches. Without these constraints, identities may gain excessive permissions, leading to privilege escalation and non-compliance with regulations like GDPR and HIPAA. Properly constrained access helps maintain the principle of least privilege, reducing the attack surface.
  description: This check verifies that IAM policies include resource constraints, ensuring that roles and permissions are limited to specific resources. To verify, review IAM policies for the presence of resource-level conditions. Remediation involves updating IAM policies to include specific resources and conditions for each role, ensuring that identities cannot access resources outside their scope. Use 'resource' and 'condition' fields in your IAM policy bindings to enforce these constraints.
  references:
  - https://cloud.google.com/iam/docs/resource-based-policies
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/conditions-overview
- rule_id: gcp.iam.policy.minimum_length_14
  service: iam
  resource: policy
  requirement: Minimum Length 14
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce IAM Policy Password Minimum Length of 14 Characters
  rationale: Setting a minimum password length of 14 characters significantly enhances account security by reducing the risk of unauthorized access through brute force attacks. Longer passwords are statistically harder to crack, aligning with industry best practices and compliance requirements for strong authentication mechanisms. This policy supports regulatory standards such as NIST SP 800-63B, which recommends longer passwords to mitigate security breaches.
  description: This rule checks if IAM policies enforce a minimum password length of 14 characters for user accounts on Google Cloud Platform. Administrators should configure the password policy settings to require a minimum of 14 characters, including a mix of letters, numbers, and special characters. Verification can be done through the IAM settings in the GCP console or using the gcloud command-line tool. Remediation involves updating the IAM password policy to comply with the length requirement, ensuring enhanced security for user accounts.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/docs/security/pci-dss
  - https://www.nist.gov/publications/digital-identity-guidelines
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.iam.policy.no_administrative_privileges
  service: iam
  resource: policy
  requirement: No Administrative Privileges
  scope: iam.policy.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure No Unnecessary Administrative Privileges in IAM Policies
  rationale: Minimizing administrative privileges reduces the risk of accidental or malicious changes that could compromise GCP resources. Excessive permissions can lead to data breaches, service disruptions, and non-compliance with frameworks such as NIST and PCI-DSS, which emphasize least privilege access. Limiting admin rights helps protect sensitive data and maintain a secure and compliant cloud environment.
  description: This rule checks IAM policies for the presence of administrative roles that are not necessary for specific user functions. It ensures users are not assigned roles like 'roles/owner', 'roles/editor', or other high-privilege roles unless absolutely required for operational duties. To verify, review IAM policy bindings and remove or replace high-privilege roles with more specific roles that follow the principle of least privilege. Remediate by restructuring roles and permissions to align with user needs while minimizing admin access.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/least-privilege
- rule_id: gcp.iam.policy.overly_permissive
  service: iam
  resource: policy
  requirement: Overly Permissive
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Identify and Restrict Overly Permissive IAM Policies
  rationale: Overly permissive IAM policies can lead to unauthorized access, data breaches, and potential financial and reputational damage. They increase the attack surface by granting more permissions than necessary, possibly violating compliance requirements such as PCI-DSS and HIPAA, which mandate strict access controls.
  description: This rule checks for IAM policies that grant overly broad permissions, such as wildcard (*) permissions or roles with extensive access across multiple resources. To verify, review IAM policies for users, groups, or service accounts, and ensure the principle of least privilege is applied. Remediation involves auditing permissions, removing unnecessary access, and replacing broad roles with custom roles that provide only the required permissions.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/docs/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.iam.policy.policy_configured
  service: iam
  resource: policy
  requirement: Policy Configured
  scope: iam.policy.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Are Properly Configured
  rationale: Proper configuration of IAM policies is crucial for controlling access to GCP resources. Misconfigured policies can lead to unauthorized access, data breaches, and non-compliance with regulations like PCI-DSS and HIPAA. Ensuring policies are correctly set up mitigates the risk of privilege escalation and data exposure.
  description: This rule checks whether IAM policies are correctly configured in GCP. It verifies that policies are not overly permissive and adhere to the principle of least privilege. Audit logs should be enabled to track policy changes. To remediate, review IAM policies, ensure they align with organizational access requirements, and adjust roles and permissions to minimize risk. Use the GCP Console or gcloud CLI to inspect and modify policies.
  references:
  - https://cloud.google.com/iam/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices-for-iam
- rule_id: gcp.iam.policy.root_hardware_mfa_enabled
  service: iam
  resource: policy
  requirement: Root Hardware MFA Enabled
  scope: iam.policy.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Root Account Has Hardware MFA Enabled
  rationale: Enabling hardware-based multi-factor authentication (MFA) for the root account adds an extra layer of security, reducing the risk of unauthorized access. This is crucial as the root account has full access to all resources within the GCP environment, making it a high-value target. Regulatory frameworks such as PCI-DSS and ISO 27001 mandate strong authentication mechanisms to protect sensitive data and systems.
  description: This rule checks if the root account on GCP has hardware MFA enabled. Hardware MFA requires a physical device, such as a security key, providing stronger authentication compared to software-based methods. To verify, review the IAM settings for the root account and ensure a hardware security key is registered. If not configured, set up a compatible hardware MFA device via the Google Cloud Console and enforce its use for root account logins.
  references:
  - https://cloud.google.com/docs/authentication/mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
- rule_id: gcp.iam.policy.root_mfa_enabled
  service: iam
  resource: policy
  requirement: Root MFA Enabled
  scope: iam.policy.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Require MFA for Root Account Access in GCP
  rationale: Enforcing multi-factor authentication (MFA) for the root account in GCP minimizes the risk of unauthorized access, which could lead to severe security breaches and data loss. MFA provides an additional layer of security beyond just a password, protecting against credential theft. Regulatory standards such as NIST and PCI-DSS mandate strong authentication measures to safeguard sensitive information.
  description: This rule verifies whether multi-factor authentication is enabled for the GCP root account. It checks the Identity and Access Management (IAM) policy settings to ensure MFA is enforced for any login attempts to the root account. To remediate, configure the security settings in the GCP Console to require MFA for root account access, following the steps in the GCP IAM documentation. Regularly audit account settings to maintain compliance and security.
  references:
  - https://cloud.google.com/iam/docs/mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.iam.policy.strong
  service: iam
  resource: policy
  requirement: Strong
  scope: iam.policy.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Enforce Strong Authentication Practices
  rationale: Weak IAM policies can lead to unauthorized access, data breaches, and compliance violations. Enforcing strong authentication practices mitigates risks of identity theft and ensures compliance with regulations such as PCI-DSS and SOC2, safeguarding sensitive data and maintaining business integrity.
  description: This rule checks if IAM policies enforce strong authentication practices by requiring multi-factor authentication (MFA) for all users with access to sensitive resources. Ensure policies are configured to require MFA and limit the use of service accounts to essential operations. Remediate by enabling MFA for all users and reviewing IAM policies for excessive permissions.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.iam.role.identity_access_attached_policies_not_admin_star
  service: iam
  resource: role
  requirement: Identity Access Attached Policies Not Admin Star
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Admin Permissions in IAM Role Policies
  rationale: Allowing IAM roles to have policies with 'admin' permissions grants them unrestricted access, posing a significant security risk, including potential data exfiltration or unauthorized service management. This is critical in maintaining least privilege, a core security principle, and ensuring compliance with regulatory standards like PCI-DSS and ISO 27001.
  description: This rule checks that IAM roles do not have attached policies granting '*' admin permissions, which could lead to broad and potentially harmful access. It requires reviewing IAM role configurations and ensuring no 'admin' level permissions are included in attached policies. Remediation involves auditing current IAM policies and revising them to remove such permissions, ensuring adherence to the principle of least privilege by granting only necessary permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/best-practices-for-enterprise-organizations
- rule_id: gcp.iam.role.identity_access_max_session_duration_reasonable
  service: iam
  resource: role
  requirement: Identity Access Max Session Duration Reasonable
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Set IAM Role Max Session Duration to 1 Hour or Less
  rationale: Setting a reasonable maximum session duration for IAM roles reduces the risk of unauthorized access through token misuse and limits the potential impact of compromised sessions. It helps align with security best practices and compliance requirements by ensuring that role sessions are not unnecessarily prolonged, which can prevent timely revocation of access if needed.
  description: This rule checks IAM roles to ensure that the maximum session duration is set to a reasonable length, such as 1 hour or less. A shorter session duration reduces the window in which compromised credentials can be used. To verify, review the 'maxSessionDuration' field in the IAM role configuration. Remediation involves updating the IAM role settings via the Google Cloud Console or using the gcloud command-line tool to set 'maxSessionDuration' to a value of 3600 seconds or less.
  references:
  - https://cloud.google.com/iam/docs/assigning-roles#custom_roles
  - https://cloud.google.com/iam/docs/understanding-custom-roles
  - https://cloud.google.com/iam/docs/creating-managing-custom-roles#changing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.iam.role.identity_access_no_inline_policies
  service: iam
  resource: role
  requirement: Identity Access No Inline Policies
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Avoid Inline Policies in GCP IAM Roles
  rationale: Inline policies in IAM roles can lead to complex and hard-to-manage permission structures, increasing the risk of unauthorized access. Without centralized policy management, it's challenging to audit and ensure policy consistency across the organization, potentially leading to non-compliance with regulatory standards such as NIST and GDPR.
  description: This rule checks for the presence of inline policies attached directly to IAM roles, which should instead utilize managed policies for better consistency and ease of management. Inline policies can lead to unintentional privilege escalation and policy sprawl. To remediate, detach any inline policies from roles and redefine them as standardized managed policies in your organization's policy repository.
  references:
  - https://cloud.google.com/iam/docs/policies#inline_and_managed_policies
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.role.identity_access_rbac_attached_policies_not_admin_star
  service: iam
  resource: role
  requirement: Identity Access RBAC Attached Policies Not Admin Star
  scope: iam.role.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict IAM Role Policies from Using Wildcard Permissions
  rationale: Allowing wildcard permissions in IAM roles can lead to excessive access, potentially exposing sensitive resources to unauthorized users. This increases the risk of data breaches and non-compliance with regulatory standards such as GDPR and HIPAA, which mandate strict access control mechanisms.
  description: This rule checks for IAM roles that have attached policies using wildcard permissions (e.g., 'roles/*') which could lead to unintentional privilege escalation. Ensure all IAM roles specify exact permissions rather than using '*'. Review and update IAM policies to restrict permissions to only what is necessary for the role's function. Use the Principle of Least Privilege to minimize access and regularly audit roles to detect and correct any over-permissioned roles.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/using-iam-securely
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 1.6
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.role.identity_access_rbac_no_inline_policies
  service: iam
  resource: role
  requirement: Identity Access RBAC No Inline Policies
  scope: iam.role.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prohibit Inline Policies on IAM Roles
  rationale: Inline IAM policies can lead to security risks due to their potential to be overlooked during audits and their tight coupling with roles, making it difficult to manage and review permissions effectively. This practice can result in unauthorized access and non-compliance with industry standards such as ISO 27001 and SOC2, which require clear and auditable access controls.
  description: This rule checks for the presence of inline policies attached directly to IAM roles within GCP. Inline policies, unlike managed policies, are embedded within a role, making them harder to discover and manage. To verify compliance, review IAM roles for any inline policies and migrate them to standalone managed policies. This practice enhances auditability and simplifies permission management.
  references:
  - https://cloud.google.com/iam/docs/policies
  - https://cloud.google.com/security/compliance/soc-2/
  - https://cloud.google.com/security/compliance/iso-27001/
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.iam.role.identity_access_rbac_session_duration_reasonable__applicable
  service: iam
  resource: role
  requirement: Identity Access RBAC Session Duration Reasonable Applicable
  scope: iam.role.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Reasonable RBAC Session Duration for IAM Roles
  rationale: Limiting IAM role session duration reduces the window of opportunity for credential abuse. Prolonged sessions can increase the risk of unauthorized access if credentials are compromised. Adhering to a reasonable session duration supports compliance with standards such as PCI-DSS and enhances overall security posture by minimizing potential lateral movement within the cloud environment.
  description: This rule verifies that IAM roles have a session duration that is set to a reasonable time limit, typically not exceeding 1 hour. To assess, review the session duration setting in the role's trust policy and adjust any durations that exceed this limit. Properly configuring session duration reduces the risk of credential misuse and helps maintain compliance with security best practices.
  references:
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.role.identity_access_trust_principals_allowlist_only
  service: iam
  resource: role
  requirement: Identity Access Trust Principals Allowlist Only
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict IAM Role Trust Principals to Allowlist
  rationale: Restricting IAM role trust principals to an allowlist minimizes the risk of unauthorized access by ensuring only predefined, trusted identities can assume roles. This reduces the likelihood of privilege escalation attacks and supports compliance with regulations requiring strict access controls, such as PCI-DSS and ISO 27001.
  description: This rule checks that IAM roles in GCP are configured with trust policies restricted to an allowlist of explicitly permitted identities. Ensure that the role's trust policy specifies only known and validated service accounts or users. Regularly review and update the allowlist to accommodate legitimate changes while preventing unauthorized entities from assuming critical roles. Remediation involves editing the role's trust policy to align with organizational security policies and compliance requirements.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/understanding-roles#custom-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://cloud.google.com/iam/docs/best-practices-for-using-roles
- rule_id: gcp.iam.role.identity_access_trust_requires_external_id_or_audi_supported
  service: iam
  resource: role
  requirement: Identity Access Trust Requires External Id Or Audi Supported
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce External Id or Audi for Identity Access Trust
  rationale: Requiring an external ID or audience (audi) claim for identity trusts mitigates the risk of unauthorized access by ensuring that only specifically intended entities can assume roles. This requirement helps prevent impersonation attacks and strengthens the security posture by reducing the attack surface, aligning with compliance mandates like NIST SP 800-53 and ISO 27001.
  description: This rule checks that identity access trusts in GCP IAM roles are configured to require either an external ID or an audience (audi) claim. Without these, roles may be assumed by unintended entities, leading to potential misuse of permissions. To remediate, ensure that identity trust configurations specify an external ID or audi claim, which can be verified through the IAM policy and adjusted via the Google Cloud Console or gcloud CLI. Regularly review and update these configurations to align with security policies and compliance requirements.
  references:
  - https://cloud.google.com/iam/docs/service-accounts
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
- rule_id: gcp.iam.role.kms_enforce_separation_of_duties
  service: iam
  resource: role
  requirement: KMS Enforce Separation Of Duties
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce KMS Separation of Duties in IAM Roles
  rationale: Separation of duties in Key Management Service (KMS) roles reduces the risk of unauthorized access to cryptographic keys and prevents potential insider threats. It helps ensure that no single individual has excessive control over critical security functions, thus minimizing the risk of data breaches and aligning with compliance mandates like PCI-DSS and ISO 27001.
  description: This rule checks that IAM roles assigned to users interacting with Google Cloud KMS are configured to enforce separation of duties. Specifically, it verifies that roles do not allow a single user to both manage key permissions and use keys to encrypt or decrypt data. To remediate, review and adjust IAM policies to ensure distinct roles are assigned to manage key access and key usage, following the principle of least privilege.
  references:
  - https://cloud.google.com/kms/docs/separation-of-duties
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.role.least_privilege
  service: iam
  resource: role
  requirement: Least Privilege
  scope: iam.role.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure IAM Roles Adhere to Least Privilege Principle
  rationale: Adhering to the principle of least privilege minimizes the risk of unauthorized access and potential data breaches by ensuring users and services have only the permissions necessary to perform their tasks. Over-permissioned roles can lead to inadvertent data exposure, increased attack surface, and non-compliance with regulations such as GDPR, PCI-DSS, and HIPAA.
  description: This rule checks IAM roles to ensure they do not possess permissions beyond what is necessary for their function. Verify roles by reviewing the permissions assigned against the tasks they need to perform. Remediation involves adjusting the permissions to align with the specific requirements of the role, potentially breaking roles into smaller, more specific ones, and regularly auditing roles for compliance with least privilege.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis#gcp-1-1
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/using-custom-roles
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.role.sa_enforce_separation_of_duties
  service: iam
  resource: role
  requirement: Sa Enforce Separation Of Duties
  scope: iam.role.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Separation of Duties for Service Account Roles
  rationale: Separation of duties minimizes the risk of malicious activities by ensuring that no single individual has excessive privileges that could lead to unauthorized access or data manipulation. This is crucial for meeting regulatory requirements such as PCI-DSS and SOC2, which mandate the segregation of critical duties to prevent fraud and errors.
  description: This rule checks for adherence to the principle of separation of duties in IAM roles associated with service accounts. It ensures that roles are not aggregating permissions that could allow a single entity to both modify configurations and access sensitive data. Verification involves reviewing IAM role bindings and restructuring them to separate duties effectively. Remediation includes adjusting roles to limit permissions based on least privilege and functional requirements.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices/identity
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.service_account.access_approval_enabled
  service: iam
  resource: service_account
  requirement: Access Approval Enabled
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enable Access Approval for Service Accounts
  rationale: Enabling Access Approval for service accounts is crucial for maintaining an audit trail and ensuring that sensitive operations are authorized before execution. This mitigates the risk of unauthorized access and actions, protecting sensitive data and operations from potential misuse or breaches. It also supports compliance with regulations requiring strict access controls and audit capabilities.
  description: This rule checks whether Access Approval is enabled for service accounts within your GCP projects. Access Approval requires explicit user consent before sensitive actions can be undertaken by service accounts. To verify, ensure that the Access Approval API is configured and active on relevant projects. Remediation involves enabling Access Approval in the GCP Console under the 'Security' section by following the official guidelines for setting up Access Approval.
  references:
  - https://cloud.google.com/access-approval/docs
  - https://cloud.google.com/access-approval/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.service_account.account_key_rotation_90_days_enforced
  service: iam
  resource: service_account
  requirement: Account Key Rotation 90 Days Enforced
  scope: iam.service_account.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Enforce 90-Day Rotation for Service Account Keys
  rationale: Frequent rotation of service account keys mitigates the risk of unauthorized access due to key compromise. It limits the window of opportunity for malicious actors to exploit stolen keys, reducing potential data breaches and service disruptions. Regular rotation aligns with security best practices and helps meet compliance requirements such as PCI-DSS and ISO 27001.
  description: This rule checks that all service account keys in GCP are rotated every 90 days. Service account keys should not be used indefinitely as prolonged usage increases the likelihood of key leakage. To verify compliance, review the 'Last used' and 'Created' timestamps in the Google Cloud Console or via the gcloud CLI. Remediation involves scripting or automating key rotation using Cloud Functions or Cloud Scheduler to generate new keys and delete old ones securely.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
- rule_id: gcp.iam.service_account.account_role_check_admin_privileges
  service: iam
  resource: service_account
  requirement: Account Role Check Admin Privileges
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Service Accounts Lack Admin Privileges
  rationale: Service accounts with excessive privileges pose significant security risks, including unauthorized access and potential lateral movement within the cloud environment. Limiting service account privileges to the necessary minimum helps mitigate risks of data breaches and aligns with compliance mandates such as PCI-DSS and ISO 27001, which require least privilege principles.
  description: This rule checks for service accounts in GCP that have been granted admin privileges, which is contrary to the principle of least privilege. Admin roles should be restricted to human users for accountability and traceability. To remediate, review the roles assigned to each service account and revoke any that are unnecessary or overly permissive, especially roles like Owner or Editor. Use IAM policies to enforce appropriate role assignments and regularly audit role usage.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.iam.service_account.account_sod_configured
  service: iam
  resource: service_account
  requirement: Account Sod Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Service Account Separation of Duties Configured
  rationale: Separation of Duties (SoD) in service accounts is crucial to prevent conflicts of interest and reduce the risk of fraud. Without proper SoD, a single compromised service account can lead to unauthorized access or modification of resources, potentially causing data breaches or service outages. Additionally, regulatory frameworks often mandate SoD to ensure security and compliance.
  description: This rule checks whether GCP service accounts have appropriate Separation of Duties (SoD) configured by ensuring that roles assigned to service accounts do not conflict. SoD prevents the same account from having permissions to perform multiple critical operations which should be segregated. To verify, review the roles assigned to each service account and ensure they align with the principle of least privilege and are not cross-functional unless justified. Remediation involves auditing roles and permissions to guarantee they are exclusive and align with organizational policies.
  references:
  - https://cloud.google.com/iam/docs/service-accounts
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloudsecurityalliance.org/artifacts/best-practices-for-securing-workloads-in-gcp/
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.iam.service_account.account_user_managed_keys_absent
  service: iam
  resource: service_account
  requirement: Account User Managed Keys Absent
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disallow User-Managed Keys for Service Accounts
  rationale: User-managed keys for service accounts pose a security risk as they can be leaked or mismanaged, potentially leading to unauthorized access to sensitive resources. Ensuring that service accounts do not have user-managed keys helps reduce the attack surface and aligns with best practices for cloud-native security. This measure is critical for maintaining the integrity of identity and access management in GCP environments.
  description: This rule checks that no user-managed keys are associated with service accounts, as they should only rely on system-managed keys. User-managed keys can be difficult to track and rotate, increasing the risk of exposure. To verify compliance, review service account key settings in the GCP console or via the gcloud command-line tool. Remediation involves deleting any existing user-managed keys and ensuring future keys are system-managed, thereby leveraging GCP's key management capabilities.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts#best_practices
  - https://cloud.google.com/iam/docs/manage-service-account-keys
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 1.9
  - NIST SP 800-53 Rev. 5 AC-2, AC-3
  - PCI-DSS v3.2.1 7.1, 8.1
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.service_account.administrative_privileges
  service: iam
  resource: service_account
  requirement: Administrative Privileges
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Service Accounts Have No Excessive Privileges
  rationale: Service accounts with unnecessary administrative privileges pose a significant security risk as they can be exploited by attackers to gain unauthorized access and perform malicious activities. Limiting privileges aligns with the principle of least privilege, reducing the potential impact of compromised credentials and aiding in compliance with regulations like PCI-DSS and ISO 27001.
  description: This rule checks for service accounts that have been granted administrative roles, such as Owner or Editor, which exceed their operational needs. Verify by reviewing IAM policies for each service account, ensuring they are granted only the permissions necessary for their function. Remediate by adjusting IAM roles to adhere to the least privilege principle, using predefined roles or creating custom roles with minimal permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles#service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.service_account.identity_access_instance_profile_no_instance_profile_wi_star
  service: iam
  resource: service_account
  requirement: Identity Access Instance Profile No Instance Profile Wi Star
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Wildcard Use in Service Account Instance Profiles
  rationale: Allowing wildcard permissions in instance profiles can lead to unintended access and privilege escalation risks, potentially enabling unauthorized users to assume roles or access resources. This can compromise sensitive data and violate compliance requirements such as PCI-DSS and ISO 27001, which mandate strict access controls.
  description: This rule checks for the use of wildcard ('*') permissions within the identity access instance profiles associated with GCP service accounts. Such configurations can inadvertently grant excessive permissions beyond the intended scope. To remediate, review and specify more granular permissions in the IAM policy for each service account, limiting access to only what is necessary for the service's operation.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/best-practices-for-using-iam
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.iam.service_account.identity_access_instance_profile_role_attached
  service: iam
  resource: service_account
  requirement: Identity Access Instance Profile Role Attached
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Service Account Identity Access with Instance Profile Role
  rationale: Improper assignment of instance profile roles to service accounts can lead to unauthorized access to resources, increasing the risk of data breaches and privilege escalation. This configuration is essential to control identity-based access, ensuring compliance with security regulations such as PCI-DSS and SOC2, and protecting sensitive data from exposure.
  description: This rule checks if any service account has an instance profile role attached that may grant unintended access. Ensure that only necessary roles are assigned to service accounts and regularly audit the permissions to maintain a least privilege model. Remediation involves reviewing the IAM policy binding for the service account and detaching any role that is not required for its operation. Verification can be done via the Google Cloud Console or using gcloud CLI to list the IAM policies attached to service accounts.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/assigning-roles-to-service-accounts
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.iam.service_account.identity_access_instance_profile_role_policies_lea_privilege
  service: iam
  resource: service_account
  requirement: Identity Access Instance Profile Role Policies Lea Privilege
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege on Service Account IAM Role Bindings
  rationale: Service accounts with excessive permissions pose a significant security risk as they can be exploited by attackers to access or modify sensitive resources. Implementing least privilege helps mitigate the risk of data breaches and unauthorized access, aligning with compliance requirements such as PCI-DSS and SOC 2 by minimizing access to only what is necessary for business operations.
  description: This rule checks that IAM role bindings for service accounts follow the principle of least privilege by ensuring that only necessary permissions are granted. It verifies that service accounts do not have overly permissive roles, such as 'Owner' or 'Editor', unless explicitly required. To mitigate risks, review and adjust IAM policies to assign the minimum set of permissions needed for the service accounts to perform their functions. Regularly audit role assignments and use predefined roles or custom roles with specific permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security-command-center/docs/concepts-iam-analyzer
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.iam.service_account.identity_access_keys_not_used_or_rotated_90_days_or_less
  service: iam
  resource: service_account
  requirement: Identity Access Keys Not Used Or Rotated 90 Days Or Less
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Rotate and Use Service Account Keys Every 90 Days
  rationale: Unused or stale service account keys pose a significant security risk as they may be exploited by malicious actors to gain unauthorized access to resources. Regular rotation and usage verification of these keys help to mitigate the risk of data breaches and ensure compliance with security best practices and regulatory requirements such as PCI-DSS and ISO 27001.
  description: This rule checks for service account identity access keys that haven't been used or rotated in the last 90 days. Stale keys should be rotated to reduce exposure to potential threats. Administrators can use the GCP Console or CLI to identify and remove unused keys, and implement an automated process to rotate keys regularly. Additionally, ensure that all services and applications depending on these keys are updated accordingly.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/monitoring-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-using-service-accounts
- rule_id: gcp.iam.service_account.identity_access_no_user_managed_long_lived_keys
  service: iam
  resource: service_account
  requirement: Identity Access No User Managed Long Lived Keys
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Prevent Long-Lived User-Managed Keys for Service Accounts
  rationale: User-managed keys that are long-lived can pose significant security risks as they increase the surface area for potential unauthorized access if compromised. Such keys are not automatically rotated like Google-managed keys, leading to potential exposure. Ensuring that only short-lived, automatically rotated keys are used helps to mitigate unauthorized access and aligns with security best practices and compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks for the presence of long-lived user-managed keys on service accounts in GCP. It ensures that only short-lived keys, preferably Google-managed keys, are used, which are automatically rotated. To verify compliance, audit service accounts for user-managed keys and transition to using Google-managed keys. Remove any long-lived user-managed keys and implement key rotation policies to enhance security.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/key-management
- rule_id: gcp.iam.service_account.identity_access_scopes_or_roles_least_privilege
  service: iam
  resource: service_account
  requirement: Identity Access Scopes Or Roles Least Privilege
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Service Accounts Use Least Privilege Access Scopes or Roles
  rationale: Implementing least privilege for service accounts minimizes potential attack vectors by limiting access to essential resources only. Over-permissioned service accounts can be exploited by attackers to gain unauthorized access, leading to data breaches or service disruptions. This practice is crucial for maintaining compliance with standards like PCI-DSS and NIST, which require stringent access controls.
  description: This rule checks that GCP service accounts are not granted excessive identity access scopes or roles beyond what is necessary for their function. It verifies that service accounts adhere to the principle of least privilege by examining IAM policies and role assignments. To remediate, audit current permissions and adjust roles to provide only the permissions necessary for the service account's tasks. Use predefined roles where possible, and regularly review permissions for appropriateness.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.service_account.identity_access_workload_identity_federation_used__supported
  service: iam
  resource: service_account
  requirement: Identity Access Workload Identity Federation Used Supported
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Workload Identity Federation for Service Account Authentication
  rationale: Workload Identity Federation enhances security by allowing external identities to access Google Cloud resources without the need of long-lived service account keys, reducing the risk of key compromise. It supports seamless integration with external identity providers, aligning with zero-trust principles and supporting compliance with standards such as NIST and ISO 27001.
  description: This rule checks if Workload Identity Federation is used for service account authentication, ensuring that service accounts are configured to authenticate using external identity providers. Verify that the service account is configured with a workload identity pool and provider, and that the workload identity bindings are correctly set up. Remediation involves configuring a workload identity pool, setting up a provider, and binding it to the service account. This reduces dependency on static keys and enhances security posture.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-using-workload-identity-federation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-207.pdf
  - https://cloud.google.com/blog/products/identity-security/using-workload-identity-federation-to-improve-cloud-security
  - https://cloud.google.com/architecture/identity-federation-best-practices
- rule_id: gcp.iam.service_account.level_service_account_role_configured
  service: iam
  resource: service_account
  requirement: Level Service Account Role Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Service Account Has Least Privilege Role
  rationale: Assigning least privilege roles to service accounts minimizes the risk of unauthorized access and reduces the potential impact of a compromised account by limiting access to only necessary resources. This practice is critical in maintaining compliance with security standards and preventing privilege escalation attacks.
  description: This rule verifies that service accounts in GCP are assigned roles that align with the principle of least privilege. It checks for overly permissive roles that may grant excessive access to resources. To remediate, review the roles assigned to each service account and ensure they only include the permissions necessary for their intended function. Use IAM policies to fine-tune permissions and regularly audit service account roles.
  references:
  - https://cloud.google.com/iam/docs/service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#identity_and_access_management
  - https://cloud.google.com/iam/docs/understanding-roles#service-accounts
  - NIST SP 800-53 Rev. 5 AC-6 Least Privilege
  - 'PCI-DSS Requirement 7: Restrict Access to Cardholder Data by Business Need to Know'
- rule_id: gcp.iam.service_account.no_guest_accounts_with_permissions_configured
  service: iam
  resource: service_account
  requirement: No Guest Accounts With Permissions Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Guest Access to Service Accounts
  rationale: Allowing guest accounts to have permissions on service accounts can lead to unauthorized access and potential data breaches. It increases the risk of privilege escalation and data exfiltration if a guest account is compromised. This practice is contrary to least privilege principles and may violate compliance standards such as PCI-DSS and ISO 27001.
  description: This rule checks for any permissions configured for guest accounts on GCP service accounts. Guest accounts, typically having an '@gmail.com' or similar unmanaged identity, should not have any permissions on service accounts to prevent unauthorized access. Remediation involves removing any IAM policy bindings that grant access to guest accounts and ensuring that only trusted, managed identities have the necessary permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.iam.service_account.organization_essential_contacts_configured
  service: iam
  resource: service_account
  requirement: Organization Essential Contacts Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Essential Contacts Configured for Service Accounts
  rationale: Configuring essential contacts for service accounts ensures that critical security and operational notifications are promptly received by the appropriate personnel. This reduces the risk of missing crucial alerts related to service account activities, which could otherwise lead to unauthorized access or operational disruptions. Compliance with frameworks like ISO 27001 and SOC2 often requires timely incident response, which relies on effective notification mechanisms.
  description: This check verifies that essential contacts are configured for each service account within the organization. Essential contacts are critical for receiving important notifications related to security incidents and operational issues. To verify, ensure that each service account in the IAM settings has designated essential contacts. Remediation involves accessing the Google Cloud Console, navigating to the IAM settings, and adding or updating essential contacts for each service account, ensuring they are current and monitored.
  references:
  - https://cloud.google.com/resource-manager/docs/managing-notification-contacts
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/best-practices-for-securing-service-accounts
- rule_id: gcp.iam.service_account.role_conflict_configured
  service: iam
  resource: service_account
  requirement: Role Conflict Configured
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Prevent Role Conflicts in GCP Service Accounts
  rationale: Misconfigured roles for service accounts can lead to privilege escalation and unauthorized access, increasing the risk of data breaches and non-compliance with regulatory mandates. Ensuring that service accounts have no role conflicts helps maintain the principle of least privilege, reducing the attack surface and safeguarding sensitive resources.
  description: This rule checks for conflicting roles assigned to GCP service accounts, which might grant excessive permissions. Verify that each service account is assigned roles without overlaps that could lead to unintended access. Remediation involves reviewing assigned roles, removing unnecessary permissions, and ensuring roles are aligned with the account's specific purpose. Use IAM policy analysis tools to detect and resolve conflicts effectively.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/docs/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/manage-access-service-accounts
- rule_id: gcp.iam.service_account.roles_at_project_level
  service: iam
  resource: service_account
  requirement: Roles At Project Level
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit Service Account Roles at Project Level
  rationale: Service accounts with excessive roles at the project level can lead to unauthorized access and privilege escalation, posing significant security risks. This can result in data breaches, loss of service integrity, and non-compliance with regulations like PCI-DSS and SOC2. Ensuring that service accounts have the least privilege necessary reduces the attack surface and helps maintain a secure cloud environment.
  description: This rule checks for service accounts that have roles assigned at the project level, which may grant more permissions than necessary. It is important to assign roles at the most granular level possible, such as specific resources, to adhere to the principle of least privilege. Remediation involves reviewing the roles assigned to service accounts and restricting them to only those necessary for their intended purpose, preferably at the resource level rather than the project level.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.iam.service_account.sa_no_administrative_privileges_configured
  service: iam
  resource: service_account
  requirement: Sa No Administrative Privileges Configured
  scope: iam.service_account.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Service Accounts Lack Admin Privileges
  rationale: Granting administrative privileges to service accounts increases the risk of privilege escalation attacks, unauthorized access, and potential data exfiltration. Limiting privileges aligns with the principle of least privilege, mitigating potential security threats and ensuring compliance with regulatory standards like PCI-DSS and ISO 27001.
  description: This rule checks if any service accounts are assigned roles with administrative privileges, which may include roles like 'roles/editor' or 'roles/owner'. To verify, review the IAM policy bindings for each service account and ensure they only have the necessary permissions for their intended functions. Remediation involves auditing the assigned roles and removing any unnecessary administrative roles, potentially replacing them with more granular permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.service_account.service_account_read_only_image_repo_access
  service: iam
  resource: service_account
  requirement: Service Account Read Only Image Repo Access
  scope: iam.service_account.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict Service Account to Read-Only Access on Image Repos
  rationale: Limiting service accounts to read-only access on image repositories reduces the risk of unauthorized image modifications, which could lead to the deployment of compromised applications. It also helps in minimizing the attack surface, ensuring that only approved processes can modify images, thereby supporting compliance with security frameworks like PCI-DSS and SOC2.
  description: This rule checks if the service account is granted only read-only permissions (roles/storage.objectViewer) on image repositories in Google Cloud Storage. To verify, review the IAM policy bindings for the service account and ensure it does not have write or admin roles on storage buckets hosting image repositories. Remediation involves adjusting IAM policies to assign the least privilege necessary, specifically the objectViewer role, to service accounts accessing image repositories.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/storage/docs/access-control/iam-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.iam.user.accesskey_unused
  service: iam
  resource: user
  requirement: Accesskey Unused
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Unused GCP IAM User Access Keys
  rationale: Unused access keys pose a significant security risk as they can be exploited by unauthorized actors if compromised. Regularly rotating or disabling these keys helps mitigate unauthorized access and aligns with best practices for identity and access management. Compliance with frameworks like NIST and PCI-DSS often requires monitoring and controlling access key usage to protect sensitive information.
  description: This rule checks for IAM user access keys in GCP that have not been used within a specified timeframe, indicating potential security vulnerabilities. To verify, review IAM user access key usage in the GCP Console and identify keys with no recent activity. Remediation involves disabling or rotating these unused keys to prevent unauthorized access. Implement automation to alert on or auto-disable keys that remain inactive beyond your defined security policy.
  references:
  - https://cloud.google.com/iam/docs/managing-access-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/security-best-practices
- rule_id: gcp.iam.user.console_access_unused
  service: iam
  resource: user
  requirement: Console Access Unused
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Identify and Revoke Unused Console Access for IAM Users
  rationale: Unused console access for IAM users can lead to potential security risks such as unauthorized access or exploitation by malicious actors. Regularly auditing and revoking unnecessary access helps maintain a principle of least privilege, reducing the attack surface and ensuring compliance with security standards like CIS and ISO 27001.
  description: This rule checks for IAM users who have not accessed the GCP console for a specified period, indicating unused console access. Administrators should verify these users' roles and determine if their access is still required. If not, the console access should be revoked or the account disabled to prevent unauthorized use. This action helps in maintaining a secure, compliant IAM posture by minimizing exposure.
  references:
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/docs/security/best-practices-for-enterprise-organizations
  - https://cloud.google.com/blog/products/identity-security/iam-best-practices-guide
- rule_id: gcp.iam.user.group_membership
  service: iam
  resource: user
  requirement: Group Membership
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Proper User Group Membership in IAM
  rationale: Group membership in IAM helps manage access control efficiently and securely. Without proper group management, users may have unauthorized access to critical resources, leading to potential data breaches and non-compliance with regulations like GDPR and HIPAA. Ensuring correct group memberships mitigates risks related to privilege escalation and insider threats.
  description: This rule checks if users are correctly assigned to IAM groups, ensuring that access permissions are aligned with their roles and responsibilities. Verify user group memberships through the GCP Console or by using the 'gcloud iam groups describe' command. Remediation involves reviewing user roles and updating group assignments to reflect appropriate access levels, reducing the risk of excessive permissions.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/manage-access-groups
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0, Control 1.1
  - 'NIST SP 800-53 Rev. 5: AC-2, AC-3'
  - https://www.iso.org/standard/73906.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.iam.user.in_group
  service: iam
  resource: user
  requirement: In Group
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Users Are Assigned to Appropriate Groups
  rationale: Assigning IAM users to appropriate groups helps streamline access management, reduce the risk of excessive permissions, and enforce the principle of least privilege. This approach minimizes potential security breaches and non-compliance with regulations such as PCI-DSS and SOC2 by ensuring users only have necessary access.
  description: This rule checks if IAM users in GCP are members of appropriate groups to facilitate effective access management. It evaluates the configurations to ensure users are grouped according to their roles, enhancing permission oversight. To verify, review and update group memberships in the Google Cloud Console or via gcloud commands. Remediation involves assessing current IAM policies and adjusting group memberships to align with organizational policies.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/manage-group-memberships
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.user.managed_key_rotate_90_days
  service: iam
  resource: user
  requirement: Managed Key Rotate 90 Days
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce 90-Day Rotation for User-Managed IAM Keys
  rationale: Regularly rotating IAM user-managed keys mitigates the risk of key compromise due to theft or brute force attacks. It also aligns with industry best practices and compliance requirements, reducing the potential for unauthorized access to critical resources. Adhering to a 90-day rotation policy helps maintain a strong security posture and meets regulatory standards such as PCI-DSS and ISO 27001.
  description: This rule checks if IAM user-managed keys are rotated within a 90-day period. To verify, query the IAM API for key creation and last rotation timestamps. Remediate by implementing a key rotation policy using automation tools like Cloud Functions or Cloud Workflows to ensure keys are rotated every 90 days. Regular audits and alerts can be configured in Google Cloud's Security Command Center to monitor compliance.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts#managing_service_account_keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security-command-center/docs/how-to-use-sec-command-center
  - https://cloud.google.com/security/key-management
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
- rule_id: gcp.iam.user.managed_key_unused
  service: iam
  resource: user
  requirement: Managed Key Unused
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Mitigate Risks of Unused User-Managed IAM Keys
  rationale: Unused user-managed IAM keys pose a security risk as they may become vulnerable to unauthorized access or compromise over time, especially if not rotated regularly. Such keys can be leveraged by malicious actors to gain unauthorized access to resources, leading to potential data breaches and compliance violations. Maintaining active and necessary keys aligns with regulatory requirements and minimizes the attack surface.
  description: This rule checks for user-managed IAM keys that have not been used for more than 90 days. Unused keys should be identified and either rotated or deleted to prevent unauthorized access. Administrators can verify this by using the GCP Console or the gcloud command-line tool to list keys and their last used date. Remediation involves auditing key usage and removing those that are unused or unnecessary, while ensuring essential keys are regularly rotated.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/best-practices/identity
- rule_id: gcp.iam.user.managed_keys
  service: iam
  resource: user
  requirement: Managed Keys
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure User-Managed Keys are Rotated and Secured
  rationale: Proper management of user-managed keys in GCP is critical to prevent unauthorized access and data breaches. Unrotated or improperly secured keys can lead to compromised accounts and unauthorized access to sensitive resources, posing significant security and compliance risks, especially under frameworks such as PCI-DSS and ISO 27001.
  description: This rule verifies that user-managed keys in GCP are rotated regularly and stored securely. It checks for keys that have not been rotated within a specified period and ensures they adhere to best practices for storage and usage. Users should implement automated key rotation policies and store keys in secure locations like Cloud KMS. Immediate rotation and secure storage of outdated or improperly stored keys mitigate the risk of key compromise.
  references:
  - https://cloud.google.com/iam/docs/managing-service-account-keys
  - https://cloud.google.com/security/compliance/cis#gcp_cis
  - https://www.nist.gov/publications/nist-special-publication-800-57-part-1-revision-5
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloud.google.com/security/best-practices/identity-and-access-management
- rule_id: gcp.iam.user.mfa_enabled
  service: iam
  resource: user
  requirement: MFA Enabled
  scope: iam.user.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure MFA is Enabled for All IAM Users
  rationale: Requiring Multi-Factor Authentication (MFA) for IAM users enhances security by adding an additional layer of protection against unauthorized access. This is crucial in preventing account compromise from credential theft and mitigating risks such as data breaches and unauthorized data manipulation. Enforcing MFA aligns with compliance mandates for frameworks like PCI-DSS and ISO 27001, ensuring that access control measures meet industry standards.
  description: This rule checks whether Multi-Factor Authentication (MFA) is enabled for all users in the Google Cloud IAM service. MFA involves requiring users to present two or more separate authentication factors to verify their identity, significantly reducing the likelihood of unauthorized access. To verify, review the 'Security' section in the Google Cloud Console under 'IAM & Admin > IAM' and ensure that MFA is required for all user accounts. If MFA is not enabled, configure it by setting up security keys or Google Authenticator for each user. Regularly audit user accounts to ensure compliance.
  references:
  - https://cloud.google.com/iam/docs/managing-mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.user.mfa_enabled_console_access
  service: iam
  resource: user
  requirement: MFA Enabled Console Access
  scope: iam.user.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA for GCP Console Access by Users
  rationale: Enabling Multi-Factor Authentication (MFA) for users accessing the GCP console mitigates the risk of account compromise due to stolen credentials. It adds a layer of security by requiring a second form of verification, which is crucial for protecting sensitive operations and data. This is essential for compliance with standards like NIST SP 800-63B and PCI-DSS that mandate strong authentication mechanisms.
  description: This rule verifies that all users with console access to Google Cloud Platform have MFA enabled. It checks the IAM settings to ensure that MFA is configured for user accounts, which can be done by requiring security keys or Google Prompt. To remediate, administrators should enforce MFA by setting up an Organization Policy that mandates MFA for all users accessing the console. They should also educate users on configuring and maintaining their MFA devices.
  references:
  - https://cloud.google.com/iam/docs/using-mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/identity-and-access-management
- rule_id: gcp.iam.user.no_inline_policies
  service: iam
  resource: user
  requirement: No Inline Policies
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Users Have No Inline IAM Policies
  rationale: Inline IAM policies tied directly to users can lead to complex and hard-to-track permissions, increasing the risk of privilege escalation and breaches. By using centralized IAM roles, organizations can maintain better control and visibility over user permissions, aligning with regulatory requirements and reducing the attack surface.
  description: This rule checks whether any inline policies are directly attached to IAM users. Inline policies can create security risks due to their decentralized nature, making it difficult to audit permissions across the organization. To ensure a secure and manageable access control environment, use IAM roles with predefined permissions and attach them to users as needed. Remove any inline policies and migrate permissions to roles for better governance.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/using-manage
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.iam.user.policy_lowercase
  service: iam
  resource: user
  requirement: Policy Lowercase
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policies Use Lowercase Identifiers
  rationale: Using lowercase identifiers in IAM policies helps prevent discrepancies and potential misconfigurations that can arise from case sensitivity issues. This practice mitigates the risk of unauthorized access due to misinterpretation of policy conditions or roles. Consistent use of lowercase also facilitates easier auditing and alignment with compliance standards that require clear and uniform policy definitions.
  description: This rule checks that all IAM policy identifiers, such as user emails and resource names, are in lowercase. IAM policies with mixed or uppercase identifiers may lead to unintended access controls. Remediation involves reviewing and updating IAM policies to ensure all identifiers are lowercase. This can be verified by manually inspecting IAM policies in the Google Cloud Console or using Cloud Deployment Manager scripts to enforce naming conventions.
  references:
  - https://cloud.google.com/iam/docs/policies
  - https://cloud.google.com/security/compliance/cis-gcp-1-1-0
  - https://www.nist.gov/itl/ssd/nist-cloud-computing-program
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.user.policy_minimum_length_14
  service: iam
  resource: user
  requirement: Policy Minimum Length 14
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: IAM User Password Policy Minimum Length 14
  rationale: Enforcing a minimum password length of 14 characters reduces the risk of unauthorized access by making brute force attacks more difficult. Longer passwords increase the complexity and entropy, enhancing security posture. This requirement aligns with compliance standards such as NIST SP 800-63B, which recommend strong password policies to protect sensitive information and prevent data breaches.
  description: This rule checks if IAM user password policies enforce a minimum length of at least 14 characters. To verify, review the IAM password policy settings in the GCP Console or using the gcloud CLI. If the minimum length is set below 14, update the policy to meet the requirement. This can be done by navigating to 'Security' in the IAM settings and adjusting the password length parameter. Adhering to this rule helps mitigate risks associated with weak passwords.
  references:
  - https://cloud.google.com/iam/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://cloud.google.com/security/compliance
  - https://pages.nist.gov/800-63-3/sp800-63b.html
- rule_id: gcp.iam.user.policy_number
  service: iam
  resource: user
  requirement: Policy Number
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Limit IAM User Policies to Essential Permissions
  rationale: Excessive IAM user policies can lead to unauthorized access, elevating the risk of data breaches and compliance violations. By limiting user policies to only what is necessary, organizations reduce their attack surface and align with best practices in identity management. This is crucial for maintaining integrity and confidentiality in cloud environments, supporting compliance with standards like ISO 27001 and PCI-DSS.
  description: This rule checks for the number of IAM user policies assigned to each user, ensuring they are not excessive and only include essential permissions. To verify, review the IAM policies in the GCP Console and ensure they follow the principle of least privilege. Remediation involves auditing current permissions, removing unnecessary ones, and regularly reviewing policy assignments to prevent privilege creep.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/publications/nist-cybersecurity-framework
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.iam.user.policy_reuse_24
  service: iam
  resource: user
  requirement: Policy Reuse 24
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Unique IAM Policy Assignments for Users
  rationale: Reusing IAM policies for multiple users can lead to unintended access permissions, increasing the risk of data breaches and non-compliance with security standards. Organizations must ensure policies are tailored to specific user roles to maintain the principle of least privilege and adhere to compliance frameworks like NIST and PCI-DSS.
  description: This rule checks for the reuse of IAM policies across different user accounts to ensure that each user has a unique, role-appropriate policy. Verify that IAM policies are not reused by multiple users unless absolutely necessary, and adjust permissions to align with job functions. Remediation involves auditing existing policies, creating new roles as needed, and assigning permissions based on the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/auditing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.iam.user.policy_reuse_24_gcp_cloud_build_project_no_secrets_i_logging
  service: iam
  resource: user
  requirement: Policy Reuse 24 Gcp Cloud Build Project No Secrets I Logging
  scope: iam.user.logging
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Prevent Secret Exposure in Cloud Build Logs
  rationale: Exposing secrets in Cloud Build logs can lead to unauthorized access and potential data breaches, compromising sensitive information. This practice violates compliance standards such as PCI-DSS and ISO 27001, which mandate strict control over access to sensitive data. Preventing secrets in logs is crucial to maintain the integrity and confidentiality of the application environment.
  description: This rule audits Cloud Build configurations to ensure that sensitive information such as API keys, passwords, and other secrets are not included in build logs. To comply, ensure that all secrets are stored in Secret Manager and accessed securely during builds. Validate configurations through IAM roles and permissions, ensuring logs are free from sensitive data. Remediation involves reviewing build scripts and IAM policies to eliminate secret exposure risks.
  references:
  - https://cloud.google.com/build/docs/securing-builds/configure-access-to-secrets
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.iam.user.policy_symbol
  service: iam
  resource: user
  requirement: Policy Symbol
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM Policy Symbols Are Defined for User Accounts
  rationale: Defining clear IAM policy symbols for user accounts helps prevent unauthorized access by ensuring that only appropriate permissions are granted. Misconfigured IAM policies can lead to privilege escalation and data breaches, impacting business operations and violating compliance requirements such as GDPR and ISO 27001. Properly defined policies are crucial for maintaining a secure and compliant cloud environment.
  description: This rule checks whether IAM policies for user accounts in GCP have clearly defined symbols, ensuring that permissions are explicitly granted and managed. Users with undefined or overly permissive policies increase the risk of unauthorized access and data exposure. To remediate, review user IAM policies and ensure each permission is necessary and properly defined, removing any wildcard entries or unnecessary roles.
  references:
  - https://cloud.google.com/iam/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.user.policy_uppercase
  service: iam
  resource: user
  requirement: Policy Uppercase
  scope: iam.user.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure IAM User Policies are Defined in Uppercase
  rationale: Enforcing uppercase in policy definitions helps maintain consistency and reduces the risk of errors or misinterpretations in policy management. This can prevent unauthorized access and ensure that identity and access management policies are implemented accurately, which is crucial for regulatory compliance and reducing the risk of security breaches.
  description: This rule checks if user policies in IAM are defined in uppercase letters. This is important for maintaining a standardized policy format that minimizes the risk of human error during policy evaluation. To verify compliance, review IAM policies to ensure all policy attributes and values are in uppercase. If not compliant, update the policy definitions to adhere to uppercase formatting. This practice aids in preventing unauthorized access and maintaining a secure environment.
  references:
  - https://cloud.google.com/iam/docs/policies
  - https://cloud.google.com/security/compliance/cis#gcp-cis-1-iam
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.iam.user.unused_credentials
  service: iam
  resource: user
  requirement: Unused Credentials
  scope: iam.user.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Unused IAM User Credentials
  rationale: Unused IAM user credentials present a significant security risk as they can be exploited by attackers due to lack of oversight and monitoring. Inactive credentials may not be subject to routine password updates or multi-factor authentication, increasing the likelihood of unauthorized access. Regularly reviewing and disabling unused credentials helps organizations comply with security standards such as NIST SP 800-53 and PCI-DSS, reducing the attack surface.
  description: This rule checks for IAM user credentials that have not been used within a specified period, typically 90 days, indicating potential neglect or abandonment. To verify, administrators can review the 'LastUsed' timestamp for each credential in the IAM console or via API. Remediation involves disabling or deleting the unused credentials or enforcing a policy that requires periodic credential usage verification. This action reduces the risk of credential compromise and ensures compliance with security policies.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/monitoring-iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/blog/products/identity-security/security-key-enforcement-enhancements
- rule_id: gcp.iam.workload_identity_pool.identity_access_oidc_allowed_client_ids_audiences_restricted
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Oidc Allowed Client Ids Audiences Restricted
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Restrict OIDC Client IDs in Workload Identity Pools
  rationale: Restricting OIDC allowed client IDs and audiences in Workload Identity Pools minimizes unauthorized access risks by ensuring only designated clients can request tokens. This is crucial for maintaining secure authentication practices, preventing privilege escalation, and aligning with compliance requirements such as NIST SP 800-63-3 and ISO 27001, which emphasize strict identity management controls.
  description: This rule checks if Workload Identity Pools have restricted OIDC allowed client IDs and audiences to prevent unauthorized token requests. Administrators should configure OIDC providers to specify only trusted client IDs and audiences. Verification involves reviewing the workload identity pool's configuration settings to ensure that allowed client IDs and audiences are explicitly defined. Remediation includes updating the workload identity pool to limit these fields to only necessary and verified values.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.locations.workloadIdentityPools.providers
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.workload_identity_pool.identity_access_oidc_issuer_https_and_matches_discovery
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Oidc Issuer HTTPS And Matches Discovery
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: OIDC Issuer HTTPS & Discovery Match for Workload Identity
  rationale: Ensuring the OIDC issuer URL is HTTPS and matches the discovery document is critical to prevent man-in-the-middle attacks, unauthorized access, and misconfigurations in identity federation. This safeguards organizational resources by ensuring only verified identities from trusted providers can access GCP workloads, aligning with compliance requirements such as ISO 27001 and SOC2.
  description: This rule checks if the OIDC issuer URL in a GCP Workload Identity Pool is using HTTPS and matches the OIDC discovery document. A mismatch could indicate a configuration error or a potential security risk. Verify that the issuer URL is correctly configured and secured with HTTPS. Remediation involves updating the issuer URL to ensure it uses HTTPS and accurately reflects the OIDC discovery document configuration.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.locations.workloadIdentityPools
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf
- rule_id: gcp.iam.workload_identity_pool.identity_access_oidc_thumbprints_or_jwks_pinning_configured
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Oidc Thumbprints Or Jwks Pinning Configured
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure OIDC Thumbprints or JWKS Pinning Configured for Workload Identity Pools
  rationale: Configuring OIDC thumbprints or JWKS pinning for workload identity pools helps prevent man-in-the-middle attacks by ensuring the authenticity of the identity provider. This configuration is crucial for maintaining the integrity of identity assertions, protecting against unauthorized access, and ensuring compliance with industry standards such as NIST and ISO 27001.
  description: This rule checks if OIDC thumbprints or JWKS pinning are configured for workload identity pools in GCP. Proper configuration requires specifying either the OIDC provider's HTTPS thumbprints or JWKS URL to verify the identity token's signature. To verify, navigate to the Workload Identity Pool settings in the GCP Console and ensure that either thumbprints or JWKS URLs are set. Remediation involves updating your workload identity pool with the correct thumbprint or JWKS settings obtained from the identity provider's security documentation.
  references:
  - https://cloud.google.com/iam/docs/configuring-workload-identity-federation
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf
  - https://cloud.google.com/security/best-practices
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.iam.workload_identity_pool.identity_access_oidc_token_lifetime_reasonable
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Oidc Token Lifetime Reasonable
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Reasonable OIDC Token Lifetime for Workload Identity Pool
  rationale: Excessive OIDC token lifetimes increase the risk of unauthorized access if tokens are compromised, as they remain valid for longer periods. Shorter token lifetimes mitigate the impact of token theft, align with the principle of least privilege, and help comply with security standards that require periodic re-authentication to reduce the attack surface.
  description: This rule checks if the OIDC token lifetime for a Workload Identity Pool is set to a reasonable duration, typically not exceeding 1 hour. Configure the lifetime through the IAM settings to ensure that tokens expire promptly, minimizing potential misuse. Verify the current settings via the Google Cloud Console or gcloud CLI, and adjust the token lifetime by modifying the 'token_lifetime' parameter in the Workload Identity Pool configuration.
  references:
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.locations.workloadIdentityPools
  - https://cloud.google.com/identity/docs/workload-identity-federation
  - CIS Google Cloud Computing Foundations Benchmark v1.1.0, Section 1.6
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.iam.workload_identity_pool.identity_access_saml_assertion_lifetime_reasonable
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Saml Assertion Lifetime Reasonable
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SAML Assertion Lifetime is Reasonable for Workload Identity Pools
  rationale: Setting a reasonable lifetime for SAML assertions in Workload Identity Pools minimizes the risk of session hijacking and unauthorized access due to long-lived tokens. It helps to align with security best practices and regulatory requirements for session management, ensuring that breached session tokens have a limited impact.
  description: This rule checks if the SAML assertion lifetime configured in a GCP Workload Identity Pool is set to a reasonable duration, typically less than one hour. A shorter assertion lifetime reduces the window of opportunity for malicious actors to exploit stolen tokens. To verify, review the IAM settings for the identity pool and adjust the assertion lifetime as needed using the GCP Console or gcloud CLI. Remediation involves setting the assertion lifetime to a value that balances security and operational needs.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.locations.workloadIdentityPools
  - https://cloud.google.com/security/compliance/cis#cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.iam.workload_identity_pool.identity_access_saml_audience_restriction_configured
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Saml Audience Restriction Configured
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SAML Audience Restriction Configured for Workload Identity Pools
  rationale: Configuring SAML audience restrictions helps prevent impersonation attacks by ensuring that tokens are only accepted by intended services. Without this configuration, malicious entities could potentially exploit tokens to gain unauthorized access to sensitive resources, leading to data breaches and non-compliance with standards like ISO 27001 and SOC2.
  description: This rule checks if the SAML audience restriction is configured for workload identity pools in GCP. The SAML audience is a critical component that specifies the intended recipient of a token, preventing misuse. Verify this configuration by reviewing the audience field in the workload identity pool settings. To remediate, configure the audience restriction by updating the workload identity pool settings to include a valid audience field that matches your intended applications or services.
  references:
  - https://cloud.google.com/iam/docs/manage-workload-identity-pools
  - https://cloud.google.com/iam/docs/saml#saml_audience_validation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://docs.microsoft.com/en-us/security/compass/identity-management-best-practices
- rule_id: gcp.iam.workload_identity_pool.identity_access_saml_certificates_not_expired
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Saml Certificates Not Expired
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SAML Certificates in Workload Identity Pools Are Valid
  rationale: Expired SAML certificates in workload identity pools can lead to authentication failures, disrupting access to critical resources and potentially violating regulatory compliance requirements. Keeping certificates valid mitigates the risk of unauthorized access and ensures continuous and secure service operations.
  description: This rule checks whether the SAML certificates used in GCP workload identity pools are expired. It verifies the validity of each certificate by comparing the current date against their expiration dates. To remediate, administrators should renew any expired certificates and update the identity pool configurations to reference the new certificates, ensuring uninterrupted and secure authentication processes.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/reference/rest/v1/projects.locations.workloadIdentityPools
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-63b.pdf
- rule_id: gcp.iam.workload_identity_pool.identity_access_saml_idp_metadata_signed
  service: iam
  resource: workload_identity_pool
  requirement: Identity Access Saml Idp Metadata Signed
  scope: iam.workload_identity_pool.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SAML IdP Metadata is Signed in Workload Identity Pools
  rationale: Unsigned SAML IdP metadata can lead to potential impersonation attacks, where a malicious entity could present falsified metadata to gain unauthorized access to resources. Ensuring the metadata is signed helps maintain trust in the identity provider's authenticity and integrity, aligning with compliance frameworks like NIST and ISO 27001 that mandate secure authentication practices.
  description: This rule checks that SAML IdP metadata for Workload Identity Pools is signed, which is crucial for verifying the identity of an external identity provider. To verify, ensure that the SAML metadata XML file includes a valid signature element. Remediation involves configuring your identity provider to sign the metadata and updating the IdP configuration in GCP to use the signed metadata file. This prevents unauthorized access by ensuring that identity assertions are from a legitimate source.
  references:
  - https://cloud.google.com/iam/docs/workload-identity-federation
  - https://cloud.google.com/iam/docs/concepts/workload-identity-federation
  - https://cloud.google.com/security-compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.logging.bucket.access_rbac_least_privilege
  service: logging
  resource: bucket
  requirement: Access RBAC Least Privilege
  scope: logging.bucket.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Logging Bucket Access Uses Least Privilege RBAC
  rationale: Implementing least privilege for access to logging buckets minimizes the risk of unauthorized data exposure and reduces the attack surface. This approach helps protect sensitive log data from insider threats and external breaches, aligning with compliance requirements such as PCI-DSS and ISO 27001 by ensuring only necessary permissions are granted.
  description: This rule checks that access to GCP logging buckets is configured using role-based access control (RBAC) with the principle of least privilege. It verifies that only essential roles, such as 'Log Viewer' or custom roles with minimal permissions, are assigned to users or service accounts. To remediate, review the IAM policy for the logging bucket and remove any unnecessary roles or permissions, ensuring that users have the least amount of access required to perform their functions.
  references:
  - https://cloud.google.com/logging/docs/access-control
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/using-iam-securely#least_privilege
- rule_id: gcp.logging.bucket.encryption_at_rest_cmek
  service: logging
  resource: bucket
  requirement: Encryption At Rest Cmek
  scope: logging.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Logging Buckets Use Customer-Managed Encryption Keys
  rationale: Using Customer-Managed Encryption Keys (CMEK) for Cloud Logging buckets allows organizations to maintain control over the encryption keys for their sensitive log data, mitigating risks associated with unauthorized access. This practice is crucial in meeting compliance mandates such as GDPR and CCPA, which require robust data protection measures. Additionally, CMEK provides enhanced audit capabilities, enabling organizations to track key usage and access patterns.
  description: This rule checks whether Cloud Logging buckets are configured to use Customer-Managed Encryption Keys (CMEK) for encryption at rest. To verify compliance, ensure that each logging bucket specifies a CMEK by configuring the bucket's encryption settings. Remediation involves assigning a Cloud Key Management Service (KMS) key to the logging bucket. This can be done via the Google Cloud Console or gcloud CLI by updating the bucket configuration to include a specified KMS key.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.logging.bucket.immutability_or_object_lock_where_supported
  service: logging
  resource: bucket
  requirement: Immutability Or Object Lock Where Supported
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Immutability or Object Lock for Logging Buckets
  rationale: Implementing immutability or object locks on logging buckets protects critical log data from accidental or malicious alterations. This security measure helps organizations maintain data integrity, supports forensic investigations, and meets compliance requirements such as those found in PCI-DSS and ISO 27001. By ensuring logs cannot be deleted or modified, businesses can safeguard against data breaches and unauthorized data manipulation.
  description: This rule checks if logging buckets have immutability or object lock settings enabled, ensuring that logs cannot be altered or deleted unintentionally. To verify, inspect the bucket configuration for 'Retention Policy' or 'Object Lock' settings in the Google Cloud Console under Cloud Storage. If not configured, enable a retention policy or object lock to protect log data. This can be done using the 'gsutil' command-line tool or through the web interface by setting a specific time period for data retention.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.logging.bucket.permission_changes_enabled
  service: logging
  resource: bucket
  requirement: Permission Changes Enabled
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Logging for Permission Changes on Buckets
  rationale: Enabling logging for permission changes on Google Cloud Storage buckets helps organizations detect unauthorized access or misconfigurations, reducing the risk of data breaches. This practice is crucial for audit trails and compliance with standards like ISO 27001, which require auditing of access controls. Without this logging, enterprises face increased risk of unnoticed malicious activities and potential non-compliance with regulatory requirements.
  description: This rule checks whether logging is enabled for permission changes on Cloud Storage buckets. To verify, ensure that Audit Logs are configured to capture 'Admin Read' and 'Data Write' activities for the 'storage.googleapis.com' service in your project settings. Remediation involves enabling these logging activities via the Google Cloud Console by navigating to 'IAM & Admin' > 'Audit Logs' and checking the appropriate boxes for each bucket. This configuration helps track and respond to unauthorized permission modifications.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/storage/docs/access-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.bucket.read_events_enabled
  service: logging
  resource: bucket
  requirement: Read Events Enabled
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Buckets Have Read Events Enabled
  rationale: Enabling read events logging on buckets is crucial for tracking access patterns and detecting unauthorized access attempts, which helps mitigate data breaches and maintain the integrity of sensitive information. This practice supports compliance with standards such as PCI-DSS and helps organizations in forensic investigations by providing a detailed access log.
  description: This rule checks if read events logging is enabled on Google Cloud Storage buckets, which is essential for auditing access operations. By verifying the configuration in the 'Logging' settings of each bucket, organizations can ensure that all read operations are logged. Remediation involves enabling the 'logBucket' option under 'Logging' settings in the Cloud Console or via the `gsutil logging set` command to specify the log bucket for read events.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/storage/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.logging.bucket.retention_days_minimum
  service: logging
  resource: bucket
  requirement: Retention Days Minimum
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Minimum Retention Days for Logging Buckets
  rationale: Setting a minimum retention period for logging buckets ensures that audit logs are preserved for a sufficient duration to support investigations of security incidents, compliance audits, and operational troubleshooting. Insufficient retention periods can lead to loss of critical logs that are necessary for forensic analysis and meeting regulatory requirements such as PCI-DSS and SOC2.
  description: This rule checks whether logging buckets in GCP have a minimum retention period set. The retention period determines how long log data is preserved before it is automatically deleted. To verify, review the configuration of each logging bucket and ensure the retention period meets or exceeds organizational and regulatory requirements. Remediation involves updating the bucket settings in the GCP Console or via CLI to enforce the required retention period.
  references:
  - https://cloud.google.com/logging/docs/retention
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.bucket.server_access_logging_enabled
  service: logging
  resource: bucket
  requirement: Server Access Logging Enabled
  scope: logging.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Server Access Logging for GCP Buckets
  rationale: Enabling server access logging for GCP buckets provides visibility into access patterns and potential unauthorized access. It helps in auditing and forensic investigations by offering detailed logs of who accessed the buckets and what actions were performed. This is crucial for compliance with regulations such as PCI-DSS and HIPAA, which require detailed access logs for sensitive data.
  description: This rule checks if server access logging is enabled for GCP buckets, ensuring that all access events are recorded. To verify, inspect the bucket's logging configuration to ensure it is set to output logs to a specified logging bucket. Remediation involves configuring the GCP bucket to enable logging via the Cloud Console, CLI, or API, directing logs to a designated log bucket for analysis.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/storage/docs/enabling-bucket-logging
- rule_id: gcp.logging.bucket.write_events_enabled
  service: logging
  resource: bucket
  requirement: Write Events Enabled
  scope: logging.bucket.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Buckets Have Write Events Enabled
  rationale: Enabled write events for logging buckets help track changes and access patterns, providing visibility into unauthorized modifications and potential security breaches. This is crucial for detecting data loss or tampering, ensuring compliance with regulations like PCI-DSS and HIPAA, and maintaining the integrity of logging data for forensic analysis.
  description: This check verifies that logging buckets have write events enabled, which logs operations such as data writes and modifications. To ensure compliance, configure the logging buckets in GCP to capture write events using the Stackdriver Logging. Remediation involves enabling audit logs for the bucket through GCP Console or using gcloud CLI, thereby capturing detailed information about write activities.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
- rule_id: gcp.logging.log.access_rbac_least_privilege
  service: logging
  resource: log
  requirement: Access RBAC Least Privilege
  scope: logging.log.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Logging Access via RBAC
  rationale: Implementing least privilege in access controls minimizes potential security breaches by limiting permissions to only those necessary for users to perform their jobs. This reduces the risk of unauthorized access to sensitive log data, which can lead to data breaches or compliance violations, especially relevant in regulated industries like finance and healthcare.
  description: This rule checks if IAM roles assigned to users or service accounts on GCP logging resources adhere to the principle of least privilege. Specifically, it ensures that roles granting access to log data are limited to necessary permissions without excessive access. To verify, audit the assigned roles for logging resources and adjust permissions to align with operational duties. Remediation involves reviewing current IAM policies and modifying roles to remove unnecessary permissions through the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/logging/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.logging.log.logging_configuration_verification
  service: logging
  resource: log
  requirement: Logging Configuration Verification
  scope: logging.log.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Verify and Secure GCP Logging Configurations
  rationale: Ensuring accurate logging configurations in GCP is crucial for maintaining visibility into system operations and detecting anomalous activities. Improper logging can lead to unmonitored security incidents, data breaches, and non-compliance with regulatory standards. Effective logging supports forensic investigations and satisfies audit requirements for frameworks like PCI-DSS and HIPAA.
  description: This rule checks whether all required log sinks are configured properly to capture and export relevant logs for critical resources in GCP. It verifies if audit logs, including Admin Activity and Data Access logs, are enabled and exported to a secure destination such as BigQuery, Cloud Storage, or Pub/Sub. Remediation involves reviewing logging configurations in the GCP Console or using gcloud commands to ensure all necessary logs are captured and stored securely. Update IAM policies to restrict access to log data to authorized personnel only.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/index.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.log.metric_custom_role_changes_alerts_enabled
  service: logging
  resource: log
  requirement: Metric Custom Role Changes Alerts Enabled
  scope: logging.log.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Alert on Custom Role Changes in GCP
  rationale: Monitoring custom role changes is crucial to detect unauthorized or unintentional modifications that could lead to privilege escalation or misuse of resources. This ensures that any changes to role permissions are quickly identified, minimizing the risk of security breaches and aiding in compliance with frameworks like ISO 27001 and SOC 2, which require rigorous access control and monitoring.
  description: This rule verifies that alerts are configured for changes to custom roles in GCP. It checks for the existence of log-based metrics and alerting policies that notify administrators when modifications are made to custom roles. To verify, ensure that a log metric captures role change events and an alert policy is set to notify relevant stakeholders. Remediation involves creating a log-based metric for 'iam.role.update' events and setting up a corresponding alerting policy in Cloud Monitoring.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/iam/docs/understanding-custom-roles
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.logging.log.metric_filter_alerts_vpc_changes
  service: logging
  resource: log
  requirement: Metric Filter Alerts VPC Changes
  scope: logging.log.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Alert on VPC Configuration Changes via Log Metrics
  rationale: Monitoring VPC changes is crucial to detect unauthorized modifications that could expose sensitive data or disrupt services. It helps in identifying potential misconfigurations or malicious activities, fulfilling compliance requirements such as NIST SP 800-53 and ISO 27001 which emphasize the importance of audit trails and change management.
  description: This rule checks if a log-based metric filter and alert are configured to monitor changes in VPC configurations. The filter should capture logs related to VPC alterations and trigger an alert when these logs are detected. To verify, ensure that a log metric is created with a filter for VPC change events and an alert policy is linked to this metric. Remediation involves setting up a log metric filter in Google Cloud Logging and creating an alerting policy in Google Cloud Monitoring.
  references:
  - https://cloud.google.com/logging/docs/logs-based-metrics
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/vpc/docs/audit-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.logging.log.metric_project_ownership_assignments_monitored
  service: logging
  resource: log
  requirement: Metric Project Ownership Assignments Monitored
  scope: logging.log.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Monitor Project Ownership Assignment with Log Metrics
  rationale: Monitoring project ownership assignments is critical to ensure that unauthorized changes do not compromise the security posture of your cloud environment. Unmonitored changes can lead to privilege escalation or unauthorized access, impacting compliance with regulations like SOC2 and ISO 27001. Regular monitoring helps in early detection of potential security breaches, reducing the risk to business operations.
  description: This rule checks if logging metrics are configured to monitor project ownership assignments within GCP. Ensure that appropriate logging and alerting mechanisms are in place to track changes in project ownership. Verify that logs capture all relevant assignment events and set up alerts for critical changes. Remediation involves configuring log-based metrics to track 'roles/owner' assignment changes and setting up alerts for any modifications.
  references:
  - https://cloud.google.com/logging/docs/logs-based-metrics
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.logging.log.privacy_audit_logs_centralized_and_encrypted
  service: logging
  resource: log
  requirement: Privacy Audit Logs Centralized And Encrypted
  scope: logging.log.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Centralized and Encrypted Privacy Audit Logs in GCP
  rationale: Centralizing and encrypting Privacy Audit Logs ensures that sensitive information is protected from unauthorized access and reduces the risk of data breaches. This practice supports compliance with regulatory requirements such as GDPR, HIPAA, and PCI-DSS, which mandate strict access controls and data protection measures for audit logs. It also facilitates efficient auditing and monitoring, enabling timely detection and response to security incidents.
  description: This rule checks that all Privacy Audit Logs in GCP are centralized in a designated logging project and encrypted using Cloud KMS. To verify, ensure that Cloud Logging is configured to route logs to a centralized log bucket with CMEK (Customer-Managed Encryption Keys) enabled. Remediation involves setting up a centralized logging architecture and configuring log sinks to use CMEK for encryption, providing an additional layer of control over log data.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/architecture/logging-centralization
- rule_id: gcp.logging.log.privacy_audit_retention_days_minimum
  service: logging
  resource: log
  requirement: Privacy Audit Retention Days Minimum
  scope: logging.log.audit_logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Minimum Retention Period for Privacy Audit Logs
  rationale: Retaining privacy audit logs for a minimum period is crucial for forensic analysis, compliance with regulatory mandates, and ensuring accountability. Insufficient log retention can obstruct incident investigation, hinder regulatory audits, and may lead to non-compliance with standards like GDPR or HIPAA, exposing your organization to legal risks and potential fines.
  description: This rule checks if GCP privacy audit logs are retained for at least the minimum required number of days. Ensure that the LogRouter is configured to retain these logs by setting the 'retentionDays' property in the logging sink configuration. Verify the settings through the GCP Console or gcloud CLI, and adjust retention policies to meet compliance requirements. Remediation involves updating the logging sink settings to ensure logs are retained for a specified minimum period.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/cis/benchmark
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/reference/tools/gcloud-logging
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.logging.log.storage_encrypted
  service: logging
  resource: log
  requirement: Storage Encrypted
  scope: logging.log.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Log Storage Encryption at Rest is Enabled
  rationale: Encrypting logs at rest protects sensitive data from unauthorized access, ensuring confidentiality and integrity. Unencrypted log data could be exposed in case of a breach, leading to potential data leaks, non-compliance with regulations such as GDPR and CCPA, and damage to company reputation.
  description: This rule checks if Google Cloud Logging is configured to encrypt logs at rest using customer-managed encryption keys (CMEK) or Google-managed keys. To verify, review the configuration settings in Google Cloud Console under Logging > Logs Storage and ensure encryption is enabled. Remediation involves configuring the logs storage to use CMEK via Google Cloud Key Management Service (KMS) or ensuring Google-managed encryption is active.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 8.4 Ensure that Cloud Audit Logging is configured properly
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - 'PCI-DSS Requirement 3: Protect stored cardholder data'
  - https://cloud.google.com/security/encryption-at-rest
- rule_id: gcp.logging.metric.alert_policy_audit_configuration_changes
  service: logging
  resource: metric
  requirement: Alert Policy Audit Configuration Changes
  scope: logging.metric.audit_logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Monitor Audit Config Changes for Enhanced Security Posture
  rationale: Monitoring audit configuration changes is crucial as unauthorized or untracked changes can lead to data breaches, regulatory non-compliance, and operational disruptions. It helps in identifying insider threats and misconfigurations that could expose sensitive data or disrupt services. Ensuring audit policies are intact supports compliance with frameworks like SOC 2 and ISO 27001.
  description: This rule checks for changes to audit configurations in GCP logging metrics. It ensures that any modifications to audit settings are tracked and alerts are generated for review. Administrators should verify that audit logs are enabled for all critical services and that alerts notify the appropriate teams. Remediation involves reviewing change logs, confirming the legitimacy of changes, and reverting unauthorized modifications.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.logging.metric.alert_vpc_firewall_rule_changes
  service: logging
  resource: metric
  requirement: Alert VPC Firewall Rule Changes
  scope: logging.metric.network_security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Alert on VPC Firewall Rule Modifications
  rationale: Monitoring changes to VPC firewall rules is critical as unauthorized modifications can lead to exposure of sensitive data and systems to external threats. This control helps in identifying potential security breaches, insider threats, and ensures compliance with security frameworks that mandate rigorous access controls and logging of network activities.
  description: This rule monitors audit logs for any changes to VPC firewall rules and triggers alerts when modifications are detected. It involves setting up log-based metrics in Google Cloud Logging to capture specific audit log entries related to 'compute.firewalls.update' and 'compute.firewalls.insert' actions. Verification can be done by checking the configured alert policies in Cloud Monitoring. To remediate issues, review the changes, revert unauthorized modifications, and ensure access permissions are adequately restricted.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/compute/docs/vpc/firewall-rules
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/monitoring/alerts
- rule_id: gcp.logging.metric.iam_permission_changes_storage
  service: logging
  resource: metric
  requirement: IAM Permission Changes Storage
  scope: logging.metric.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Monitor IAM Permission Changes via Logs-Based Metrics
  rationale: Monitoring changes to IAM permissions is crucial to prevent unauthorized access and to ensure that only intended changes are made, supporting both security and compliance efforts. Unauthorized or accidental modifications to IAM policies can lead to privilege escalation, data breaches, and significant financial and reputational damage. Continuous logging of IAM permission changes helps organizations meet compliance requirements such as SOC 2, ISO 27001, and NIST SP 800-53.
  description: This rule checks for the presence of a logs-based metric that tracks IAM permission changes within Google Cloud. Ensure that the appropriate logs-based metric is configured to capture 'iam.setIamPolicy' events in Cloud Logging. To verify, navigate to the Logging section in the GCP Console, and confirm that a metric is set up for 'iam.setIamPolicy' events. Remediate by creating a logs-based metric if absent, and configure alerts to notify security teams of any changes.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.logging.metric.log_metric_filter_alarm_configured
  service: logging
  resource: metric
  requirement: Log Metric Filter Alarm Configured
  scope: logging.metric.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Log Metric Filter Alarm is Configured
  rationale: Configuring log metric filter alarms is crucial for proactive monitoring and incident response. It helps in real-time detection of suspicious activities and policy violations, reducing the impact of potential security incidents. This practice is vital for maintaining compliance with security standards like PCI-DSS and ISO 27001, which require continuous monitoring and logging.
  description: This check verifies that a log metric filter alarm is configured for specific log metrics in Google Cloud's logging service. It involves setting up alerts that trigger based on custom log queries, ensuring critical events donâ€™t go unnoticed. Remediation involves identifying essential log metrics and configuring alerts through Google Cloud Monitoring to receive notifications via email, SMS, or other channels.
  references:
  - https://cloud.google.com/logging/docs/logs-based-metrics
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.audit_logging_enabled
  service: logging
  resource: sink
  requirement: Audit Logging Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Audit Logs are Enabled for GCP Logging Sinks
  rationale: Enabling audit logging for GCP logging sinks is crucial for maintaining an accurate record of access and changes to your logging infrastructure. This helps in identifying unauthorized access attempts, analyzing incident timelines, and ensuring accountability for actions within the logging environment, which is essential for meeting compliance requirements such as GDPR, PCI-DSS, and HIPAA.
  description: This rule checks whether audit logging is enabled for all logging sinks within your GCP project. Audit logs capture important events related to access and modifications, providing visibility into the integrity and security of your logging data. To verify, ensure that logging sinks include audit log configurations by accessing the Cloud Console or using gcloud CLI commands. Remediate by configuring the sink to include the requisite audit logs through IAM roles and permissions settings.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.logging.sink.bucket_retention_policy_locked
  service: logging
  resource: sink
  requirement: Bucket Retention Policy Locked
  scope: logging.sink.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sink Bucket Retention Policies Are Locked
  rationale: Locking bucket retention policies prevents accidental or malicious changes to retention settings, ensuring audit logs are preserved for the required duration. This is crucial for forensic investigations, compliance with legal and regulatory requirements, and maintaining data integrity. Failure to lock retention policies can lead to premature log deletion, impacting compliance with standards like PCI-DSS and HIPAA.
  description: 'This rule checks if the retention policy for buckets used by Cloud Logging sinks is locked, preventing modifications to the retention period. To verify, inspect the bucket''s configuration in the Google Cloud Console or use the gcloud CLI to ensure the ''retentionPolicy.isLocked'' property is set to true. To remediate, use the gcloud CLI to lock the retention policy: ''gcloud alpha storage buckets update [BUCKET_NAME] --retention-policy-is-locked''.'
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2#destination-bucket
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.logging.sink.changes_enabled
  service: logging
  resource: sink
  requirement: Changes Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Change Auditing for Logging Sinks
  rationale: Enabling change auditing for logging sinks is crucial for maintaining transparency and accountability within your cloud environment. Unauthorized or inadvertent modifications to logging configurations can lead to data loss or exposure, impacting compliance with standards like PCI-DSS and ISO 27001. Additionally, having detailed logs of changes aids in incident response and forensic investigations, reducing business risk.
  description: This rule checks if change logging is enabled for Google Cloud logging sinks, ensuring that any modifications are captured and auditable. To verify, ensure that Cloud Audit Logs are configured to log admin activity and data access for logging sinks. Remediation involves setting up audit configurations via the 'gcloud' command-line tool or Google Cloud Console to capture necessary logs for review.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/logging/docs/reference/tools/gcloud-logging
  - https://cloud.google.com/logging/docs/export
- rule_id: gcp.logging.sink.configuration_changes_enabled
  service: logging
  resource: sink
  requirement: Configuration Changes Enabled
  scope: logging.sink.configuration_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Configuration Changes Logging for GCP Logging Sinks
  rationale: Enabling logging of configuration changes for sinks in GCP ensures that any modifications to logging behavior are tracked, providing traceability and accountability. This is crucial for detecting unauthorized changes and meeting compliance requirements such as auditing and monitoring policies. It mitigates risks related to unauthorized access or data leakage by ensuring that all configuration changes are logged and can be reviewed.
  description: This rule checks whether configuration changes for logging sinks are being logged in GCP. To verify, ensure that audit logs for 'Admin Read' and 'Admin Write' activities are enabled for the logging service. Remediation involves configuring audit logging for your GCP project to include sink configuration activities. This can be done through the Google Cloud Console by navigating to the IAM & Admin section, selecting Audit Logs, and enabling the necessary logs for the Logging API.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.sink.configuration_configured
  service: logging
  resource: sink
  requirement: Configuration Configured
  scope: logging.sink.configuration_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sinks are Properly Configured
  rationale: Proper configuration of logging sinks is crucial for maintaining a comprehensive audit trail of activities within GCP. Misconfigured sinks can lead to incomplete logging, which might hinder forensic analysis and compliance with regulatory requirements such as PCI-DSS or SOC2. This can expose the organization to risks such as data breaches, lack of accountability, and inability to detect unauthorized access.
  description: This rule checks for the correct configuration of logging sinks within Google Cloud Platform. It evaluates whether sinks are set up to export logs to the appropriate destinations, such as Cloud Storage, BigQuery, or Pub/Sub, ensuring that all necessary logs are retained and accessible. To verify, review the sink configuration in the Logging section of the GCP Console, ensuring that each sink has a valid destination and appropriate filters. Remediation involves updating or creating sinks with the correct settings to ensure all critical logs are captured and stored securely.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.logging.sink.cross_account_destinations_restricted
  service: logging
  resource: sink
  requirement: Cross Account Destinations Restricted
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Logging Sink Cross-Account Destinations
  rationale: Allowing logging sinks to export logs to destinations in other accounts can lead to unauthorized access and data exfiltration. Misconfigured sinks could expose sensitive data, resulting in compliance violations with standards like PCI-DSS and HIPAA. Restricting cross-account destinations mitigates the risk of data breaches and ensures data integrity within the organization.
  description: This check verifies that logging sinks do not have destinations set in external accounts. It requires examining sink configurations to ensure destinations are within the same GCP organization or account. To remediate, review and update the sink destination settings to point to resources under the same account. Use IAM policies to enforce restrictions and regularly audit sink configurations.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/security/compliance/cis#gcp_cis_benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/hipaa
- rule_id: gcp.logging.sink.destination_encrypted
  service: logging
  resource: sink
  requirement: Destination Encrypted
  scope: logging.sink.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Logging Sink Destinations are Encrypted at Rest
  rationale: Encrypting logging sink destinations is crucial to protect sensitive log data from unauthorized access and potential breaches. Without encryption, logs containing sensitive information can be exposed, leading to data leaks, compliance violations, and reputational damage. Ensuring encryption aligns with regulatory standards such as GDPR and HIPAA, safeguarding data privacy and integrity.
  description: This rule checks whether the destination of logging sinks in GCP uses encryption at rest to protect log data. Verify that the storage destination, such as Cloud Storage or BigQuery, has encryption either through Google-managed or customer-managed keys. To remediate, configure the destination to use encryption by selecting an appropriate key management option in the GCP Console or using gcloud commands.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://cloud.google.com/security/compliance
  - NIST SP 800-57 Part 1
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.destination_endpoint_private_networking
  service: logging
  resource: sink
  requirement: Destination Endpoint Private Networking
  scope: logging.sink.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Log Sink Destinations Use Private Networking
  rationale: Using private networking for log sink destinations minimizes exposure to the public internet, reducing the risk of data interception and unauthorized access. This is critical to maintaining the confidentiality and integrity of log data, which often contains sensitive information. Additionally, private networking is a key requirement for compliance with many security frameworks and regulations.
  description: This check ensures that the destination endpoints for log sinks are configured to use private networking, such as VPC peering or Private Google Access. Verify that log sinks are not set up with public IP addresses and that they use secure, private connections. Remediation involves updating the log sink's destination to utilize a private network path, ensuring secure data transfer within your GCP environment.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/architecture/best-practices-for-private-connectivity
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 6.6
  - 'NIST SP 800-53 Rev. 5: AC-17 Remote Access'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.sink.destination_iam_policy_least_privilege
  service: logging
  resource: sink
  requirement: Destination IAM Policy Least Privilege
  scope: logging.sink.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Logging Sink IAM Policies
  rationale: Granting excessive permissions on logging sinks can lead to unauthorized access or data leaks, impacting business confidentiality and integrity. Minimizing permissions reduces the risk of exposure to insider threats and external attacks while aligning with compliance regulations such as GDPR and PCI-DSS that mandate strict access controls.
  description: This rule checks that IAM policies attached to logging sinks grant only the necessary permissions required for their function. Review and adjust the IAM roles associated with sink destinations to ensure they follow the principle of least privilege. Verify permissions using the GCP Console or gcloud CLI, and remove any unnecessary roles to mitigate potential security risks.
  references:
  - https://cloud.google.com/logging/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/iam/docs/granting-changing-revoking-access
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.sink.destination_least_privilege
  service: logging
  resource: sink
  requirement: Destination Least Privilege
  scope: logging.sink.least_privilege
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: high
  title: Ensure Logging Sink Destination Uses Least Privilege Access
  rationale: Implementing least privilege access for logging sink destinations minimizes the risk of unauthorized access and potential data breaches. This practice reduces the attack surface by ensuring only necessary permissions are granted, aligning with compliance requirements such as GDPR and CCPA, which mandate strict access controls on sensitive data. It also helps mitigate insider threats and accidental data exposure by restricting access to only those with a legitimate need.
  description: This rule checks that logging sink destinations are configured with the principle of least privilege, ensuring that only necessary permissions are granted. Verify that roles assigned to service accounts associated with logging sinks have only the essential permissions to write logs to the destination resources, such as Cloud Storage buckets, BigQuery datasets, or Pub/Sub topics. To remediate, review and adjust IAM policies for the sink's destination to limit permissions to only those required for logging operations, utilizing predefined roles where possible.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
- rule_id: gcp.logging.sink.destination_tls_min_1_2_enforced
  service: logging
  resource: sink
  requirement: Destination TLS Min 1 2 Enforced
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sink Uses TLS 1.2+ for Destination Security
  rationale: Enforcing TLS 1.2 or higher for logging sink destinations protects data in transit from eavesdropping and man-in-the-middle attacks. This practice helps maintain data integrity and confidentiality, which is crucial for compliance with standards such as PCI-DSS and HIPAA. Failure to enforce this can lead to unauthorized access and potential data breaches, resulting in financial and reputational damage.
  description: This rule checks that all logging sink destinations in GCP enforce a minimum of TLS 1.2 for secure data transmission. Verify that the logging sink configuration specifies a TLS version of 1.2 or higher by reviewing the sink's destination settings in the GCP Console or via the gcloud CLI. To remediate, update the sink configuration to utilize a TLS 1.2+ endpoint, ensuring that all data is encrypted during transmission.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf
- rule_id: gcp.logging.sink.instance_configuration_change_alert_enabled
  service: logging
  resource: sink
  requirement: Instance Configuration Change Alert Enabled
  scope: logging.sink.configuration_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Alerts for Instance Configuration Changes
  rationale: Monitoring instance configuration changes is critical for ensuring the integrity and security of your GCP environment. Unauthorized or unplanned changes can lead to vulnerabilities, service disruptions, and non-compliance with industry regulations, such as PCI-DSS and NIST. Alerts help in quickly identifying and responding to potential security incidents.
  description: This rule checks whether alerts are configured for changes to instance configurations via log sinks in Google Cloud. To verify, ensure that sink filters are set to capture configuration changes and that notifications are configured to alert the appropriate teams. Remediate by setting up logging sinks with filters for 'compute.instances.update' and configuring alert policies in Cloud Monitoring to notify stakeholders.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/compute/docs/instances/viewing-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/security-overview
- rule_id: gcp.logging.sink.kms_encryption_enabled
  service: logging
  resource: sink
  requirement: KMS Encryption Enabled
  scope: logging.sink.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption is Enabled for Logging Sinks
  rationale: Enabling KMS encryption for logging sinks protects sensitive log data from unauthorized access and meets regulatory requirements for data protection, such as GDPR and HIPAA. This reduces the risk of data breaches and unauthorized data manipulation, ensuring that only authorized entities can decrypt and access the logs.
  description: This rule checks if log sinks in GCP have KMS encryption enabled, which requires specifying a Customer Managed Encryption Key (CMEK) for log entries. To verify, ensure that each log sink configuration has a 'writerIdentity' with the necessary permissions to use the specified KMS key. Remediation involves updating the sink configuration to include a CMEK by using the Google Cloud Console or gcloud command-line tool.
  references:
  - https://cloud.google.com/logging/docs/routing/using-sinks
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - CIS GCP Benchmark v1.3.0, 6.8 Ensure Cloud Audit Logging is configured properly
  - NIST SP 800-53 Rev. 5, SC-13 Cryptographic Protection
  - PCI DSS v3.2.1, Requirement 3.5.1
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.logging.sink.log_analysis_enabled
  service: logging
  resource: sink
  requirement: Log Analysis Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Log Analysis is Enabled for GCP Logging Sinks
  rationale: Enabling log analysis on GCP logging sinks enhances the ability to detect and respond to security incidents by ensuring that all relevant logs are aggregated and accessible for analysis. This is vital for identifying potential threats, understanding the impact of security events, and meeting compliance obligations such as monitoring and auditing controls mandated by frameworks like NIST and PCI-DSS.
  description: This rule checks if log analysis is enabled on GCP logging sinks to ensure that logs are properly aggregated and available for inspection. To verify, ensure that each sink has an associated BigQuery dataset, Pub/Sub topic, or Cloud Storage bucket configured for analysis. Remediation involves configuring a destination for each logging sink and enabling log exports. Regularly review and update these configurations to ensure comprehensive log coverage.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.log_file_validation_enabled
  service: logging
  resource: sink
  requirement: Log File Validation Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Log File Validation is Enabled for Logging Sinks
  rationale: Enabling log file validation ensures the integrity and authenticity of log data, which is crucial for forensic investigations and compliance with regulations such as PCI-DSS and SOC2. It reduces the risk of tampering with logs by ensuring that any unauthorized modifications are detectable, enhancing trust in log data for security analysis and audits.
  description: This rule checks whether log file validation is enabled for all logging sinks in GCP. Log file validation uses checksums to verify the integrity of log data, providing assurance that logs have not been altered. To ensure compliance, configure your logging sinks to include log file validation by setting up a 'LOG_VALIDATION' feature in the Cloud Logging settings. This involves updating the sink configuration to include the validation option, which can be done through the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2#creating_sink
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.log_metric_filter_and_alert_for_audit_configuration__enabled
  service: logging
  resource: sink
  requirement: Log Metric Filter And Alert For Audit Configuration Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Log Metric Filter and Alert for Audit Configurations
  rationale: Monitoring and alerting on audit configuration changes are crucial for detecting unauthorized or unintentional modifications that could compromise logging integrity. This helps in maintaining accountability, ensuring traceability of changes, and supporting compliance with regulations such as PCI-DSS and NIST, which require robust logging and monitoring practices.
  description: This rule checks that log metric filters and alerts are configured for changes to audit configurations in Google Cloud Logging. It ensures that any modifications to audit configurations are captured and alerts are generated, enabling prompt investigation of potential security incidents. To verify, ensure that a log metric filter is set up for 'AuditConfig' changes and that an alerting policy is linked to this metric. Remediate by creating a log metric for 'AuditConfig' changes and establishing an alert policy to notify security operations teams.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/logging/docs/reference/tools/gcloud-logging
- rule_id: gcp.logging.sink.log_metric_filter_and_alert_for_project_ownership_ch_enabled
  service: logging
  resource: sink
  requirement: Log Metric Filter And Alert For Project Ownership Ch Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Alert on Project Ownership Changes via Log Metric
  rationale: Monitoring changes to project ownership is crucial for maintaining control over permissions and preventing unauthorized access. Unauthorized changes can lead to data breaches, loss of intellectual property, and non-compliance with regulations such as GDPR and HIPAA. By alerting on these changes, organizations can quickly respond to potential security incidents.
  description: This rule checks for the existence of log metric filters that detect changes in project ownership within GCP. These filters should be configured to trigger alerts when ownership changes are logged, ensuring that security teams are promptly notified. To verify, ensure that a log-based alert is set up for the 'IAM Policy Change' audit logs with a condition that matches 'protoPayload.methodName="SetIamPolicy"'. Remediation involves creating or updating log sinks and alert policies to capture and notify such events.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/monitoring/alerts/log-based-alerts
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Control 4.1
- rule_id: gcp.logging.sink.logging_enabled
  service: logging
  resource: sink
  requirement: Logging Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Logging is Enabled for All Logging Sinks
  rationale: Enabling logging for all sinks is crucial for tracking and auditing data flow within Google Cloud. It provides visibility into operations, aiding in the detection of unauthorized access attempts and misconfigurations. This is essential for meeting compliance standards such as PCI-DSS and NIST, and it strengthens the overall security posture by ensuring all data transfers are monitored.
  description: This rule verifies that logging is enabled for all sinks in your GCP environment. A logging sink allows you to export log entries outside of Cloud Logging, for example, to BigQuery, Pub/Sub, or Cloud Storage. To verify, check the logging configuration in the GCP Console under Logging > Logs Router, and ensure that each sink has a valid destination with logging enabled. Remediation involves ensuring each sink is properly configured with the necessary permissions and a valid logging destination.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/pci_security/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.logging.sink.multi_region_enabled
  service: logging
  resource: sink
  requirement: Multi Region Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Multi-Region Logging Sinks for Redundancy
  rationale: Enabling multi-region logging sinks ensures that audit logs are replicated across multiple regions, providing redundancy and resilience against regional failures. This setup enhances the availability of logs for incident response and forensic analysis, which is crucial for maintaining compliance with regulations that require reliable audit trails such as PCI-DSS and SOC 2.
  description: This rule verifies that logging sinks are configured to use multi-region storage locations, which can be set by specifying a valid multi-region bucket in Cloud Storage as the destination. To verify, check the destination property of each logging sink. Remediation involves updating the sink's destination to a multi-region bucket, ensuring logs are stored across multiple geographic locations for added durability and availability.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/storage/docs/locations#location-mr
- rule_id: gcp.logging.sink.resource_enabled
  service: logging
  resource: sink
  requirement: Resource Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sinks Are Enabled for Audit Trails
  rationale: Enabling logging sinks is crucial for capturing audit trails, which help in detecting unauthorized access and potential security breaches. Without enabled logging sinks, organizations may fail to meet compliance requirements such as NIST, PCI-DSS, and ISO 27001, leading to financial penalties and reputational damage. Effective logging allows for timely incident response and forensic analysis.
  description: This rule verifies that logging sinks in Google Cloud are enabled, ensuring that log entries are exported to specified destinations for long-term retention and analysis. Administrators should check the configuration of sinks by using the 'gcloud logging sinks list' command to verify their status. To enable a sink, use 'gcloud logging sinks create' with appropriate destination details. Ensure that any disabled sinks are re-enabled to maintain comprehensive audit logging.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.pcisecuritystandards.org/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.logging.sink.route_change_alert_configured
  service: logging
  resource: sink
  requirement: Route Change Alert Configured
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Route Change Alerts are Configured for Logging Sinks
  rationale: Properly configured route change alerts are crucial for detecting unauthorized network modifications, which could indicate potential security breaches or compliance violations. By monitoring these changes, organizations can quickly respond to potential threats and maintain alignment with regulatory requirements such as PCI-DSS and SOC2.
  description: This check verifies that alerting is configured for logging sinks to monitor route changes in the Google Cloud Platform. It ensures that the logging sink is set up to capture route change logs and that relevant alerts are established to notify security teams of any alterations. To verify, navigate to the Logging section in the GCP console, ensure that the sink captures route logs, and configure alerts in the Monitoring section. Remediation involves setting up appropriate alert policies and filters to track and respond to route changes.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.logging.sink.route_changes_enabled
  service: logging
  resource: sink
  requirement: Route Changes Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sink for Route Changes is Enabled
  rationale: Enabling logging for route changes is crucial for monitoring and auditing network configuration changes, which helps in detecting unauthorized access and potential network misconfigurations. This visibility supports compliance with frameworks like ISO 27001 and assists in forensic investigations by providing a detailed trail of changes impacting network security.
  description: This rule verifies that all Google Cloud Logging sinks are configured to log changes in network routing. To check this, ensure that audit logs for 'vpc_accessible_services' and 'routes' are enabled. Remediation involves configuring logging sinks to capture these changes by setting up audit configurations in IAM policies to monitor 'Compute Engine Network' changes. This can be done via the GCP Console or CLI by enabling appropriate 'Admin Activity' logs.
  references:
  - https://cloud.google.com/logging/docs/audit#enabling_audit_logs
  - https://cloud.google.com/network-connectivity/docs/router/how-to/logging
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/compute/docs/networking/routes
- rule_id: gcp.logging.sink.rule_changes_enabled
  service: logging
  resource: sink
  requirement: Rule Changes Enabled
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Audit Logging for Sink Rule Changes
  rationale: Enabling audit logging for sink rule changes in GCP ensures that all modifications to logging sinks are captured and reviewed. This is crucial for tracking unauthorized changes, maintaining data integrity, and meeting compliance requirements such as PCI-DSS and SOC 2, where audit trails are mandatory. Failure to log these changes can result in undetected configuration drifts, leading to potential data exfiltration and reputational damage.
  description: This rule checks whether audit logging is enabled for changes to logging sink configurations in Google Cloud. It involves verifying that appropriate IAM roles are assigned to capture sink rule changes and that logs are stored securely. Remediation involves configuring Cloud Audit Logs to monitor ADMIN_ACTIVITY and DATA_ACCESS logs for the logging sink resource. Regularly review these logs to ensure compliance and detect unauthorized changes promptly.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/logging/docs/export/configure_export_v2
- rule_id: gcp.logging.sink.run_task_definition_logging_enabled
  service: logging
  resource: sink
  requirement: Run Task Definition Logging Enabled
  scope: logging.sink.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Logging for Run Task Definitions in GCP Sinks
  rationale: Enabling logging for run task definitions in GCP sinks is crucial for maintaining an audit trail of system operations and user activities. It helps organizations detect unauthorized access, troubleshoot issues, and comply with regulatory requirements such as PCI-DSS and SOC2. Failure to enable such logging can result in undetected security breaches and non-compliance with industry standards.
  description: This rule checks whether logging is enabled for run task definitions within GCP logging sinks. Organizations must configure their sinks to capture and store logs related to task executions, which can be verified by inspecting the sink's configuration settings. To remediate, ensure the logging sink is correctly set up to include all relevant task definition logs by using the gcloud command-line tool or GCP Console. This involves specifying the appropriate log filters and destination settings.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/architecture/best-practices-for-logging-and-monitoring
- rule_id: gcp.logging.sink.sink_created_configured
  service: logging
  resource: sink
  requirement: Sink Created Configured
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Logging Sinks Are Properly Configured
  rationale: Proper configuration of logging sinks is crucial for capturing and storing audit logs, which helps in monitoring activities, detecting anomalies, and ensuring compliance with security policies. Misconfigured sinks may lead to loss of critical log data, affecting the organization's ability to respond to incidents and meet regulatory requirements such as PCI-DSS and HIPAA.
  description: This rule checks that logging sinks are created and configured correctly to export log entries to their designated destinations like BigQuery, Cloud Storage, or Pub/Sub. Proper configuration includes specifying the correct log filter, destination, and permissions. To verify, ensure that sinks are actively capturing the required logs and that the IAM policies allow appropriate access. Remediation involves reviewing and updating the sink configuration and permissions as needed to ensure all critical logs are captured and stored securely.
  references:
  - https://cloud.google.com/logging/docs/export/configure_export_v2
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 6.5 Ensure that sinks are configured
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.logging.sink.stream_encryption_at_rest_enabled
  service: logging
  resource: sink
  requirement: Stream Encryption At Rest Enabled
  scope: logging.sink.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Logging Sink Encryption at Rest is Enabled
  rationale: Encrypting log data at rest is critical for protecting sensitive information from unauthorized access and potential data breaches. This is crucial to maintain confidentiality, integrity, and compliance with data protection laws like GDPR, HIPAA, and PCI-DSS. Failure to encrypt can lead to severe reputational damage and financial penalties.
  description: This rule checks if Google Cloud Logging sinks have encryption at rest enabled by verifying the configuration of Cloud KMS keys that encrypt log data. To ensure encryption, configure your logging sinks to use customer-managed encryption keys (CMEK). You can verify this setting in the Google Cloud Console under Logging > Sinks or by using the gcloud CLI to confirm that a CMEK is associated with the sink. Remediation involves updating the sink configuration to specify a valid Cloud KMS key for encryption.
  references:
  - https://cloud.google.com/logging/docs/routing/overview
  - https://cloud.google.com/logging/docs/routing/using-sinks
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
- rule_id: gcp.logging.sink.stream_ingestion_auth_required
  service: logging
  resource: sink
  requirement: Stream Ingestion Auth Required
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Stream Ingestion Authentication for Logging Sinks
  rationale: Enforcing authentication for stream ingestion in logging sinks reduces the risk of unauthorized access to log data, which could lead to data breaches or manipulation. Secure logging practices are crucial for maintaining data integrity, supporting forensic investigations, and meeting compliance mandates such as PCI-DSS and SOC 2.
  description: This rule verifies that stream ingestion to logging sinks is authenticated in GCP, ensuring that only authorized entities can write logs. To check this, review the sink's configuration in the Google Cloud Console or via gcloud CLI to confirm that authentication mechanisms like OAuth tokens are enabled. Remediate by configuring the sink to require token-based authentication, enhancing the security of your logging infrastructure.
  references:
  - https://cloud.google.com/logging/docs/export
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.logging.sink.stream_retention_days_minimum
  service: logging
  resource: sink
  requirement: Stream Retention Days Minimum
  scope: logging.sink.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Minimum Retention Days for Logging Sinks
  rationale: Proper retention of log data is crucial for forensic analysis, compliance, and operational troubleshooting. Short retention periods can hinder the ability to investigate security incidents, leading to potential data loss and compliance issues with frameworks like PCI-DSS and SOC2. A minimum retention period ensures that critical log data is retained long enough to meet business and regulatory needs.
  description: This rule checks that all logging sinks in GCP have a minimum retention period set to ensure logs are retained for an adequate duration. Verify the 'retentionDays' field in each sink configuration to ensure it meets or exceeds the organizational policy. If the retention period is too short, update the sink configuration to extend the retention days, ensuring alignment with compliance and operational requirements.
  references:
  - https://cloud.google.com/logging/docs/routing/overview#sinks
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.acls_alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Acls Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Policies for ACLs are Configured in GCP Monitoring
  rationale: Alert policies on ACLs help identify unauthorized access attempts and potential breaches, mitigating security risks and ensuring compliance with regulations such as NIST and HIPAA. Properly configured alerts enable timely responses to access violations, reducing the risk of data exposure and maintaining trust with stakeholders.
  description: This rule verifies that alert policies are configured for Access Control Lists (ACLs) within GCP Monitoring. It checks for notifications on changes or access attempts that deviate from defined security policies. To remediate, configure alert policies in the GCP Console by setting conditions that trigger on ACL changes. Regularly review and update these policies to align with organizational security protocols.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.hipaajournal.com/wp-content/uploads/2020/09/HIPAA-Security-Rule.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.monitoring.alert_policy.alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Policies Have Alarms Configured in GCP Monitoring
  rationale: Configuring alarms for alert policies in GCP Monitoring is crucial for timely detection of anomalies and potential security incidents, which can prevent data breaches and mitigate operational risks. It ensures that stakeholders are immediately informed about critical issues, aligning with compliance requirements such as ISO 27001 and SOC 2, which mandate effective monitoring and alerting mechanisms.
  description: This rule checks for the presence of alarms in configured alert policies within GCP Monitoring. An alarm must be set up to trigger notifications when specific conditions are met, ensuring prompt response to incidents. To verify, review the alert policy configuration in the GCP Console under Monitoring > Alerting. Ensure that each alert policy has a 'Notification Channels' section filled out. Remediation involves configuring the missing alarms and linking them to appropriate notification channels like email or SMS.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/monitoring/support/notification-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
- rule_id: gcp.monitoring.alert_policy.alert_alarm_actions_configured
  service: monitoring
  resource: alert_policy
  requirement: Alert Alarm Actions Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Alarm Actions Are Configured for Monitoring Policies
  rationale: Configuring alarm actions for alert policies ensures timely notification and response to potential security incidents or system failures. Without alarm actions, critical alerts may go unnoticed, leading to prolonged exposure to risks and non-compliance with industry standards such as NIST or PCI-DSS.
  description: This rule checks if alert policies in Google Cloud Monitoring have alarm actions configured. Alarm actions notify stakeholders or trigger automated responses when an alert is triggered. To verify, review the configuration of each alert policy and ensure that notification channels or automated actions are set up. Remediation involves accessing the Google Cloud Console, navigating to Monitoring, and configuring the necessary alarm actions for each policy.
  references:
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://www.nist.gov/document-162
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.monitoring.alert_policy.alert_critical_alarms_enabled
  service: monitoring
  resource: alert_policy
  requirement: Alert Critical Alarms Enabled
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Critical Alarms in GCP Monitoring Alert Policies
  rationale: Enabling critical alarms in GCP Monitoring alert policies is crucial for timely detection and response to potential incidents that could impact system availability and security. It ensures that critical issues are promptly addressed, minimizing potential downtime and data breaches, thereby supporting regulatory compliance and maintaining trust with stakeholders.
  description: This rule checks whether GCP Monitoring alert policies have critical alarms enabled. It ensures that alerts are configured to notify relevant personnel when critical thresholds are breached, enabling prompt remediation actions. To verify, navigate to the GCP Console, access Monitoring, and review the alert policies for critical alarm settings. Remediation involves configuring alert policies to include critical alerts with appropriate notification channels such as email or SMS to ensure rapid response.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/architecture/best-practices-for-cloud-operations
- rule_id: gcp.monitoring.alert_policy.alert_destinations_authenticated
  service: monitoring
  resource: alert_policy
  requirement: Alert Destinations Authenticated
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Destinations Are Authenticated
  rationale: Authenticating alert destinations in GCP Monitoring is crucial to prevent unauthorized access to sensitive alert data, which can lead to data breaches and non-compliance with regulations like GDPR and HIPAA. It ensures that alerts are only received by intended parties, maintaining the integrity and confidentiality of alert information.
  description: This rule checks that all alert destinations in GCP Monitoring are properly authenticated, ensuring that alerts are sent securely and only to authorized endpoints. Verify that alert policies utilize secure channels such as HTTPS and OAuth 2.0 for authentication. Remediation involves updating alert policies to use HTTPS endpoints and configuring authentication credentials, such as service accounts or API keys, to ensure secure communication.
  references:
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.monitoring.alert_policy.anomaly_alerts_configured
  service: monitoring
  resource: alert_policy
  requirement: Anomaly Alerts Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Anomaly Alerts Are Configured for Monitoring
  rationale: Configuring anomaly alerts in GCP Monitoring is critical for identifying unusual patterns or deviations from normal operations, which could indicate security incidents or system failures. This helps in minimizing downtime and potential data breaches by enabling timely responses to suspicious activities. Additionally, it supports compliance with regulatory requirements by ensuring continuous monitoring of critical metrics.
  description: This rule checks if anomaly detection alerts are configured in GCP Monitoring alert policies. To verify, ensure that alert policies include conditions that monitor for unexpected changes in metrics. This can be configured through the GCP Console or using Terraform to specify 'anomaly detection' as the condition type. Remediation involves creating or updating alert policies to include anomaly detection, thereby enhancing proactive monitoring capabilities.
  references:
  - https://cloud.google.com/monitoring/alerts/using-alerting-policies
  - https://cloud.google.com/monitoring/alerts/concepts-indepth#anomaly-detection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.anomaly_detectors_enabled
  service: monitoring
  resource: alert_policy
  requirement: Anomaly Detectors Enabled
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Anomaly Detectors for GCP Alert Policies
  rationale: Enabling anomaly detectors in GCP alert policies is crucial for identifying unusual patterns that may indicate security breaches or operational issues. This helps in proactively addressing potential threats and minimizing the impact on business operations. Additionally, anomaly detection supports compliance with regulatory standards by ensuring continuous monitoring and timely incident response.
  description: This rule checks if anomaly detectors are enabled within GCP alert policies, which are essential for identifying deviations from normal behavior in monitored metrics. To verify, navigate to the Monitoring section in the GCP Console and ensure anomaly detection is configured for critical alert policies. Remediation involves enabling anomaly detectors within alert policies, allowing for early detection and notification of irregular activity, thereby improving incident response and system reliability.
  references:
  - https://cloud.google.com/monitoring/alerts/concepts-indepth
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - NIST SP 800-53 Rev. 5 CM-8
  - ISO/IEC 27001:2013 A.12.4.1
  - https://cloud.google.com/blog/products/operations/advanced-anomaly-detection-in-cloud-operations
- rule_id: gcp.monitoring.alert_policy.anomaly_training_data_sources_approved
  service: monitoring
  resource: alert_policy
  requirement: Anomaly Training Data Sources Approved
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Approved Anomaly Training Data Sources
  rationale: Using approved data sources for anomaly detection ensures the accuracy and reliability of alerting processes, minimizes false positives, and protects against potential data breaches. Properly vetted data sources comply with organizational policies and regulatory requirements, reducing the risk of unauthorized access and ensuring data integrity.
  description: This rule verifies that anomaly detection alert policies in GCP Monitoring use only approved training data sources. It checks configurations to ensure data sources have been validated and authorized according to organizational security policies. To remediate, review the list of data sources used for anomaly training and ensure they are approved by security teams. Regular audits and updates to the approved sources list are recommended to maintain compliance.
  references:
  - https://cloud.google.com/monitoring/alerts
  - CIS GCP Benchmark v1.3.0 - Section 4.2
  - NIST SP 800-53 Rev. 4 - SI-4
  - https://cloud.google.com/security/compliance
  - https://security.googleblog.com/2020/09/monitoring-best-practices.html
- rule_id: gcp.monitoring.alert_policy.capacity_alerts_configured
  service: monitoring
  resource: alert_policy
  requirement: Capacity Alerts Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Capacity Alerts are Configured
  rationale: Configuring capacity alerts in GCP is crucial for proactively managing resource utilization. Without these alerts, businesses risk unexpected service disruptions due to resource exhaustion, potentially leading to financial loss and customer dissatisfaction. Proper alert configuration aids in maintaining service availability and supports compliance with operational monitoring standards.
  description: This rule checks whether capacity alerts are configured in GCP's Monitoring service. Capacity alerts notify administrators when resource usage approaches predetermined thresholds, allowing timely intervention. To verify compliance, ensure that alert policies are set up for critical resources such as CPU, memory, and storage. Remediation involves creating or updating alert policies to include capacity thresholds, ensuring notifications are sent to the responsible teams.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/monitoring/docs/alerting/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/73906.html
- rule_id: gcp.monitoring.alert_policy.changes_to_vpcs_alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Changes To Vpcs Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Monitor Changes to VPCs with Alert Policies
  rationale: Monitoring changes to VPC configurations is crucial as unauthorized or erroneous modifications can lead to network vulnerabilities, such as open access to critical services or exposure of sensitive data. By configuring alerts for VPC changes, organizations can quickly detect and respond to potential security incidents, ensuring compliance with regulatory requirements and maintaining the integrity of their cloud infrastructure.
  description: This rule checks if an alert policy is configured to monitor changes to VPCs in GCP. It ensures that any modification to the VPC settings triggers an alert, allowing security teams to investigate and mitigate risks promptly. To verify, ensure that the alert policy includes conditions for 'VPC Network Changes' under the monitored metrics and that notifications are set up for the relevant security personnel. Remediation involves creating or updating an alert policy in the GCP Monitoring service to include these conditions.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/vpc/docs/security-overview
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, section 6.5
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 5, CA-7 Continuous Monitoring
  - ISO/IEC 27001:2013 - A.12.4 Logging and Monitoring
- rule_id: gcp.monitoring.alert_policy.cpu_utilization_alert_configured
  service: monitoring
  resource: alert_policy
  requirement: Cpu Utilization Alert Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure CPU Utilization Alerts Are Configured
  rationale: Configuring CPU utilization alerts helps quickly identify and respond to potential performance issues that could impact business operations. High CPU usage may indicate resource contention, inefficient code, or potential denial of service attacks. Timely alerts ensure that administrators can take corrective action before user experience or service availability is affected, thereby meeting compliance requirements for system monitoring and availability.
  description: This rule checks whether alert policies are in place to monitor CPU utilization on GCP resources. Specifically, it verifies that there is an alert policy configured to trigger when CPU utilization exceeds a defined threshold. To verify, navigate to the Alerting section of the GCP Monitoring dashboard and ensure there are policies configured for CPU utilization. Remediation involves setting up an alert policy to notify administrators when CPU usage exceeds a safe threshold, based on the needs of the applications and services.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/monitoring/alerts/concepts-indicators
  - https://cloud.google.com/monitoring/settings
  - https://cloud.google.com/monitoring/alerts/using-alerting-policies
- rule_id: gcp.monitoring.alert_policy.data_analytics_admin_activity_logging_enabled
  service: monitoring
  resource: alert_policy
  requirement: Data Analytics Admin Activity Logging Enabled
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Ensure Data Analytics Admin Activity Logging is Enabled
  rationale: Enabling admin activity logging for Data Analytics services on GCP is crucial for tracking access and modification events, helping in detecting unauthorized changes and potential security breaches. It provides a comprehensive audit trail necessary for compliance with standards like PCI-DSS and HIPAA, which require detailed logging of user activities. Without it, organizations risk undetected data tampering and non-compliance penalties.
  description: This rule verifies that logging of admin activities in GCP Data Analytics services, such as BigQuery and Dataflow, is enabled by checking the alert policies configuration. Admin activity logs capture actions taken by administrators, which are critical for operational and security auditing. To remediate, ensure that audit logging is configured for these services through the GCP Console or gcloud CLI, and validate settings in the Cloud Audit Logs section.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/bigquery/docs/reference/auditlogs
  - https://cloud.google.com/dataflow/docs/guides/logging
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.monitoring.alert_policy.data_analytics_logs_retention_days_minimum
  service: monitoring
  resource: alert_policy
  requirement: Data Analytics Logs Retention Days Minimum
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Sufficient Retention of Data Analytics Logs
  rationale: Maintaining an adequate retention period for Data Analytics logs is crucial for forensic investigations, troubleshooting, and compliance with various regulations such as GDPR and HIPAA. Insufficient log retention can result in the inability to detect, analyze, and respond to security incidents, potentially leading to data breaches and financial penalties.
  description: This rule checks that the retention period for Data Analytics logs in GCP is set to a minimum acceptable number of days, as per organizational policies or compliance requirements. To verify, ensure that the retention period is configured in the Google Cloud Console under the Logging settings for each relevant project. Remediation involves accessing the Cloud Logging settings and adjusting the retention period to meet or exceed the minimum requirement. Regular audits should be conducted to ensure compliance with updated policies.
  references:
  - https://cloud.google.com/logging/docs/logs-retention
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/best-practices-for-enterprise-organizations
- rule_id: gcp.monitoring.alert_policy.data_analytics_query_access_logging_enabled
  service: monitoring
  resource: alert_policy
  requirement: Data Analytics Query Access Logging Enabled
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Data Analytics Query Access Logging is Enabled
  rationale: Enabling data analytics query access logging is crucial for tracking access patterns, identifying unauthorized access, and ensuring accountability. It helps in detecting anomalies, preventing data breaches, and maintaining compliance with regulatory standards such as GDPR and HIPAA, which require detailed audit trails for data access.
  description: This rule checks whether access logging for data analytics queries is enabled within GCP monitoring alert policies. Access logging must be configured to capture detailed logs of who accessed which data, when, and how. To verify, ensure that logging is enabled in the Stackdriver Logging settings for each alert policy. Remediate by configuring the necessary IAM roles and permissions to enable logging for data analytics queries if not already set.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/monitoring/settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.monitoring.alert_policy.dr_alert_destinations_configured
  service: monitoring
  resource: alert_policy
  requirement: DR Alert Destinations Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Policies Have DR Destinations Configured
  rationale: Configuring Disaster Recovery (DR) alert destinations ensures that critical alerts are received by the appropriate teams even during outages, minimizing downtime and potential data loss. Without proper DR alert configurations, organizations risk failing to respond to critical incidents promptly, which can result in significant operational disruptions and potential non-compliance with industry standards.
  description: This rule checks that all Google Cloud Monitoring alert policies have at least one DR alert destination configured, such as an external email address or a third-party incident management system. To verify, review the alert policy configurations in the Cloud Console under Monitoring. Remediate by adding reliable and redundant notification channels to each alert policy to ensure alerts are received during DR scenarios.
  references:
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - https://cloud.google.com/security/compliance/cis-cloud-platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/designing-for-business-continuity
- rule_id: gcp.monitoring.alert_policy.dr_alerts_for_backup_failures_configured
  service: monitoring
  resource: alert_policy
  requirement: DR Alerts For Backup Failures Configured
  scope: monitoring.alert_policy.backup_recovery
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure DR Alerts for Backup Failures are Configured
  rationale: Configuring alerts for backup failures is crucial for maintaining data integrity and availability. It helps organizations promptly respond to backup issues, minimizing the risk of data loss and ensuring business continuity. This is especially important for meeting compliance requirements related to data protection and disaster recovery, such as those outlined in ISO 27001 and NIST SP 800-53.
  description: This rule checks whether alert policies are configured to notify administrators of backup failures in GCP. Specifically, it verifies that Google Cloud Monitoring has alert policies targeting backup services, such as Cloud SQL or Cloud Storage. To remediate, configure alert policies in Cloud Monitoring by setting conditions that trigger when a backup operation fails and ensure notification channels are properly set up to inform responsible personnel.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/solutions/backup-and-disaster-recovery
  - https://www.iso.org/standard/54534.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/sql/docs/mysql/backup-recovery
  - https://cloud.google.com/storage/docs/managing-lifecycles
- rule_id: gcp.monitoring.alert_policy.dr_alerts_for_replication_lag_configured
  service: monitoring
  resource: alert_policy
  requirement: DR Alerts For Replication Lag Configured
  scope: monitoring.alert_policy.replication
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Configure DR Alerts for Replication Lag in GCP
  rationale: Replication lag can lead to stale data being served, impacting DR strategies and potentially causing data loss during failovers. Configuring alerts for replication lag ensures timely notifications, allowing for quick remedial action to maintain data integrity and business continuity. This is crucial for compliance with data protection regulations and industry standards.
  description: This rule checks if alert policies are configured to monitor replication lag in disaster recovery setups on GCP. Specifically, it verifies that alert policies are set to notify administrators when replication lag exceeds a predefined threshold, indicating potential data inconsistency issues. To remediate, configure an alert policy in GCP Monitoring with conditions that track replication delay metrics. Regularly review and adjust alert thresholds to align with your recovery point objectives (RPO).
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/solutions/dr-scenarios
  - CIS Google Cloud Platform Foundation Benchmark
  - 'NIST SP 800-53: CP-9 System and Communications Protection'
  - https://cloud.google.com/solutions/best-practices-for-enterprise-organizations
  - https://cloud.google.com/architecture/designing-hybrid-cloud-disaster-recovery
- rule_id: gcp.monitoring.alert_policy.dr_alerts_for_rpo_rto_breach_configured
  service: monitoring
  resource: alert_policy
  requirement: DR Alerts For Rpo Rto Breach Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure DR Alerts for RPO/RTO Breaches Are Configured
  rationale: Configuring Disaster Recovery (DR) alerts for Recovery Point Objective (RPO) and Recovery Time Objective (RTO) breaches is crucial to minimize downtime and data loss, which can have significant business impacts. Without these alerts, organizations may not be promptly notified of potential disruptions, leading to extended service outages and non-compliance with regulatory requirements such as ISO 27001 and PCI-DSS.
  description: This rule checks if alert policies are configured in GCP Monitoring to trigger notifications for breaches in RPO and RTO objectives. To verify, ensure that GCP Monitoring alert policies include conditions for key metrics indicating DR capability breaches. Set up notifications to alert responsible teams instantly. Remediation involves creating or updating alert policies to monitor RPO/RTO metrics and configuring notification channels to ensure timely alerts.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/architecture/disaster-recovery-planning-guide
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.monitoring.alert_policy.gateways_alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Gateways Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Policies for Gateway Monitoring are Configured
  rationale: Properly configured alert policies for gateways enable timely detection and response to potential security incidents, minimizing downtime and data breaches. They help organizations meet compliance requirements by providing evidence of monitoring and alerting capabilities, which are crucial for maintaining the integrity and availability of cloud resources.
  description: This rule checks whether alert policies are configured for monitoring gateway activities within GCP. It ensures that critical events, such as unauthorized access attempts or resource misconfigurations, trigger alerts. To verify, review the alert policies in the GCP Console under Monitoring and ensure they are set up for gateway events. Remediation involves setting up or updating alert policies to include specific conditions and notification channels tailored to gateway security needs.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance/cis
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/monitoring/docs/alerting/alert-policy-best-practices
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.group_retention_policy_specific_days_enabled_gcp_sto_enabled
  service: monitoring
  resource: alert_policy
  requirement: Group Retention Policy Specific Days Enabled Gcp Sto Enabled
  scope: monitoring.alert_policy.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enforce Specific Days for Alert Policy Retention
  rationale: Configuring specific retention periods for alert policies ensures that monitoring data is retained only as long as necessary, reducing storage costs and minimizing the risk of data exposure. It also supports compliance with regulatory requirements that dictate how long certain types of data must be retained. Properly configured alert retention policies help organizations efficiently manage their monitoring data lifecycle and maintain security and privacy obligations.
  description: This rule verifies that alert policies in Google Cloud Monitoring are configured with a specific retention period for the retention of alerting data. To comply, ensure that each alert policy has a defined retention period that aligns with your organization's data management policies. Remediation involves reviewing alert policies and setting an appropriate retention period via the Google Cloud Console or using gcloud CLI commands. This practice helps in managing storage costs and adhering to data governance policies.
  references:
  - https://cloud.google.com/monitoring/alerts/using-alerting
  - https://cloud.google.com/monitoring/settings#retention
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.monitoring.alert_policy.logging_metric_filter_console_root_login_detected_fi_present
  service: monitoring
  resource: alert_policy
  requirement: Logging Metric Filter Console Root Login Detected Fi Present
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Alert on Console Root Login Attempts Detected
  rationale: Detecting root login attempts via the console is crucial because it may indicate unauthorized access attempts or insider threats. Such activities could lead to data breaches or system compromise, especially if the root account is misused. Monitoring these attempts helps meet compliance with regulatory standards like PCI-DSS and HIPAA, which require the logging and monitoring of access to sensitive systems.
  description: This rule checks for the presence of a logging metric filter that detects root login attempts via the GCP console. The configuration should include setting up an alert policy that triggers notifications when such events are logged, ensuring timely response to potential security incidents. Verification involves confirming that the alert policy is active and configured correctly, and remediation includes creating or updating the policy to cover all relevant projects and accounts.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance/pci-dss
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/hipaa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
- rule_id: gcp.monitoring.alert_policy.logging_metric_filter_iam_policy_change_detected_fil_present
  service: monitoring
  resource: alert_policy
  requirement: Logging Metric Filter IAM Policy Change Detected Fil Present
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Alert on IAM Policy Changes via Logging Metric Filter
  rationale: Monitoring IAM policy changes helps detect unauthorized access modifications, which can lead to privilege escalations or data breaches. This practice is crucial for maintaining compliance with regulatory requirements like PCI-DSS and ensuring data integrity and security within your GCP environment.
  description: This rule checks for the presence of a logging metric filter that triggers alerts on IAM policy changes. Ensure that the alert policy is correctly configured to detect and notify on any changes to IAM policies. Remediation involves setting up a metric filter in Cloud Logging that captures policy change events and configuring an alert policy to notify security teams promptly.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.logging_metric_filter_kms_key_deletion_or_disable_de_present
  service: monitoring
  resource: alert_policy
  requirement: Logging Metric Filter KMS Key Deletion Or Disable De Present
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: high
  title: Alert on KMS Key Deletion or Disablement Events
  rationale: Detecting KMS key deletion or disablement is critical as these actions can render encrypted data inaccessible, leading to potential data loss and service disruption. Such alerts help mitigate risks associated with accidental or malicious actions that could violate data security policies and compliance requirements like PCI-DSS and ISO 27001.
  description: This rule checks for the presence of a logging metric filter and alert policy designed to capture and notify on events related to the deletion or disablement of KMS keys. Ensure that the necessary logging metrics are configured in Cloud Logging to track these specific actions, and establish an alerting policy in Cloud Monitoring to promptly inform security teams. Remediation involves setting up a log-based metric filter for 'kms.googleapis.com/keyDelete' and 'kms.googleapis.com/keyDisable' events, followed by configuring an alert policy to trigger notifications.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/kms/docs/algorithms
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.monitoring.alert_policy.logging_metric_filter_network_acl_or_sg_change_detec_present
  service: monitoring
  resource: alert_policy
  requirement: Logging Metric Filter Network ACL Or Sg Change Detec Present
  scope: monitoring.alert_policy.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Network ACL or SG Change Alert Policies are Configured
  rationale: Monitoring changes to Network ACLs or Security Groups is vital to maintaining network security posture, as unauthorized modifications can lead to exposure of sensitive data or unauthorized access. This helps organizations quickly detect and respond to potential security incidents, aligning with compliance requirements such as PCI DSS and ensuring protection against insider threats.
  description: This rule verifies that alert policies are configured to detect changes in Network ACLs or Security Groups using logging metric filters. It ensures that any modification to these critical network configurations triggers an alert, facilitating prompt investigation and remediation. To implement, create a logging metric filter for changes in network configurations and associate it with an alert policy in GCP Monitoring. Remediate by establishing a consistent process for reviewing and updating alert policies to cover all network configuration changes.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.monitoring.alert_policy.memory_utilization_alert_configured
  service: monitoring
  resource: alert_policy
  requirement: Memory Utilization Alert Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Memory Utilization Alert is Configured in GCP
  rationale: Configuring memory utilization alerts is critical for maintaining optimal performance and preventing service disruptions. High memory usage can lead to application crashes or degraded performance, impacting business operations and customer satisfaction. Proactive alerting helps in early detection of potential issues, ensuring compliance with operational best practices and reducing the risk of downtime.
  description: This rule checks whether an alert policy for memory utilization is configured in Google Cloud Monitoring. It ensures that alerts are set to trigger when memory usage exceeds a specified threshold, enabling timely intervention. Verify by checking the alert policies in the GCP Console under Monitoring, ensuring thresholds align with operational needs. Remediate by creating or updating policies to include memory utilization alerts with appropriate notification channels.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/docs/security
- rule_id: gcp.monitoring.alert_policy.metric_filter_policy_changes_gcp_iam_password_policy__unused
  service: monitoring
  resource: alert_policy
  requirement: Metric Filter Policy Changes Gcp IAM Password Policy Unused
  scope: monitoring.alert_policy.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Monitor IAM Password Policy Changes via Metric Filters
  rationale: Monitoring changes to IAM password policies is crucial for identifying unauthorized or accidental modifications that could weaken account security, leading to potential unauthorized access. It also helps organizations comply with regulatory requirements by ensuring consistent enforcement of password strength policies.
  description: This rule checks for the presence and configuration of metric filters that monitor changes to the IAM password policy. Specifically, it ensures that any modifications are logged and trigger alerts, allowing for timely response to potential security incidents. To verify, ensure that an alert policy is configured to trigger when Cloud Logging detects changes to the IAM password policy. Remediation involves setting up or updating alert policies in Google Cloud Monitoring to include these metric filters.
  references:
  - https://cloud.google.com/iam/docs/auditing
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/monitoring/alerts
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.route_tables_alarm_configured
  service: monitoring
  resource: alert_policy
  requirement: Route Tables Alarm Configured
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Route Tables Alert Policies are Configured in GCP Monitoring
  rationale: Configuring alert policies for route tables is crucial for detecting unauthorized changes that could expose internal networks to unintended routes, potentially leading to data breaches or service disruptions. Proactive monitoring and alerting help organizations respond quickly to misconfigurations and align with compliance requirements such as PCI-DSS and NIST SP 800-53 that mandate network security monitoring.
  description: This rule checks whether alert policies are configured for route table changes in GCP Monitoring. It involves setting up alerts for any changes to route tables to ensure immediate notification of potential security incidents. To verify, ensure that alert policies include conditions that monitor changes to the 'routes' resource type. Remediation involves creating an alert policy with conditions that capture route table modifications and setting appropriate notification channels.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.monitoring.alert_policy.sampling_rule_access_rbac_least_privilege
  service: monitoring
  resource: alert_policy
  requirement: Sampling Rule Access RBAC Least Privilege
  scope: monitoring.alert_policy.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Alert Policy Sampling Rule Access
  rationale: Implementing least privilege for alert policy sampling rules minimizes the risk of unauthorized data access and potential data breaches. By restricting permissions to only necessary users or roles, organizations can reduce attack surfaces and improve compliance with security standards such as NIST and ISO 27001. This is crucial for maintaining the integrity and confidentiality of sensitive monitoring data.
  description: This rule checks that IAM roles granted to users or service accounts for accessing alert policy sampling rules are configured with the principle of least privilege. Verify that only necessary permissions are assigned by auditing IAM policies and removing excessive access rights. Use the 'gcloud' command-line tool or GCP Console to inspect and modify IAM roles associated with alert policies. Remediation involves adjusting IAM policy bindings to align with security best practices, ensuring users have only the permissions required for their roles.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/monitoring/alerts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.monitoring.alert_policy.sampling_rule_rules_present
  service: monitoring
  resource: alert_policy
  requirement: Sampling Rule Rules Present
  scope: monitoring.alert_policy.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Sampling Rules are Configured for Alert Policies
  rationale: Sampling rules in alert policies help manage alert noise by allowing finer control over alert conditions and frequency. This is crucial for maintaining effective monitoring and ensuring critical alerts are not overlooked due to alert fatigue. Proper configuration supports compliance with regulations requiring effective monitoring and logging, such as PCI-DSS and SOC2.
  description: This check verifies that sampling rules are configured for GCP alert policies to control the rate at which notifications are sent. Without these rules, you may receive excessive alerts, leading to potential important alerts being ignored. To verify, review the alert policy configurations in Google Cloud Console under Monitoring and ensure sampling settings are correctly applied. Remediation involves setting appropriate thresholds and conditions for each alert policy to manage alert volume effectively.
  references:
  - https://cloud.google.com/monitoring/alerts/concepts-indepth
  - https://cloud.google.com/monitoring/alerts
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.aicpa.org/interestareas/frc/assuranceadvisoryservices/sorhome.html
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.alert_policy.sampling_rule_storage_encrypted
  service: monitoring
  resource: alert_policy
  requirement: Sampling Rule Storage Encrypted
  scope: monitoring.alert_policy.encryption
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Alert Policy Storage Encryption for Sampling Rules
  rationale: Encrypting storage for sampling rules in alert policies is crucial to protect sensitive monitoring data from unauthorized access and potential data breaches. This measure reduces the risk of exposing information that could be exploited by attackers, thereby safeguarding the organization's operational integrity and compliance with data protection regulations.
  description: This rule checks if the storage for sampling rules in GCP alert policies is encrypted. To verify, ensure that the storage associated with alert policies utilizes Google's default encryption or customer-managed encryption keys (CMEK). Remediation involves enabling encryption for any unencrypted storage of alert policies via the GCP Console or gcloud command-line tool, reinforcing data protection measures.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-144.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.dashboard.public_embeds_disabled
  service: monitoring
  resource: dashboard
  requirement: Public Embeds Disabled
  scope: monitoring.dashboard.public_access
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Disable Public Embeds for GCP Monitoring Dashboards
  rationale: Enabling public embeds on GCP Monitoring Dashboards can lead to unauthorized access to sensitive operational data, posing a significant security risk. This exposure can lead to data breaches and compliance violations under standards like PCI-DSS and HIPAA, which require strict access controls. Ensuring dashboards are not publicly accessible helps mitigate these threats and protects critical business information.
  description: This rule checks that GCP Monitoring Dashboards do not have public embeds enabled, preventing unauthorized public access. To verify, review the dashboard settings in the GCP Console and ensure 'Public access' is disabled. Remediation involves navigating to the dashboard's sharing settings and ensuring only authorized users and groups have access, removing any public links. Regular audits should be conducted to maintain compliance and security posture.
  references:
  - https://cloud.google.com/monitoring/dashboards
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/monitoring/access-control
- rule_id: gcp.monitoring.dashboard.sharing_restricted_to_org
  service: monitoring
  resource: dashboard
  requirement: Sharing Restricted To Org
  scope: monitoring.dashboard.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Restrict Monitoring Dashboard Sharing to Organization
  rationale: Restricting dashboard sharing to within the organization minimizes the risk of exposing sensitive operational data to unauthorized external entities. This helps prevent potential data breaches that could result from unauthorized access, ensuring compliance with internal data governance policies and regulatory requirements. It also safeguards against inadvertent data exposure that could undermine business operations and competitive advantage.
  description: This check verifies that Google Cloud Monitoring dashboards are only shared within the organization's domain. To verify, inspect the IAM policies associated with each dashboard for permissions granted outside the organization. Remediation involves adjusting the sharing settings in the Cloud Console to limit access to organizational users only, ensuring that no external accounts have been granted view or edit permissions.
  references:
  - https://cloud.google.com/monitoring/dashboards/overview
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/conditions-overview
- rule_id: gcp.monitoring.dashboard.sso_required
  service: monitoring
  resource: dashboard
  requirement: Sso Required
  scope: monitoring.dashboard.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enforce SSO for GCP Monitoring Dashboards
  rationale: Requiring SSO for GCP Monitoring Dashboards mitigates unauthorized access risks, ensuring that only authenticated and authorized users can view or modify sensitive monitoring data. This aligns with best practices for identity management, reducing potential insider threats and enhancing data protection. Compliance with regulatory frameworks often necessitates robust access controls, making SSO a critical component of a secure cloud posture.
  description: This rule checks if Single Sign-On (SSO) is enforced for accessing GCP Monitoring Dashboards. To verify, ensure that your organization's identity provider is configured to manage access to these dashboards, integrating with GCP's IAM policies. Remediation involves configuring identity federation and enforcing SSO via the Google Admin console, ensuring all dashboard access is routed through the organization's SSO mechanism.
  references:
  - https://cloud.google.com/monitoring/docs/access-control
  - https://cloud.google.com/identity-platform/docs/how-to-enable-single-sign-on
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/itl/applied-cybersecurity/nist-cybersecurity-framework
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.group.kms_encryption_enabled
  service: monitoring
  resource: group
  requirement: KMS Encryption Enabled
  scope: monitoring.group.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Monitoring Groups
  rationale: Enabling KMS encryption for monitoring groups is crucial to protect sensitive monitoring data from unauthorized access and potential data breaches. This encryption helps meet regulatory compliance requirements such as GDPR and HIPAA, and mitigates risks associated with data interception in transit or at rest, safeguarding business-critical information and maintaining customer trust.
  description: This rule verifies that all monitoring groups in GCP have Cloud KMS encryption enabled to protect data at rest. To ensure compliance, check that each monitoring group's configuration specifies a KMS key. Remediation involves updating the configuration to include a suitable KMS key, which can be managed through the GCP Console or via gcloud CLI commands. Ensure the key is regularly rotated and access controls are appropriately configured.
  references:
  - https://cloud.google.com/monitoring/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance
- rule_id: gcp.monitoring.group.retention
  service: monitoring
  resource: group
  requirement: Retention
  scope: monitoring.group.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Monitoring Group Data Retention Policy is Configured
  rationale: Proper retention of monitoring group data is crucial for historical analysis, compliance, and forensic investigations. Insufficient retention can lead to data loss, hindering the ability to conduct long-term trend analysis and identify security incidents. It also impacts compliance with regulations that require retaining logs for specific periods.
  description: This check verifies that a data retention policy is configured for GCP Monitoring groups to ensure logs and metrics are retained as per organizational and regulatory requirements. Administrators should configure retention policies via the GCP Console or API to specify the duration for which monitoring data should be stored. Remediation involves adjusting the retention settings to meet or exceed the minimum required by applicable compliance standards.
  references:
  - https://cloud.google.com/monitoring/docs
  - https://cloud.google.com/monitoring/settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library
- rule_id: gcp.monitoring.group.retention_policy_specific_days_enabled
  service: monitoring
  resource: group
  requirement: Retention Policy Specific Days Enabled
  scope: monitoring.group.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Enable Specific Day Retention Policy for Monitoring Groups
  rationale: Configuring specific day retention policies for monitoring groups is crucial for managing log data efficiently and ensuring that critical information is retained as per business and compliance needs. This helps mitigate risks associated with data loss and ensures that logs are available for forensic analysis, regulatory audits, and operational troubleshooting.
  description: This rule verifies that a specific day retention policy is enabled for monitoring groups in GCP, which allows organizations to define how long logs should be retained. To ensure compliance and operational efficiency, configure the retention settings under Monitoring > Settings > Retention Policies. If specific day retention is not enabled, logs may either be retained indefinitely, incurring unnecessary costs, or purged too early, leading to potential data loss. Remediation involves setting a predefined number of retention days that align with organizational policy.
  references:
  - https://cloud.google.com/monitoring/settings#retention
  - 'CIS GCP Benchmark: Ensure that Cloud Monitoring log retention periods are set to meet organizational compliance requirements'
  - 'NIST SP 800-53: AU-11 Audit Record Retention'
  - ISO/IEC 27001:2013 A.12.4.3 Protection of Log Information
- rule_id: gcp.monitoring.notification_channel.dr_communication_channels_configured
  service: monitoring
  resource: notification_channel
  requirement: DR Communication Channels Configured
  scope: monitoring.notification_channel.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure DR Notification Channels Are Configured
  rationale: Properly configured Disaster Recovery (DR) communication channels ensure that key stakeholders are promptly informed during incidents, reducing downtime and minimizing financial impacts. Without these channels, critical alerts may be missed, leading to prolonged service interruptions and potential data loss, which can harm business operations and reputation. Compliance with regulations such as ISO 27001 requires effective communication plans for incident response.
  description: This rule checks whether notification channels are configured for disaster recovery scenarios within GCP's monitoring services. It verifies that channels such as email, SMS, or webhooks are set up to alert the right personnel. To ensure these are properly configured, review the notification channels in the Google Cloud Console under Monitoring > Alerting > Notification Channels, and ensure contact information is accurate and up to date. Remediation involves setting up necessary channels and testing their functionality.
  references:
  - https://cloud.google.com/monitoring/alerts/using-channels-api
  - https://cloud.google.com/monitoring/support/notification-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-34r1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.monitoring.notification_channel.dr_communication_destinations_access_least_privilege
  service: monitoring
  resource: notification_channel
  requirement: DR Communication Destinations Access Least Privilege
  scope: monitoring.notification_channel.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege on DR Notification Channels
  rationale: Implementing least privilege on disaster recovery (DR) communication channels minimizes the risk of unauthorized access and potential data breaches. Misconfigured access can lead to unauthorized users modifying or receiving alert data, compromising incident response and compliance with standards such as PCI-DSS and ISO 27001.
  description: This rule checks for overly permissive IAM roles assigned to notification channels used for DR communications in GCP Monitoring. Verify that only necessary roles, such as 'roles/monitoring.notificationChannelEditor', are granted. Remediate by reviewing IAM policies and ensuring that only essential users or service accounts have access, removing any broad permissions.
  references:
  - https://cloud.google.com/monitoring/alerts/using-channels-api
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/iam-best-practices
- rule_id: gcp.monitoring.notification_channel.dr_communication_no_public_webhooks_without_auth
  service: monitoring
  resource: notification_channel
  requirement: DR Communication No Public Webhooks Without Auth
  scope: monitoring.notification_channel.public_access
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Disable Public Webhooks Without Auth for Notification Channels
  rationale: Allowing public webhooks without authentication can expose your monitoring alerts to unauthorized actors, leading to potential data leakage or malicious alert manipulation. This practice can result in compliance violations with frameworks like NIST and ISO 27001, and increases the risk of security breaches that could compromise business operations.
  description: This rule checks that all Google Cloud Monitoring notification channels using webhooks are configured to require authentication if they are publicly accessible. To verify, review the webhook URLs in the Monitoring notification settings and ensure they are not publicly accessible without authentication. Remediation involves updating webhook configurations to include authentication or restricting access to trusted IPs only.
  references:
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.nist.gov/document-162
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.monitoring.notification_channel.incident_escalation_change_audit_logging_enabled
  service: monitoring
  resource: notification_channel
  requirement: Incident Escalation Change Audit Logging Enabled
  scope: monitoring.notification_channel.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for Incident Escalation Changes
  rationale: Enabling audit logging for incident escalation changes in GCP ensures that all modifications to notification channels are tracked, providing visibility into potential unauthorized changes. This is crucial for maintaining the integrity of incident management processes and supports compliance with regulations that require detailed auditing of security-related activities.
  description: This rule checks if audit logging is enabled for changes to incident escalation configurations in GCP notification channels. To verify, ensure that the appropriate logging options are configured in the Google Cloud Console under the Monitoring section. If audit logging is not enabled, set up audit logs for 'admin read', 'admin write', and 'data write' activities to capture all relevant changes. This helps in monitoring and responding to unauthorized access or changes.
  references:
  - https://cloud.google.com/logging/docs/audit/configure-data-access
  - https://cloud.google.com/monitoring/support/notification-options
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
  - https://cloud.google.com/iam/docs/audit-logging
- rule_id: gcp.monitoring.notification_channel.incident_escalation_oncall_contacts_verified
  service: monitoring
  resource: notification_channel
  requirement: Incident Escalation Oncall Contacts Verified
  scope: monitoring.notification_channel.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Verify Oncall Contacts for Incident Escalation
  rationale: Ensuring that oncall contacts in notification channels are verified is crucial to maintaining effective incident response. Unverified or incorrect contact information can lead to missed alerts or delayed responses, increasing the risk of prolonged service disruptions and potential breaches. This practice supports compliance with industry regulations that mandate timely incident responses.
  description: This rule checks that all oncall contacts listed in GCP Monitoring notification channels are verified to ensure reliable incident escalation. Verification includes confirming that the contact details are current and that recipients can be reached promptly during incidents. To remediate, review notification channel settings in the GCP Console, update any outdated contact information, and ensure that all contacts are verified through the platform's verification process.
  references:
  - https://cloud.google.com/monitoring/alerts/using-notification-channels
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/blog/products/management-tools/alerts-and-notifications-in-google-cloud
  - https://cloud.google.com/security
- rule_id: gcp.monitoring.notification_channel.incident_escalation_policy_exists_for_critical_severity
  service: monitoring
  resource: notification_channel
  requirement: Incident Escalation Policy Exists For Critical Severity
  scope: monitoring.notification_channel.policy_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Critical Incident Escalation Policy in GCP Monitoring
  rationale: Having an incident escalation policy for critical severity incidents ensures that critical issues are promptly addressed, reducing potential downtime and financial loss. It minimizes the risk of prolonged exposure to security threats and helps maintain compliance with industry standards that require timely incident response.
  description: This rule checks if an escalation policy is configured for notification channels handling critical severity incidents in GCP Monitoring. Without a defined policy, critical alerts may not reach the right personnel for timely resolution. Verify that each critical alert notification channel has an associated escalation policy by reviewing the settings in the Google Cloud Console under Monitoring > Alerting > Notification Channels. Remediate by configuring an escalation policy that includes multiple contacts and specifies appropriate response actions.
  references:
  - https://cloud.google.com/monitoring/alerts/concepts-indepth
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-61r1.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.monitoring.notification_channel.incident_notification_destinations_configured
  service: monitoring
  resource: notification_channel
  requirement: Incident Notification Destinations Configured
  scope: monitoring.notification_channel.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Notification Channels for Incident Alerts Are Configured
  rationale: Configuring incident notification channels in GCP Monitoring is essential to promptly alert relevant stakeholders about potential security incidents, ensuring quick response and mitigation. Without proper notification mechanisms, critical alerts might be missed, leading to prolonged exposure to risks and potential non-compliance with standards like PCI-DSS and ISO 27001.
  description: This rule verifies that all Google Cloud Monitoring notification channels are appropriately configured to send incident alerts. Check if channels such as email, SMS, or webhooks are set up to notify the right personnel or systems about incidents. To remediate, navigate to the Google Cloud Console, access Monitoring, and ensure that notification channels are configured and tested for all relevant alerting policies.
  references:
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/monitoring/alerts
- rule_id: gcp.monitoring.notification_channel.incident_notification_endpoints_authenticated
  service: monitoring
  resource: notification_channel
  requirement: Incident Notification Endpoints Authenticated
  scope: monitoring.notification_channel.security
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Notification Endpoints are Authenticated
  rationale: Authenticating notification endpoints is crucial to prevent unauthorized access to sensitive incident data, which could lead to data breaches or compromised system integrity. Proper authentication ensures that notifications are only sent to verified recipients, protecting against malicious actors intercepting or tampering with incident alerts. This is vital for maintaining trust and compliance with data protection regulations.
  description: This rule checks that all incident notification channels in GCP are configured with authentication mechanisms such as OAuth tokens or API keys. To verify, inspect the notification channel settings in the Google Cloud Console or use the gcloud CLI to list channels and check their authentication configurations. Remediation involves updating channels to include appropriate authentication details, ensuring only authorized endpoints receive incident alerts.
  references:
  - https://cloud.google.com/monitoring/support/notification-options
  - https://cloud.google.com/monitoring/alerts/using-channels-api
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/authentication
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.monitoring.notification_channel.incident_notification_message_encryption_in_transit
  service: monitoring
  resource: notification_channel
  requirement: Incident Notification Message Encryption In Transit
  scope: monitoring.notification_channel.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption of Incident Notifications in Transit
  rationale: Encrypting incident notifications in transit protects sensitive data from interception by unauthorized entities, reducing the risk of data breaches and ensuring compliance with data protection regulations such as GDPR and HIPAA. Unencrypted notifications could expose critical system vulnerabilities or sensitive information, posing significant security and privacy threats.
  description: This rule checks whether notifications sent from Google Cloud Monitoring are encrypted during transmission to ensure data integrity and confidentiality. To verify, ensure that all notification channels support TLS encryption. If a channel does not support encryption, configure it to use a secure transport mechanism or switch to a service that does. Remediation involves updating your notification channels or consulting GCP documentation to enable TLS for existing channels.
  references:
  - https://cloud.google.com/monitoring/alerts/using-channels-documentation
  - https://cloud.google.com/security/compliance/cis-gcp-benchmarks
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-in-transit
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.monitoring.notification_channel.incident_notification_no_public_webhooks
  service: monitoring
  resource: notification_channel
  requirement: Incident Notification No Public Webhooks
  scope: monitoring.notification_channel.public_access
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: critical
  title: Prevent Public Webhooks for Incident Notifications
  rationale: Public webhooks for incident notifications can expose sensitive information to unauthorized users, leading to data breaches or service disruptions. This poses significant business risks, including loss of customer trust and potential financial penalties. Ensuring incident notifications are sent to secure, authenticated endpoints aligns with compliance standards such as NIST SP 800-53 and ISO 27001.
  description: This rule checks if any notification channels are configured with public webhooks for incident notifications in Google Cloud Monitoring. Publicly accessible webhooks can be exploited by attackers to intercept or manipulate incident data. To remediate, configure notification channels to use private, authenticated endpoints or use other secure methods like email or SMS. Verify by reviewing notification channel configurations in the Google Cloud Console or via the gcloud CLI.
  references:
  - https://cloud.google.com/monitoring/alerts/using-channels-api
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.multi.region.region_enabled
  service: multi
  resource: region
  requirement: Region Enabled
  scope: multi.region.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Critical Regions Are Enabled for Multi-Region Resources
  rationale: Enabling specific regions for multi-region resources is crucial to ensure compliance with data residency requirements and to optimize latency for users in various geographic locations. Failing to configure regions properly could lead to data breaches, increased latency, and non-compliance with international data protection regulations such as GDPR.
  description: This rule checks whether critical geographical regions are enabled for multi-region resources in GCP. To verify, review the region settings in the Google Cloud Console under the specific resource settings. Ensure that the regions where data is processed or stored comply with business requirements and legal obligations. Remediation involves adding or enabling the necessary regions through the resource configuration settings.
  references:
  - https://cloud.google.com/compute/docs/regions-zones
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/best-practices
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.notebooks.instance.data_governance_ai_notebook_encrypted
  service: notebooks
  resource: instance
  requirement: Data Governance Ai Notebook Encrypted
  scope: notebooks.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Notebook Instances are Encrypted at Rest
  rationale: Encrypting AI Notebook instances at rest mitigates risks of unauthorized data access, ensuring sensitive information is protected against potential breaches. This aligns with regulatory requirements such as GDPR and HIPAA, which mandate encryption to protect personal and health-related data. Encryption at rest also reduces the impact of data exposure in scenarios involving physical theft or unauthorized access to underlying storage infrastructure.
  description: This rule checks if AI Notebook instances in Google Cloud are configured to use Customer-Managed Encryption Keys (CMEK) or Google-managed encryption for data stored at rest. To verify, ensure the 'encryptionConfig.kmsKeyName' field is set in the instance configuration. Remediation involves updating the instance settings to specify a valid CMEK or confirming that Google-managed encryption is enabled by default. Encrypting data at rest protects against unauthorized access and is a vital step in data governance.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/security
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/cmek
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.notebooks.instance.data_governance_ai_notebook_execution_role_least_privilege
  service: notebooks
  resource: instance
  requirement: Data Governance Ai Notebook Execution Role Least Privilege
  scope: notebooks.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for AI Notebook Execution Roles
  rationale: Applying the principle of least privilege to AI Notebook execution roles reduces the attack surface by ensuring users and services have only the permissions necessary for their tasks. This minimizes potential misuse or exploitation of elevated privileges, protecting sensitive data and meeting compliance requirements like PCI-DSS and NIST which mandate stringent access controls.
  description: This rule checks whether AI Notebook instances in GCP have execution roles configured with only the necessary permissions to perform their tasks. To verify, audit the IAM policies associated with Notebook instances and ensure they do not exceed required permissions. Remediation involves adjusting IAM roles to eliminate excessive permissions, possibly using predefined roles tailored for specific tasks.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/using-iam-securely
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/ai-platform/notebooks/docs/security
- rule_id: gcp.notebooks.instance.data_governance_ai_notebook_idle_shutdown_configured
  service: notebooks
  resource: instance
  requirement: Data Governance Ai Notebook Idle Shutdown Configured
  scope: notebooks.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Idle Shutdown is Configured for AI Notebooks
  rationale: Configuring idle shutdown for AI notebooks helps prevent unauthorized access and potential data breaches by automatically shutting down instances when not in use. This practice not only reduces unnecessary costs but also aligns with data protection regulations by minimizing exposure time of sensitive data, thereby mitigating the risk of data exfiltration and ensuring compliance with privacy standards.
  description: This rule checks whether AI Notebook instances in Google Cloud have idle shutdown configured. Idle shutdown automatically powers down the notebook after a specified period of inactivity, reducing attack surfaces and operational costs. To verify, ensure that notebook instances have the 'idle_shutdown' and 'idle_shutdown_time' parameters set in their metadata. Remediation involves configuring these parameters via the Google Cloud Console or gcloud command-line tool to enforce the shutdown policy.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/configure-instance-settings
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 7.5
  - 'NIST SP 800-53 Rev. 5: AC-11'
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.notebooks.instance.data_governance_ai_notebook_no_public_access
  service: notebooks
  resource: instance
  requirement: Data Governance Ai Notebook No Public Access
  scope: notebooks.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Access to AI Notebook Instances
  rationale: Public access to AI Notebook instances can expose sensitive data and computational resources to unauthorized users, increasing the risk of data breaches and unauthorized access. This can lead to significant financial loss, reputational damage, and non-compliance with data protection regulations such as GDPR and HIPAA.
  description: This rule checks that AI Notebook instances on GCP are not configured to allow public access. Public IP addresses should be avoided, and instead, private IP addresses within a VPC should be used. Verify the instance's network configuration and remove any public IP assignments. Implement Identity and Access Management (IAM) controls to restrict access to authorized users only.
  references:
  - https://cloud.google.com/notebooks/docs/security
  - https://cloud.google.com/security/compliance/cis#section-5
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/vpc/docs/vpc
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.notebooks.instance.data_privacy_ai_notebook_encrypted
  service: notebooks
  resource: instance
  requirement: Data Privacy Ai Notebook Encrypted
  scope: notebooks.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure AI Notebook Instances Use Encryption at Rest
  rationale: Encrypting data at rest protects against unauthorized access and data breaches, particularly in cases of physical theft or unauthorized access to underlying storage systems. This is crucial for maintaining data confidentiality and integrity, especially when handling sensitive or personally identifiable information (PII), and is often a regulatory requirement under frameworks like GDPR, HIPAA, and PCI-DSS.
  description: This rule checks whether AI Notebook instances in GCP have encryption at rest enabled. To verify, ensure that each notebook instance is configured to use customer-managed encryption keys (CMEK) or Google-managed keys. Remediation involves updating the notebook configuration to specify the use of encryption keys, which can be managed via the GCP Console or gcloud CLI. This helps to protect stored data and meet compliance requirements.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/security/compliance/cis-gcp-1-0-0
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.notebooks.instance.data_privacy_ai_notebook_execution_role_least_privilege
  service: notebooks
  resource: instance
  requirement: Data Privacy Ai Notebook Execution Role Least Privilege
  scope: notebooks.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure AI Notebook Roles Have Least Privilege
  rationale: Assigning excessive permissions to AI Notebook execution roles can result in unauthorized access to sensitive data, increasing the risk of data breaches. Least privilege principles reduce the attack surface and mitigate insider threats, aligning with compliance requirements such as HIPAA and GDPR.
  description: This rule checks if AI Notebook instances are assigned roles with only the necessary permissions for their intended function. Verify that each notebook's execution role does not include permissions beyond what is needed for specific tasks. To remediate, audit current permissions, remove unnecessary roles, and apply the principle of least privilege by using custom roles tailored to the notebook's functions.
  references:
  - https://cloud.google.com/notebooks/docs/security
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/resource-manager/docs/organization-policy/defining-policies
- rule_id: gcp.notebooks.instance.data_privacy_ai_notebook_idle_shutdown_configured
  service: notebooks
  resource: instance
  requirement: Data Privacy Ai Notebook Idle Shutdown Configured
  scope: notebooks.instance.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Ai Notebook Idle Shutdown is Configured
  rationale: Configuring idle shutdown for AI Notebooks minimizes the risk of unauthorized access and potential data leaks by ensuring that notebooks do not remain active unnecessarily. This reduces the attack surface and helps control costs by preventing unintended resource usage. It aligns with data privacy standards by ensuring data is not kept active longer than needed.
  description: This rule checks that AI Notebooks have idle shutdown configured to prevent them from staying active when not in use. It requires setting a timeout interval after which the notebook will automatically shut down if idle, enhancing data privacy and cost efficiency. To verify, review the notebook instance settings to ensure the idle shutdown feature is enabled and set to an appropriate duration. Remediation involves accessing the AI Notebook instance settings and configuring the 'idle shutdown' option.
  references:
  - https://cloud.google.com/vertex-ai/docs/workbench/managing-notebooks#idle-shutdown
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.notebooks.instance.data_privacy_ai_notebook_no_public_access
  service: notebooks
  resource: instance
  requirement: Data Privacy Ai Notebook No Public Access
  scope: notebooks.instance.public_access
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: critical
  title: Ensure AI Notebooks Have No Public Access
  rationale: Publicly accessible AI Notebooks can expose sensitive data and intellectual property, leading to potential data breaches and unauthorized access. Ensuring no public access aligns with compliance requirements such as GDPR and HIPAA, which mandate strict data privacy controls and reduce the risk of data exfiltration and unauthorized data manipulation.
  description: This rule checks if AI Notebook instances are configured with public access. Instances should be restricted to private IPs or specific IP ranges to prevent exposure to the internet. Verify that no instances have public IPs assigned. To remediate, configure network settings to remove public IPs and apply firewall rules to restrict access. Use Identity and Access Management (IAM) policies to control user permissions.
  references:
  - https://cloud.google.com/ai-platform/notebooks/docs/security-best-practices
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0 - 6.2 Ensure that Cloud AI Platform Notebooks instances are not publicly accessible
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.notebooks.instance.machine_learning_notebook_encrypted
  service: notebooks
  resource: instance
  requirement: Machine Learning Notebook Encrypted
  scope: notebooks.instance.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure ML Notebooks are Encrypted at Rest
  rationale: Encrypting machine learning notebooks at rest is crucial to protect sensitive data from unauthorized access, particularly in scenarios involving data breaches or insider threats. This practice is essential to comply with various data protection regulations and standards, ensuring the confidentiality and integrity of data used in machine learning models.
  description: This rule checks whether machine learning notebooks on GCP are encrypted at rest using Customer-Managed Encryption Keys (CMEK) or Google-managed encryption. To verify, check the encryption configuration settings of each notebook instance in the Google Cloud Console. If not encrypted, enable encryption by selecting CMEK or Google-managed options in the instance settings. This ensures that data is protected using strong encryption standards.
  references:
  - https://cloud.google.com/notebooks/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.notebooks.instance.machine_learning_notebook_execution_role_least_privilege
  service: notebooks
  resource: instance
  requirement: Machine Learning Notebook Execution Role Least Privilege
  scope: notebooks.instance.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for ML Notebook Execution Roles
  rationale: Implementing least privilege for machine learning notebook execution roles minimizes the risk of unauthorized access and potential data breaches. This approach protects sensitive data and operations from being exposed if a notebook is compromised, ensuring compliance with regulatory standards such as PCI-DSS and HIPAA, which mandate strict access controls.
  description: This rule checks that machine learning notebook instances in GCP are assigned the minimum permissions necessary for execution. It verifies that roles attached to notebook instances do not exceed required permissions, thereby reducing attack surfaces. To remediate, review the IAM policies associated with notebook instances and modify roles to ensure only necessary permissions are granted. Use the principle of least privilege by assigning custom roles tailored to specific operational needs.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.notebooks.instance.machine_learning_notebook_idle_shutdown_configured
  service: notebooks
  resource: instance
  requirement: Machine Learning Notebook Idle Shutdown Configured
  scope: notebooks.instance.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Configure Idle Shutdown for ML Notebook Instances
  rationale: Unattended machine learning notebooks can lead to unnecessary costs and potential security risks if left running with sensitive data. Configuring idle shutdown minimizes resource wastage and mitigates the risk of unauthorized access to computational resources and data during idle periods. This practice aligns with cost management and security best practices, potentially aiding compliance with frameworks like ISO 27001 and NIST.
  description: This rule checks whether a Google Cloud Platform machine learning notebook instance has an idle shutdown configuration. Idle shutdown ensures that the instance automatically shuts down after a specified period of inactivity, reducing costs and limiting exposure risk. To verify, ensure the 'idleShutdown' and 'idleShutdownTimeout' settings are configured in the notebook's metadata. Remediation involves setting the `idleShutdown` to true and specifying an appropriate `idleShutdownTimeout` value in the instance settings.
  references:
  - https://cloud.google.com/vertex-ai/docs/workbench/managed/analyze
  - https://cloud.google.com/security/compliance/cis
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/billing/docs/how-to/manage-budgets
  - https://cloud.google.com/vertex-ai/docs/workbench/managed/protect-notebooks
  - https://cloud.google.com/architecture/best-practices-for-enterprise-organizations
- rule_id: gcp.notebooks.instance.machine_learning_notebook_no_public_access
  service: notebooks
  resource: instance
  requirement: Machine Learning Notebook No Public Access
  scope: notebooks.instance.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Restrict Public Access to ML Notebooks on GCP
  rationale: Allowing public access to Machine Learning Notebooks on GCP can expose sensitive data and intellectual property to unauthorized users, increasing the risk of data breaches and unauthorized use of resources. This can lead to financial losses, reputational damage, and non-compliance with regulatory frameworks such as GDPR and HIPAA, which mandate stringent data protection measures.
  description: This rule checks if any Machine Learning Notebook instances within GCP are accessible from the public internet without proper security controls. To mitigate risks, ensure that all notebook instances are configured with private IPs and access is restricted to authorized users only via Identity and Access Management (IAM) policies. Remediation involves modifying the instance's network configuration to remove external IP addresses and implementing VPC Service Controls.
  references:
  - https://cloud.google.com/notebooks/docs/security
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.osconfig.guest_policy.patch_compliance_configured
  service: osconfig
  resource: guest_policy
  requirement: Patch Compliance Configured
  scope: osconfig.guest_policy.compliance
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: medium
  title: Ensure Patch Compliance via OSConfig Guest Policies
  rationale: Configuring patch compliance for guest policies is critical to maintain system integrity and protect against vulnerabilities. Unpatched systems can be exploited by attackers, leading to data breaches, unauthorized access, and service disruptions. Ensuring patches are applied timely helps meet compliance requirements such as PCI-DSS and HIPAA, mitigating risks associated with outdated software.
  description: This rule checks whether patch compliance is configured using OSConfig guest policies in GCP. Specifically, it verifies that guest policies are set to manage patch deployment and compliance checks on virtual machines. To remediate, define and apply guest policies through the OSConfig service, ensuring that patch schedules and compliance settings are correctly configured to automatically update and report on patch status. Use the GCP Console or gcloud CLI to set these configurations.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/compute/docs/os-patch-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/sites/default/files/hipaa-simplification-201303.pdf
- rule_id: gcp.osconfig.patch_deployment.compliant_patching
  service: osconfig
  resource: patch_deployment
  requirement: Compliant Patching
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Deployment Compliance for OS Config
  rationale: Timely patch deployment is crucial to mitigate vulnerabilities that can be exploited by attackers, potentially leading to data breaches or service disruptions. Compliant patching ensures that systems are up-to-date with the latest security patches, reducing the risk of exploitation and aligning with compliance requirements such as PCI-DSS and HIPAA.
  description: This rule checks if patch deployments in GCP OS Config are configured to ensure that all VM instances receive necessary updates in a timely manner. It verifies the schedule and scope of patch deployments to ensure they are consistent with organizational security policies. Remediation involves configuring patch deployments appropriately, ensuring they are set to run at regular intervals and cover all critical and security patches. This can be done through the GCP Console or by using the gcloud command-line tool.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/compute/docs/osconfig/os-patch-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/document_library
  - https://www.hhs.gov/hipaa/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.osconfig.patch_deployment.config_managed_compliant_patching_configured
  service: osconfig
  resource: patch_deployment
  requirement: Config Managed Compliant Patching Configured
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Managed Compliant Patching is Configured
  rationale: Configuring managed compliant patching is crucial to maintain the security and integrity of systems by ensuring that all instances receive the latest security patches. Failure to implement managed patching can lead to vulnerabilities being exploited by attackers, resulting in data breaches and non-compliance with regulatory frameworks such as PCI-DSS and HIPAA.
  description: This rule checks whether managed compliant patching is configured for patch deployments in GCP's OS Config service. It requires that patch deployments are set up to automatically apply patches to instances, ensuring they remain up-to-date with security vulnerabilities addressed. Verification involves checking the patch deployment configuration settings in the Cloud Console or using gcloud commands. To remediate, configure a patch deployment with a managed compliant patching schedule in the OS Config section of the Google Cloud Console.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/compute/docs/osconfig-management#patch-management
- rule_id: gcp.osconfig.patch_deployment.managed_compliant_patching
  service: osconfig
  resource: patch_deployment
  requirement: Managed Compliant Patching
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Managed Compliant Patching for OS Configurations
  rationale: Automated patch management reduces the risk of vulnerabilities being exploited by ensuring systems are up-to-date with the latest security patches. Non-compliant patching can lead to security breaches, potentially resulting in data loss, service disruptions, and non-compliance with regulatory standards such as PCI-DSS and HIPAA.
  description: This rule verifies that all patch deployments are configured to manage and apply patches in compliance with organizational policies. It checks that patch deployments are scheduled, managed, and monitored using GCP OS Config to ensure timely updates. Remediation involves setting up and configuring patch deployments in the OS Config service, specifying schedules, and ensuring that all instances are enrolled in these deployments.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/compute/docs/os-patch-management
  - https://cloud.google.com/security/compliance/gcp-cis-benchmarks
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-171.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.osconfig.patch_deployment.managed_compliant_patching_gcp_gke_cluster_uses_a_su_enabled
  service: osconfig
  resource: patch_deployment
  requirement: Managed Compliant Patching Gcp Gke Cluster Uses A Su Enabled
  scope: osconfig.patch_deployment.patch_management
  domain: container_and_kubernetes_security
  subcategory: security_monitoring
  severity: medium
  title: Ensure GKE Cluster Patching Uses SU-Enabled Configuration
  rationale: Utilizing a 'su' enabled configuration for patch management in GKE clusters ensures that security updates and patches are applied with the necessary privileges, reducing the risk of vulnerabilities being exploited. This is critical for maintaining the security and integrity of workloads running on GKE, as unpatched systems can lead to unauthorized access, data breaches, and service disruptions. Ensuring compliant patch deployment aligns with regulatory requirements and helps protect organizational assets against emerging threats.
  description: This rule checks whether patch deployments for GKE clusters are configured with 'su' enabled, which is essential for applying patches with elevated privileges. To verify compliance, review the patch deployment configuration in the Google Cloud Console or using the gcloud command-line tool to ensure 'su' is enabled for all patch operations. Remediation involves updating the patch deployment configurations to include the necessary privilege settings, ensuring that all security updates can be applied without interruption.
  references:
  - https://cloud.google.com/kubernetes-engine/docs/how-to/patching
  - https://cloud.google.com/compute/docs/os-patch-management
  - CIS Google Cloud Platform Foundation Benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.osconfig.patch_deployment.patch_deployment_exists_configured
  service: osconfig
  resource: patch_deployment
  requirement: Patch Deployment Exists Configured
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Deployments are Configured for OS Management
  rationale: Properly configured patch deployments in GCP reduce the risk of vulnerabilities by automating the application of security patches across managed instances. This minimizes the attack surface and helps organizations maintain compliance with security standards such as PCI-DSS and NIST, thus protecting against potential data breaches and service disruptions.
  description: This rule checks whether patch deployments are configured in Google Cloud OS Config. Patch deployments should be scheduled to automatically apply updates and patches to managed instances, ensuring they remain secure and up-to-date. Verify that the patch deployment schedules are defined and active. Remediation involves configuring patch deployment settings within the OS Config service to automate patch management processes.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/blog/products/identity-security/best-practices-for-securing-your-gcp-environment
  - https://cloud.google.com/compute/docs/instances/os-patch-management
- rule_id: gcp.osconfig.patch_deployment.vuln_baseline_exemptions_have_expiry_and_owner
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Baseline Exemptions Have Expiry And Owner
  scope: osconfig.patch_deployment.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Patch Exemptions Have Expiry and Assigned Owner
  rationale: Exemptions without expiry or specified ownership can lead to unmanaged vulnerabilities, increasing the risk of security breaches. Lack of clear ownership may result in delayed response times during a security incident. Compliance with industry standards often requires defined accountability and timely review of security exceptions.
  description: This rule checks that vulnerability baseline exemptions in GCP's OS Config Patch Deployment have set expiration dates and designated owners. This ensures that exemptions are temporary and reviewed regularly, preventing long-term security gaps. To verify, review the patch deployment configurations for missing expiry dates and owner information. Remediation involves updating the exemption policies to include these critical details.
  references:
  - https://cloud.google.com/compute/docs/osconfig
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/blog/products/identity-security/how-to-configure-and-use-os-patch-management-on-gcp
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.osconfig.patch_deployment.vuln_baseline_required_controls_present
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Baseline Required Controls Present
  scope: osconfig.patch_deployment.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Vulnerability Baseline Controls in Patch Deployment
  rationale: Establishing a vulnerability baseline in patch deployments is crucial to mitigate risks associated with unpatched security vulnerabilities, which can lead to unauthorized access, data breaches, and service disruptions. By maintaining these controls, organizations can reduce the attack surface and adhere to industry compliance standards such as PCI-DSS and ISO 27001, ultimately protecting sensitive information and maintaining customer trust.
  description: This rule checks if patch deployment configurations in GCP OS Config include necessary vulnerability baseline controls. It verifies that patch deployments are set to address critical vulnerabilities and comply with organizational security policies. To ensure compliance, review and configure patch deployment settings to include a vulnerability baseline, ensuring timely application of security patches. Remediation involves adjusting patch deployment schedules and settings via the GCP console or CLI to align with baseline requirements.
  references:
  - https://cloud.google.com/compute/docs/os-config-management
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.osconfig.patch_deployment.vuln_maintenance_execution_roles_least_privilege
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Maintenance Execution Roles Least Privilege
  scope: osconfig.patch_deployment.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege Roles for Patch Deployment Execution
  rationale: Applying the principle of least privilege to roles associated with vulnerability maintenance execution minimizes the risk of unauthorized access or accidental changes to critical systems. Overly permissive roles can lead to potential data breaches or service disruptions, undermining business continuity and violating regulatory obligations such as PCI-DSS and HIPAA.
  description: This rule checks that roles assigned to osconfig patch deployment resources are configured with the least privilege necessary for their function. Review IAM policies to ensure only required permissions are granted. Remediation involves auditing current roles, removing unnecessary permissions, and adhering to predefined GCP IAM roles where possible. Validate configurations by using Cloud IAM to monitor and adjust roles as necessary.
  references:
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/iam/docs/audit-logging
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.osconfig.patch_deployment.vuln_maintenance_window_defined
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Maintenance Window Defined
  scope: osconfig.patch_deployment.security
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Define Vulnerability Maintenance Windows for Patch Deployments
  rationale: Defining maintenance windows for patch deployments helps minimize downtime and ensures critical vulnerabilities are addressed in a timely manner. This proactive approach reduces the risk of exploitation, maintaining system integrity and compliance with security standards.
  description: This rule checks if a maintenance window is defined for patch deployments in Google Cloud OS Config. Properly configured maintenance windows allow for controlled deployment of patches during non-peak hours, reducing the impact on business operations. To remediate, specify a maintenance window in the patch deployment configuration, ensuring it aligns with organizational policies for vulnerability management.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/compute/docs/instances/startup-scripts
  - https://cloud.google.com/security/compliance
- rule_id: gcp.osconfig.patch_deployment.vuln_patch_approval_rules_defined
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Patch Approval Rules Defined
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Define Vulnerability Patch Approval Rules in OS Config
  rationale: Defining vulnerability patch approval rules is crucial to ensure that only authorized and verified patches are applied to your VMs, reducing the risk of introducing vulnerabilities through unapproved updates. This helps maintain system integrity and availability, and aligns with compliance frameworks such as NIST and PCI-DSS by ensuring patches are vetted and controlled.
  description: This rule checks for the presence of defined vulnerability patch approval rules in OS Config patch deployments. Approval rules should specify criteria such as CVE severity thresholds, testing requirements, and authorized approvers. To verify, ensure that patch deployment configurations include these rules. Remediation involves updating patch deployment configurations in the GCP Console or using gcloud commands to include appropriate approval rules.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/compute/docs/osconfig-management/create-patch-deployments
  - 'CIS GCP Benchmark: Ensure that OS Config Patch Management is configured'
  - 'NIST SP 800-53 Rev. 5: Security and Privacy Controls for Information Systems'
  - 'PCI DSS Requirement 6: Develop and maintain secure systems and applications'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.osconfig.patch_deployment.vuln_patch_baseline_defined_for_families
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Patch Baseline Defined For Families
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Patch Baselines Include Vulnerability Patches for OS Families
  rationale: Defining a vulnerability patch baseline for operating system families ensures that instances receive critical and security updates in a timely manner, reducing the risk of exploitation from known vulnerabilities. This practice helps maintain the integrity and security of the workloads, minimizes potential downtime, and supports compliance with security frameworks such as NIST and PCI-DSS.
  description: This rule checks whether patch deployments in Google Cloud OS Config have defined vulnerability patch baselines for applicable operating system families. It requires configuring patch deployments to include vulnerability classifications, which can be verified by reviewing the patch deployment settings in the Google Cloud Console under the OS Config service. To remediate, ensure that patch deployments specify vulnerability patches and are scheduled regularly to cover all relevant OS families.
  references:
  - https://cloud.google.com/compute/docs/osconfig-management
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/compute/docs/patch-management
- rule_id: gcp.osconfig.patch_deployment.vuln_patch_maintenance_windows_configured
  service: osconfig
  resource: patch_deployment
  requirement: Vuln Patch Maintenance Windows Configured
  scope: osconfig.patch_deployment.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Vulnerability Patch Maintenance Windows Must Be Configured
  rationale: Configuring vulnerability patch maintenance windows is crucial for minimizing the exposure of systems to known vulnerabilities, thus reducing the risk of exploitation by attackers. Properly timed patching helps ensure service availability and compliance with industry standards such as PCI-DSS and SOC 2, which mandate regular updates for security reasons.
  description: This rule checks if patch deployments on GCP have maintenance windows configured to apply security patches. Maintenance windows should be scheduled to minimize disruption while ensuring timely patching of vulnerabilities. Administrators can verify this by checking the 'timeZone' and 'duration' fields in the patch deployment configuration. To remediate, define maintenance windows in the patch deployment settings using the GCP Console or gcloud CLI.
  references:
  - https://cloud.google.com/compute/docs/os-patch-management
  - https://cloud.google.com/architecture/security-foundations/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.pubsub.subscription.data_analytics_event_cross_account_sharing_restricted
  service: pubsub
  resource: subscription
  requirement: Data Analytics Event Cross Account Sharing Restricted
  scope: pubsub.subscription.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Sharing of Pub/Sub Subscriptions
  rationale: Restricting cross-account sharing of Pub/Sub subscriptions is crucial to prevent unauthorized access to sensitive data analytics events. This control helps mitigate risks such as data leakage and unauthorized data manipulation, ensuring compliance with data protection regulations like GDPR and CCPA. It also supports maintaining the integrity and confidentiality of your organization's data by limiting access to trusted accounts only.
  description: This rule checks whether Google Cloud Pub/Sub subscriptions, specifically those handling data analytics events, are shared across different GCP accounts. To ensure security, configure IAM policies to restrict access to trusted accounts only. Verify the IAM policy bindings for each subscription and remove any unintended cross-account permissions. Remediation involves auditing and updating IAM roles to align with the principle of least privilege.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.pubsub.subscription.data_analytics_event_destination_encrypted
  service: pubsub
  resource: subscription
  requirement: Data Analytics Event Destination Encrypted
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Subscriptions Use Encryption for Data Analytics Events
  rationale: Encrypting data analytics events in Pub/Sub subscriptions protects sensitive data from unauthorized access and potential breaches, ensuring compliance with data protection regulations like GDPR and CCPA. Failure to encrypt can lead to data exposure, financial loss, and reputational damage.
  description: This rule checks if Pub/Sub subscriptions for data analytics events are encrypted using Google-managed keys or customer-managed encryption keys (CMEK) at rest. To verify, ensure that the 'enableMessageOrdering' and 'kmsKeyName' fields are correctly configured in the subscription settings. Remediation involves updating the subscription to specify an appropriate KMS key for encryption.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.pubsub.subscription.data_analytics_event_destination_least_privilege
  service: pubsub
  resource: subscription
  requirement: Data Analytics Event Destination Least Privilege
  scope: pubsub.subscription.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Pub/Sub Subscription Uses Least Privilege for Data Analytics
  rationale: Minimizing permissions for Pub/Sub subscriptions used in data analytics can prevent unauthorized data access and help mitigate the risk of data exfiltration. Over-privileged access can lead to accidental data leaks or malicious activities, which may result in compliance violations with frameworks such as GDPR or HIPAA.
  description: This rule checks that Pub/Sub subscriptions configured as event destinations for data analytics use the least privilege principle by granting only necessary permissions. Verify that IAM roles attached to these subscriptions are limited and do not exceed what is necessary for their purpose. Remediation involves auditing current roles and permissions, then adjusting them to ensure they align with operational needs while minimizing excess access.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 4.2
  - NIST SP 800-53 Rev. 5, AC-6 Least Privilege
  - https://cloud.google.com/architecture/security-foundations
  - ISO/IEC 27001:2013 A.9.1.2 Access Control
- rule_id: gcp.pubsub.subscription.data_warehouse_event_cross_account_sharing_restricted
  service: pubsub
  resource: subscription
  requirement: Data Warehouse Event Cross Account Sharing Restricted
  scope: pubsub.subscription.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Sharing for Pub/Sub Data Warehouse Events
  rationale: Restricting cross-account sharing of Pub/Sub data warehouse events is crucial to prevent unauthorized access and data breaches. Unauthorized sharing can lead to exposure of sensitive data and violate compliance requirements such as GDPR and CCPA, which mandate stringent data protection measures. It also helps in maintaining data integrity and ensuring that only authorized accounts have access to critical data.
  description: This rule checks if Google Cloud Pub/Sub subscriptions for data warehouse events are configured to prevent cross-account sharing. Ensure that IAM policies do not grant access to external accounts unless explicitly required. Review and modify IAM roles and permissions to restrict access to only necessary accounts. Use the GCP Console or gcloud CLI to audit current permissions and apply least privilege principles to remediate any violations.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/pubsub/docs/admin
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.pubsub.subscription.data_warehouse_event_destination_encrypted
  service: pubsub
  resource: subscription
  requirement: Data Warehouse Event Destination Encrypted
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Subscription Data Warehouse Events Are Encrypted
  rationale: Encrypting data at rest for Pub/Sub subscriptions mitigates the risk of unauthorized access to sensitive data, protecting against potential data breaches and compliance violations. This is crucial for businesses dealing with sensitive or regulated data, such as financial transactions or personal information, which must adhere to various data protection laws and standards.
  description: This rule checks if Pub/Sub subscription configurations for data warehouse event destinations are encrypted using Google-managed keys. Verify that the Pub/Sub subscription utilizes encryption at rest by inspecting the subscription's encryption configuration. To remediate, enable encryption for the subscription by setting the appropriate IAM roles to allow key access and specifying the use of Google-managed keys or customer-managed encryption keys (CMEK) through the GCP Console or CLI.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Section 5.1
  - 'NIST SP 800-57: Recommendation for Key Management'
  - 'PCI-DSS Requirement 3: Protect stored cardholder data'
  - 'ISO/IEC 27001:2013 - A.10.1: Cryptographic controls'
- rule_id: gcp.pubsub.subscription.data_warehouse_event_destination_least_privilege
  service: pubsub
  resource: subscription
  requirement: Data Warehouse Event Destination Least Privilege
  scope: pubsub.subscription.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Data Warehouse Event Subscriptions
  rationale: Applying the principle of least privilege to Pub/Sub subscriptions used for data warehouse event ingestion minimizes the risk of unauthorized access. Inadequate permission settings can lead to data breaches, regulatory non-compliance, and potential financial losses. Ensuring least privilege aligns with compliance requirements such as PCI-DSS, HIPAA, and ISO 27001, reducing the attack surface.
  description: This rule checks that Pub/Sub subscriptions targeting data warehouses have permissions limited to necessary roles only. It verifies if the service account used by the subscription has minimal access rights, preventing unnecessary data exposure. To remediate, audit the IAM policies for these subscriptions and remove any excess privileges, ensuring compliance with security best practices. Implement strict role-based access controls to limit access to only what is necessary for the subscription's function.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.pubsub.subscription.streaming_consumer_access_least_privilege
  service: pubsub
  resource: subscription
  requirement: Streaming Consumer Access Least Privilege
  scope: pubsub.subscription.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Pub/Sub Subscription Streaming Access
  rationale: Implementing least privilege access for Pub/Sub subscriptions minimizes potential damage from compromised credentials by restricting permissions to only what is necessary. This reduces the attack surface and helps prevent unauthorized data access or service disruptions, aligning with compliance frameworks like PCI-DSS and ISO 27001 that mandate strict access controls to protect sensitive data.
  description: This check ensures that IAM roles associated with Pub/Sub subscription streaming consumers are configured with the least privilege necessary to perform their functions. Verify that only necessary roles, such as 'roles/pubsub.subscriber', are granted and avoid overly permissive roles like 'roles/editor' or 'roles/owner'. Remediation involves reviewing IAM policy bindings for each subscription and adjusting roles to adhere to the principle of least privilege, using tools like the GCP IAM Policy Analyzer for analysis.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
- rule_id: gcp.pubsub.subscription.streaming_consumer_auth_required
  service: pubsub
  resource: subscription
  requirement: Streaming Consumer Auth Required
  scope: pubsub.subscription.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Require Auth for Pub/Sub Streaming Consumers
  rationale: Enforcing authentication for streaming consumers in Pub/Sub ensures that only authorized applications can access your data streams, reducing the risk of data breaches. This is crucial for maintaining the confidentiality and integrity of sensitive data, as unauthorized access could lead to data leaks or malicious data manipulation. Additionally, it supports compliance with regulations by ensuring that data access is controlled and monitored.
  description: This rule checks whether streaming consumers accessing Pub/Sub subscriptions have authentication enabled. To verify, ensure that all consumers use OAuth 2.0 authentication when connecting to Pub/Sub. This can be achieved by configuring service accounts with the necessary IAM roles and ensuring that consumers authenticate using these service accounts. To remediate, update consumer applications to use OAuth 2.0 tokens generated from valid service accounts with appropriate permissions.
  references:
  - https://cloud.google.com/pubsub/docs/authentication
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.pubsub.subscription.streaming_destination_encrypted
  service: pubsub
  resource: subscription
  requirement: Streaming Destination Encrypted
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Subscription Streaming Destination is Encrypted
  rationale: Encrypting streaming destinations for Pub/Sub subscriptions protects sensitive data from unauthorized access during transit and at rest. This is crucial for maintaining data confidentiality and integrity, especially when dealing with personally identifiable information or sensitive business data. Regulatory standards such as GDPR, HIPAA, and PCI-DSS mandate encryption to safeguard data privacy and prevent data breaches.
  description: This rule verifies that all Pub/Sub subscription streaming destinations are encrypted using Google-managed or customer-managed encryption keys. To ensure compliance, check that the 'encryptionConfig' field is set with a valid KMS key name for each subscription. Remediation involves enabling encryption by specifying a Customer-Managed Encryption Key (CMEK) in your subscription configuration or verifying that Google-managed encryption is enabled. Use the Google Cloud Console or gcloud CLI to update subscription settings appropriately.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance/resource-center/compliance-certifications
  - https://cloud.google.com/kms/docs
- rule_id: gcp.pubsub.subscription.streaming_destination_least_privilege
  service: pubsub
  resource: subscription
  requirement: Streaming Destination Least Privilege
  scope: pubsub.subscription.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Pub/Sub Subscriptions Use Least Privilege for Streaming
  rationale: Implementing least privilege for Pub/Sub subscriptions minimizes the risk of unauthorized access and data exfiltration. It prevents over-permissioned roles from accessing sensitive data and reduces potential attack vectors. This aligns with regulatory compliance requirements such as PCI-DSS and NIST by enforcing strong access control measures.
  description: This rule checks that Pub/Sub subscription IAM policies are configured to grant only the necessary permissions for streaming operations. To verify, review the IAM roles assigned to subscriptions and ensure they do not exceed the required permissions such as 'Subscriber' role for data consumption. Remediation involves adjusting IAM policies to remove excess permissions and adhering to the principle of least privilege, ensuring roles are strictly limited to necessary actions.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.pubsub.subscription.streaming_encryption_at_rest_enabled
  service: pubsub
  resource: subscription
  requirement: Streaming Encryption At Rest Enabled
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Pub/Sub Subscription Encryption at Rest is Enabled
  rationale: Enabling streaming encryption at rest for Pub/Sub subscriptions protects sensitive data from unauthorized access and potential breaches. This is crucial for maintaining data integrity and confidentiality, reducing risks associated with data leaks, and ensuring compliance with regulatory frameworks such as GDPR and HIPAA.
  description: This rule checks whether Google Cloud Pub/Sub subscriptions have streaming encryption at rest enabled. To verify, ensure the 'kmsKeyName' field is specified in the subscription configuration, which indicates the use of a Customer Managed Encryption Key (CMEK). Remediation involves configuring the subscription to use CMEK by setting the 'kmsKeyName' during creation or updating existing subscriptions via the Google Cloud Console or CLI.
  references:
  - https://cloud.google.com/pubsub/docs/cmek
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
- rule_id: gcp.pubsub.subscription.streaming_logging_enabled
  service: pubsub
  resource: subscription
  requirement: Streaming Logging Enabled
  scope: pubsub.subscription.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Pub/Sub Subscription Streaming Logging is Enabled
  rationale: Enabling streaming logging for Pub/Sub subscriptions is crucial for maintaining a comprehensive audit trail of message operations. This enhances visibility into subscription activities, aiding in the detection and investigation of unauthorized access or anomalous behavior. It supports compliance requirements by ensuring that message transactions can be audited as part of security and regulatory frameworks.
  description: This rule verifies that streaming logging is enabled for Google Cloud Pub/Sub subscriptions. Logging should be configured to capture subscription events, ensuring that all message pulls, acknowledgments, and modifications are logged. To verify, check the subscription's logging settings in the Google Cloud Console or via the gcloud command-line tool. Remediation involves enabling logging in the Pub/Sub configuration to ensure that all relevant events are captured for analysis and compliance purposes.
  references:
  - https://cloud.google.com/pubsub/docs/admin
  - https://cloud.google.com/logging/docs/audit
  - CIS Google Cloud Platform Foundation Benchmark, v1.3.0 - 5.3
  - NIST SP 800-53 Rev. 5 - AU-2 Audit Events
  - 'PCI-DSS Requirement 10: Track and monitor all access to network resources and cardholder data'
  - https://cloud.google.com/security/compliance
- rule_id: gcp.pubsub.subscription.streaming_private_network_only_where_supported
  service: pubsub
  resource: subscription
  requirement: Streaming Private Network Only Where Supported
  scope: pubsub.subscription.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Pub/Sub Subscriptions Use Private Networks Where Supported
  rationale: Restricting Pub/Sub subscriptions to private networks reduces exposure to unauthorized access, mitigating risks of data exfiltration and unauthorized data manipulation. This approach aligns with best practices for network security by ensuring that sensitive data is not accessible over the public internet, which is crucial for maintaining data integrity and privacy compliance, particularly under regulations like GDPR and HIPAA.
  description: This rule checks if Pub/Sub subscriptions are configured to use private network endpoints where available. It ensures that subscriptions leverage VPC Service Controls, reducing the attack surface by restricting network access to internal IPs only. Verify configurations by reviewing the network settings in the Pub/Sub console and updating the subscription to enforce private connectivity through the use of private Google Access. Remediation involves enabling private endpoints and updating the IAM roles to limit access to authorized networks.
  references:
  - https://cloud.google.com/pubsub/docs/private-ip
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/vpc-service-controls/docs
  - https://cloud.google.com/pubsub/docs/security-overview
  - https://cloud.google.com/network-connectivity/docs/
- rule_id: gcp.pubsub.subscription.streaming_video_encryption_at_rest_enabled
  service: pubsub
  resource: subscription
  requirement: Streaming Video Encryption At Rest Enabled
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Pub/Sub Subscription Encryption for Streaming Video
  rationale: Encrypting streaming video data at rest helps protect sensitive information from unauthorized access and data breaches. This is crucial for complying with regulatory standards such as GDPR and HIPAA, which mandate data protection measures. Failing to encrypt data at rest can lead to significant financial penalties and reputational damage in the event of a data leak.
  description: This rule checks if Google Cloud Pub/Sub subscriptions used for streaming video data have encryption at rest enabled. To verify, ensure that the Pub/Sub subscription is configured with a Customer-Managed Encryption Key (CMEK) for additional security over the default Google-managed encryption. Remediation involves setting up a CMEK in Cloud Key Management Service (KMS) and associating it with your Pub/Sub subscription.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - 'CIS GCP Benchmark: 4.1 - Ensure that Cloud Pub/Sub topics and subscriptions have CMEK configured'
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/index.html
- rule_id: gcp.pubsub.subscription.streaming_video_encryption_in_transit_required
  service: pubsub
  resource: subscription
  requirement: Streaming Video Encryption In Transit Required
  scope: pubsub.subscription.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption In Transit for Pub/Sub Video Streaming
  rationale: Encrypting streaming video data in transit is crucial to protect sensitive information from being intercepted by unauthorized parties. This is particularly important for compliance with regulations like GDPR and HIPAA, which mandate data protection measures. Failing to encrypt data can lead to data breaches, legal liabilities, and a loss of customer trust.
  description: This rule checks whether Pub/Sub subscriptions for streaming video data enforce encryption in transit using TLS. To verify compliance, ensure that all Pub/Sub endpoints are configured to require TLS 1.2 or higher for data transmission. Remediation involves updating your Pub/Sub subscription settings to enforce the use of secure transmission protocols, thus protecting data integrity and confidentiality.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-in-transit
- rule_id: gcp.pubsub.subscription.streaming_video_retention_days_minimum_configured
  service: pubsub
  resource: subscription
  requirement: Streaming Video Retention Days Minimum Configured
  scope: pubsub.subscription.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Minimum Retention for Pub/Sub Streaming Video Data
  rationale: Configuring a minimum retention period for streaming video data in Pub/Sub ensures that critical video logs are retained for analysis and compliance purposes. This helps in forensic investigations, understanding user interactions, and meeting compliance mandates such as GDPR and CCPA that require data retention policies to be defined and auditable. Without proper retention settings, valuable data could be lost, leading to gaps in security monitoring and potential compliance violations.
  description: This rule checks whether a minimum retention period is configured for streaming video data subscriptions in Google Cloud Pub/Sub. The retention setting ensures that messages are retained for a specified period, allowing for adequate time to process and analyze the data. To verify, check the 'retentionDuration' field in the subscription configuration and ensure it meets the organization's policy requirements. Remediation involves updating the subscription settings to define a suitable retention period using the GCP Console or CLI.
  references:
  - https://cloud.google.com/pubsub/docs/subscriber
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/architecture/cloud-pubsub-audit-logs
  - https://cloud.google.com/pubsub/docs/managing-subscription
- rule_id: gcp.pubsub.subscription.topic_subscription_configured
  service: pubsub
  resource: subscription
  requirement: Topic Subscription Configured
  scope: pubsub.subscription.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure All Pub/Sub Subscriptions Have Associated Topics
  rationale: Configuring Pub/Sub subscriptions without associated topics can result in missed messages and data loss, impacting business operations and compliance with data retention policies. An unconfigured subscription may indicate misconfigurations that could lead to security gaps and monitoring blind spots, thereby increasing vulnerability to unauthorized access or data breaches.
  description: This rule verifies that all Google Cloud Pub/Sub subscriptions are correctly configured with an associated topic. Subscriptions that lack a topic may not receive messages, leading to potential data loss and operational inefficiencies. To remediate, ensure each subscription is linked to an appropriate topic by reviewing the subscription settings in the Google Cloud Console or using the gcloud command-line tool. This ensures that messages are properly routed and monitored, maintaining data integrity and system reliability.
  references:
  - https://cloud.google.com/pubsub/docs/subscriber
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/pubsub/docs/overview
  - https://cloud.google.com/pubsub/docs/admin
- rule_id: gcp.pubsub.topic.data_protection_storage_queue_auth_required
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Queue Auth Required
  scope: pubsub.topic.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Topic Requires Authentication for Access
  rationale: Requiring authentication for Pub/Sub topics is crucial to protect sensitive data from unauthorized access, which can lead to data breaches and compromise of business operations. Ensuring that only authenticated users and services can publish or subscribe to topics helps mitigate risks such as data leaks and unauthorized data manipulation. Compliance with regulations like PCI-DSS and HIPAA often mandates strict access controls for data handling systems.
  description: This rule checks that Pub/Sub topics are configured to require authentication for accessing their data. To verify, ensure IAM policies are set to prevent anonymous or unauthenticated access by reviewing the permissions granted to 'allUsers' and 'allAuthenticatedUsers'. Remediation involves removing excessive permissions and ensuring that only specific, trusted identities have access to the topics, using roles such as 'roles/pubsub.publisher' and 'roles/pubsub.subscriber' with identity-based policies.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org
  - https://www.hhs.gov/hipaa/index.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
- rule_id: gcp.pubsub.topic.data_protection_storage_queue_cross_account_send__restricted
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Queue Cross Account Send Restricted
  scope: pubsub.topic.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Restrict Cross-Account Pub/Sub Topic Data Transfers
  rationale: Limiting cross-account data transfers for Pub/Sub topics is crucial to prevent unauthorized access and data exfiltration. Ensuring that only trusted accounts can publish messages to your topics helps maintain data integrity and compliance with privacy regulations such as GDPR and CCPA. This measure mitigates risks associated with data breaches and unauthorized data sharing.
  description: This rule checks that Google Cloud Pub/Sub topics do not allow cross-account message publishing unless explicitly required. By default, restrict topic permissions to ensure that only designated accounts can send messages. Verify permissions using Google Cloud Console or gcloud CLI and adjust IAM policies to limit access. Remediation involves reviewing and updating IAM policies to only include necessary service accounts or identities.
  references:
  - https://cloud.google.com/pubsub/docs/security
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.pubsub.topic.data_protection_storage_queue_encryption_at_rest_enabled
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Queue Encryption At Rest Enabled
  scope: pubsub.topic.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Pub/Sub Topics Have Encryption at Rest Enabled
  rationale: Enabling encryption at rest for Pub/Sub topics protects sensitive data from unauthorized access and potential data breaches. It is crucial for maintaining data confidentiality and integrity, particularly in meeting compliance requirements such as GDPR and CCPA. Without encryption, data stored in Pub/Sub topics could be vulnerable to unauthorized access by malicious actors or internal threats.
  description: This rule checks whether Google Cloud Pub/Sub topics have encryption at rest enabled by using either Google-managed or customer-managed encryption keys. To verify, ensure that the 'kmsKeyName' field is set in the topic's configuration. If not configured, use the Google Cloud Console or the gcloud CLI to enable encryption by specifying a Cloud KMS key. This action will encrypt the data stored in Pub/Sub, aligning with best practices for data protection.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/encrypt-decrypt
- rule_id: gcp.pubsub.topic.data_protection_storage_queue_private_network_only_supported
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Queue Private Network Only Supported
  scope: pubsub.topic.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Pub/Sub Topics Use Private Network for Data Protection
  rationale: Using private networks for Pub/Sub topics reduces exposure to unauthorized access and data breaches by restricting the network pathways through which data can travel. This setup is crucial for maintaining data confidentiality and integrity, especially for organizations handling sensitive information or subject to regulatory requirements such as GDPR or HIPAA.
  description: This rule checks whether Pub/Sub topics are configured to only allow data flow over private networks, preventing public internet exposure. To verify, ensure that the Pub/Sub topic's IAM policies are set to restrict access to specific private IP ranges or VPC networks. Remediation involves updating network configurations and IAM policies to enforce traffic through private connections only.
  references:
  - https://cloud.google.com/pubsub/docs/network-restriction
  - CIS Google Cloud Platform Foundation Benchmark
  - NIST SP 800-53
  - PCI-DSS Requirement 1.2.1
  - https://cloud.google.com/security/compliance
- rule_id: gcp.pubsub.topic.data_protection_storage_stream_consumer_auth_required
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Stream Consumer Auth Required
  scope: pubsub.topic.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Topic Consumers Have Authenticated Access
  rationale: Requiring authentication for Pub/Sub topic consumers is crucial to prevent unauthorized data access, which could lead to data breaches and compromise sensitive information. This control is vital for maintaining data integrity and confidentiality, and helps in adhering to compliance requirements like GDPR and HIPAA, which mandate strict access controls to protect personal data.
  description: This rule checks if all consumers accessing Pub/Sub topics are authenticated. Ensure that IAM policies are configured to grant permission only to intended and authenticated service accounts. Verify this by reviewing the topic's IAM policy bindings and ensuring no 'allUsers' or 'allAuthenticatedUsers' roles are granted. Remediation involves updating IAM policies to restrict access to specific identities that require access, such as service accounts with least privilege necessary.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/document_library
- rule_id: gcp.pubsub.topic.data_protection_storage_stream_encryption_at_rest_enabled
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Stream Encryption At Rest Enabled
  scope: pubsub.topic.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Pub/Sub Topic Encryption at Rest is Enabled
  rationale: Enabling encryption at rest for Pub/Sub topics is crucial to protect sensitive data from unauthorized access, particularly in the event of a data breach. It mitigates the risk of data exposure by ensuring that data is encrypted when stored, addressing compliance requirements such as PCI-DSS and HIPAA, which mandate data protection measures.
  description: This rule checks if Google Cloud Pub/Sub topics have encryption at rest enabled, using either Google-managed or customer-managed encryption keys (CMEK). Verification can be performed by examining the topic's configuration in the GCP Console or using the gcloud command-line tool. To remediate, configure the topic to use encryption by navigating to the Pub/Sub topic settings and selecting an appropriate encryption key.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://cloud.google.com/security/compliance/hipaa/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/docs/security/security-best-practices
- rule_id: gcp.pubsub.topic.data_protection_storage_stream_private_network_onl_supported
  service: pubsub
  resource: topic
  requirement: Data Protection Storage Stream Private Network Onl Supported
  scope: pubsub.topic.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Ensure Pub/Sub Topic Accessible Only via Private Network
  rationale: Restricting Pub/Sub topic access to private networks minimizes the risk of unauthorized data interception and exfiltration. This enhances data confidentiality and integrity by ensuring that sensitive data streams are not exposed to the public internet, aligning with compliance requirements such as PCI-DSS and HIPAA that mandate secure data transmission.
  description: This rule checks if Pub/Sub topics are configured to allow access only from private IP addresses. To verify, ensure that IAM policies are set to restrict access to VPC Service Controls or use private Google Access. Remediation involves configuring a VPC Service Perimeter to include the Pub/Sub service or setting up private Google Access for secure data handling.
  references:
  - https://cloud.google.com/pubsub/docs/vpc
  - https://cloud.google.com/vpc-service-controls/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.pubsub.topic.streaming_analytics_checkpoints_and_outputs_encrypted
  service: pubsub
  resource: topic
  requirement: Streaming Analytics Checkpoints And Outputs Encrypted
  scope: pubsub.topic.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Pub/Sub Checkpoints and Outputs Are Encrypted
  rationale: Encrypting Pub/Sub checkpoints and outputs mitigates the risk of unauthorized data access, ensuring data confidentiality and integrity. This is crucial in protecting sensitive information from potential threats such as data breaches and insider threats, and is often required to comply with regulations like GDPR and HIPAA.
  description: This rule checks that all Pub/Sub topics used for streaming analytics checkpoints and outputs are encrypted using Google-managed or customer-managed encryption keys. To verify, ensure that the Pub/Sub topic's encryption configuration is set to leverage Cloud Key Management Service (KMS). Remediation involves configuring Pub/Sub topics to use CMEK (Customer-Managed Encryption Keys) through the GCP Console or CLI, thereby enhancing data protection.
  references:
  - https://cloud.google.com/pubsub/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/hipaa
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.pubsub.topic.streaming_analytics_network_private_only
  service: pubsub
  resource: topic
  requirement: Streaming Analytics Network Private Only
  scope: pubsub.topic.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Enforce Private Network for Streaming Analytics Pub/Sub Topics
  rationale: Ensuring Pub/Sub topics are only accessible from private networks reduces the risk of unauthorized access and potential data breaches, protecting sensitive analytics data. This is particularly important for compliance with regulations such as GDPR and HIPAA, which mandate robust data protection measures to prevent data leakage and unauthorized exposure.
  description: This rule checks if Pub/Sub topics used for streaming analytics are configured to only allow access from private IPs. Verify that IAM policies and network settings restrict access to private networks, preventing exposure to the public internet. Remediation involves configuring VPC Service Controls to enforce private connectivity and updating IAM roles to limit public access. Use the GCP Console or CLI to review and modify topic access settings as needed.
  references:
  - https://cloud.google.com/pubsub/docs/security
  - https://cloud.google.com/vpc-service-controls/docs/overview
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0, Control 4.1
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 5, AC-19
- rule_id: gcp.pubsub.topic.streaming_analytics_role_least_privilege
  service: pubsub
  resource: topic
  requirement: Streaming Analytics Role Least Privilege
  scope: pubsub.topic.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Pub/Sub Topics Use Least Privilege for Streaming Analytics
  rationale: Implementing least privilege access for Pub/Sub topics minimizes the risk of unauthorized data access and potential data breaches. Over-privileged roles can lead to inadvertent data exposure or misuse, impacting business operations and regulatory compliance. Adhering to strict access controls helps in maintaining data integrity and confidentiality, which is essential for compliance with standards like NIST and ISO 27001.
  description: This rule checks if Pub/Sub topics used in streaming analytics are assigned the least privilege necessary to perform required tasks. It examines IAM policies to ensure roles include only essential permissions. To verify, review IAM roles associated with Pub/Sub topics and adjust permissions to align with the principle of least privilege. Remediation involves removing unnecessary roles and permissions, thereby reducing the attack surface.
  references:
  - https://cloud.google.com/pubsub/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.resourcemanager.folder.governance_ou_auto_tag_policies_enabled_where_supported
  service: resourcemanager
  resource: folder
  requirement: Governance Ou Auto Tag Policies Enabled Where Supported
  scope: resourcemanager.folder.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Auto Tag Policies in GCP Folders for Governance
  rationale: Auto Tag Policies in GCP folders help maintain a consistent application of metadata across resources, aiding in security monitoring and compliance with data management policies. Without these policies, it is challenging to enforce governance rules, potentially leading to non-compliance with regulatory frameworks like ISO 27001 and increased risk of unauthorized resource access or data leakage.
  description: This rule checks whether auto tag policies are enabled in GCP folders to ensure all resources inherit necessary tags for governance purposes. To verify, review the folder settings in the GCP Console or via CLI to confirm auto tag policies are active. Remediation involves configuring policies in the GCP Console under 'Resource Manager' to automatically apply tags based on organizational unit requirements, ensuring consistent metadata application across resources.
  references:
  - https://cloud.google.com/resource-manager/docs/tag-policies
  - https://cloud.google.com/security/compliance/iso-27001
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/architecture/tagging-strategies
  - https://cloud.google.com/resource-manager/docs/creating-managing-projects
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.resourcemanager.folder.governance_ou_no_overly_permissive_exceptions
  service: resourcemanager
  resource: folder
  requirement: Governance Ou No Overly Permissive Exceptions
  scope: resourcemanager.folder.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Overly Permissive Roles in GCP Folders
  rationale: Overly permissive IAM roles within GCP folders can lead to unauthorized access, data leaks, and non-compliance with regulatory standards. Limiting exceptions to governance policies prevents potential breaches and ensures adherence to the principle of least privilege, reducing the risk surface and aligning with compliance mandates such as GDPR, PCI-DSS, and HIPAA.
  description: This rule checks for folders in GCP where overly permissive IAM roles have been granted that deviate from established governance policies. It identifies exceptions to organizational policies that allow excessive access and recommends remediation by adjusting IAM roles to align with the principle of least privilege. To verify, review the IAM policy bindings for folders and ensure they adhere to organizational standards. Remediation involves auditing current permissions and removing or adjusting any that exceed the necessary access levels.
  references:
  - https://cloud.google.com/resource-manager/docs/access-control-folders
  - https://cloud.google.com/security/compliance/cis#overview
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.resourcemanager.folder.governance_ou_required_scps_attached
  service: resourcemanager
  resource: folder
  requirement: Governance Ou Required Scps Attached
  scope: resourcemanager.folder.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Folders Have Required Governance Policies
  rationale: Attaching required governance policies to folders in GCP ensures that organizational security and compliance policies are uniformly applied and enforced. This minimizes risks such as non-compliance with regulatory frameworks and reduces the potential for unauthorized access or configuration drift within the organization's cloud resources.
  description: This rule verifies that all folders within the GCP Resource Manager have the mandatory governance policies, such as Organization Policy Constraints, attached. It checks for the presence of Service Control Policies (SCPs) that dictate the allowed and denied actions within the folder. To remediate, ensure that your folders have the necessary SCPs applied by navigating to the GCP Console, selecting the folder, and adding the required policies under 'Governance Policies'.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://cloud.google.com/resource-manager/docs/creating-managing-folders
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0, Section 1.5
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - NIST SP 800-53 Rev. 5 CM-6 Configuration Settings
  - ISO/IEC 27001:2013 - A.9.2 Access Control
- rule_id: gcp.resourcemanager.organization.governance_org_all_accounts_enrolled_in_org
  service: resourcemanager
  resource: organization
  requirement: Governance Org All Accounts Enrolled In Org
  scope: resourcemanager.organization.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure All Accounts Are Enrolled in Governance Organization
  rationale: Enrolling all accounts in the Governance Organization ensures centralized oversight and consistent application of security policies across the organization. This reduces the risk of shadow IT and potential security breaches by ensuring all accounts adhere to governance standards and regulatory compliance requirements, such as PCI-DSS or ISO 27001.
  description: This rule verifies that all Google Cloud accounts are enrolled within the designated Governance Organization. This ensures that organizational policies and security controls are consistently applied across all accounts. To verify, review the Organization Policy in the Cloud Console under 'Resource Manager', ensuring all projects and accounts are listed. Remediation involves enrolling any standalone accounts into the Governance Organization using Google Cloud CLI or Console.
  references:
  - https://cloud.google.com/resource-manager/docs/creating-managing-organization
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/overview/whitepaper
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.resourcemanager.organization.governance_org_block_leave_org_by_policy
  service: resourcemanager
  resource: organization
  requirement: Governance Org Block Leave Org By Policy
  scope: resourcemanager.organization.policy_management
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Prevent Unauthorized Organization Leaving via Policy
  rationale: Blocking the ability for organization members to leave by policy mitigates risks of unauthorized or accidental removal, which could lead to loss of critical access and data integrity issues. This control is crucial for maintaining governance, ensuring compliance with organizational policies, and reducing insider threat scenarios. It supports regulatory frameworks that emphasize strong governance and access controls.
  description: This rule ensures that Google Cloud Platform's Organization Policy restricts members from leaving the organization without authorization. To verify, navigate to the Organization Policy settings in the GCP Console and confirm that constraints/gcp.restrictLeaveOrganization is enforced. Remediate by setting this constraint to 'Allow' only for specific roles that require such permissions, ensuring governance policies are adhered to and reducing the risk of data and resource exposure.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/constraints#restrict_leave_organization
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/resource-manager/docs/access-control-org
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.resourcemanager.organization.governance_org_restrict_region_usage_by_policy_wher_required
  service: resourcemanager
  resource: organization
  requirement: Governance Org Restrict Region Usage By Policy Wher Required
  scope: resourcemanager.organization.policy_management
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Restrict Organization Region Usage by Policy
  rationale: Restricting region usage by policy mitigates risks associated with data residency laws and enhances compliance with geographic data regulations. It helps prevent unauthorized data processing in regions that do not meet your organization's security or compliance requirements, thereby reducing exposure to potential legal and financial penalties.
  description: This rule checks if your organization has policies in place to restrict the usage of specific Google Cloud regions as required by your governance framework. Verification involves ensuring that Organization Policy Constraints are set to limit the regions where resources can be deployed. Remediation includes defining and applying the 'constraints/gcp.resourceLocations' policy at the organization level to enforce geographic restrictions.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/blog/topics/identity-security/how-to-use-organization-policy-service-to-configure-restricted-locations
- rule_id: gcp.resourcemanager.organization.governance_org_scps_enabled
  service: resourcemanager
  resource: organization
  requirement: Governance Org Scps Enabled
  scope: resourcemanager.organization.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Organization Policies are Enforced with Org SCPs
  rationale: Enabling Organization Service Control Policies (SCPs) is crucial for maintaining governance at scale across Google Cloud Platform environments. It allows centralized management of policies, ensuring consistent security and compliance across all projects within an organization. Without SCPs, there is an increased risk of policy drift and non-compliance, leading to potential security vulnerabilities and regulatory penalties.
  description: This rule checks whether Organization Service Control Policies are enabled in your GCP organization. SCPs allow you to define and enforce a set of policies that apply to all resources within the organization. To verify, navigate to the GCP Console, access the 'Resource Manager' under 'IAM & Admin', and ensure that SCPs are configured and enforced. Remediation involves setting up SCPs that align with organizational security policies and compliance requirements.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_lockout_threshold_defined
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Lockout Threshold Defined
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Define Password Lockout Threshold for Identity Access
  rationale: Defining a password lockout threshold mitigates the risk of brute force attacks by limiting the number of failed login attempts before a temporary lockout occurs. This helps protect sensitive organizational data and resources from unauthorized access, reducing potential security breaches and maintaining compliance with security frameworks that mandate account protection measures.
  description: This rule checks if a password lockout threshold is defined within the organization's identity access policy. It ensures that after a specified number of failed login attempts, the user account is temporarily locked, preventing further unauthorized access attempts. To verify, access the GCP Console, navigate to 'IAM & Admin,' and review the password policy settings under the organization. Remediation involves setting a lockout threshold by updating the identity access policy to define the maximum number of failed attempts allowed.
  references:
  - https://cloud.google.com/iam/docs/policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_max_age_90_days_or_less
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Max Age 90 Days Or Less
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce 90-Day Max Age for Identity Password Policies
  rationale: Implementing a maximum password age of 90 days or less reduces the risk of unauthorized access by ensuring that compromised passwords are not used indefinitely. Regular password changes mitigate the risk of brute-force attacks and comply with industry standards like NIST and PCI-DSS, which demand stringent password management practices.
  description: This rule verifies that the organization's identity access password policy enforces a maximum password age of 90 days or less. To check compliance, review the organization's password policy settings under the Identity and Access Management (IAM) configurations in GCP. If the policy exceeds 90 days, update it within the IAM settings to align with best practices and regulatory requirements. Regularly auditing these settings ensures continued adherence to security policies.
  references:
  - https://cloud.google.com/iam/docs/reference/rest/v1/organizations/getIamPolicy
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_min_length_14
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Min Length 14
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Min Password Length of 14 for IAM Policies
  rationale: Setting a minimum password length of 14 characters enhances the strength of user passwords, reducing the risk of unauthorized access through brute force attacks. This policy aids in fulfilling compliance requirements such as NIST SP 800-63B, which recommends longer passwords to increase entropy and security. It also protects sensitive organizational data and resources by preventing weak password practices.
  description: This rule checks whether the Identity Access Management (IAM) policies within your GCP organization enforce a minimum password length of 14 characters. To verify, inspect the organization's password policy settings in the GCP Console under Security -> Settings. Remediation involves updating the password policy to require a minimum length of 14 characters through the IAM settings or by using the gcloud command-line tool to configure the policy programmatically. This ensures stronger passwords across all users in the organization.
  references:
  - https://cloud.google.com/iam/docs/password-policy
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://pages.nist.gov/800-63-3/sp800-63b.html#memsectr
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_prevent_reuse_last_24
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Prevent Reuse Last 24
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Password Policy to Prevent Reuse of Last 24 Passwords
  rationale: Implementing a password reuse prevention policy strengthens identity security by reducing the risk of credential stuffing and brute-force attacks. It ensures that compromised passwords cannot be easily recycled, thus protecting sensitive organizational data and complying with regulations like PCI-DSS and ISO 27001.
  description: This rule checks whether the organization's identity access management settings enforce a password policy that prevents the reuse of the last 24 passwords. To verify, review the IAM settings in the GCP Console under Security > Identity and Access Management > Password Policies. If not configured, update the policy to disallow the reuse of the last 24 passwords. This can be done via the Google Cloud CLI by setting the 'password_policy_prevent_reuse' field to '24'.
  references:
  - https://cloud.google.com/iam/docs/managing-password-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
- rule_id: gcp.resourcemanager.organization.identity_access_password_policy_require_upper_lower__special
  service: resourcemanager
  resource: organization
  requirement: Identity Access Password Policy Require Upper Lower Special
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Enforce Complex Password Policy for Organization IAM
  rationale: Enforcing a complex password policy is essential to prevent unauthorized access by reducing the risk of credential-based attacks such as brute force or dictionary attacks. Weak passwords can lead to compromised accounts, which may result in data breaches, financial loss, and non-compliance with standards like PCI-DSS and ISO 27001.
  description: This rule checks if the password policy for the organization enforces the use of upper and lower case letters, numbers, and special characters. A strong password policy helps safeguard user accounts against unauthorized access. To verify, check the IAM settings in the Google Cloud Console under Security > Password Policy. Remediation involves configuring the policy to require a mix of character types in passwords.
  references:
  - https://cloud.google.com/iam/docs/reference/rest/v1/organizations/setIamPolicy
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://cloud.google.com/security/best-practices/identity
  - https://cloud.google.com/docs/security-overview
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_api_access_keys_root_or_ow_disallowed
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant API Access Keys Root Or Ow Disallowed
  scope: resourcemanager.organization.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable API Access Keys for Root or Owner Accounts
  rationale: Allowing API access keys for root or owner accounts increases the risk of unauthorized access and potential data exfiltration. Compromised keys can lead to privilege escalation and security breaches, undermining organizational security and violating compliance mandates such as PCI-DSS and ISO 27001.
  description: This rule checks if API access keys are enabled for root or owner accounts within the organization. To enhance security, these accounts should rely on more secure authentication methods like OAuth tokens or IAM roles. Verify by auditing the IAM policies attached to these accounts and remove any API keys. Utilize Identity and Access Management (IAM) policies to enforce this restriction and regularly review account permissions.
  references:
  - https://cloud.google.com/iam/docs/best-practices-for-managing-iam-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/docs/security
  - https://cloud.google.com/iam/docs/using-iam-securely
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_break_glass_accounts_mfa_enforced
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Break Glass Accounts MFA Enforced
  scope: resourcemanager.organization.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA on Break Glass Accounts for GCP Organizations
  rationale: Enforcing Multi-Factor Authentication (MFA) on break glass accounts mitigates the risk of unauthorized access to critical GCP resources, reducing the impact of compromised credentials. It ensures compliance with security best practices and frameworks like NIST and PCI-DSS, thereby protecting sensitive data and maintaining organizational trust.
  description: This rule checks that all break glass accounts within a GCP organization have MFA enforced, ensuring an additional layer of security beyond passwords. Administrators should configure these accounts to require MFA using Google Cloud Identity. Verification involves auditing IAM policies to ensure MFA is enabled for these critical accounts. Remediation includes applying an MFA policy to break glass accounts and regularly reviewing access logs for anomalies.
  references:
  - https://cloud.google.com/identity-platform/docs/multi-factor-authentication
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/iam/docs/using-mfa
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_console_mfa_required_org_wide
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Console MFA Required Org Wide
  scope: resourcemanager.organization.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA for All GCP Organization Console Access
  rationale: Requiring multi-factor authentication (MFA) for console access helps prevent unauthorized access to the GCP organization, mitigating risks of data breaches and unauthorized changes. This is crucial for protecting sensitive organizational data and maintaining compliance with security standards such as PCI-DSS and ISO 27001. Failure to enforce MFA increases susceptibility to phishing attacks and credential theft.
  description: This rule ensures that all users accessing the GCP management console for the organization must authenticate using MFA. Verify this by checking the organization's IAM policy to ensure MFA is enforced for all accounts. Remediate by configuring Identity and Access Management (IAM) policies to require MFA for console logins. This can be achieved by enforcing security policies through Google Identity, setting up MFA for all users, and ensuring that policies are applied organization-wide.
  references:
  - https://cloud.google.com/docs/authentication/end-user-mfa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_inactive_user_disable_thre_configured
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Inactive User Disable Thre Configured
  scope: resourcemanager.organization.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Configure Inactive User Auto-Disable in Identity Access Settings
  rationale: Inactive users with access to your GCP organization pose a security threat as they can be exploited by attackers to gain unauthorized access. Implementing an auto-disable policy for inactive accounts helps mitigate risks associated with stale credentials and supports compliance with regulatory requirements such as ISO 27001 and SOC2, which emphasize strict access control policies.
  description: This rule checks that the Identity Access Management (IAM) settings for your GCP organization include a policy to automatically disable user accounts after a period of inactivity. To verify, navigate to the IAM settings in the Google Cloud Console and ensure the 'Auto-disable inactive users' policy is configured with a threshold appropriate to your organization's security policy. Remediation involves setting this policy to a recommended duration, such as 90 days, to ensure inactive accounts are automatically disabled, reducing the risk of unauthorized access.
  references:
  - https://cloud.google.com/iam/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r5.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/best-practices/iam-users
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_password_policy_compliant
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Password Policy Compliant
  scope: resourcemanager.organization.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure Tenant Password Policy Compliance in GCP Organizations
  rationale: Enforcing a strong password policy within a GCP organization mitigates risks of unauthorized access due to weak or compromised passwords. It is crucial for protecting sensitive organizational data and maintaining trust in cloud infrastructure. Compliance with regulatory standards such as NIST and ISO 27001 further enforces best practices and helps meet legal obligations.
  description: This rule checks whether the identity access tenant password policy in the GCP organization is compliant with recommended security standards. A compliant password policy should include minimum length, complexity, and expiration settings. To verify, review the Identity and Access Management settings in the GCP Console under Security > Identity & Security > Password policies. Remediation involves adjusting the password policy to align with security best practices, ensuring robust authentication mechanisms are in place.
  references:
  - https://cloud.google.com/iam/docs/managing-password-policies
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/docs/security#security-and-compliance
- rule_id: gcp.resourcemanager.organization.identity_access_tenant_sso_federation_configured_w_supported
  service: resourcemanager
  resource: organization
  requirement: Identity Access Tenant Sso Federation Configured W Supported
  scope: resourcemanager.organization.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: medium
  title: Ensure SSO Federation Configured for Identity Access Tenant
  rationale: Configuring SSO federation for your GCP organization enhances security by centralizing authentication and leveraging existing identity providers, reducing the risk of unauthorized access. It also aids in compliance with standards that require strong identity and access management controls, such as ISO 27001 and SOC2. Without SSO federation, organizations may face increased risk of credential theft or misuse.
  description: This rule checks if Single Sign-On (SSO) federation is configured correctly for the organization's identity access tenant in GCP. Proper configuration involves setting up a trusted identity provider and ensuring that user authentication requests are redirected and validated against this provider. To verify, review the IAM settings in the GCP console and confirm that the SSO federation is active and correctly linked to the supported identity provider. Remediation involves configuring SSO settings according to the GCP documentation and your identity provider's guidelines.
  references:
  - https://cloud.google.com/identity/docs/setup-sso
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/blog/products/identity-security/enhancing-security-in-gcp-with-federated-id-and-sso
  - https://cloud.google.com/iam/docs/understanding-service-accounts
- rule_id: gcp.resourcemanager.project.configuration_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Configuration Management in GCP Projects
  rationale: Enabling configuration management in GCP projects ensures that all resources are consistently and correctly configured, reducing the risk of misconfigurations that could lead to security breaches or compliance issues. This approach helps organizations meet regulatory requirements and maintain a secure and efficient cloud environment.
  description: This rule checks whether configuration management settings are enabled for GCP projects to ensure consistent resource configurations. Verify that tools like Google Cloud Deployment Manager or Terraform are used for managing project configurations. To remediate, implement a configuration management process that automates resource provisioning and management, ensuring compliance with organizational policies.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/itl/nist-cybersecurity-framework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/ci-cd
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.resourcemanager.project.configuration_management_aggregation_auto_enroll_ne_accounts
  service: resourcemanager
  resource: project
  requirement: Configuration Management Aggregation Auto Enroll Ne Accounts
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Auto Enrollment for Configuration Management Aggregation
  rationale: Automatically enrolling new accounts in configuration management aggregation ensures consistent application of security policies and settings across all projects. This reduces the risk of misconfigurations, which can lead to vulnerabilities and non-compliance with security frameworks. It enhances visibility and control over the cloud environment, reducing the likelihood of unauthorized access or data breaches.
  description: This rule checks if new accounts are automatically enrolled in configuration management aggregation within Google Cloud. To verify, ensure that the appropriate settings in the Resource Manager are enabled, allowing new projects to inherit security configurations automatically. Remediation involves configuring the Resource Manager to include auto-enrollment policies for new accounts, ensuring they adhere to organizational security standards from inception.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.configuration_management_aggregation_org_aggregator_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Aggregation Org Aggregator Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Org Aggregator for Config Management in Projects
  rationale: Enabling an organization aggregator for configuration management facilitates centralized visibility and control over cloud resources across multiple projects. This minimizes configuration drift, reduces the risk of misconfigurations, and supports compliance with industry standards by providing a unified view of resource settings, thus enhancing overall security posture.
  description: This rule checks if the organization-level aggregator for configuration management is enabled, allowing resource configuration data from various projects to be aggregated at the organization level. To verify, ensure that the GCP Organization has an aggregator configured within Cloud Asset Inventory. Remediate by setting up an organization aggregator in the Cloud Asset Inventory console, which consolidates configuration data for analysis and reporting.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.resourcemanager.project.configuration_management_baseline_conformance_pack_deployed
  service: resourcemanager
  resource: project
  requirement: Configuration Management Baseline Conformance Pack Deployed
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Baseline Conformance Pack Is Deployed for GCP Projects
  rationale: Deploying a Configuration Management Baseline Conformance Pack helps ensure consistent security configurations across GCP projects. This reduces the risk of configuration drift, which can lead to security vulnerabilities and non-compliance with industry standards such as ISO 27001 and PCI-DSS. Proactively managing configurations supports robust security postures and aligns with best practice frameworks.
  description: This rule checks whether a Configuration Management Baseline Conformance Pack is deployed within GCP projects. It evaluates the presence of predefined policies that enforce security settings and configurations. To verify compliance, ensure that your project uses a baseline conformance pack with policies relevant to your organizational requirements. If not present, deploy a suitable conformance pack via the GCP Console or Command Line Interface (CLI).
  references:
  - https://cloud.google.com/config-connector/docs/reference/resource-docs/resourcemanager/project
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/docs/security/security-overview
  - https://cloud.google.com/resource-manager/docs/creating-managing-projects
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.resourcemanager.project.configuration_management_baseline_parameters_set
  service: resourcemanager
  resource: project
  requirement: Configuration Management Baseline Parameters Set
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Project Configuration Management Baseline is Set
  rationale: Setting configuration management parameters for GCP projects is crucial in maintaining a consistent security posture across all resources. It mitigates risks of misconfigurations that could lead to unauthorized access or data breaches. Adhering to a baseline also supports compliance with industry standards and regulatory requirements such as NIST and PCI-DSS.
  description: This rule checks if baseline parameters for configuration management are set for GCP projects. It involves ensuring that all projects follow predefined configurations that align with the organization's security policies. Verification requires reviewing project settings against the baseline configuration. To remediate, establish and apply configuration management baselines using tools like Google Cloud Deployment Manager or Terraform, ensuring all projects comply with these standards.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/deployment-manager/docs
- rule_id: gcp.resourcemanager.project.configuration_management_compliance_evidence_expo_configured
  service: resourcemanager
  resource: project
  requirement: Configuration Management Compliance Evidence Expo Configured
  scope: resourcemanager.project.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Compliance Evidence Expo for Projects Configured
  rationale: Enabling Configuration Management Compliance Evidence Expo ensures that projects maintain auditability and accountability, which is crucial for detecting misconfigurations and unauthorized changes. This is important for minimizing risks associated with data breaches and non-compliance with regulatory standards such as SOC 2 and ISO 27001, which could lead to financial penalties and loss of customer trust.
  description: This rule verifies that Configuration Management Compliance Evidence Expo is enabled for GCP projects. This setting ensures that evidence of configuration compliance is collected and exposed for auditing and analysis. To verify, ensure that the necessary API is enabled and properly configured in the Google Cloud Console. Remediation involves enabling the relevant API and configuring it to collect and expose compliance evidence as per organizational policies.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security/compliance/soc-2/
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/audit-logging
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.project.configuration_management_compliance_reporting_dashbo_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Compliance Reporting Dashbo Enabled
  scope: resourcemanager.project.compliance
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Compliance Reporting Dashboard for Configuration Mgmt
  rationale: Enabling the Configuration Management Compliance Reporting Dashboard helps organizations track compliance with internal policies and external regulations. This enhances security posture by identifying misconfigurations and potential vulnerabilities, thus reducing the risk of data breaches and ensuring adherence to standards such as PCI-DSS, ISO 27001, and HIPAA.
  description: This rule checks if the Compliance Reporting Dashboard for Configuration Management is enabled in GCP projects. The dashboard provides a centralized view of compliance status across resources, helping to quickly identify and remediate deviations. To verify, ensure that the dashboard is activated within the GCP console under the Security Command Center. Remediation involves enabling Security Command Center Premium to access the dashboard and configuring proper IAM roles for access.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-enable-scc
  - https://cloud.google.com/security-command-center/docs/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security-command-center
- rule_id: gcp.resourcemanager.project.configuration_management_delivery_destination_acce_privilege
  service: resourcemanager
  resource: project
  requirement: Configuration Management Delivery Destination Acce Privilege
  scope: resourcemanager.project.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Configuration Management Destinations
  rationale: Excessive access privileges to configuration management delivery destinations can lead to unauthorized data exposure, configuration manipulation, and potential data breaches. It is crucial to enforce the principle of least privilege to minimize the risk of insider threats and ensure compliance with security frameworks such as NIST and ISO 27001.
  description: This rule checks that access privileges for configuration management delivery destinations within a project are limited to only necessary roles. Verify that no user, group, or service account has more permissions than required for their task. Remediation involves auditing IAM policies, removing unnecessary roles, and using predefined roles where possible to restrict access effectively.
  references:
  - https://cloud.google.com/resource-manager/docs/access-control-proj
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.resourcemanager.project.configuration_management_delivery_kms_encryption_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Delivery KMS Encryption Enabled
  scope: resourcemanager.project.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption for Configuration Management in GCP Projects
  rationale: Enabling KMS encryption for configuration management in GCP projects protects sensitive configuration data from unauthorized access and potential data breaches. This is crucial for maintaining data integrity and confidentiality, reducing the risk of data exposure during transmission and storage. Compliance with regulatory requirements, such as GDPR, HIPAA, and PCI-DSS, necessitates strong encryption practices.
  description: This check verifies that Google Cloud Platform projects are configured to use Google Key Management Service (KMS) for encrypting configuration management data. Ensure that the configuration management delivery process utilizes customer-managed encryption keys (CMEK) to enhance security by providing control over encryption keys. To enable KMS encryption, navigate to the Google Cloud Console, access the appropriate project, and configure the KMS settings under the security section. Regularly audit these settings to ensure compliance with organizational policies.
  references:
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0 - 4.1.1
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://www.nist.gov/news-events/news/2018/12/nist-releases-updates-security-and-privacy-controls
- rule_id: gcp.resourcemanager.project.configuration_management_delivery_secure_destinat_configured
  service: resourcemanager
  resource: project
  requirement: Configuration Management Delivery Secure Destinat Configured
  scope: resourcemanager.project.configuration_management
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Secure Configuration Management Delivery Destinations
  rationale: Unsecured configuration management delivery can expose sensitive data, configuration files, and infrastructure details to unauthorized access. This can lead to potential data breaches, non-compliance with regulatory requirements like PCI-DSS and SOC2, and increased operational risk. Securing these delivery paths is crucial to maintaining the integrity and confidentiality of your cloud environment.
  description: This rule checks that all configuration management delivery destinations are securely configured within GCP projects. It ensures that transport channels are encrypted, destinations are authenticated, and access is restricted to authorized entities only. Verification involves ensuring that services like Google Cloud Storage and Pub/Sub are configured with proper IAM policies, and data is transmitted using TLS. Remediation steps include enabling encryption in transit, setting appropriate IAM roles, and reviewing access logs regularly.
  references:
  - https://cloud.google.com/security/encryption-in-transit
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Computing Foundations Benchmark v1.3.0
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/pubsub/docs/security
- rule_id: gcp.resourcemanager.project.configuration_management_drift_alerts_configured
  service: resourcemanager
  resource: project
  requirement: Configuration Management Drift Alerts Configured
  scope: resourcemanager.project.configuration_management
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Configuration Management Drift Alerts are Enabled
  rationale: Configuration drift can lead to security vulnerabilities and operational inconsistencies. By enabling drift alerts, organizations can promptly detect and address unauthorized or accidental changes in their GCP environment, reducing the risk of data breaches or compliance violations. This is crucial for maintaining a secure and stable infrastructure that aligns with regulatory requirements like ISO 27001 and SOC2.
  description: This rule checks if configuration management drift alerts are configured for GCP projects. It ensures that alerts are set up to notify responsible parties when there are deviations from the desired configuration state. To verify, confirm that Cloud Monitoring is configured with alerting policies targeting configuration changes. Remediation involves setting up alerting policies in Cloud Monitoring to track and notify on configuration changes using Stackdriver alerts.
  references:
  - https://cloud.google.com/monitoring/alerts
  - https://cloud.google.com/resource-manager/docs/configuration-management
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/blog/products/management-tools/implementing-cloud-configuration-management
- rule_id: gcp.resourcemanager.project.configuration_management_drift_critical_resources_dr_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Drift Critical Resources DR Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Configuration Drift Detection for Critical Resources
  rationale: Configuration drift in critical resources can lead to security vulnerabilities, increased operational risk, and compliance failures. Proactively monitoring and managing configuration changes ensures resources remain in a secure state, supporting business continuity and regulatory requirements.
  description: This rule checks whether configuration drift detection is enabled for critical Google Cloud resources, ensuring deviations from the desired configuration are identified promptly. Verify through Google Cloud Console or API that Cloud Config is enabled and properly configured to monitor changes. Remediate by deploying Google Cloud's Configuration Management tool to define and enforce configuration states, reducing risk of unauthorized changes.
  references:
  - https://cloud.google.com/config-management/docs
  - https://cloud.google.com/architecture/drift-detection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.configuration_management_recorder_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Recorder Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Configuration Management Recorder is Enabled
  rationale: Enabling the Configuration Management Recorder on GCP projects is crucial for maintaining visibility into configuration changes, which helps in identifying unauthorized modifications, reducing the risk of misconfigurations, and ensuring compliance with security policies. This is essential for protecting data integrity and supporting incident response activities by providing a detailed audit trail.
  description: This rule checks if the Configuration Management Recorder is enabled for GCP projects, which is part of the Cloud Asset Inventory service. To verify, ensure that the asset inventory feature is activated and configured to record all relevant changes within the project. Remediation involves enabling the Cloud Asset Inventory API and setting up appropriate policies to capture configuration changes across all assets.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security/compliance/cis#cis_google_cloud_platform_foundations_benchmark
  - https://cloud.google.com/security/compliance/nist
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.configuration_management_recorder_global_resource_ty_tracked
  service: resourcemanager
  resource: project
  requirement: Configuration Management Recorder Global Resource Ty Tracked
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Track Global Resource Type in Configuration Management
  rationale: Tracking global resource types in GCP ensures that changes to critical resources are monitored, reducing the risk of configuration drift and unauthorized modifications. This aids in maintaining a secure and compliant environment by providing visibility into resource changes that could impact security posture and regulatory compliance.
  description: This rule checks whether the Configuration Management Recorder is set up to track changes to global resource types within a GCP project. Ensuring this setting is enabled allows for comprehensive auditing and monitoring of resource configurations. To verify, review the configuration settings of your resource manager to ensure global resources are tracked, and update policies to include these if they are not already monitored.
  references:
  - https://cloud.google.com/resource-manager/docs/configuration-management
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.project.configuration_management_remediation_auto_remediation_e_high
  service: resourcemanager
  resource: project
  requirement: Configuration Management Remediation Auto Remediation E High
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enable Auto-Remediation for High-Risk Configurations
  rationale: Enabling auto-remediation for high-risk configuration issues in GCP projects helps mitigate potential security threats promptly, reducing the risk of unauthorized access or data breaches. This proactive approach supports compliance with regulatory requirements and ensures that configurations are swiftly corrected, maintaining a secure cloud environment.
  description: This rule checks whether auto-remediation is enabled for high-risk configuration issues within GCP projects, specifically within the resource manager scope. To verify, ensure that configuration management tools are set to automatically remediate detected high-risk issues. Remediation involves setting up policies that automatically apply security patches or revert to secure configurations when deviations are detected.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security-command-center/docs/concepts-vm-scanner
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.resourcemanager.project.configuration_management_remediation_manual_runbook_attached
  service: resourcemanager
  resource: project
  requirement: Configuration Management Remediation Manual Runbook Attached
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Attach Manual Runbook for Configuration Management Remediation
  rationale: Attaching a manual runbook for configuration management remediation is essential for ensuring that any deviations or misconfigurations are promptly corrected, minimizing downtime and potential security risks. This practice supports business continuity by providing a structured approach to address configuration issues, aligning with regulatory compliance requirements such as SOC2 and ISO 27001.
  description: This rule checks if a manual runbook for configuration management remediation is attached to each GCP project. A manual runbook provides step-by-step instructions for rectifying configuration issues, ensuring consistent and efficient resolution processes. To verify, ensure that each project within the Resource Manager has an associated document outlining remediation steps for configuration issues. Remediation involves creating or updating a runbook to include detailed procedures for addressing findings from configuration scans.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.resourcemanager.project.configuration_management_rule_remediation_targets_bound
  service: resourcemanager
  resource: project
  requirement: Configuration Management Rule Remediation Targets Bound
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Enforce Configuration Management Remediation Targets in Project
  rationale: Binding configuration management remediation targets in GCP projects ensures that any configuration drifts are automatically corrected, minimizing the risk of security vulnerabilities and non-compliance. This is crucial for maintaining the integrity of cloud resources and adhering to industry regulations such as ISO 27001 and PCI-DSS, which require stringent configuration management processes.
  description: This rule verifies that all GCP projects are configured with defined remediation targets for configuration management, ensuring automated corrections for configuration drifts. It examines the presence of policies or tools that enforce configuration state, such as Google Cloud's Config Connector or third-party solutions. Remediation involves setting up or integrating these tools to automatically enforce desired configurations and correct deviations.
  references:
  - https://cloud.google.com/anthos-config-management/docs/concepts/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/blog/products/identity-security/continuous-configuration-validation-with-policy-controller
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.resourcemanager.project.configuration_management_rule_required_rules_present
  service: resourcemanager
  resource: project
  requirement: Configuration Management Rule Required Rules Present
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Project Configuration Management Rules are Defined
  rationale: Defining configuration management rules for GCP projects is crucial to ensure consistent security and compliance across cloud resources. Without these rules, projects may lack enforced security policies, leading to potential vulnerabilities and compliance failures. This is particularly important for meeting regulatory requirements and preventing unauthorized access or misconfigurations.
  description: This rule checks that all GCP projects have specific configuration management rules applied, ensuring that security policies are consistently enforced across the cloud environment. Verify that configuration management tools such as Terraform or Ansible are used to maintain desired state configurations. Remediation involves implementing or updating these tools to include rules for access control, logging, and encryption settings.
  references:
  - https://cloud.google.com/resource-manager/docs/creating-managing-projects
  - https://cloud.google.com/docs/terraform/get-started-with-terraform
  - https://cloud.google.com/security/compliance
  - CIS GCP Benchmark v1.2.0 - 1.1 Ensure that there is no 'default' network
  - 'NIST SP 800-53: CM-2 Configuration Management'
  - https://cloud.google.com/architecture/security-foundations
- rule_id: gcp.resourcemanager.project.configuration_management_rule_rules_enabled
  service: resourcemanager
  resource: project
  requirement: Configuration Management Rule Rules Enabled
  scope: resourcemanager.project.configuration_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: low
  title: Ensure Configuration Management Rules are Enabled for Projects
  rationale: Enabling configuration management rules helps maintain consistent and secure project settings across GCP environments. This reduces the risk of misconfigurations that can lead to unauthorized access, data breaches, and non-compliance with standards such as NIST and ISO 27001. Consistent configuration management is essential for meeting regulatory requirements and protecting sensitive data.
  description: This rule verifies that configuration management rules are enabled for all GCP projects, ensuring that project settings adhere to security best practices. Administrators should use tools like Google Cloud's Policy Analyzer to assess and enforce configuration policies. To remediate, review the configuration rules in the Google Cloud Console and enable required settings to align with organizational security policies.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/docs/ci-cd/security-best-practices
  - CIS Google Cloud Platform Foundation Benchmark
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.resourcemanager.project.governance_config_rule_remediation_targets_bound
  service: resourcemanager
  resource: project
  requirement: Governance Config Rule Remediation Targets Bound
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enforce Governance Config Rule Remediation Targets
  rationale: Binding governance config rule remediation targets helps ensure that security incidents are promptly addressed, reducing the risk of data breaches and non-compliance with regulatory frameworks. This practice supports maintaining a robust security posture by ensuring that all remediation activities are tracked and executed as part of a structured governance process.
  description: This rule checks whether projects in GCP have specific remediation targets set for governance configuration rules. Having defined remediation targets ensures that any non-compliance issues are automatically assigned to responsible parties for resolution. Verify by reviewing project settings in the Google Cloud Console under IAM & Admin to ensure appropriate roles and responsibilities are assigned. To remediate, configure the Cloud Resource Manager to include explicit governance remediation targets by updating IAM policies and setting clear responsibility chains.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
  - https://cloud.google.com/security/compliance
- rule_id: gcp.resourcemanager.project.governance_config_rule_required_rules_present
  service: resourcemanager
  resource: project
  requirement: Governance Config Rule Required Rules Present
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Required Governance Config Rules Exist for Projects
  rationale: Having essential governance configurations in place is crucial for maintaining a consistent security posture across GCP projects. Without these rules, projects may deviate from organizational security policies, leading to increased risk of unauthorized access, data breaches, and non-compliance with industry standards such as ISO 27001 and SOC2.
  description: This rule verifies that all required governance configuration rules are present in GCP projects to ensure adherence to security and compliance policies. It checks for predefined rules that enforce security settings, such as identity and access management policies, logging configurations, and data protection measures. To remediate, review the project settings and ensure that the necessary governance rules are configured according to your organization's security baseline.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/resource-manager/docs/creating-managing-projects
- rule_id: gcp.resourcemanager.project.governance_config_rule_rules_enabled
  service: resourcemanager
  resource: project
  requirement: Governance Config Rule Rules Enabled
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Governance Config Rules Are Enabled for Projects
  rationale: Enabling governance config rules helps organizations maintain compliance with security policies and frameworks by automatically monitoring and managing cloud resources. This reduces the risk of misconfigurations that could lead to data breaches or unauthorized access, ensuring that projects adhere to security best practices and regulatory requirements.
  description: This rule checks whether governance configuration rules are enabled for projects within Google Cloud Platform. It requires users to configure specific rules that monitor and enforce security policies, ensuring that all projects comply with organizational governance standards. To verify, navigate to the Google Cloud Console, access the 'Security Command Center', and ensure that 'Security Health Analytics' findings are active. Remediation involves enabling these rules through the 'Security Command Center' to automate compliance monitoring.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-security-health-analytics-overview
  - https://cloud.google.com/security-command-center/docs/how-to-configure-security-health-analytics
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0 - 1.1
  - NIST SP 800-53 Rev. 5 - CA-7 Continuous Monitoring
  - ISO/IEC 27001:2013 - A.12.1.2 Change Management
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.resourcemanager.project.governance_editors_rbac_least_privilege
  service: resourcemanager
  resource: project
  requirement: Governance Editors RBAC Least Privilege
  scope: resourcemanager.project.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Editors Use Least Privilege in GCP Projects
  rationale: Over-privileged accounts increase the risk of accidental or malicious changes that can compromise critical resources or data. Assigning the least privilege necessary for governance editors reduces the attack surface and potential for insider threats, helping to maintain compliance with regulatory frameworks such as PCI-DSS and ISO 27001.
  description: This rule checks for the assignment of the 'Editor' role within GCP projects to ensure it is granted only to users who truly require it for governance purposes. It verifies that no unnecessary permissions are granted, adhering to the principle of least privilege. Remediation involves reviewing role assignments and removing or adjusting permissions to align with actual job responsibilities, ensuring that only necessary access is granted.
  references:
  - https://cloud.google.com/resource-manager/docs/access-control-proj
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/document_library
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.resourcemanager.project.governance_no_public_admin_roles
  service: resourcemanager
  resource: project
  requirement: Governance No Public Admin Roles
  scope: resourcemanager.project.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Admin Role Assignments in GCP Projects
  rationale: Allowing public admin roles poses significant security risks as it grants unrestricted access to critical resources, potentially leading to data breaches, unauthorized resource modifications, and compliance violations. Ensuring that admin roles are not publicly accessible mitigates these risks and aligns with regulatory standards such as ISO 27001 and SOC 2, which mandate strict access control measures.
  description: This rule checks for any GCP project where administrative roles are assigned to public identities, such as 'allUsers' or 'allAuthenticatedUsers'. It ensures these roles are not publicly accessible by reviewing IAM policies. To remediate, audit IAM policies and remove any public admin role bindings, replacing them with specific user or group assignments as necessary. Verify changes by reviewing the IAM policy bindings for each project.
  references:
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.resourcemanager.project.governance_no_wildcard_admin_actions
  service: resourcemanager
  resource: project
  requirement: Governance No Wildcard Admin Actions
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: critical
  title: Prevent Wildcard Admin Permissions in GCP Projects
  rationale: Allowing wildcard admin permissions poses significant security risks as it grants broad and unrestricted access to all resources, potentially leading to unmonitored and unauthorized activities. This exposure increases the risk of data breaches, loss of data integrity, and non-compliance with regulatory standards such as PCI-DSS and ISO 27001. Ensuring specific, least-privilege access aligns with security best practices and reduces potential attack surfaces.
  description: This rule checks for IAM policies in GCP projects that grant 'roles/*admin' or similar wildcard permissions, which can lead to excessive privileges. Verifying IAM policies for such configurations helps in enforcing the principle of least privilege. Remediation involves auditing and modifying IAM policies to replace wildcards with specific role assignments that limit access rights to necessary resources only.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/docs/security-overview
- rule_id: gcp.resourcemanager.project.governance_required_guardrail_policies_attached
  service: resourcemanager
  resource: project
  requirement: Governance Required Guardrail Policies Attached
  scope: resourcemanager.project.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Ensure Governance Guardrail Policies are Attached to Projects
  rationale: Attaching governance guardrail policies to projects is crucial for maintaining consistent security and compliance standards across your organization. This reduces the risk of misconfigurations and unauthorized access, thereby minimizing potential data breaches and ensuring compliance with regulatory frameworks such as PCI-DSS and HIPAA.
  description: This rule checks if required governance guardrail policies, such as IAM policies and organization policies, are attached to all active projects. These policies enforce security controls and compliance requirements, ensuring that projects adhere to organizational standards. To verify, review the IAM policy bindings and organization policies for each project and ensure they include necessary guardrails. Remediation involves attaching any missing policies through the GCP Console or using the Cloud SDK.
  references:
  - https://cloud.google.com/resource-manager/docs/organization-policy/overview
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0, Section 1.1
  - NIST SP 800-53 Rev. 5
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.resourcemanager.project.governance_scp_mandatory_block_disabling_guardrail_services
  service: resourcemanager
  resource: project
  requirement: Governance Scp Mandatory Block Disabling Guardrail Services
  scope: resourcemanager.project.security
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: low
  title: Enforce Mandatory Guardrail Services for Project Resilience
  rationale: Disabling essential guardrail services can lead to critical security gaps, increasing the risk of unauthorized access and data breaches. Ensuring these services remain enabled supports compliance with regulatory requirements and enhances the overall resilience of the cloud environment against potential threats and data loss scenarios.
  description: This rule checks that essential guardrail services within Google Cloud projects are not disabled, ensuring consistent security posture. Verify that IAM policies are applied to prevent disabling services like Cloud Logging, Cloud Monitoring, and Security Command Center. Remediation involves setting policies in the Resource Manager to enforce these guardrails and regularly auditing these configurations to comply with organizational governance policies.
  references:
  - https://cloud.google.com/resource-manager/docs
  - https://cloud.google.com/security-command-center/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.resourcemanager.project.governance_scp_mandatory_deny_iam_star_admin
  service: resourcemanager
  resource: project
  requirement: Governance Scp Mandatory Deny IAM Star Admin
  scope: resourcemanager.project.security
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent IAM Roles with Excessive Permissions
  rationale: Allowing IAM roles with wildcard permissions ('*') poses a significant security risk as it grants overly broad access, potentially leading to unauthorized data exposure and manipulation. This violates the principle of least privilege and can result in non-compliance with regulations like GDPR, PCI-DSS, and HIPAA, which require strict access controls to protect sensitive information.
  description: This rule ensures that IAM roles do not incorporate wildcard permissions at the project level, which can give users excessive access rights. It checks for roles that include 'roles/*' or similar patterns that allow broad administrative access. To remediate, review and restrict such roles to specific, necessary permissions only. Use custom roles to grant precise permissions and periodically audit roles for compliance with security policies.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/security/compliance/cis-google-cloud-platform-foundations-benchmark
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/iam/docs/creating-custom-roles
  - https://cloud.google.com/security/best-practices/identity
- rule_id: gcp.resourcemanager.project.governance_scp_mandatory_require_mfa_for_console_w_supported
  service: resourcemanager
  resource: project
  requirement: Governance Scp Mandatory Require MFA For Console W Supported
  scope: resourcemanager.project.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce MFA for GCP Console Access
  rationale: Requiring multi-factor authentication (MFA) for GCP console access significantly reduces the risk of unauthorized access, protecting sensitive data and operations. It mitigates threats such as credential theft and account takeover, which can lead to data breaches and compliance violations. Regulatory standards often mandate strong authentication mechanisms to safeguard user accounts.
  description: This rule ensures that all GCP console users are required to use MFA, which adds an additional layer of security beyond just username and password. Verify that all user accounts have MFA enabled through the Google Cloud Console under IAM & Admin settings. To remediate, administrators should enforce MFA by configuring identity and access management policies to include MFA as a requirement for all users. This can be achieved through the Google Workspace Admin console by enabling 2-Step Verification.
  references:
  - https://cloud.google.com/iam/docs/using-mfa
  - https://cloud.google.com/identity/security-best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.resourcemanager.project.governance_scp_no_allow_star_on_star_resources
  service: resourcemanager
  resource: project
  requirement: Governance Scp No Allow Star On Star Resources
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Prevent Broad Permissions with Allow * on All Resources
  rationale: Allowing wildcard permissions with '*' on all resources can lead to significant security risks, such as unauthorized data access and resource manipulation. This practice can violate compliance requirements by not adhering to the principle of least privilege, increasing the attack surface and potential for data breaches.
  description: This rule checks for IAM policies within a project that grant overly permissive permissions using 'Allow *' on all resources. To mitigate risks, audit and restrict IAM policies to use specific resources and actions. Remediation involves modifying IAM policies to replace wildcards with precise resource and action specifications, ensuring adherence to the principle of least privilege.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/iam/docs/conditions-overview
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.isc2.org/Research/Enterprise-Security-Architecture
- rule_id: gcp.resourcemanager.project.governance_security_services_mandatory_enabled
  service: resourcemanager
  resource: project
  requirement: Governance Security Services Mandatory Enabled
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Services are Enabled on GCP Projects
  rationale: Enabling mandatory security services on GCP projects ensures that essential security measures are automatically applied, reducing the risk of misconfigurations and security breaches. This practice supports compliance with frameworks like ISO 27001 and PCI-DSS by maintaining consistent security monitoring and governance across all projects.
  description: This rule checks for the activation of key security services such as Security Command Center, Cloud Audit Logs, and IAM policy analysis on GCP projects. To verify, ensure these services are enabled in the Google Cloud Console or via gcloud CLI. Remediation involves enabling these services under the project's settings, ensuring continuous security monitoring and threat detection capabilities.
  references:
  - https://cloud.google.com/security-command-center/docs
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/iam/docs/managing-policies
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.resourcemanager.project.governance_versioning_and_change_audit_enabled
  service: resourcemanager
  resource: project
  requirement: Governance Versioning And Change Audit Enabled
  scope: resourcemanager.project.audit_logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Governance and Audit for Project Resource Changes
  rationale: Enabling governance versioning and change audit is critical for ensuring accountability and traceability of modifications to GCP projects. This helps in identifying unauthorized changes and supports compliance with regulatory frameworks like SOC 2 and ISO 27001, which require detailed logging of access and modifications. It also mitigates risks related to insider threats and misconfigurations that could lead to data breaches.
  description: This rule checks whether audit logging is enabled for changes in GCP projects. Specifically, it verifies that Data Access and Admin Activity logging is enabled in Cloud Audit Logs for the Resource Manager service. To verify, ensure that the 'Audit Logs' settings in 'IAM & Admin' are configured to capture all necessary events. Remediation involves enabling the appropriate log types through the GCP Console under the 'Audit Logs' section of the project's IAM settings.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/audit-logging
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.resourcemanager.project.part_of_organizations
  service: resourcemanager
  resource: project
  requirement: Part Of Organizations
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Projects Are Part of an Organization
  rationale: Ensuring that projects are part of an organization is crucial for centralized management and enforcement of security policies. Without organizational association, projects may bypass inherited security controls, increasing the risk of misconfigurations and unauthorized access. This setup also aids in meeting compliance requirements by providing a clear hierarchy and control over resources, supporting frameworks like ISO 27001 and SOC 2.
  description: This rule checks if a Google Cloud project is associated with an organization. Projects outside an organization cannot benefit from the centralized policies and IAM roles that an organization provides, which may lead to inconsistent security postures. To verify, navigate to the Cloud Console, ensure that the project is listed under an organization. If not, move the project into an organization using the Resource Manager API or Console. This will enforce consistent security policies and access controls across all projects.
  references:
  - https://cloud.google.com/resource-manager/docs/creating-managing-organization
  - https://cloud.google.com/resource-manager/docs/migrating-projects-billing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/iam/docs/overview
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.resourcemanager.project.recorder_all_regions_enabled
  service: resourcemanager
  resource: project
  requirement: Recorder All Regions Enabled
  scope: resourcemanager.project.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Cloud Asset Inventory Enabled in All Regions
  rationale: Enabling Cloud Asset Inventory across all regions is crucial for comprehensive visibility into your GCP assets, ensuring that no resources are left unmonitored. This enhances your ability to manage risk, meet compliance requirements, and quickly respond to security incidents. Without complete regional monitoring, you may miss critical asset changes that could lead to unauthorized access or data breaches.
  description: This rule verifies that the Cloud Asset Inventory is enabled in all GCP regions for the specified project. To ensure all assets are monitored, configure the Cloud Asset Inventory API to record asset data from every region in your organization. You can verify this by checking the 'resourceTypes' field for comprehensive regional coverage in your asset metadata exports. Remediation involves setting up or updating the Cloud Asset Inventory API to include all regions, ensuring your asset management policies and tools are configured accordingly.
  references:
  - https://cloud.google.com/asset-inventory/docs/overview
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.secretmanager.secret.compliance
  service: secretmanager
  resource: secret
  requirement: Compliance
  scope: secretmanager.secret.compliance
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Ensure Secret Manager Secrets Follow Compliance Standards
  rationale: Secrets stored in GCP Secret Manager must comply with industry standards to prevent unauthorized access and data breaches. Non-compliance can lead to security incidents that compromise sensitive data, resulting in financial loss, reputational damage, and regulatory penalties. Compliance with frameworks like NIST and PCI-DSS ensures robust security practices are applied to manage and protect secrets.
  description: This rule checks that secrets in GCP Secret Manager are configured in accordance with compliance requirements. It verifies settings such as access policies, encryption methods, and audit logging to ensure they meet standards like NIST and PCI-DSS. To remediate non-compliance, ensure secrets are encrypted using Google-managed encryption keys and that IAM policies restrict access on a need-to-know basis. Regularly audit access logs and adhere to key rotation policies.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/secret-manager/docs/access-control
- rule_id: gcp.secretmanager.secret.compliant_patching
  service: secretmanager
  resource: secret
  requirement: Compliant Patching
  scope: secretmanager.secret.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Secret Manager Secrets Have Up-to-Date Patches
  rationale: Regular patching of Secret Manager secrets is crucial to prevent unauthorized access and protect sensitive information. Unpatched secrets can be exploited by attackers to gain access to confidential data, leading to potential data breaches, financial loss, and reputational damage. Compliance with industry regulations like PCI-DSS and SOC2 often requires maintaining up-to-date security patches as part of a robust security posture.
  description: This rule checks whether secrets stored in Google Cloud Secret Manager are maintained with the latest security patches. It involves verifying that the underlying infrastructure and access configurations are regularly updated to mitigate vulnerabilities. Remediation includes scheduling regular updates, enabling automatic updates where possible, and auditing secret access policies to ensure they conform with security best practices.
  references:
  - https://cloud.google.com/secret-manager/docs/security
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.secretmanager.secret.manager_enabled
  service: secretmanager
  resource: secret
  requirement: Manager Enabled
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: medium
  title: Enable Secret Manager for Secure Key Storage
  rationale: Enabling Secret Manager ensures that sensitive information such as API keys, passwords, and certificates are stored securely. This reduces the risk of unauthorized access and data breaches. It is crucial for meeting regulatory requirements like GDPR and PCI-DSS, as well as protecting business-critical data against insider threats and external attacks.
  description: This check verifies that the Secret Manager service is enabled for all secrets within GCP projects. It is important to configure the Secret Manager to manage and access secrets securely by applying appropriate IAM policies. Remediation involves enabling Secret Manager for any secret where it is disabled and setting strict access controls. To verify, navigate to the Secret Manager in the GCP Console and ensure it's enabled and properly configured for all secrets.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.secretmanager.secret.patching
  service: secretmanager
  resource: secret
  requirement: Patching
  scope: secretmanager.secret.patch_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Secrets in Secret Manager Are Regularly Reviewed and Updated
  rationale: Regularly reviewing and updating secrets in Secret Manager mitigates the risk of unauthorized access due to stale or compromised credentials, which can lead to data breaches and non-compliance with security standards such as PCI-DSS and HIPAA. Keeping secrets up-to-date ensures that only current, authorized applications and users have access, reducing the attack surface.
  description: This rule checks whether secrets stored in GCP Secret Manager are subject to regular review and updates. Ensure that secrets are rotated and updated according to a defined schedule and security policy, which includes patching known vulnerabilities in associated components. Use automated workflows to manage secret lifecycles and verify compliance with patch management policies through audit logs and monitoring tools.
  references:
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-compliance-requirements/
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/security/overview/white-paper
- rule_id: gcp.secretmanager.secret.secret_rotation_enabled
  service: secretmanager
  resource: secret
  requirement: Secret Rotation Enabled
  scope: secretmanager.secret.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Rotation is Enabled for GCP Secrets
  rationale: Enabling secret rotation mitigates the risk of credential exposure due to long-lived secrets, reducing the attack surface for unauthorized access. It supports compliance with security standards and regulations that require regular credential updates, thereby protecting sensitive data and maintaining system integrity.
  description: This rule checks if secret rotation is enabled for secrets stored in Google Cloud Secret Manager. Secret rotation ensures that secrets are regularly updated, minimizing the risk associated with static, long-term credentials. To verify, navigate to the Secret Manager in the GCP Console, check the 'Rotation' tab for each secret, and ensure a rotation schedule is configured. Remediation involves setting a rotation schedule and automating it using cloud functions or other automation tools.
  references:
  - https://cloud.google.com/secret-manager/docs/rotation
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
- rule_id: gcp.secretmanager.secret.secrets_access_rbac_least_privilege
  service: secretmanager
  resource: secret
  requirement: Secrets Access RBAC Least Privilege
  scope: secretmanager.secret.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict Secret Manager Access with RBAC Least Privilege
  rationale: Implementing least privilege for accessing secrets in GCP minimizes the risk of unauthorized access to sensitive information, such as API keys and passwords. This approach reduces potential attack vectors, aligns with regulatory requirements like PCI-DSS and HIPAA, and safeguards against insider threats by ensuring only necessary permissions are granted.
  description: This rule verifies that access to secrets in Secret Manager is limited to the minimum set of users, roles, or service accounts necessary to perform specific tasks. It checks IAM policies for adherence to least privilege principles, ensuring only essential roles like 'roles/secretmanager.secretAccessor' are assigned. Remediation involves auditing current IAM bindings and removing any excessive permissions, maintaining a strict access control policy.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/docs/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.secretmanager.secret.secrets_alias_points_to_active_key
  service: secretmanager
  resource: secret
  requirement: Secrets Alias Points To Active Key
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Aliases Point to Active Keys
  rationale: When a secret alias does not point to an active key, it can lead to failures in accessing the secret data, which can disrupt application functionality and lead to potential security vulnerabilities. Ensuring that aliases point to active keys minimizes the risk of data breaches and supports compliance with data protection regulations by maintaining the integrity of your secret management system.
  description: This check verifies that all secret aliases in Secret Manager point to active versions of the secret. If an alias points to an inactive or deleted key, secret access may fail, potentially causing application downtime or security exposure. To remediate, update the alias to reference an active key version. This involves ensuring that the key version is enabled and properly configured to be accessed by necessary services.
  references:
  - https://cloud.google.com/secret-manager/docs/managing-secrets
  - https://github.com/GoogleCloudPlatform/inspec-gcp-cis-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.secretmanager.secret.secrets_ca_access_rbac_least_privilege
  service: secretmanager
  resource: secret
  requirement: Secrets Ca Access RBAC Least Privilege
  scope: secretmanager.secret.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Secret Manager Access
  rationale: Implementing least privilege for accessing secrets in Secret Manager minimizes the risk of unauthorized access, which could lead to data breaches and non-compliance with regulations such as GDPR and HIPAA. Inadequate access controls can expose sensitive information, resulting in financial loss and reputational damage.
  description: This rule verifies that access to secrets in GCP Secret Manager is granted strictly on a need-to-know basis. It checks for excessive permissions and ensures that only specific roles are assigned for accessing secrets. To remediate, review and adjust IAM policies to assign minimal permissions required for users or service accounts. Use predefined roles like 'roles/secretmanager.secretAccessor' instead of overly broad roles.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
- rule_id: gcp.secretmanager.secret.secrets_ca_key_in_hsm_where_supported
  service: secretmanager
  resource: secret
  requirement: Secrets Ca Key In Hsm Where Supported
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Use HSM-backed Keys in Supported Regions
  rationale: Utilizing HSM-backed keys for secret encryption enhances security by providing stronger physical and logical protections, reducing the risk of key extraction or misuse. This approach supports compliance with industry regulations such as PCI-DSS and ISO 27001, which mandate stringent controls on cryptographic key management. Organizations can mitigate potential data breaches and unauthorized access by ensuring key material is securely managed within a Hardware Security Module (HSM).
  description: This rule checks if secrets in Google Cloud Secret Manager are encrypted using keys stored in an HSM, where such support is available. To verify, inspect the key configuration associated with each secret to ensure it specifies an HSM-backed key. If not, reconfigure the secret to use a Cloud KMS key that is managed by a supported HSM. This setup ensures that cryptographic operations are performed in a highly secure environment, providing enhanced protection against key compromise.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/kms/docs/hsm
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.secretmanager.secret.secrets_crl_or_ocsp_configured
  service: secretmanager
  resource: secret
  requirement: Secrets Crl Or Ocsp Configured
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure OCSP or CRL is Configured for GCP Secrets
  rationale: Configuring Online Certificate Status Protocol (OCSP) or Certificate Revocation Lists (CRL) is crucial for verifying the validity of certificates used in secret management. This configuration helps prevent the use of compromised or expired certificates, reducing the risk of unauthorized access to sensitive data. It is also essential for compliance with security regulations like PCI-DSS and ISO 27001, which require robust certificate management practices.
  description: This rule checks if the GCP Secret Manager is configured to use OCSP or CRL to validate the status of certificates in use. Without this configuration, there is a risk that expired or revoked certificates could be used, potentially leading to unauthorized access to secrets. To verify, ensure that secret manager settings include OCSP or CRL endpoints. Remediation involves updating the secret manager configuration to include these settings, which can be done via the GCP Console or CLI.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://csrc.nist.gov/publications/detail/sp/800-57/part-1/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.sans.org/reading-room/whitepapers/bestprac/understanding-online-certificate-status-protocol-ocsp-33904
- rule_id: gcp.secretmanager.secret.secrets_key_has_alias
  service: secretmanager
  resource: secret
  requirement: Secrets Key Has Alias
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Keys in Secret Manager Have Aliases
  rationale: Assigning aliases to secret keys in GCP Secret Manager is crucial for maintaining organized and easily identifiable resources. Having aliases reduces the risk of mismanagement and accidental exposure of sensitive data, which can lead to breaches and non-compliance with standards such as NIST, PCI-DSS, and ISO 27001. Proper aliasing facilitates effective secret rotation and auditing processes.
  description: This rule checks that each secret key in GCP Secret Manager has an assigned alias. An alias provides a human-readable identifier, improving the manageability and traceability of secrets. To remediate, ensure that all secret keys are configured with appropriate aliases via the GCP Console or gcloud CLI. This involves navigating to the Secret Manager, selecting the secret, and updating its settings to include a descriptive alias.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/secret-manager/docs/best-practices
- rule_id: gcp.secretmanager.secret.secrets_key_length_minimum
  service: secretmanager
  resource: secret
  requirement: Secrets Key Length Minimum
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secret Key Length Meets Minimum Requirements
  rationale: Shorter key lengths can lead to weaker encryption, increasing the risk of unauthorized access to sensitive information stored in Secret Manager. This can result in data breaches, financial loss, and non-compliance with security regulations like NIST and PCI-DSS, which mandate strong cryptographic controls.
  description: This rule checks whether the keys used for encrypting secrets in Secret Manager meet a minimum length requirement of 256 bits. Verifying key length ensures that your cryptographic practices are robust against modern attack vectors. To remediate, configure key management settings to use keys of at least 256 bits. Review key policies and update any existing keys that do not meet this standard.
  references:
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/key-rotation
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.secretmanager.secret.secrets_kms_constraints_present
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Constraints Present
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Enforce KMS Constraints for Secret Manager Secrets
  rationale: Using Customer Managed Encryption Keys (CMEK) for managing secrets in GCP ensures that you maintain control over the encryption keys, offering enhanced security and compliance. Without KMS constraints, there is a risk of unauthorized access or data exposure, which can lead to significant business losses, reputational damage, and non-compliance with data protection regulations.
  description: This rule checks if secrets stored in Secret Manager have Key Management System (KMS) constraints present, ensuring they are encrypted using CMEK. To verify, review the Secret Manager configurations and ensure that a valid KMS key is specified for each secret. Remediation involves updating the secret configuration to include a specific KMS key, thereby enforcing the use of customer-managed encryption.
  references:
  - https://cloud.google.com/secret-manager/docs/encrypting-secrets
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.nist.gov/document/nist-special-publication-800-57-part-1-revision-5
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.secretmanager.secret.secrets_kms_encryption_enabled
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Encryption Enabled
  scope: secretmanager.secret.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enable KMS Encryption for Secret Manager Secrets
  rationale: Encrypting secrets with Customer-Managed Encryption Keys (CMEK) enhances data protection by allowing more control over the cryptographic keys used. This reduces the risk of unauthorized data access, supports compliance with regulations like PCI-DSS and HIPAA, and mitigates threats such as data breaches and insider threats by ensuring secrets are not left vulnerable.
  description: This rule checks if secrets stored in Google Cloud Secret Manager are encrypted using customer-managed keys from Cloud Key Management Service (KMS). To verify, ensure that the encryption field for each secret specifies a KMS key. If not configured, enable KMS encryption by updating the secret's configuration to use a designated KMS key. This provides enhanced security control over the encryption process and key lifecycle management.
  references:
  - https://cloud.google.com/secret-manager/docs/encrypting-secrets
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/kms/docs/cmek
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.secretmanager.secret.secrets_kms_grantee_principal_valid
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Grantee Principal Valid
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Valid Principals for KMS Grantees in Secret Manager
  rationale: Managing valid principals for KMS grantees in Secret Manager is critical to ensure that only authorized entities can access and decrypt sensitive secrets. Misconfigured or invalid principals could lead to unauthorized access, potentially resulting in data breaches, compliance violations, and loss of customer trust. Adhering to this requirement helps meet compliance frameworks such as PCI-DSS and ISO 27001.
  description: This rule checks whether the principals granted access to the KMS keys associated with secrets in Secret Manager are valid and correctly configured. It verifies that the IAM policies for KMS keys do not contain obsolete or unauthorized principals. To remediate, audit all IAM policies attached to your KMS keys and ensure that only authorized and valid principals are listed. Regularly review and update these policies to align with the principle of least privilege.
  references:
  - https://cloud.google.com/secret-manager/docs/securing-secrets
  - https://cloud.google.com/security/compliance/cis-gcp-benchmark
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.secretmanager.secret.secrets_kms_key_configured
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Key Configured
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Use Customer-Managed KMS Keys
  rationale: Configuring secrets with customer-managed encryption keys (CMEK) in Secret Manager enhances control over data encryption and decryption processes. This reduces the risk of unauthorized access through key compromise and aligns with compliance mandates like PCI-DSS and HIPAA, which require stringent data protection measures. Using CMEK allows organizations to rotate keys and enforce key access policies, mitigating threats such as data breaches and unauthorized data exposure.
  description: This rule checks if secrets in Secret Manager are encrypted with customer-managed keys rather than Google-managed keys. To verify, ensure that each secret specifies a KMS key in its configuration. If a secret is not configured with a CMEK, update its settings to associate it with a customer-managed key in Cloud KMS. This involves creating or selecting an existing key ring and key, and updating the secret to reference this key, thereby enhancing the security posture of sensitive data.
  references:
  - https://cloud.google.com/secret-manager/docs/encryption
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/kms/docs/cmek
- rule_id: gcp.secretmanager.secret.secrets_kms_lessthan_wildcard_permissions
  service: secretmanager
  resource: secret
  requirement: Secrets KMS Lessthan Wildcard Permissions
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets KMS Key Permissions Do Not Use Wildcards
  rationale: Using wildcard permissions for KMS keys in Secret Manager can lead to unauthorized access, increasing the risk of data breaches. Precise permissions help ensure that only authorized users and services can decrypt and access sensitive information, supporting compliance with standards such as PCI-DSS and GDPR.
  description: This rule checks for the use of wildcard permissions in the Key Management Service (KMS) policies associated with secrets in Secret Manager. Wildcard permissions may unintentionally grant more access than intended, compromising secret confidentiality. Verify KMS policies linked to secrets do not use wildcards and specify exact permissions. Remediate by updating policies to include specific roles and members, reducing exposure to unauthorized users.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/kms/docs/reference/permissions-and-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/pci-dss
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
- rule_id: gcp.secretmanager.secret.secrets_no_plaintext_exposed_in_policy_or_tags
  service: secretmanager
  resource: secret
  requirement: Secrets No Plaintext Exposed In Policy Or Tags
  scope: secretmanager.secret.policy_management
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Plaintext Exposure in Secret Manager Policies
  rationale: Exposing plaintext secrets in policies or tags can lead to unauthorized access and data breaches, potentially compromising sensitive information and violating compliance standards such as PCI-DSS and HIPAA. This risk is amplified if these secrets are accidentally leaked or accessed by malicious actors, leading to significant business impact and loss of trust.
  description: This rule checks for any instances where plaintext secrets are included within IAM policies or tags associated with GCP Secret Manager resources. Organizations should ensure that all secrets are stored securely and not exposed in configuration metadata. To remediate, review IAM policies and tags for sensitive information and remove any plaintext secrets, ensuring that access to secrets is managed through proper roles and permissions.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-security-rule/
  - https://cloud.google.com/security/best-practices
  - https://cloud.google.com/secret-manager/docs/best-practices
- rule_id: gcp.secretmanager.secret.secrets_not_expired
  service: secretmanager
  resource: secret
  requirement: Secrets Not Expired
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets in Secret Manager Have Expiry Dates Set
  rationale: Secrets without expiration dates may remain active indefinitely, increasing the risk of unauthorized access if compromised. Implementing expiration dates helps mitigate potential security breaches by limiting the lifespan of exposed secrets. Additionally, it aligns with compliance frameworks that mandate regular key rotation and lifecycle management to protect sensitive data.
  description: This rule checks that all secrets stored in Google Cloud Secret Manager have an expiration date configured. Secrets should be set to expire to ensure they do not remain valid longer than necessary, reducing potential exposure. Administrators can set an expiration date during the creation of a secret or by updating existing secrets using the Google Cloud Console, CLI, or API. It is recommended to regularly review and update secrets and their expiration dates to adhere to security policies.
  references:
  - https://cloud.google.com/secret-manager/docs/set-secret-expiration
  - https://cloud.google.com/secret-manager/docs/managing-secrets
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
- rule_id: gcp.secretmanager.secret.secrets_path_length_constraints_set
  service: secretmanager
  resource: secret
  requirement: Secrets Path Length Constraints Set
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Path Length Meets Security Standards
  rationale: Enforcing a minimum path length for secrets in Secret Manager helps prevent unauthorized access by making it harder for attackers to guess or brute force secret names. This is crucial for protecting sensitive information, which if exposed, could lead to data breaches and significant financial and reputational damage. Compliance with standards such as PCI-DSS and ISO 27001 often requires stringent access controls and secure storage of sensitive data.
  description: This rule checks that all secret paths in Google Cloud's Secret Manager meet a predefined length constraint, ensuring they are not easily discoverable. To verify, review and configure secret names to be at least 10 characters long, including a mix of alphanumeric and special characters. Remediation involves renaming existing secrets and enforcing naming policies for new ones. Use IAM policies to restrict access and regularly audit secret usage and access logs.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.secretmanager.secret.secrets_rotation_configured_where_applicable
  service: secretmanager
  resource: secret
  requirement: Secrets Rotation Configured Where Applicable
  scope: secretmanager.secret.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Have Rotation Configured in GCP Secret Manager
  rationale: Regular rotation of secrets mitigates the risk of unauthorized access due to credential exposure. It helps in reducing the potential impact of leaked secrets by limiting the time window a compromised secret can be exploited. Compliance with standards like PCI-DSS and ISO 27001 often mandates regular key and secret rotation to protect sensitive data.
  description: This rule checks that secrets stored in GCP Secret Manager have a rotation period configured, which is crucial for maintaining security hygiene. The recommended practice is to set a rotation schedule, typically every 90 days, to automatically generate new secret versions. Verification involves reviewing the 'rotation' settings in the Secret Manager and ensuring a defined schedule. To remediate, configure the 'rotation' attribute with a time period using the Google Cloud Console, gcloud CLI, or Terraform.
  references:
  - https://cloud.google.com/secret-manager/docs/managing-secrets#managing_secret_versions
  - https://cloud.google.com/secret-manager/docs/rotation
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r4.pdf
- rule_id: gcp.secretmanager.secret.secrets_rotation_enabled_for_rotatable_secrets
  service: secretmanager
  resource: secret
  requirement: Secrets Rotation Enabled For Rotatable Secrets
  scope: secretmanager.secret.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Rotation is Enabled for All Rotatable Secrets
  rationale: Enabling secrets rotation for rotatable secrets in Secret Manager reduces the risk of unauthorized access by ensuring that secrets are regularly updated and old versions are invalidated. This practice mitigates potential threats from compromised secrets and aligns with compliance requirements for maintaining strong security controls over sensitive information.
  description: This rule checks whether secrets in the Secret Manager have rotation enabled, which is crucial for maintaining the security and integrity of sensitive data stored in GCP. To verify, ensure that each secret has an associated rotation schedule configured. Remediation involves setting up a rotation policy that automatically updates secrets at regular intervals, using the Secret Manager API or Google Cloud Console. This configuration helps in minimizing the risks associated with static secret values.
  references:
  - https://cloud.google.com/secret-manager/docs/rotation
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - NIST SP 800-57 Part 1
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/secret-manager/docs/best-practices
  - https://cloud.google.com/security/key-management
- rule_id: gcp.secretmanager.secret.secrets_secure_string_type_for_sensitive
  service: secretmanager
  resource: secret
  requirement: Secrets Secure String Type For Sensitive
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Use Secure String Type for Sensitive Data
  rationale: Storing sensitive data in an insecure format can lead to unauthorized access and data breaches. Ensuring that secrets use a secure string type protects against exposure of sensitive information such as API keys and passwords. This practice is essential for meeting compliance requirements like PCI-DSS and HIPAA, which mandate robust data protection measures.
  description: This rule checks that all secrets in Google Cloud Secret Manager are stored using a secure string type, preventing plaintext exposure. Verify that secrets are encrypted both in transit and at rest, and ensure that access controls are properly configured to restrict unauthorized access. To remediate, update any secrets not using a secure string type to utilize GCP's native encryption mechanisms.
  references:
  - https://cloud.google.com/secret-manager/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://cloud.google.com/secret-manager/docs/best-practices
- rule_id: gcp.secretmanager.secret.secrets_trusted_issuer
  service: secretmanager
  resource: secret
  requirement: Secrets Trusted Issuer
  scope: secretmanager.secret.security
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Ensure Secrets Use Trusted Issuer for Authentication
  rationale: Using a trusted issuer for secret authentication ensures that only authorized entities can access sensitive data, reducing the risk of unauthorized data exposure or breaches. It helps maintain data integrity and confidentiality, which is crucial for regulatory compliance with frameworks like PCI-DSS and SOC2. Failure to implement this can lead to unauthorized data access, resulting in potential financial and reputational damage.
  description: This rule checks if secrets in the Secret Manager are configured to authenticate using a trusted issuer, such as Google-issued service account tokens or external identity providers that comply with enterprise security policies. Verify the configuration of each secret to ensure it uses a trusted issuer by examining IAM policies and binding settings. Remediate any non-compliance by updating IAM policies to restrict access to identities authenticated by trusted issuers, and regularly audit configurations to ensure adherence to security policies.
  references:
  - https://cloud.google.com/secret-manager/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/document_library
- rule_id: gcp.securitycenter.automation.vuln_automation_change_audit_logging_enabled
  service: securitycenter
  resource: automation
  requirement: Vuln Automation Change Audit Logging Enabled
  scope: securitycenter.automation.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Vulnerability Automation Change Audit Logging
  rationale: Audit logging for vulnerability automation changes is critical for maintaining accountability and transparency within your GCP environment. It helps in identifying unauthorized access and potential security breaches by recording changes made to vulnerability management settings. This is essential for compliance with frameworks like ISO 27001 and NIST, which require detailed logging and monitoring of security-related activities.
  description: This rule checks if audit logging is enabled for changes made to vulnerability automation settings within Google Cloud Security Command Center. To verify, ensure that logs are being generated and stored in Cloud Logging, capturing all modifications to vulnerability configurations. Remediation involves configuring audit logs for the Security Command Center project, specifically focusing on admin activity logs related to vulnerability automation changes. Enable these logs through the Cloud Console or gcloud CLI to ensure comprehensive monitoring.
  references:
  - https://cloud.google.com/security-command-center/docs/audit-logging
  - https://cloud.google.com/iam/docs/audit-logging-overview
  - 'CIS Google Cloud Platform Foundation v1.2.0: Section 4.3'
  - 'NIST SP 800-53 Rev. 5: AU-2 Audit Events'
  - ISO/IEC 27001:2013 A.12.4.1 Event Logging
  - https://cloud.google.com/logging/docs/audit
- rule_id: gcp.securitycenter.automation.vuln_automation_execution_roles_least_privilege
  service: securitycenter
  resource: automation
  requirement: Vuln Automation Execution Roles Least Privilege
  scope: securitycenter.automation.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for Vulnerability Automation Roles
  rationale: Assigning least privilege to vulnerability automation execution roles minimizes the risk of unauthorized access and potential exploitation. Over-privileged roles may lead to accidental or malicious data exposure or system compromise, posing a significant threat to sensitive data and compliance with standards such as PCI-DSS and ISO 27001.
  description: This check ensures that roles assigned to vulnerability automation processes in Google Cloud Security Command Center adhere to the principle of least privilege. Review and adjust IAM policies to grant only necessary permissions for automation execution. Verification can be done by auditing IAM policies and ensuring roles like 'roles/securitycenter.automationRunner' do not have extraneous permissions. Remediation involves modifying IAM bindings to remove unnecessary permissions.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-vulnerability-scanning
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/security-command-center/docs/iam-permissions#automation_runner
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.securitycenter.finding.center_enabled
  service: securitycenter
  resource: finding
  requirement: Center Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Command Center is Enabled
  rationale: Enabling Security Command Center (SCC) is crucial for maintaining visibility into your GCP resources' security posture. Without SCC, organizations may miss critical security findings, leading to potential vulnerabilities being exploited. This can result in data breaches, non-compliance with regulations such as PCI-DSS and ISO 27001, and financial and reputational damage.
  description: This rule checks whether the Security Command Center is enabled, ensuring continuous monitoring and alerting of security threats and vulnerabilities within GCP resources. To verify, check the SCC settings in your GCP console and ensure it's configured to monitor all relevant projects. Remediation involves enabling SCC through the GCP console and configuring it for comprehensive threat detection, following GCP's best practice guidelines.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart-security-command-center
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security-command-center/docs/concepts-overview
- rule_id: gcp.securitycenter.finding.center_is_enabled
  service: securitycenter
  resource: finding
  requirement: Center Is Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Command Center is Enabled
  rationale: Enabling Security Command Center is crucial for real-time threat detection and asset monitoring in GCP. It provides comprehensive insights into security posture, helping organizations identify vulnerabilities and threats before they escalate. Failing to enable this service can lead to undetected security misconfigurations and compliance violations, increasing the risk of security breaches and data loss.
  description: This rule checks if the Security Command Center is activated on your GCP account, as it is essential for centralized security visibility and management. Ensure that the Security Command Center is enabled by navigating to the GCP Console, selecting the Security Command Center service, and verifying that it is in the active state. If not enabled, activate it to start receiving critical security findings and insights. Regularly review and act on the findings reported to maintain a strong security posture.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-get-started
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/compliance
  - 'CIS GCP Benchmark: https://www.cisecurity.org/benchmark/google_cloud_computing_platform'
  - 'NIST SP 800-53: CA-7 Continuous Monitoring'
  - ISO 27001:2013 A.12.6.1 Management of technical vulnerabilities
- rule_id: gcp.securitycenter.finding.center_no_high_severity_findings
  service: securitycenter
  resource: finding
  requirement: Center No High Severity Findings
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure No High Severity Findings in Security Center
  rationale: High severity findings in the Security Command Center indicate critical security vulnerabilities that could lead to unauthorized access, data breaches, or service disruptions. Addressing these findings promptly helps to mitigate potential threats and ensures compliance with industry standards and regulations, such as PCI-DSS and ISO 27001, which mandate robust security monitoring and incident response.
  description: This rule checks for the absence of high severity findings in the Google Cloud Security Command Center. High severity findings typically involve significant security issues such as exposed sensitive data, misconfigured access controls, or compromised resources. It is crucial to regularly review the findings in the Security Command Center and remediate any high severity issues by adjusting resource configurations, enforcing stricter access controls, or deploying additional security measures. Remediation steps can be verified by ensuring no outstanding high severity issues remain in the console.
  references:
  - https://cloud.google.com/security-command-center/docs/overview
  - https://cloud.google.com/security-command-center/docs/finding-severity
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.securitycenter.finding.command_center_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Security Command Center for Enhanced Monitoring
  rationale: Enabling Security Command Center is crucial for comprehensive visibility into the security posture of your GCP environment. It helps in identifying and mitigating potential risks, ensuring that policy violations and misconfigurations are addressed promptly. This enhances compliance with regulatory standards like GDPR and CCPA by maintaining secure data environments.
  description: This rule checks whether the Security Command Center is enabled in your GCP project. Security Command Center aggregates security findings from various GCP services, providing a centralized view of threats and vulnerabilities. To verify, navigate to Security Command Center in the GCP Console and ensure it is activated. If not enabled, activate it by selecting the appropriate project and clicking 'Enable Security Command Center'. This will facilitate the detection and management of security risks effectively.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0 - 7.1
  - NIST SP 800-53 Rev. 4 - CA-7 Continuous Monitoring
  - PCI DSS v3.2.1 - Requirement 10
  - https://cloud.google.com/security-command-center/docs/how-to-security-command-center
- rule_id: gcp.securitycenter.finding.command_center_guardduty_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Guardduty Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Security Command Center for Threat Detection
  rationale: Enabling Security Command Center's threat detection capabilities is crucial for identifying and mitigating potential security threats in real-time. Without it, organizations may face undetected vulnerabilities, leading to potential data breaches and non-compliance with security regulations such as NIST and PCI-DSS. Proactive threat detection helps in maintaining business continuity and protecting sensitive information.
  description: This rule checks whether Security Command Center's threat detection features, such as GuardDuty, are enabled to monitor, detect, and respond to potential security threats in your GCP environment. Ensure that Security Command Center is set up with appropriate permissions and integrated with your existing security operations. Remediation involves navigating to the Security Command Center in the GCP Console, enabling threat detection features, and configuring notifications for timely alerts.
  references:
  - https://cloud.google.com/security-command-center/docs/overview
  - https://cloud.google.com/security-command-center/docs/quickstart
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security-command-center/docs/best-practices
- rule_id: gcp.securitycenter.finding.command_center_is_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Is Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Command Center Is Enabled for Monitoring
  rationale: Enabling Security Command Center (SCC) is crucial for providing continuous visibility into your GCP environment's security posture. Without SCC, organizations may miss critical security findings, increasing the risk of undetected vulnerabilities and compliance violations, which can lead to data breaches and financial penalties.
  description: This rule checks whether the Google Cloud Security Command Center is enabled, ensuring real-time security monitoring and threat detection. To verify, access the GCP Console, navigate to Security Command Center, and ensure it is activated for your organization. If not enabled, activate SCC to gain insights into potential security threats and vulnerabilities across your GCP resources, aligning with best practices for cloud security management.
  references:
  - https://cloud.google.com/security-command-center/docs/quickstart
  - https://cloud.google.com/security-command-center/docs/concepts
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security-command-center/docs/how-to-enable-scc
- rule_id: gcp.securitycenter.finding.command_center_no_high_severity_findings_configured
  service: securitycenter
  resource: finding
  requirement: Command Center No High Severity Findings Configured
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure High Severity Findings Are Configured in Security Command Center
  rationale: Configuring high severity findings in Google Cloud's Security Command Center is crucial for promptly identifying and mitigating critical security vulnerabilities. This reduces the risk of data breaches, financial loss, and damage to organizational reputation. Furthermore, it supports compliance with regulatory standards that require continuous monitoring of security risks.
  description: This rule checks if high severity findings are configured in the Security Command Center, which is essential for effective threat detection and response. To verify, ensure that the Security Command Center is activated and configured to identify and report high severity vulnerabilities across your GCP resources. Remediation involves enabling high severity alerts, ensuring stakeholders are notified, and implementing automated responses based on predefined security policies.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/how-to-manage-findings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
- rule_id: gcp.securitycenter.finding.command_center_security_center_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Security Center Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Security Command Center for Threat Monitoring
  rationale: Enabling Security Command Center is crucial for identifying and mitigating security threats in real-time, reducing potential business disruptions and data breaches. This service provides centralized insights, helping organizations meet compliance requirements by maintaining visibility into the security posture of their GCP resources.
  description: This rule checks whether the Security Command Center is enabled to ensure continuous monitoring and detection of security threats across GCP resources. To verify, navigate to the Security Command Center in the GCP Console and ensure it is activated. Remediation involves enabling the Security Command Center and configuring appropriate roles to allow access to security insights, thus enhancing overall security monitoring capabilities.
  references:
  - https://cloud.google.com/security-command-center/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/best-practices
- rule_id: gcp.securitycenter.finding.command_center_vulnerability_assessment_enabled
  service: securitycenter
  resource: finding
  requirement: Command Center Vulnerability Assessment Enabled
  scope: securitycenter.finding.vulnerability_management
  domain: compute_and_workload_security
  subcategory: instance_configuration
  severity: medium
  title: Ensure Vulnerability Assessments are Enabled in Security Command Center
  rationale: Enabling vulnerability assessments in Security Command Center helps organizations identify and mitigate security vulnerabilities in their cloud environment, reducing the risk of exploitation by attackers. This is crucial for maintaining the integrity and confidentiality of data, supporting compliance with security standards, and minimizing potential business disruptions.
  description: This rule checks whether the Security Command Center's vulnerability assessment feature is enabled, which is a critical component for proactively identifying security risks. To verify, ensure that the 'Vulnerability Scanning' service is activated in your Security Command Center settings. Remediation involves navigating to the Security Command Center in your GCP Console, selecting Settings, and enabling the 'Vulnerability Scanning' feature. This will enhance your cloud security posture by providing continuous monitoring and assessment of your environment.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-vulnerability-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security-command-center/docs/concepts-vulnerability-management
  - https://cloud.google.com/security-command-center/docs/overview
- rule_id: gcp.securitycenter.finding.incident_automation_artifacts_encrypted_and_private
  service: securitycenter
  resource: finding
  requirement: Incident Automation Artifacts Encrypted And Private
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Incident Artifacts Encrypted & Private
  rationale: Incident automation artifacts in Google Cloud may contain sensitive data that, if exposed, could lead to unauthorized access and data breaches. Encrypting these artifacts at rest and ensuring their access is restricted mitigates risks of data exfiltration and aligns with compliance mandates like GDPR and HIPAA that require robust data protection measures.
  description: This rule checks if incident automation artifacts stored in Google Security Command Center are encrypted at rest using Customer-Managed Encryption Keys (CMEK) and are configured to be private. Verify by ensuring that the storage buckets used for these artifacts have CMEK enabled and appropriate IAM policies restricting access to authorized personnel only. Remediate by updating bucket settings to enable CMEK and adjusting IAM policies to limit access.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-enable-and-configure
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-111.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3.pdf
- rule_id: gcp.securitycenter.finding.incident_automation_change_audit_logging_enabled
  service: securitycenter
  resource: finding
  requirement: Incident Automation Change Audit Logging Enabled
  scope: securitycenter.finding.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Incident Automation Change Audit Logging Activation
  rationale: Audit logging for incident automation changes helps in tracking unauthorized or unintended modifications, which could lead to potential security vulnerabilities or compliance violations. Proper logging ensures traceability and accountability, aiding in forensic investigations and demonstrating adherence to regulatory standards.
  description: This rule checks that audit logging is enabled for changes to incident automation processes within Security Command Center. Ensure that logging is configured to capture all relevant changes by setting up audit logs in the Google Cloud Console under IAM & Admin. To remediate, navigate to the Audit Logs page and configure the appropriate log sinks for Security Command Center to capture these changes.
  references:
  - https://cloud.google.com/logging/docs/audit
  - https://cloud.google.com/security-command-center/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security-command-center/docs/how-to-audit-logs
- rule_id: gcp.securitycenter.finding.incident_automation_execution_roles_least_privilege
  service: securitycenter
  resource: finding
  requirement: Incident Automation Execution Roles Least Privilege
  scope: securitycenter.finding.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Incident Automation Roles Use Least Privilege
  rationale: Using least privilege for incident automation roles minimizes the risk of unauthorized access and potential data breaches. This practice reduces the attack surface, helping to protect sensitive data and maintain compliance with standards such as NIST and PCI-DSS. It supports effective security posture by ensuring that roles have only the permissions necessary to perform their duties.
  description: This rule checks that roles assigned for incident automation have the minimum necessary permissions within Google Cloud Security Command Center. It verifies that roles are not overly permissive by reviewing IAM policies associated with automation processes. To remediate, audit and refine IAM policies, removing any unnecessary permissions from roles, and regularly reviewing role assignments to ensure adherence to least privilege principles.
  references:
  - https://cloud.google.com/iam/docs/least-privilege
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
- rule_id: gcp.securitycenter.finding.incident_response_access_rbac_least_privilege
  service: securitycenter
  resource: finding
  requirement: Incident Response Access RBAC Least Privilege
  scope: securitycenter.finding.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Incident Response Roles Follow Least Privilege Principle
  rationale: Implementing least privilege access for incident response roles minimizes the risk of unauthorized access and potential data breaches. This approach reduces the attack surface, ensuring that only authorized personnel have the necessary access during security incidents, thereby safeguarding sensitive information and meeting compliance requirements like PCI-DSS and SOC2.
  description: This rule checks whether roles assigned to incident response teams in GCP Security Command Center adhere to the principle of least privilege. It requires that users have only the permissions necessary to perform their specific duties, which can be verified by auditing IAM policies in the Security Command Center. To remediate, review and adjust IAM roles to eliminate excessive permissions and ensure each role only includes the necessary permissions for incident response tasks.
  references:
  - https://cloud.google.com/security-command-center/docs/access-control
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://www.nist.gov/cyberframework
- rule_id: gcp.securitycenter.finding.incident_response_storage_encrypted_and_private
  service: securitycenter
  resource: finding
  requirement: Incident Response Storage Encrypted And Private
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Incident Response Storage is Encrypted and Private
  rationale: Encrypting incident response storage protects sensitive data from unauthorized access, reducing the risk of data breaches and ensuring compliance with regulations like GDPR and HIPAA. Unencrypted data can be exploited by malicious actors, leading to potential financial and reputational damage. Ensuring privacy and encryption helps maintain trust and aligns with best practices in data protection.
  description: This rule checks that all incident response storage buckets in GCP are configured with encryption at rest and are not publicly accessible. Verify that the storage uses Customer-Managed Encryption Keys (CMEK) or Google-managed keys to encrypt data. Ensure that IAM policies do not grant public access to the storage. Remediation involves configuring encryption settings and adjusting IAM policies to restrict access to authorized personnel only.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-use-security-health-analytics
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.securitycenter.finding.incident_response_versioning_and_immutability_enabled
  service: securitycenter
  resource: finding
  requirement: Incident Response Versioning And Immutability Enabled
  scope: securitycenter.finding.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Versioning and Immutability for Incident Response Findings
  rationale: Enabling versioning and immutability for incident response findings ensures that historical data is preserved and protected against unauthorized modifications. This is crucial for forensic investigations, compliance with regulatory requirements, and maintaining the integrity of security reports. Failure to implement these measures can lead to data loss, tampering, and incomplete incident analysis, potentially resulting in non-compliance with standards like PCI-DSS and GDPR.
  description: This rule checks whether versioning and immutability are enabled for findings within Google Cloud's Security Command Center (SCC). To verify, ensure that your SCC settings include version control mechanisms that maintain historical records of findings. Implement Cloud Storage Bucket versioning and retention policies to safeguard data. To remediate, configure Cloud Storage with versioning enabled by using `gsutil versioning set on gs://[BUCKET_NAME]` and set retention policies via the Google Cloud Console or `gcloud` CLI to prevent deletion of critical security data.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-version-findings
  - https://cloud.google.com/storage/docs/using-object-versioning
  - https://cloud.google.com/storage/docs/using-object-hold
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
- rule_id: gcp.securitycenter.finding.is_enabled
  service: securitycenter
  resource: finding
  requirement: Is Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Command Center Findings Are Enabled
  rationale: Enabling findings in the Security Command Center (SCC) is crucial for identifying potential security threats and vulnerabilities across your GCP resources. Without this feature enabled, organizations might miss critical threat indicators, potentially leading to data breaches or non-compliance with regulations such as PCI-DSS or ISO 27001. By enabling findings, businesses enhance their ability to proactively manage and mitigate risks, safeguarding their infrastructure and sensitive data.
  description: This rule checks whether findings in the Security Command Center are enabled. It ensures that security alerts and insights are actively monitored and addressed in a timely manner. To verify, navigate to the SCC settings in the Google Cloud Console and ensure that all relevant detectors are turned on. Remediation involves configuring the SCC to enable findings for all necessary resources, ensuring continuous monitoring and threat detection.
  references:
  - https://cloud.google.com/security-command-center/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/how-to-enable-and-configure
- rule_id: gcp.securitycenter.finding.privacy_breach_detection_alert_destinations_configured
  service: securitycenter
  resource: finding
  requirement: Privacy Breach Detection Alert Destinations Configured
  scope: securitycenter.finding.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Configure Alert Destinations for Privacy Breach Detection
  rationale: Configuring alert destinations for privacy breaches in GCP ensures timely response to potential data leaks, minimizing the risk of unauthorized access to sensitive data. This is crucial for maintaining customer trust and complying with data protection regulations like GDPR and CCPA, which mandate prompt breach notifications.
  description: This rule checks whether alert destinations for privacy breach detection are properly configured in Google Cloud Security Command Center. Ensure that alerts are routed to responsible security teams or systems for swift response. Configure alerting policies via Pub/Sub topics or email notifications. Verification includes reviewing Security Command Center settings and testing alert delivery mechanisms.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-notifications
  - https://cloud.google.com/security-command-center/docs/concepts-findings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://www.nist.gov/document-193
  - https://cloud.google.com/security/compliance/gdpr
- rule_id: gcp.securitycenter.finding.privacy_breach_detection_integrations_authenticated
  service: securitycenter
  resource: finding
  requirement: Privacy Breach Detection Integrations Authenticated
  scope: securitycenter.finding.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Authenticate Privacy Breach Detection Integrations
  rationale: Ensuring integrations for privacy breach detection are authenticated is crucial to prevent unauthorized access to sensitive data and maintain compliance with regulatory standards. Unauthorized integrations can lead to data breaches, resulting in financial loss, reputational damage, and legal penalties. Authentication helps mitigate risks by ensuring that only approved systems can access and process sensitive information.
  description: This rule checks if configured privacy breach detection integrations in Security Command Center are authenticated. It verifies that all integrations use strong authentication mechanisms such as OAuth tokens or service accounts. To remediate, ensure that all external systems interfacing with your Security Command Center findings are authenticated using GCP-managed authentication methods. Regularly audit and update permissions to adhere to the principle of least privilege.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-api
  - https://cloud.google.com/security-command-center/docs/best-practices
  - https://www.iso.org/standard/54534.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.securitycenter.finding.resource_enabled
  service: securitycenter
  resource: finding
  requirement: Resource Enabled
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Security Findings are Enabled and Monitored
  rationale: Enabling security findings is crucial for effective threat detection and response. Without real-time monitoring, organizations may be unaware of vulnerabilities or active threats, leading to potential data breaches, financial losses, and non-compliance with regulations such as PCI-DSS and HIPAA.
  description: This rule checks whether Security Command Center findings are enabled, ensuring that potential security issues are identified and addressed promptly. To verify, ensure that Security Command Center is activated with the appropriate roles and permissions assigned. Remediate by enabling the Security Command Center in the GCP Console and configuring notifications for critical findings to ensure timely response.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-use-security-command-center
  - https://cloud.google.com/security-command-center/docs/pricing
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://www.hhs.gov/hipaa/for-professionals/security/index.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.securitycenter.finding.threat_finding_alert_destinations_configured
  service: securitycenter
  resource: finding
  requirement: Threat Finding Alert Destinations Configured
  scope: securitycenter.finding.security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: low
  title: Ensure Threat Alert Destinations are Properly Configured
  rationale: Configuring threat alert destinations is crucial for timely incident response and minimizing potential damage from security threats. Without proper configuration, critical security alerts may be missed, leading to unmitigated risks and potential breaches. This is vital for compliance with frameworks like NIST and ISO 27001, which emphasize timely threat detection and response.
  description: This rule verifies that Google Cloud Security Command Center (SCC) is set up to send threat findings to designated alerting destinations, such as Pub/Sub, for further processing and response. Ensure that SCC finding sources are integrated with alerting systems to facilitate rapid incident response. Remediation involves configuring Pub/Sub topics to route findings alerts to your incident response or monitoring systems, ensuring that all critical alerts are captured and actioned.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-notifications-configure
  - https://cloud.google.com/security-command-center/docs/how-to-configure-notifications
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/iso-iec-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/concepts-findings
- rule_id: gcp.securitycenter.finding.threat_finding_archival_export_encrypted
  service: securitycenter
  resource: finding
  requirement: Threat Finding Archival Export Encrypted
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption for Threat Finding Archival Exports
  rationale: Encrypting threat finding archival exports is crucial for protecting sensitive data from unauthorized access and potential breaches. Unencrypted data can lead to exposure of critical security insights, impacting business integrity, and violating compliance with regulations such as GDPR and HIPAA which mandate data protection measures.
  description: This rule checks whether threat finding archival exports in Google Cloud Security Command Center are encrypted at rest. To verify, ensure that the storage solution used for these exports, such as Cloud Storage, is configured with Customer-Managed Encryption Keys (CMEK) or Customer-Supplied Encryption Keys (CSEK). Remediation involves updating the storage settings to enable encryption using these keys, ensuring compliance and enhanced data protection.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-enable-and-configure#enable_scc
  - https://cloud.google.com/storage/docs/encryption/customer-supplied-keys
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.securitycenter.finding.threat_finding_suppression_rules_documented_and_scoped
  service: securitycenter
  resource: finding
  requirement: Threat Finding Suppression Rules Documented And Scoped
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Threat Finding Suppression Rules are Documented and Scoped
  rationale: Properly documented and scoped threat finding suppression rules help organizations manage alerts effectively, reducing noise while ensuring critical threats are not overlooked. Without these controls, there's a risk of missing genuine security incidents, potentially leading to data breaches or compliance failures under frameworks such as NIST or ISO 27001.
  description: This rule checks that all threat finding suppression rules in Google Cloud Security Command Center are documented and scoped appropriately. To verify, ensure that each suppression rule includes a detailed description, justification for its implementation, and the specific conditions under which it applies. Remediation involves reviewing all existing rules, documenting them comprehensively, and revising any rules that are overly broad or lack sufficient context.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-findings-suppression
  - https://cloud.google.com/security-command-center/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.securitycenter.finding.vuln_assessment_policy_store_encrypted
  service: securitycenter
  resource: finding
  requirement: Vuln Assessment Policy Store Encrypted
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Vulnerability Assessment Policy Store is Encrypted
  rationale: Encrypting the Vulnerability Assessment Policy Store is crucial to protect sensitive security data from unauthorized access and potential data breaches. Failure to encrypt this data can lead to exposure of critical vulnerabilities, increasing the risk of exploitation and non-compliance with data protection regulations such as GDPR and CCPA.
  description: This rule checks if the Vulnerability Assessment Policy Store in Google Cloud's Security Command Center is encrypted at rest. Ensure that encryption keys are properly managed and that the store is configured to use either Google-managed keys or customer-managed keys for encryption. Verify the encryption settings through the Cloud Console or by using the gcloud command-line tool. To remediate, enable encryption for the policy store using the appropriate configuration settings.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-encryption
  - https://cloud.google.com/security-command-center/docs/how-to-manage-findings
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.securitycenter.finding.vuln_assessment_roles_least_privilege
  service: securitycenter
  resource: finding
  requirement: Vuln Assessment Roles Least Privilege
  scope: securitycenter.finding.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for Vulnerability Assessment Roles
  rationale: Assigning excessive permissions to vulnerability assessment roles can lead to unauthorized access to sensitive data and systems, increasing the risk of data breaches and compliance violations. Least privilege ensures that roles only have permissions necessary for their function, reducing potential attack vectors and aligning with compliance standards like NIST and PCI-DSS.
  description: This rule evaluates the permissions assigned to roles used for vulnerability assessments in Google Cloud Security Command Center. It checks if the roles exceed the necessary permissions for their tasks, potentially violating the principle of least privilege. To ensure compliance, review assigned roles and remove unnecessary permissions. Use predefined roles where possible, and regularly audit role permissions against security best practices.
  references:
  - https://cloud.google.com/security-command-center/docs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.securitycenter.finding.vuln_scan_agents_or_scanners_deployed
  service: securitycenter
  resource: finding
  requirement: Vuln Scan Agents Or Scanners Deployed
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Deployment of Vulnerability Scanning Agents or Scanners
  rationale: Deploying vulnerability scanning agents or scanners is crucial for identifying and mitigating security weaknesses in your GCP environment. This helps prevent potential exploits by malicious actors and ensures compliance with security standards that require regular vulnerability assessments. Failure to deploy these tools could lead to undetected vulnerabilities, increasing the risk of data breaches and non-compliance with regulations such as PCI-DSS and SOC2.
  description: This rule checks whether vulnerability scanning agents or scanners are deployed within your GCP environment. It verifies the presence of agents that can assess system and application vulnerabilities. To ensure compliance, configure GCP Security Command Center to integrate with vulnerability management tools like Qualys or Tenable. Remediation involves deploying these agents across all critical resources and regularly reviewing scan results to address identified vulnerabilities.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-use-command-center
  - https://cloud.google.com/security-command-center/docs/security-health-analytics-overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.soc2.com/soc-2/
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
- rule_id: gcp.securitycenter.finding.vuln_scan_results_export_destination_encrypted
  service: securitycenter
  resource: finding
  requirement: Vuln Scan Results Export Destination Encrypted
  scope: securitycenter.finding.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Vulnerability Scan Export Destinations Are Encrypted
  rationale: Encrypting vulnerability scan results is crucial for protecting sensitive information against unauthorized access and potential data breaches. Unencrypted data at rest poses significant risks to data integrity and confidentiality, potentially leading to compliance violations and reputational damage. Encryption ensures that even if data is accessed, it remains unreadable without the proper decryption keys, aligning with best practices and regulatory requirements.
  description: This rule checks whether the destination where vulnerability scan results are exported is encrypted. Ensuring encryption involves configuring Cloud Storage buckets or BigQuery datasets with customer-managed or Google-managed encryption keys. Verify encryption settings by reviewing the storage configuration and ensuring encryption is enabled. Remediation includes updating storage settings to enforce encryption at rest using Google Cloud's built-in encryption capabilities.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-use-vulnerability-scanning
  - https://cloud.google.com/storage/docs/encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 2.1 Ensure that Cloud Storage buckets use the uniform bucket-level access
  - https://csrc.nist.gov/publications/detail/sp/800-57-part-1/rev-5/final
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.securitycenter.finding.vuln_scan_scope_includes_all_asset_groups
  service: securitycenter
  resource: finding
  requirement: Vuln Scan Scope Includes All Asset Groups
  scope: securitycenter.finding.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure All Asset Groups Included in Vulnerability Scans
  rationale: Including all asset groups in vulnerability scans ensures comprehensive coverage of the organization's attack surface, reducing the risk of missing critical vulnerabilities. Incomplete scan scopes can lead to undetected security weaknesses, potentially resulting in data breaches, financial losses, and non-compliance with regulations such as PCI-DSS and ISO 27001.
  description: This rule checks whether the vulnerability scan scope in Google Cloud's Security Command Center includes all asset groups. To verify, review the Security Command Center's scan configuration to ensure all asset groups are selected. Remediate by updating the scan settings to include all asset groups, ensuring no resources are left unmonitored, thus maintaining a robust security posture.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-vulnerability-scanning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations/
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/architecture/security-best-practices-on-gcp
- rule_id: gcp.securitycenter.source.threat_custom_identifier_source_trusted
  service: securitycenter
  resource: source
  requirement: Threat Custom Identifier Source Trusted
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Trusted Custom Threat Identifiers in Security Center
  rationale: Utilizing trusted threat identifiers helps organizations detect and respond to potential threats more effectively, reducing the risk of data breaches and operational disruptions. Trusted sources ensure that threat information is reliable and actionable, which is crucial for maintaining compliance with standards such as NIST, PCI-DSS, and ISO 27001.
  description: This rule checks whether custom threat identifiers in GCP Security Center are sourced from trusted origins. Proper configuration involves verifying that custom identifiers are created following best practices and are sourced from reliable threat intelligence feeds. Remediation involves reviewing and updating threat sources to ensure credibility and alignment with organizational security policies.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-custom-threat-detection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/how-to-custom-threat-detection
- rule_id: gcp.securitycenter.source.threat_custom_identifier_storage_encrypted
  service: securitycenter
  resource: source
  requirement: Threat Custom Identifier Storage Encrypted
  scope: securitycenter.source.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption at Rest for Threat Custom Identifiers
  rationale: Encrypting data at rest protects sensitive threat intelligence from unauthorized access and data breaches. This is crucial for maintaining confidentiality and integrity, especially in environments subject to compliance frameworks like PCI-DSS and HIPAA, which mandate encryption measures. Failure to encrypt could lead to exposure of sensitive information, resulting in potential financial and reputational damage.
  description: This rule checks whether custom threat identifiers stored in Google Cloud Security Command Center are encrypted at rest using Google-managed or customer-managed encryption keys. To verify, ensure that all securitycenter.source resources have encryption enabled. Remediation involves configuring encryption settings through the GCP Console or CLI to enforce encryption at rest, preferably using Cloud Key Management Service for enhanced control.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/how-to-manage-sources
  - https://cloud.google.com/security/encryption-at-rest
  - CIS GCP Benchmark v1.3.0, Control 7.1
  - 'NIST SP 800-57 Part 1: Recommendation for Key Management'
  - 'PCI-DSS Requirement 3.5: Encryption at Rest'
- rule_id: gcp.securitycenter.source.threat_detector_enabled_in_all_regions
  service: securitycenter
  resource: source
  requirement: Threat Detector Enabled In All Regions
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Threat Detector in All GCP Regions
  rationale: Enabling Threat Detectors across all regions ensures comprehensive monitoring and detection of security threats, reducing the risk of regional blind spots. This is crucial for organizations with global operations, as it protects against region-specific threats and supports compliance with international security standards.
  description: This rule checks whether the Security Command Center's Threat Detectors are activated in all available GCP regions. Ensuring that Threat Detectors are enabled in every region helps maintain a consistent security posture and timely threat detection. To verify this, navigate to the Security Command Center and review the Threat Detector settings for each region. Remediation involves enabling Threat Detectors in any regions where they are currently disabled.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-threat-detection
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4-0.pdf
  - https://cloud.google.com/architecture/security-best-practices-for-gcp
- rule_id: gcp.securitycenter.source.threat_detector_finding_export_encrypted_destination
  service: securitycenter
  resource: source
  requirement: Threat Detector Finding Export Encrypted Destination
  scope: securitycenter.source.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Encryption for Threat Detector Findings Export
  rationale: Encrypting threat detector findings ensures that sensitive security data is protected against unauthorized access, mitigating the risk of data breaches and fulfilling compliance requirements like GDPR and HIPAA. Unencrypted data can be intercepted or improperly accessed, leading to potential data leaks and reputational damage.
  description: This rule verifies that threat detector findings from Security Command Center are exported to a destination where data is encrypted at rest. Ensure that your export destination, such as a Cloud Storage bucket, is configured with encryption keys. Remediation involves enabling default encryption for storage buckets or using customer-managed encryption keys (CMEK) to enhance data protection.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-export-findings
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-210.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.securitycenter.source.threat_hub_auto_enroll_new_accounts
  service: securitycenter
  resource: source
  requirement: Threat Hub Auto Enroll New Accounts
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Enable Auto Enrollment for New Accounts in Threat Hub
  rationale: Automatically enrolling new accounts in Threat Hub ensures that they are immediately subject to threat detection and monitoring. This proactive measure helps organizations quickly identify and respond to potential security incidents, reducing the risk of breaches. It also supports compliance with security frameworks that require continuous monitoring of cloud environments.
  description: This rule checks whether new accounts are automatically enrolled in Google Cloud's Threat Hub. To verify, navigate to the Security Command Center settings and ensure that the 'Auto Enroll New Accounts' option is enabled. If it is not, adjust the configuration to automatically include new accounts in threat monitoring, which helps maintain a secure and compliant cloud environment by ensuring consistent security oversight.
  references:
  - https://cloud.google.com/security-command-center/docs/how-to-use-security-command-center
  - https://cloud.google.com/security-command-center/docs/concepts-threat-hub
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.securitycenter.source.threat_hub_master_member_configured
  service: securitycenter
  resource: source
  requirement: Threat Hub Master Member Configured
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Threat Hub Master Member is Configured
  rationale: Configuring a Threat Hub Master Member in GCP Security Command Center enables centralized threat detection and monitoring across multiple projects, enhancing threat visibility and response capabilities. This setup helps mitigate risks such as delayed threat detection and insufficient incident response, which can lead to increased exposure to adversaries and potential data breaches. Compliance with regulations such as NIST and ISO 27001 often requires centralized security monitoring and incident management.
  description: This rule verifies that a Threat Hub Master Member is configured within the Security Command Center. It checks for the presence of a valid configuration that allows for centralized threat data aggregation and analysis across multiple projects. To verify, ensure that the Security Command Center is enabled and a master member is assigned in the organization settings. Remediation involves configuring the Security Command Center with the necessary permissions and roles to support a master member setup, ensuring comprehensive security monitoring coverage.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-threat-intelligence
  - https://cloud.google.com/security-command-center/docs/quickstart-scc
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security-command-center/docs/how-to-security-sources
- rule_id: gcp.securitycenter.source.threat_ip_set_sources_trusted
  service: securitycenter
  resource: source
  requirement: Threat Ip Set Sources Trusted
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Trusted Threat IP Set Sources in Security Command Center
  rationale: Validating that threat intelligence IP set sources are trusted helps mitigate the risk of ingesting false positives or malicious data into your security monitoring systems. This is crucial for maintaining the integrity of your threat detection and response capabilities, ensuring accurate security alerts, and adhering to compliance requirements such as PCI-DSS and NIST SP 800-53.
  description: This rule checks if all threat intelligence IP set sources configured in Google Cloud Security Command Center are from verified and trusted origins. It involves reviewing the list of IP set sources to confirm their authenticity and reliability. To remediate, update the IP set list to exclude any unverified sources and include only those from trusted threat intelligence providers. Regularly audit and update the list to reflect current trusted sources.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-scc-overview
  - https://cloud.google.com/security-command-center/docs/set-up
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
- rule_id: gcp.securitycenter.source.threat_ip_set_storage_encrypted
  service: securitycenter
  resource: source
  requirement: Threat Ip Set Storage Encrypted
  scope: securitycenter.source.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Threat IP Set Storage is Encrypted at Rest
  rationale: Encrypting threat IP set storage protects sensitive threat intelligence data from unauthorized access and potential breaches. This is crucial for maintaining trust and compliance with data protection regulations, such as GDPR and CCPA, which mandate the safeguarding of personal and sensitive information. Moreover, encryption mitigates the risk of data exposure in the event of a storage compromise, ensuring business continuity and reputation.
  description: This rule verifies that all threat IP sets stored within Google Cloud Security Command Center are encrypted at rest using Google-managed encryption keys. To ensure this, administrators should enable default encryption settings on the storage resources associated with Security Command Center. Verification can be done via the Google Cloud Console or by using the gcloud CLI to check the encryption status of the storage. Remediation involves configuring the storage settings to use Google Cloud's default encryption, ensuring compliance and data security.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-core-features#encryption
  - https://cloud.google.com/security/compliance/gdpr
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://www.iso.org/isoiec-27001-information-security.html
- rule_id: gcp.securitycenter.source.threat_ip_set_used_by_detectors
  service: securitycenter
  resource: source
  requirement: Threat Ip Set Used By Detectors
  scope: securitycenter.source.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Ensure Threat IP Sets Are Utilized by Security Detectors
  rationale: Utilizing threat IP sets in security detectors is crucial for identifying and mitigating potential threats in real-time. Failing to do so can result in undetected security incidents, leading to data breaches or service disruptions. Compliance with security standards such as ISO 27001 and NIST requires effective threat detection mechanisms.
  description: This rule checks if threat IP sets are being used by detectors in Google Cloud Security Command Center. Proper configuration involves linking known malicious IP addresses with security detectors to enhance threat detection capabilities. Verify that your detectors are configured to utilize updated threat IP lists. Remediation involves configuring Security Command Center to automatically update and use threat IP sets in its detection logic.
  references:
  - https://cloud.google.com/security-command-center/docs/concepts-overview
  - https://cloud.google.com/security-command-center/docs/how-to-use
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/standard/54534.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.services.service.keys_active_service_configured
  service: services
  resource: service
  requirement: Keys Active Service Configured
  scope: services.service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Active Service Keys are Properly Configured
  rationale: Misconfigured service keys can lead to unauthorized access and data breaches, risking sensitive data exposure. Proper configuration ensures compliance with regulatory standards such as PCI-DSS and HIPAA, protecting both business interests and customer privacy.
  description: This rule checks if service keys are actively configured and managed according to best practices in your GCP environment. It verifies that keys are rotated regularly, used with minimal permissions, and have proper expiration policies set. Remediation involves reviewing key configurations, implementing automated key rotation, and ensuring least privilege access for service accounts.
  references:
  - https://cloud.google.com/iam/docs/managing-service-account-keys
  - https://cloud.google.com/security/compliance
  - CIS Google Cloud Platform Foundation Benchmark v1.1.0, Section 1.2
  - https://cloud.google.com/docs/security/encryption-at-rest
  - NIST SP 800-57 Part 1
  - PCI DSS Requirement 3
- rule_id: gcp.services.service.keys_restrictions_enforced
  service: services
  resource: service
  requirement: Keys Restrictions Enforced
  scope: services.service.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Enforce API Key Restrictions for Enhanced Security
  rationale: Unrestricted API keys pose a significant risk as they can be easily exploited if accidentally exposed, leading to unauthorized access and potential data breaches. Ensuring that API keys are restricted by IP addresses, referrer URLs, or service calls reduces the attack surface and aligns with compliance requirements such as PCI-DSS and SOC2, which mandate strict access controls.
  description: This rule checks whether API keys used in Google Cloud services have appropriate restrictions applied to them. Keys should be restricted by specifying allowed IP addresses, HTTP referrers, or service accounts to minimize misuse if exposed. To verify, review the API key settings in the Google Cloud Console under 'APIs & Services' and apply necessary restrictions. Remediation involves configuring these restrictions to enforce the principle of least privilege.
  references:
  - https://cloud.google.com/docs/authentication/api-keys#api_key_restrictions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://doi.org/10.6028/NIST.SP.800-53r5
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.services.service.keys_rotation_90_days_configured
  service: services
  resource: service
  requirement: Keys Rotation 90 Days Configured
  scope: services.service.key_rotation
  domain: secrets_and_key_management
  subcategory: key_management
  severity: high
  title: Enforce 90-Day Rotation for GCP Service Account Keys
  rationale: Regularly rotating service account keys mitigates the risk of key compromise by limiting the time a compromised key can be used. It helps protect against unauthorized access and aligns with compliance requirements such as PCI-DSS and NIST, which mandate regular key rotation to maintain a robust security posture.
  description: This rule checks whether GCP service account keys are configured to rotate every 90 days. Service accounts without proper key rotation are at risk of unauthorized access if keys are leaked or compromised. To configure, use the IAM API or Google Cloud Console to set a key rotation schedule for each service account. Regularly review and update your key rotation policies to ensure all accounts comply with this standard.
  references:
  - https://cloud.google.com/iam/docs/creating-managing-service-account-keys
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf
  - https://cloud.google.com/iam/docs/best-practices-for-managing-service-account-keys
- rule_id: gcp.spanner.instance.backup_enabled
  service: spanner
  resource: instance
  requirement: Backup Enabled
  scope: spanner.instance.backup_recovery
  domain: resilience_and_disaster_recovery
  subcategory: backup_configuration
  severity: medium
  title: Cloud Spanner Instances Must Have Backups Enabled
  rationale: Enabling backups for Cloud Spanner instances is critical for ensuring data resilience and maintaining business continuity in the event of accidental deletion or data corruption. Without regular backups, organizations risk significant data loss, which can lead to operational downtime and non-compliance with regulations that mandate data protection and recovery capabilities.
  description: This rule checks if Cloud Spanner instances have automated backups enabled. Backups should be configured to occur regularly to ensure data can be restored to a point in time in the event of a failure. To verify, navigate to the Cloud Spanner instances in the Google Cloud Console and ensure the 'Automated Backups' setting is enabled. Remediation involves enabling automated backups for instances by configuring a backup schedule in the Cloud Console or using gcloud commands.
  references:
  - https://cloud.google.com/spanner/docs/backup
  - https://cloud.google.com/security/compliance/cis-gcp
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.bucket_object_versioning
  service: storage
  resource: bucket
  requirement: Bucket Object Versioning
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Object Versioning for Storage Buckets
  rationale: Enabling object versioning on GCP Storage Buckets helps protect against accidental or malicious data deletion or modification by maintaining a history of object versions. This is crucial for maintaining data integrity and availability, especially in environments where data retention is a compliance requirement. It mitigates risks associated with data loss and ensures recoverability, supporting business continuity and compliance with frameworks like SOC2 and ISO 27001.
  description: This rule checks if object versioning is enabled for GCP Storage Buckets. Object versioning allows the storage of multiple versions of an object, enabling recovery from unintended actions. To enable versioning, access the Google Cloud Console, navigate to Storage, select the desired bucket, and enable 'Object Versioning' under the bucket settings. This setting can also be configured via the gsutil command line tool. Proper configuration ensures data resilience and compliance with data protection standards.
  references:
  - https://cloud.google.com/storage/docs/object-versioning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.storage.bucket.bucket_policy_only_enforced
  service: storage
  resource: bucket
  requirement: Bucket Policy Only Enforced
  scope: storage.bucket.policy_management
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Bucket Policy Only is Enforced for GCP Storage Buckets
  rationale: Enforcing Bucket Policy Only on GCP storage buckets ensures that access to the bucket is controlled solely through IAM policies, reducing the risk of unauthorized access. This practice helps prevent data breaches and supports compliance with regulatory requirements such as GDPR and HIPAA by ensuring consistent access management across your cloud storage resources.
  description: This rule checks if GCP storage buckets have the 'Bucket Policy Only' setting enabled, meaning only IAM policies are used to control access. Without this setting, ACLs can also grant permissions, which may lead to inconsistent access controls. To verify, check the bucket's configuration in the Cloud Console or use the gcloud CLI. To enforce, update the bucket's settings to enable 'Bucket Policy Only' via the Cloud Console or gcloud CLI.
  references:
  - https://cloud.google.com/storage/docs/bucket-policy-only
  - https://cloud.google.com/storage/docs/access-control/iam-bucket-permissions
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/json_api/v1/buckets/patch
- rule_id: gcp.storage.bucket.cross_region_replication_encryption_enabled
  service: storage
  resource: bucket
  requirement: Cross Region Replication Encryption Enabled
  scope: storage.bucket.encryption
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: high
  title: Ensure Cross Region Replication Encryption for Buckets
  rationale: Ensuring encryption for cross-region replicated data in GCP Storage Buckets is crucial to protect sensitive data from unauthorized access and potential breaches. Without encryption, data can be exposed during transit across regions, increasing the risk of data interception. Compliance with regulations like GDPR and CCPA often requires data protection measures, including encryption, to safeguard personal and sensitive information.
  description: This rule checks if GCP Storage Buckets with cross-region replication have encryption enabled. To verify, ensure that the 'Uniform bucket-level access' is enabled and a Customer-Managed Encryption Key (CMEK) is used for data encryption. Remediation involves configuring bucket settings to use CMEK for encryption, thereby securing data in transit and at rest across regions.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/bucket-policy-only
  - CIS GCP Foundation Benchmark v1.3.0 - Section 8.1
  - https://cloud.google.com/security/compliance
  - NIST SP 800-57 Part 1 - General Guidelines for Key Management
  - https://cloud.google.com/security/encryption/default-encryption
- rule_id: gcp.storage.bucket.data_governance_enforced_on_sensitive_datasets
  service: storage
  resource: bucket
  requirement: Data Governance Enforced On Sensitive Datasets
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Data Governance on Sensitive GCP Storage Buckets
  rationale: Enforcing data governance on sensitive datasets in GCP storage buckets is crucial to mitigate risks of unauthorized access and data breaches. This practice supports compliance with regulations such as GDPR and HIPAA, protecting the organization from potential legal liabilities and financial penalties. Effective governance also enhances data management and access control, ensuring data integrity and confidentiality.
  description: This check verifies that data governance policies are applied to GCP storage buckets containing sensitive datasets. It involves ensuring that appropriate IAM roles, bucket policies, and encryption mechanisms are in place. To verify, review the bucket's IAM policy for least privilege access, ensure data classification tags are applied, and confirm that server-side encryption (SSE) is enabled. Remediation involves updating IAM roles, applying data classification tags, and enabling SSE if not already configured.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam-roles
  - https://cloud.google.com/storage/docs/encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53
  - ISO/IEC 27001:2013
- rule_id: gcp.storage.bucket.data_governance_expiration_rules_defined
  service: storage
  resource: bucket
  requirement: Data Governance Expiration Rules Defined
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Data Expiration Policies for GCP Storage Buckets
  rationale: Defining data expiration rules for GCP storage buckets helps manage data lifecycle, reduce storage costs, and mitigate risks associated with data over-retention. It safeguards against unauthorized access to obsolete data and aligns with data governance policies and regulatory requirements such as GDPR and CCPA.
  description: This rule checks if expiration policies are set for objects within GCP storage buckets to automatically delete or archive data after a specified period. To verify, ensure lifecycle management policies are configured for each bucket. Remediation involves defining and applying lifecycle rules that specify conditions for data expiration based on age or creation date, ensuring compliance with data governance standards.
  references:
  - https://cloud.google.com/storage/docs/lifecycle
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/security/compliance/ccpa
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://www.nist.gov/cyberframework
- rule_id: gcp.storage.bucket.data_governance_immutable_retention_locked_where_required
  service: storage
  resource: bucket
  requirement: Data Governance Immutable Retention Locked Where Required
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Immutable Retention Lock on GCP Storage Buckets
  rationale: Implementing immutable retention locks on storage buckets helps protect against data tampering and unauthorized deletions, crucial in maintaining data integrity and compliance with regulations like GDPR and CCPA. This is particularly important for businesses handling sensitive information that must be preserved for legal and audit purposes, thereby reducing the risk of data breaches and regulatory fines.
  description: This rule checks if GCP Storage Buckets have immutable retention policies enforced where required. Ensure that the 'locked' retention policy is set on buckets storing critical or sensitive data. To verify, access the bucket's settings in the GCP Console, check the 'Retention Policy' section, and confirm it is 'locked'. If not, configure the retention policy in the GCP Console or via the CLI using 'gsutil retention lock'. This action prevents any retention policy changes or deletions, safeguarding against data loss.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://cloud.google.com/storage/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v4.pdf
  - https://www.nist.gov/cyberframework
- rule_id: gcp.storage.bucket.data_governance_policies_defined
  service: storage
  resource: bucket
  requirement: Data Governance Policies Defined
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Data Governance Policies for GCS Buckets
  rationale: Defining data governance policies for Google Cloud Storage (GCS) buckets is crucial to maintain data integrity, security, and compliance with regulatory requirements. Without these policies, unauthorized access or data leaks could occur, leading to financial loss, reputational damage, and non-compliance with legal standards such as GDPR or HIPAA.
  description: This rule checks if data governance policies, including access controls, encryption, and lifecycle management, are defined for GCS buckets. It ensures that buckets have appropriate IAM policies, use customer-managed encryption keys (CMEK) if needed, and have lifecycle rules to automatically manage data retention. To verify, review the bucket's IAM policy, encryption settings, and lifecycle configurations in the GCP Console or via gcloud CLI. Remediate by updating IAM roles and setting CMEK in the bucket's configuration as per organizational data governance standards.
  references:
  - https://cloud.google.com/storage/docs/best-practices
  - https://cloud.google.com/iam/docs/overview
  - CIS GCP Foundation Benchmark v1.3.0, Section 7.1
  - https://cloud.google.com/security/compliance
  - NIST SP 800-53 Rev. 5
  - https://cloud.google.com/storage/docs/encryption
- rule_id: gcp.storage.bucket.data_governance_protected_from_public_override
  service: storage
  resource: bucket
  requirement: Data Governance Protected From Public Override
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Access Overrides on Storage Buckets
  rationale: Exposing storage buckets to the public can lead to unauthorized data access, resulting in data breaches and non-compliance with regulations such as GDPR, HIPAA, and PCI-DSS. Public access overrides weaken data governance policies, increasing the risk of sensitive information being accessed, altered, or deleted by unauthorized users.
  description: This rule checks if Google Cloud Storage buckets have public access overrides, which allow anyone on the internet to access the data. To verify, inspect the bucket's IAM policies and ensure no 'allUsers' or 'allAuthenticatedUsers' roles have been granted. Remediation involves removing these roles and ensuring that access is granted only to specific, authorized users or service accounts. Implement bucket-level policies that enforce strict access controls and regular audits to maintain compliance.
  references:
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/iam/docs/overview
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/best-practices
  - https://cloud.google.com/storage/docs/public-access-prevention
- rule_id: gcp.storage.bucket.data_governance_versioning_enabled_where_supported
  service: storage
  resource: bucket
  requirement: Data Governance Versioning Enabled Where Supported
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Bucket Versioning is Enabled for Data Governance
  rationale: Enabling versioning on storage buckets is critical for data integrity and recovery. Versioning helps protect against accidental deletions or overwrites by maintaining historical versions of objects, which is paramount for compliance with data governance policies and regulations such as GDPR and CCPA. This feature mitigates risks associated with data loss and unauthorized data manipulation, ensuring continuity and trust in data handling processes.
  description: This rule checks if versioning is enabled on GCP storage buckets. Versioning maintains a history of object versions, which is essential for data recovery and audit purposes. To verify, access the Google Cloud Console, navigate to Storage, select the desired bucket, and ensure versioning is enabled under the 'Lifecycle' tab. For remediation, enable bucket versioning via the Console or gcloud CLI using 'gcloud storage buckets update BUCKET_NAME --versioning', thus safeguarding against data loss and ensuring compliance with data governance standards.
  references:
  - https://cloud.google.com/storage/docs/object-versioning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/storage/docs/lifecycle
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.data_protection_access_logging_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Access Logging Enabled
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Access Logging for GCS Buckets
  rationale: Access logging for Google Cloud Storage (GCS) buckets is crucial for monitoring and investigating unauthorized access attempts and data exfiltration activities. Without logging, detecting anomalous access patterns and fulfilling regulatory compliance requirements, such as those mandated by GDPR and HIPAA, becomes challenging, potentially leading to data breaches and financial penalties.
  description: This rule verifies if access logging is enabled for GCS buckets, ensuring that detailed access logs are captured for audit and monitoring purposes. To enable logging, configure the 'gsutil logging set' command to specify a log bucket where access logs will be stored. This configuration helps maintain visibility into who accessed the data, when, and what actions were performed, thus aiding in quick incident response and forensic analysis.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/understanding-hipaa-logging-requirements/
- rule_id: gcp.storage.bucket.data_protection_block_public_access_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Block Public Access Enabled
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure Block Public Access on Storage Buckets
  rationale: Enabling block public access for storage buckets prevents unauthorized public exposure of sensitive data, which can lead to data breaches and compliance violations. Publicly accessible buckets can be exploited by attackers to access, alter, or delete critical data, impacting business operations and reputation.
  description: 'This rule checks if the block public access setting is enabled on all storage buckets, ensuring that no data is inadvertently exposed to the public. Verify that the ''public access prevention'' setting is configured to ''enforced'' for critical buckets. To remediate, navigate to the Google Cloud Console, go to the Storage section, select the bucket, click on ''Permissions'', and ensure ''Public access prevention'' is enabled. Alternatively, use the gcloud command: `gcloud storage buckets update BUCKET_NAME --public-access-prevention=enabled`.'
  references:
  - https://cloud.google.com/storage/docs/public-access-prevention
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/storage/docs/bucket-lock
- rule_id: gcp.storage.bucket.data_protection_bucket_deny_insecure_transport
  service: storage
  resource: bucket
  requirement: Data Protection Bucket Deny Insecure Transport
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Buckets Deny Insecure Transport
  rationale: Allowing insecure transport for bucket access can lead to unauthorized data interception and exposure. This poses significant security risks and non-compliance with regulations like PCI-DSS and HIPAA that mandate data protection in transit. Ensuring secure transport helps protect sensitive information and maintain customer trust.
  description: This rule checks if GCP Storage Buckets have enforced HTTPS for data transfer by setting 'Enforce HTTPS' in bucket policies. To verify, review bucket IAM policies to ensure the condition 'secureTransport' is set to 'true'. Remediation involves configuring bucket policies to deny requests that do not use secure transport, specifically HTTPS, thus enhancing data protection.
  references:
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/security/compliance/cis
  - https://csrc.nist.gov/publications/detail/sp/800-52/rev-2/final
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.storage.bucket.data_protection_bucket_deny_unencrypted_puts
  service: storage
  resource: bucket
  requirement: Data Protection Bucket Deny Unencrypted Puts
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Block Unencrypted Data Uploads to GCP Storage Buckets
  rationale: Ensuring that all data uploaded to storage buckets is encrypted protects against unauthorized access and data breaches, which can lead to significant business and reputational damage. Compliance with regulations such as GDPR, HIPAA, and PCI-DSS often mandates encryption of data at rest. Unencrypted data can expose sensitive information to malicious actors if the storage system is compromised.
  description: This rule verifies that storage buckets in GCP are configured to deny uploads of unencrypted data, ensuring that all data is encrypted at rest. To verify, check that the bucket policy includes a condition that only allows PUT operations if the data is encrypted using a Customer-Managed Encryption Key (CMEK) or Google-managed keys. Remediation involves updating bucket policies to enforce this condition, ensuring compliance with data protection requirements.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/bucket-policy-only
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/customer-managed-encryption
  - https://cloud.google.com/iam/docs/policies
- rule_id: gcp.storage.bucket.data_protection_bucket_no_public_principals
  service: storage
  resource: bucket
  requirement: Data Protection Bucket No Public Principals
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Access to GCP Storage Buckets
  rationale: Allowing public access to GCP storage buckets can lead to unauthorized data exposure, resulting in potential data breaches, non-compliance with data protection regulations like GDPR or CCPA, and financial losses from data leaks. Publicly accessible buckets are vulnerable to malicious activities, potentially impacting an organization's reputation and customer trust.
  description: This rule checks if any GCP storage bucket is configured to allow access to 'allUsers' or 'allAuthenticatedUsers', which grants public access. To verify, examine bucket IAM policies for entries with these principals. Remediation involves removing these principals and restricting access to specific users or roles within the organization. Implementing bucket-level permissions and using signed URLs for public access can help maintain data security.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/storage/docs/best-practices
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.storage.bucket.data_protection_bucket_no_wildcards_on_actions_or_principals
  service: storage
  resource: bucket
  requirement: Data Protection Bucket No Wildcards On Actions Or Principals
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Avoid Wildcards in IAM Policies for GCS Buckets
  rationale: Using wildcards in IAM policies for Google Cloud Storage buckets can unintentionally grant broader permissions than necessary, increasing the risk of unauthorized access. This can lead to data breaches or accidental exposure of sensitive information, impacting compliance with standards such as GDPR, HIPAA, and PCI-DSS.
  description: This rule checks for the use of wildcards (*) in IAM policies for Google Cloud Storage buckets, either in actions or principals. Wildcards should be replaced with specific actions and principals to enforce the principle of least privilege. To verify, review the bucket IAM policies in the GCP Console or using the gcloud command-line tool. Remediation involves editing the IAM policy to remove wildcards and specifying precise permissions.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.data_protection_cmk_cmek_configured
  service: storage
  resource: bucket
  requirement: Data Protection CMK Cmek Configured
  scope: storage.bucket.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Buckets Use Customer-Managed Encryption Keys
  rationale: Configuring Customer-Managed Encryption Keys (CMEK) for Cloud Storage buckets enhances data protection by allowing organizations to control and manage encryption keys. This reduces the risk of unauthorized data access and aligns with compliance frameworks requiring strict data encryption standards, such as PCI-DSS and HIPAA. It also mitigates threats like data breaches by ensuring that data is encrypted at rest with keys that the organization can rotate and revoke.
  description: This rule checks that Google Cloud Storage buckets are configured to use Customer-Managed Encryption Keys (CMEK) instead of Google-managed keys. To verify this configuration, you can inspect the bucket's settings in the Cloud Console or use the gcloud CLI to ensure the `encryption` field specifies a CMEK. To remediate, update the bucket's settings to use a key from Cloud Key Management Service (KMS) by setting the appropriate KMS key resource path. This ensures that only authorized users with access to the CMEK can decrypt the data.
  references:
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://cloud.google.com/security/compliance/cis#section-5.2.1
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/kms/docs/encrypt-decrypt
  - https://cloud.google.com/storage/docs/access-control/using-cmek
- rule_id: gcp.storage.bucket.data_protection_cross_region_copy_encrypted
  service: storage
  resource: bucket
  requirement: Data Protection Cross Region Copy Encrypted
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Cross-Region Bucket Copies are Encrypted
  rationale: Encrypting data copied across regions protects against unauthorized access and data breaches, which can lead to compliance violations and reputational damage. It ensures that sensitive data remains confidential and secure even when transferred across geographically dispersed locations.
  description: This rule checks if data copied from a GCP Storage Bucket to another region is encrypted using CMEK (Customer-Managed Encryption Keys) or Google-managed encryption keys. To verify, ensure that bucket policies enforce encryption during cross-region data transfers. Remediation involves configuring bucket-level settings to enforce encryption and using customer-managed keys if needed for compliance with specific regulatory requirements.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/cross-region
  - https://cloud.google.com/security/compliance
  - CIS GCP Benchmark - Storage Services
  - 'NIST SP 800-53: SC-13 Cryptographic Protection'
  - 'PCI-DSS Requirement 3.4: Protect Stored Cardholder Data'
- rule_id: gcp.storage.bucket.data_protection_encrypted
  service: storage
  resource: bucket
  requirement: Data Protection Encrypted
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure GCS Buckets are Encrypted at Rest
  rationale: Encrypting data at rest in Google Cloud Storage (GCS) buckets is crucial for protecting sensitive information from unauthorized access and data breaches. Without encryption, data stored in buckets is vulnerable to exposure through compromised credentials or insider threats. Compliance with regulations such as PCI-DSS, HIPAA, and GDPR often mandates encryption of stored data to safeguard customer privacy and maintain trust.
  description: This rule checks whether Google Cloud Storage buckets have encryption configured to protect data at rest. By default, Google Cloud encrypts all data with Google-managed keys, but users can enhance security by using Customer-Managed Encryption Keys (CMEK) for additional control. To verify, check the encryption settings of each bucket via the GCP Console or CLI. Remediation involves configuring CMEK under 'Bucket Settings' in the GCS console or using the `gsutil` command line tool to apply a CMEK.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.bucket.data_protection_encrypted_at_rest
  service: storage
  resource: bucket
  requirement: Data Protection Encrypted At Rest
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure GCP Buckets Are Encrypted At Rest
  rationale: Encrypting data at rest is crucial for protecting sensitive information from unauthorized access and potential data breaches. It mitigates risks associated with data exposure in case of a security incident or physical theft. Encryption at rest is often a requirement for compliance with standards such as PCI-DSS, HIPAA, and GDPR, ensuring that organizations meet regulatory obligations and maintain customer trust.
  description: This rule checks if Google Cloud Storage buckets have encryption at rest enabled, specifically verifying that Customer-Managed Encryption Keys (CMEK) or Google-managed keys are applied. To verify, check the bucket's encryption configuration in the GCP Console or using the 'gsutil encryption' command. Remediation involves configuring the bucket to use either CMEK or default Google-managed keys, ensuring data is encrypted automatically at rest.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://cloud.google.com/security/privacy/gdpr
  - https://cloud.google.com/blog/products/storage-data-transfer/encrypting-data-at-rest-on-google-cloud
- rule_id: gcp.storage.bucket.data_protection_encryption_at_rest_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Encryption At Rest Enabled
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Default Encryption At Rest for GCP Storage Buckets
  rationale: Enabling encryption at rest for GCP Storage buckets ensures that data is protected from unauthorized access even when stored on physical disk. This mitigates risks associated with data breaches and supports compliance with regulations such as GDPR, HIPAA, and PCI-DSS, which mandate data protection and privacy. Without encryption, sensitive data could be exposed if physical security controls are bypassed.
  description: This rule checks whether GCP Storage buckets have default encryption at rest enabled. By default, Google Cloud encrypts all data at rest using Google-managed encryption keys, but users can opt for Customer-Managed Encryption Keys (CMEK) for additional control. To enable CMEK, configure the bucket's encryption settings via the GCP Console or CLI by specifying a Cloud KMS key. This ensures compliance with internal data governance policies and external regulatory requirements.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_foundations
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - https://cloud.google.com/kms/docs/quickstart
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.storage.bucket.data_protection_fileshare_encryption_at_rest_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Fileshare Encryption At Rest Enabled
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption at Rest is Enabled for GCS Buckets
  rationale: Enabling encryption at rest for Google Cloud Storage (GCS) buckets is crucial for protecting sensitive data from unauthorized access and potential breaches. Without encryption, data could be exposed to threats such as data theft or unauthorized data manipulation, which could lead to compliance violations with frameworks like PCI-DSS, HIPAA, and GDPR, and result in significant financial and reputational damage.
  description: This rule checks if encryption at rest is enabled for GCS buckets, ensuring that all stored data is automatically encrypted using Google-managed encryption keys. To verify, review the bucket settings in the Cloud Console or use the `gsutil` command-line tool to confirm that the default `encryption` field is set. Remediation involves configuring bucket settings to ensure encryption is enabled, either by default or by specifying a customer-managed encryption key (CMEK) if custom encryption keys are preferred.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/storage/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.bucket.data_protection_fileshare_kms_key_policy_least_privilege
  service: storage
  resource: bucket
  requirement: Data Protection Fileshare KMS Key Policy Least Privilege
  scope: storage.bucket.policy_management
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Key Policies Follow Least Privilege for Storage Buckets
  rationale: Applying least privilege to KMS key policies for storage buckets mitigates the risk of unauthorized data access and potential data leaks. This is crucial for maintaining data confidentiality and integrity, especially for sensitive data stored in Google Cloud Storage. It also helps organizations meet compliance requirements by restricting key access to only those identities that require it.
  description: This rule checks if the IAM policies associated with KMS keys used to encrypt storage buckets are configured with the principle of least privilege. Specifically, it verifies that only necessary identities have roles that grant access to the key. To remediate, review the IAM policy for the KMS key, ensure roles are minimized, and access is granted only to essential identities. Audit and adjust permissions regularly to maintain a secure configuration.
  references:
  - https://cloud.google.com/kms/docs/resource-hierarchy-access-control
  - https://cloud.google.com/kms/docs/managing-keys
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57p1r4.pdf
  - https://cloud.google.com/security/encryption-at-rest/
  - https://cloud.google.com/iam/docs/understanding-roles
- rule_id: gcp.storage.bucket.data_protection_fileshare_private_network_only
  service: storage
  resource: bucket
  requirement: Data Protection Fileshare Private Network Only
  scope: storage.bucket.network_security
  domain: network_security_and_connectivity
  subcategory: network_access_control
  severity: medium
  title: Restrict GCP Bucket Access to Private Networks Only
  rationale: Restricting bucket access to private networks minimizes exposure to unauthorized access and potential data breaches. This control is essential for protecting sensitive data against malicious attacks and ensuring compliance with regulations such as PCI-DSS and HIPAA, which mandate strict data access controls.
  description: This rule checks if GCP storage buckets are configured to allow access only from specific private networks. Ensure that the bucket's IAM policies are set to deny public access and only grant permissions to VPCs or specific IP ranges within your organization's control. To verify, review the bucket's IAM policy and ensure it restricts access to known private network interfaces. Remediation involves modifying the bucket's policy to align with these requirements.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/security/compliance/cis
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://cloud.google.com/architecture/security-foundations/networking
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.data_protection_fileshare_snapshots_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Fileshare Snapshots Enabled
  scope: storage.bucket.backup_recovery
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Fileshare Snapshots for GCP Storage Buckets
  rationale: Enabling fileshare snapshots for storage buckets in GCP ensures that data can be quickly restored in the event of accidental deletion, corruption, or ransomware attacks. This enhances business continuity, reduces downtime, and supports compliance with data protection regulations by maintaining data integrity and availability.
  description: This rule checks if snapshot functionality is enabled for GCP storage buckets, which allows for point-in-time data recovery. To verify, ensure that your Google Cloud storage setup includes automated snapshot schedules for critical buckets. Remediation involves configuring lifecycle policies to regularly capture snapshots and store them in separate storage locations to prevent data loss.
  references:
  - https://cloud.google.com/storage/docs/managing-lifecycles
  - https://cloud.google.com/storage/docs/best-practices
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.storage.bucket.data_protection_not_public
  service: storage
  resource: bucket
  requirement: Data Protection Not Public
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Ensure GCS Buckets Are Not Publicly Accessible
  rationale: Publicly accessible storage buckets can lead to unauthorized data exposure, resulting in data breaches and non-compliance with regulations like GDPR, HIPAA, or PCI-DSS. This could cause significant business impact, including financial loss, reputational damage, and legal penalties. Protecting data from public access minimizes the risk of exposure to malicious actors and ensures adherence to security best practices.
  description: This rule checks for Google Cloud Storage buckets that have public access settings enabled. It verifies the IAM policies and bucket ACLs to ensure no public access permissions are granted. To remediate, review and modify the bucket's IAM policy and ACL to restrict access to authorized users only. Use the Google Cloud Console or gcloud CLI to update permissions and remove 'allUsers' or 'allAuthenticatedUsers' roles from bucket policies.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://cloud.google.com/storage/docs/access-control/iam-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/standard/73906.html
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.storage.bucket.data_protection_not_publicly_readable
  service: storage
  resource: bucket
  requirement: Data Protection Not Publicly Readable
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Read Access on GCS Buckets
  rationale: Publicly readable Cloud Storage buckets can expose sensitive data, leading to data breaches and non-compliance with regulations such as PCI-DSS and HIPAA. Unauthorized access to bucket data can result in financial loss and damage to reputation. Ensuring buckets are not publicly readable mitigates the risk of data exfiltration and unauthorized data dissemination.
  description: This rule checks for any Google Cloud Storage buckets that allow public read access. To verify, inspect the bucket's IAM policy for roles such as 'roles/storage.objectViewer' granted to 'allUsers' or 'allAuthenticatedUsers'. If found, remove or restrict these roles to authorized users only. Remediation involves setting appropriate IAM policies and configuring Bucket Policy Only to enforce strict access controls.
  references:
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://cloud.google.com/storage/docs/iam-access-control
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/pci_security/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
  - https://cloud.google.com/storage/docs/bucket-policy-only
- rule_id: gcp.storage.bucket.data_protection_object_lock_or_immutability_enable_supported
  service: storage
  resource: bucket
  requirement: Data Protection Object Lock Or Immutability Enable Supported
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Object Lock or Immutability on GCP Storage Buckets
  rationale: Enabling object lock or immutability on storage buckets helps prevent data tampering and unauthorized deletion, ensuring data integrity and compliance with regulatory requirements like GDPR and HIPAA. This feature can protect businesses from data breaches, accidental deletions, and ensure reliable data retention for audits and legal holds, reducing the risk of financial penalties and reputational damage.
  description: This rule checks whether object lock or immutability is enabled on Google Cloud Storage buckets, enhancing data protection by preventing modifications or deletions within a specified retention period. To verify, ensure that the 'Object Versioning' and 'Retention Policies' are configured on the bucket. Remediation involves configuring a retention policy in the Cloud Console or using the gcloud command-line tool, setting the desired retention period, and enabling object versioning.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/storage/docs/using-object-versioning
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
- rule_id: gcp.storage.bucket.data_protection_policy_denies_public_and_insecure
  service: storage
  resource: bucket
  requirement: Data Protection Policy Denies Public And Insecure
  scope: storage.bucket.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Access to GCP Storage Buckets
  rationale: Allowing public access to storage buckets can expose sensitive data, leading to data breaches and non-compliance with regulations like GDPR and HIPAA. Unauthorized access could result in data leaks, financial losses, and reputational damage. Implementing strict access controls helps protect against these risks and ensures data confidentiality and integrity.
  description: This rule evaluates storage bucket IAM policies to ensure no public access is granted. It checks for 'allUsers' or 'allAuthenticatedUsers' roles on buckets and denies configurations that allow public access. To remediate, remove any public roles and apply least privilege principles, only granting access to specific, authorized users or service accounts. Verification involves reviewing and modifying bucket permissions in the GCP Console or using the gcloud command-line tool.
  references:
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/storage/docs/public-data
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
  - https://cloud.google.com/security/compliance/iso-27001
  - https://cloud.google.com/security/compliance/soc-2
- rule_id: gcp.storage.bucket.data_protection_require_tls_in_transit
  service: storage
  resource: bucket
  requirement: Data Protection Require TLS In Transit
  scope: storage.bucket.security
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure TLS is Enforced for Data in Transit in GCP Storage Buckets
  rationale: Requiring TLS for data in transit helps protect sensitive data from interception and unauthorized access while being transferred over networks. This enhances data confidentiality and integrity, reducing the risk of data breaches. Compliance with regulations such as PCI-DSS and HIPAA often mandates encryption of data in transit to safeguard personal and financial information.
  description: This rule checks that all Google Cloud Storage buckets enforce the use of TLS (Transport Layer Security) to encrypt data in transit. Ensure that the bucket's client applications and services are configured to communicate over TLS. Verification involves reviewing bucket settings and confirming that the 'gs://bucket-name/*' URIs use HTTPS. Remediation includes updating applications to enforce HTTPS connections and configuring bucket policies to disallow HTTP connections.
  references:
  - https://cloud.google.com/storage/docs/encryption#encryption-in-transit
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-52r1.pdf
- rule_id: gcp.storage.bucket.data_protection_versioning_enabled
  service: storage
  resource: bucket
  requirement: Data Protection Versioning Enabled
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Versioning on GCP Storage Buckets for Data Protection
  rationale: Enabling versioning on Google Cloud Storage buckets is essential for data protection against accidental deletions or overwrites, ensuring that previous versions of objects can be recovered. This mitigates risks associated with data loss and supports compliance with data retention policies required by regulatory standards such as GDPR. By maintaining a history of object versions, businesses can safeguard critical data assets and maintain integrity and availability in the event of data corruption or malicious activity.
  description: This rule checks whether versioning is enabled on GCP Storage buckets, allowing for the retention of multiple versions of objects. To verify, access the Cloud Console, navigate to Storage, and check the bucket's 'Versioning' setting. If not enabled, use the 'gsutil versioning set on gs://[BUCKET_NAME]' command to activate it. This ensures that all changes to objects are tracked, facilitating recovery and rollback capabilities.
  references:
  - https://cloud.google.com/storage/docs/using-object-versioning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.default_encryption
  service: storage
  resource: bucket
  requirement: Default Encryption
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Default Encryption for GCP Storage Buckets
  rationale: Default encryption for GCP storage buckets is crucial to protect sensitive data from unauthorized access and potential breaches. Without encryption, data at rest is vulnerable to exposure, which can lead to significant financial loss, reputational damage, and non-compliance with regulations such as GDPR and HIPAA.
  description: This rule verifies that Google Cloud Storage buckets have a default encryption method configured. By default, GCP encrypts data at rest, but specifying a customer-managed encryption key (CMEK) provides additional control. To ensure compliance, verify that each bucket has a default encryption key configured via the Cloud Console or using the 'gsutil' command-line tool. Remediation involves setting a default bucket-level encryption key in the GCP Console or using the 'gsutil encryption' command to apply a CMEK.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://www.pcisecuritystandards.org
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.bucket.dr_documentation_access_rbac_least_privilege
  service: storage
  resource: bucket
  requirement: DR Documentation Access RBAC Least Privilege
  scope: storage.bucket.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Least Privilege for DR Documentation Access in GCS Buckets
  rationale: Implementing least privilege access for disaster recovery (DR) documentation in Google Cloud Storage (GCS) minimizes unauthorized access risks, reducing the potential for data breaches. Proper access controls are crucial to protect sensitive DR information, which may contain critical operational data. Non-compliance with least privilege principles can lead to regulatory fines and loss of customer trust.
  description: This rule checks that access to GCS buckets containing DR documentation is restricted to only those identities that require it, using Role-Based Access Control (RBAC). The rule ensures that no overly permissive roles like 'Owner' or 'Editor' are granted unless absolutely necessary. To verify, review IAM policies for the bucket and adjust roles to align with least privilege principles. Remediation involves applying more restrictive roles, such as 'Viewer' or custom roles, tailored to the specific needs of the users.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/iam/docs/understanding-service-accounts
  - https://cloud.google.com/iam/docs/overview
  - https://www.isaca.org/resources/cobit
- rule_id: gcp.storage.bucket.dr_documentation_encrypted_and_private
  service: storage
  resource: bucket
  requirement: DR Documentation Encrypted And Private
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DR Documentation Buckets Are Encrypted and Private
  rationale: Encrypting and restricting access to disaster recovery (DR) documentation stored in Cloud Storage buckets ensures data confidentiality and integrity, preventing unauthorized access and potential data breaches. This is critical for maintaining compliance with regulatory standards such as GDPR and HIPAA, which mandate rigorous data protection measures.
  description: This rule verifies that DR documentation stored in GCP Cloud Storage buckets is encrypted at rest using Customer-Managed Encryption Keys (CMEK) and is not publicly accessible. To comply, ensure buckets have CMEK configured and update IAM policies to restrict access to authorized personnel only. Remediation involves enabling CMEK for the bucket and reviewing IAM policies to remove public access.
  references:
  - https://cloud.google.com/storage/docs/encryption/customer-managed-keys
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.bucket.dr_runbook_access_rbac_least_privilege
  service: storage
  resource: bucket
  requirement: DR Runbook Access RBAC Least Privilege
  scope: storage.bucket.authorization
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure DR Runbook Access Follows Least Privilege in GCP Buckets
  rationale: Implementing least privilege access for Disaster Recovery (DR) Runbooks minimizes the risk of unauthorized access and potential data breaches. By restricting access to only those who require it, organizations can prevent insider threats and limit exposure to sensitive data, thus enhancing compliance with regulations like PCI-DSS and ISO 27001.
  description: This rule checks that access to DR Runbooks stored in GCP buckets is granted following the principle of least privilege. It involves auditing IAM policies associated with the buckets to ensure that only necessary roles have access, such as Viewer or Editor, and not Owner unless absolutely required. Remediation involves adjusting IAM policies to remove excessive permissions and regularly reviewing access logs to ensure compliance.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/security/compliance/cis-gcp-foundations-benchmark
  - https://www.nist.gov/itl/applied-cybersecurity/nist-privacy-framework
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/iso-27001
- rule_id: gcp.storage.bucket.dr_runbook_change_audit_logging_enabled
  service: storage
  resource: bucket
  requirement: DR Runbook Change Audit Logging Enabled
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for DR Runbook Changes in Buckets
  rationale: Enabling audit logging for DR runbook changes in storage buckets is crucial for detecting unauthorized access and modifications, ensuring data integrity, and maintaining a reliable disaster recovery process. It helps organizations meet compliance requirements such as PCI-DSS and ISO 27001 by providing an audit trail of activities. Without audit logging, businesses risk undetected breaches and non-compliance with regulatory standards.
  description: This rule checks if audit logging is enabled on GCP storage buckets used for disaster recovery runbooks. To verify, ensure that bucket logging is configured to capture 'write' actions and store logs in a designated logging bucket. Remediation involves setting up a logging bucket and configuring the desired bucket to send logs there, using the GCP Console or command line tools. This setup helps track changes and access patterns, providing visibility into DR processes.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/storage/docs/audit-logs
  - 'CIS GCP Benchmark: Section 4.5 - Ensure Cloud Storage Logs are Enabled'
  - https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2_1.pdf
  - 'NIST SP 800-53: AU-2 Event Logging'
  - ISO/IEC 27001:2013 - A.12.4.1 Event Logging
- rule_id: gcp.storage.bucket.dr_runbook_encrypted_and_private
  service: storage
  resource: bucket
  requirement: DR Runbook Encrypted And Private
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure DR Runbook Buckets Are Encrypted and Private
  rationale: Encrypting disaster recovery (DR) runbooks stored in Google Cloud Storage is crucial to protect sensitive operational data from unauthorized access and ensure data integrity. Misconfigured bucket access or lack of encryption could lead to data breaches, exposing critical DR plans to malicious entities, and potentially violating compliance requirements such as GDPR and HIPAA, which mandate strict data protection measures.
  description: This rule checks if the DR runbook buckets in GCP are encrypted using Cloud KMS keys and have their access permissions set to private. Ensure that all buckets containing DR runbooks use server-side encryption with customer-managed keys (CMEK) to enhance control over cryptographic operations. Verify the IAM policies to confirm that only authorized personnel have access. To remediate, configure bucket-level IAM policies to restrict access and apply encryption settings through the Google Cloud Console or gsutil command-line tool.
  references:
  - https://cloud.google.com/storage/docs/best-practices
  - https://cloud.google.com/iam/docs/overview
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0 - 5.1
  - https://cloud.google.com/security/compliance
  - NIST SP 800-57 Part 1
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.storage.bucket.encryption_enabled
  service: storage
  resource: bucket
  requirement: Encryption Enabled
  scope: storage.bucket.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure Encryption is Enabled for GCP Storage Buckets
  rationale: Enabling encryption for storage buckets protects sensitive data from unauthorized access, thereby mitigating risks such as data breaches and compliance violations. Encryption at rest is crucial for meeting regulatory standards like PCI-DSS and HIPAA, which mandate the protection of stored data to ensure privacy and security.
  description: This rule checks that Google Cloud Storage buckets have encryption enabled, ensuring data is automatically encrypted at rest. Users should configure bucket-level key management settings, selecting either Google-managed keys or customer-managed keys via Cloud KMS for enhanced control. Verification can be done through the GCP Console by navigating to the Storage browser and inspecting the 'Encryption' field. To remediate, set the desired encryption type in the bucket's settings.
  references:
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-57pt1r4.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.bucket.iam_no_public_access
  service: storage
  resource: bucket
  requirement: IAM No Public Access
  scope: storage.bucket.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Ensure Storage Buckets Have No Public IAM Access
  rationale: Public IAM access to storage buckets can lead to unauthorized data exposure, resulting in data breaches and non-compliance with regulations like GDPR and HIPAA. Limiting access prevents malicious actors from exploiting data and minimizes the risk of data loss or theft, thereby protecting organizational reputation and avoiding potential financial penalties.
  description: This rule checks that GCP storage buckets do not have IAM policies granting public access, such as 'allUsers' or 'allAuthenticatedUsers' roles. To verify, inspect the bucket's IAM policy for these roles and remove them if present. Remediation involves updating the bucket's IAM policy to restrict access to specific users or service accounts as required.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/public-access-prevention
  - https://cloud.google.com/iam/docs/granting-roles-to-service-accounts
- rule_id: gcp.storage.bucket.level_public_access_blocks
  service: storage
  resource: bucket
  requirement: Level Public Access Blocks
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Access to GCP Storage Buckets
  rationale: Publicly accessible storage buckets can expose sensitive data to unauthorized users, leading to data breaches, financial loss, and damage to reputation. Ensuring buckets are not publicly accessible mitigates risks of data exfiltration and supports compliance with regulations such as GDPR, HIPAA, and PCI-DSS that mandate data protection measures.
  description: This rule checks for and enforces the configuration of public access blocks on GCP storage buckets to prevent unauthorized public access. It ensures that all buckets have 'publicAccessPrevention' set to 'enforced', which restricts public access at the bucket level. To verify, review the 'publicAccessPrevention' setting in the bucket's IAM policies in the Google Cloud Console or via the CLI. Remediation involves configuring the bucket settings to enforce public access blocks, thereby ensuring data is only accessible by authenticated users.
  references:
  - https://cloud.google.com/storage/docs/access-control/public-access-prevention
  - https://cloud.google.com/security/compliance/cis#storage
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/compliance
  - https://www.iso.org/iso-27001-information-security.html
  - https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-53r4.pdf
- rule_id: gcp.storage.bucket.lifecycle_enabled
  service: storage
  resource: bucket
  requirement: Lifecycle Enabled
  scope: storage.bucket.lifecycle_management
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Bucket Lifecycle Policies for Cost and Security Management
  rationale: Enabling lifecycle policies on GCP storage buckets helps automate data retention and deletion processes, reducing storage costs and limiting the data exposure to unauthorized access over time. This is crucial for maintaining compliance with data protection regulations such as GDPR, which mandates the minimization of stored data and timely deletion. Furthermore, lifecycle management assists in mitigating risks associated with stale data that may become a target for data breaches.
  description: This rule checks whether lifecycle policies are enabled on GCP storage buckets. A lifecycle policy allows you to define conditions to automatically transition objects to lower-cost storage classes or delete them when they are no longer needed, optimizing storage costs and enhancing security. To verify, navigate to the GCP Console, select the bucket, and check the 'Lifecycle' configuration under 'Bucket details'. If no policies are present, configure them according to your data retention policy by specifying rules for transitioning or deleting objects.
  references:
  - https://cloud.google.com/storage/docs/managing-lifecycles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/document-2239
  - https://cloud.google.com/storage/docs/best-practices
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control
- rule_id: gcp.storage.bucket.lifecycle_enabled_gcp_compute_disk_protection_replication
  service: storage
  resource: bucket
  requirement: Lifecycle Enabled Gcp Compute Disk Protection Replication
  scope: storage.bucket.lifecycle_management
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Lifecycle Policies for GCP Storage Buckets and Disk Replication
  rationale: Enabling lifecycle policies on storage buckets with disk protection replication is crucial for automatic data management and cost optimization. It helps mitigate the risk of data loss and ensures compliance with data retention policies by automating the transition or deletion of objects. This reduces the potential attack surface by automatically removing outdated or unnecessary data, aligning with compliance frameworks that require data retention and deletion policies.
  description: This rule checks if Google Cloud Storage buckets have lifecycle policies configured to manage object transitions and deletions, particularly for replicating GCP compute disk data. Verify that lifecycle management is enabled to automate data retention and deletion. To remediate, configure lifecycle rules in the GCP Console or via the gcloud CLI to define actions like transitioning to cold storage or deletion of old objects, ensuring that disk snapshots are properly replicated and retained as per business requirements.
  references:
  - https://cloud.google.com/storage/docs/lifecycle
  - https://cloud.google.com/compute/docs/disks/snapshots
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/best-practices#lifecycle
- rule_id: gcp.storage.bucket.log_retention_policy_lock
  service: storage
  resource: bucket
  requirement: Log Retention Policy Lock
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: low
  title: Ensure Bucket Log Retention Policy is Locked
  rationale: Locking the log retention policy for storage buckets prevents accidental or malicious changes to the log retention settings, ensuring consistent audit logging practices. This helps in maintaining a reliable security posture by ensuring that audit logs are retained for the required duration, which is critical for forensic investigations and compliance with regulatory requirements such as GDPR and PCI-DSS.
  description: This rule checks if the log retention policy on a GCP storage bucket is locked, preventing alterations. To verify, ensure that the bucket's log retention policy is configured with a 'retentionPolicy.isLocked' setting as 'true'. Remediation involves setting the 'retentionPolicy.isLocked' to 'true' using the GCP Console or gcloud CLI, which ensures that the policy cannot be modified until the lock is removed.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://cloud.google.com/storage/docs/using-bucket-lock
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.storage.bucket.logging_enabled
  service: storage
  resource: bucket
  requirement: Logging Enabled
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Logging is Enabled for GCP Storage Buckets
  rationale: Enabling logging for GCP storage buckets is crucial for tracking data access and modifications, which helps in identifying unauthorized or suspicious activities. This practice supports incident response and forensic investigations, and aids in meeting compliance requirements such as PCI-DSS, which mandates logging of all access to cardholder data environments.
  description: This rule checks if logging is enabled for Google Cloud Storage buckets by verifying the presence of a logging configuration. Logging should be configured to write access logs to a designated bucket, preferably in a different project to prevent tampering by users with access to the source bucket. To enable logging, set the logging configuration using the Google Cloud Console or the `gsutil logging set` command. Ensure that the target bucket for logs is accessible only to authorized personnel.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/storage/docs/enabling-bucket-logging
  - https://cloud.google.com/security/compliance/pci-dss
  - https://cloud.google.com/security/compliance/nist-800-53
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/storage/docs/audit-logging
- rule_id: gcp.storage.bucket.multi_region_configured
  service: storage
  resource: bucket
  requirement: Multi Region Configured
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure GCP Storage Buckets are Configured with Multi-Region
  rationale: Configuring storage buckets with multi-region locations enhances data redundancy and availability, minimizing the risk of data loss due to regional failures. This configuration is crucial for business continuity, especially for applications with global reach, and aligns with compliance requirements for data availability and redundancy.
  description: This rule checks that Google Cloud Storage buckets are set to use multi-region configurations, which distribute data across multiple regions within a continent, providing higher availability compared to single-region storage. To verify, review the bucket's location setting in the GCP Console or use the `gsutil ls -Lb gs://BUCKET_NAME` command. If a bucket is not multi-region, update it via the GCP Console or CLI by creating a new bucket with the desired multi-region setting and transferring data accordingly.
  references:
  - https://cloud.google.com/storage/docs/locations#available_locations
  - https://cloud.google.com/storage/docs/gsutil/commands/ls
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 5.2
  - ISO/IEC 27001:2013 - A.11.1.4 Equipment Maintenance
  - 'NIST SP 800-53 Rev. 5 - CP-6: Alternate Storage Site'
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.object_retention_configured
  service: storage
  resource: bucket
  requirement: Object Retention Configured
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure GCS Object Retention Policies Are Configured
  rationale: Configuring object retention policies in Google Cloud Storage (GCS) is crucial to prevent premature deletion of critical data, thereby helping to meet regulatory compliance and protect against data loss. Without proper retention settings, organizations face risks such as accidental data deletion or non-compliance with legal data retention requirements, potentially resulting in financial penalties and reputational damage.
  description: This rule checks if object retention policies are configured for Google Cloud Storage buckets, which enforce minimum retention periods for objects. To verify, examine the bucket's IAM policies and retention policy settings through the Google Cloud Console or the `gsutil` command-line tool. Remediation involves setting a retention policy via the GCP Console or `gsutil retention set` command, ensuring that all stored objects adhere to the specified retention period, thus preventing deletions before the retention period expires.
  references:
  - https://cloud.google.com/storage/docs/bucket-lock
  - https://cloud.google.com/storage/docs/using-bucket-lock
  - CIS Google Cloud Computing Foundations Benchmark v1.2.0 - 6.2 Ensure that object versioning is enabled on storage buckets
  - NIST SP 800-53 Rev. 5 - CP-6, Media Protection
  - https://cloud.google.com/storage/docs/json_api/v1/buckets/lockRetentionPolicy
- rule_id: gcp.storage.bucket.object_versioning
  service: storage
  resource: bucket
  requirement: Object Versioning
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Enable Object Versioning for GCP Storage Buckets
  rationale: Object versioning is crucial for data integrity and recovery, allowing organizations to restore previous versions of objects in case of accidental deletion or corruption. This feature mitigates risks associated with data loss and unauthorized alterations, while also supporting compliance with data protection regulations like GDPR and PCI-DSS that require data retention and audit capabilities.
  description: This rule checks if object versioning is enabled for Google Cloud Storage buckets. Object versioning ensures that all versions of an object are retained, allowing recovery to any prior state. To verify this setting, check the 'Versioning' configuration of your storage buckets in the GCP Console or use the gcloud command-line tool. Remediation involves enabling the versioning option in the bucket settings to ensure data can be recovered in case of accidental or malicious modifications.
  references:
  - https://cloud.google.com/storage/docs/using-object-versioning
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://cloud.google.com/security/compliance
- rule_id: gcp.storage.bucket.policy_public_write_access
  service: storage
  resource: bucket
  requirement: Policy Public Write Access
  scope: storage.bucket.public_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Prevent Public Write Access to GCS Buckets
  rationale: Public write access to Google Cloud Storage buckets allows unauthorized users to upload or modify data, leading to potential data breaches, data loss, or service disruptions. This poses significant risks to data integrity and confidentiality, and may violate compliance requirements such as PCI-DSS or HIPAA which mandate strict access controls.
  description: This rule checks if any Google Cloud Storage bucket policies permit public write access. Public write permissions allow any user on the internet to modify bucket contents, posing serious security risks. To verify, review the bucket's IAM policy for 'allUsers' or 'allAuthenticatedUsers' with 'roles/storage.objectCreator' or 'roles/storage.legacyBucketWriter'. Remediation involves removing these permissions or restricting write access to specific authenticated users or groups.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/storage/docs/public-access-prevention
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/iam-permissions
- rule_id: gcp.storage.bucket.public_access
  service: storage
  resource: bucket
  requirement: Public Access
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Access to GCP Storage Buckets
  rationale: Publicly accessible storage buckets pose significant security risks as they may expose sensitive data to unauthorized users. This can lead to data breaches, financial losses, and compliance violations with regulations such as GDPR, HIPAA, and PCI-DSS. Ensuring that storage buckets are not publicly accessible helps protect sensitive business data and maintain customer trust.
  description: This rule checks for storage buckets that allow public access, which means that the 'allUsers' or 'allAuthenticatedUsers' members have been granted IAM permissions. To verify, check the IAM policies of each storage bucket for such entries. Remediation involves removing public access entries and ensuring that only specific, authenticated users or service accounts have access. Utilize the GCP Console or gcloud CLI to review and update bucket permissions accordingly.
  references:
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://cloud.google.com/storage/docs/access-control/making-data-public
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/best-practices
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.storage.bucket.public_read_prohibited
  service: storage
  resource: bucket
  requirement: Public Read Prohibited
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Read Access to Cloud Storage Buckets
  rationale: Ensuring that Cloud Storage buckets are not publicly readable is critical to protecting sensitive data from unauthorized access. Publicly accessible buckets can lead to data leaks, compliance violations, and potential financial and reputational damage. Many compliance frameworks such as PCI-DSS, HIPAA, and ISO 27001 require strict access controls to safeguard data.
  description: This rule checks for buckets that have public read permissions enabled. To verify, review bucket permissions in the Google Cloud Console or use the `gsutil iam get` command to inspect IAM policies. Remediation involves removing 'allUsers' or 'allAuthenticatedUsers' from the bucket's IAM policy. Implement bucket-level IAM policies and use organization policies to enforce restrictions, ensuring that only authorized users can access bucket contents.
  references:
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/storage/docs/access-control/iam
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/best-practices
  - https://cloud.google.com/iam/docs/overview
- rule_id: gcp.storage.bucket.public_write_prohibited
  service: storage
  resource: bucket
  requirement: Public Write Prohibited
  scope: storage.bucket.public_access
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: critical
  title: Prevent Public Write Access to Cloud Storage Buckets
  rationale: Allowing public write access to storage buckets can lead to unauthorized data tampering, data exfiltration, and service disruption. This poses significant security risks such as data breaches and compliance violations with standards like PCI-DSS and HIPAA, potentially resulting in financial penalties and damaged reputation.
  description: This rule ensures that Google Cloud Storage buckets do not allow public write access by checking the bucket's IAM policy and access control lists (ACLs) for entries that permit 'allUsers' or 'allAuthenticatedUsers' to write. To verify, inspect IAM permissions and ACL settings in the GCP Console or use the 'gsutil iam get' and 'gsutil acl get' commands. Remediation involves removing public write permissions by updating the IAM policy and ACLs to restrict write access to only authorized users or service accounts.
  references:
  - https://cloud.google.com/storage/docs/access-control
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf
  - https://cloud.google.com/storage/docs/iam-permissions
- rule_id: gcp.storage.bucket.replication_enabled
  service: storage
  resource: bucket
  requirement: Replication Enabled
  scope: storage.bucket.replication
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Storage Bucket Replication is Enabled
  rationale: Enabling replication for storage buckets enhances data availability and disaster recovery capabilities by automatically duplicating data across different regions. This reduces the risk of data loss due to regional outages and aligns with compliance frameworks that mandate data redundancy and resilience. Furthermore, it supports business continuity and minimizes downtime in case of infrastructural failures.
  description: This rule checks if replication is enabled for Google Cloud Storage buckets, ensuring data redundancy across geographically separate locations. To verify, review the bucket's settings in the GCP Console or using the gsutil command-line tool to confirm replication configuration. Remediation involves configuring the bucket's replication settings to automatically copy data to a secondary location, which can be done via the GCP Console or by updating the bucket's replication configuration using gsutil.
  references:
  - https://cloud.google.com/storage/docs/using-replication
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.nist.gov/cyberframework
  - https://cloud.google.com/storage/docs/disaster-recovery
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/gsutil/commands/replication
- rule_id: gcp.storage.bucket.secure_transport_policy
  service: storage
  resource: bucket
  requirement: Secure Transport Policy
  scope: storage.bucket.policy_management
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Buckets Enforce Secure Transport for Data Integrity
  rationale: Using secure transport mechanisms, such as HTTPS, to access GCP Storage Buckets helps prevent data interception and unauthorized access during data transmission. This is crucial for maintaining data integrity and confidentiality, especially for sensitive information. Compliance with regulations like GDPR, HIPAA, and PCI-DSS often mandates secure data transport to protect personal and financial information.
  description: This rule checks if Storage Buckets enforce the use of secure transport (HTTPS) by ensuring the 'Enforce TLS' policy is enabled. This can be verified by checking the bucket's IAM settings where 'secureTransport' is set to true. If not enabled, modify the bucket policy to enforce HTTPS by updating the bucket's IAM policy to include a condition that denies any request not using secure transport.
  references:
  - https://cloud.google.com/storage/docs/authentication#transport-encryption
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/using-cmek
  - https://cloud.google.com/storage/docs/best-practices
- rule_id: gcp.storage.bucket.server_access_logging_enabled
  service: storage
  resource: bucket
  requirement: Server Access Logging Enabled
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable Server Access Logging for Storage Buckets
  rationale: Enabling server access logging for storage buckets is crucial for monitoring and analyzing access patterns, which helps identify potential unauthorized access or anomalies. This enhances security posture by providing visibility into who accessed data and when, which is essential for forensic investigations and meeting compliance requirements like GDPR and HIPAA.
  description: This rule checks whether server access logging is enabled for Google Cloud Storage buckets. Access logs provide detailed records of requests made to a bucket, which are crucial for auditing and monitoring purposes. To verify, confirm that the 'logging' configuration is set for the bucket and points to a specified log bucket. Remediation involves configuring the logging settings via the GCP Console or CLI to direct logs to a designated bucket, ensuring appropriate permissions are set to prevent unauthorized access.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://www.pcisecuritystandards.org/pci_security/
  - https://cloud.google.com/logging/docs
  - https://cloud.google.com/storage/docs/enabling-bucket-logging
- rule_id: gcp.storage.bucket.server_access_logging_enabled_gcp_logging_dataevent_logging
  service: storage
  resource: bucket
  requirement: Server Access Logging Enabled Gcp Logging Dataevent Logging
  scope: storage.bucket.logging
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Enable GCP Storage Bucket Server Access Logging
  rationale: Enabling server access logging for storage buckets is crucial for monitoring and tracking access patterns, which helps in identifying unauthorized access and potential data breaches. This is important for compliance with data protection regulations like GDPR and facilitates forensic investigations and auditing processes.
  description: This rule checks whether server access logging is enabled for Google Cloud Storage buckets. Server access logging provides detailed records of requests made to a bucket, which are crucial for auditing and compliance. To verify, ensure that each bucket has log delivery configuration set with a designated logging bucket. Remediation involves configuring the bucket's logging settings via the GCP Console or gcloud CLI to record access events to a specified logging bucket.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/security/compliance/cis#gcp
  - https://www.iso.org/isoiec-27001-information-security.html
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/storage/docs/access-control/audit-logging
  - https://cloud.google.com/storage/docs/viewing-logs
- rule_id: gcp.storage.bucket.server_access_logging_enabled_gcp_logging_gcs_replication
  service: storage
  resource: bucket
  requirement: Server Access Logging Enabled Gcp Logging Gcs Replication
  scope: storage.bucket.logging
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: medium
  title: Enable Server Access Logging for GCS Buckets
  rationale: Enabling server access logging on Google Cloud Storage (GCS) buckets allows tracking of access requests, which is critical for auditing, compliance, and detecting anomalous activities. It helps in understanding data access patterns and mitigating potential threats by providing a forensic trail that is crucial for incident response and regulatory compliance, such as GDPR and SOC2.
  description: This rule checks that server access logging is enabled for GCS buckets to ensure access requests are logged and stored in a designated logging bucket. To verify, ensure the target bucket has the appropriate logging configuration via the GCP Console or gcloud CLI. Remediation involves configuring the bucket's logging settings to specify a destination for log entries, typically another GCS bucket with restricted access.
  references:
  - https://cloud.google.com/storage/docs/access-logs
  - https://cloud.google.com/security/compliance/cis#gcp_cis_v1.1_7.1
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance/soc-2
  - https://cloud.google.com/security/compliance/gdpr
  - https://cloud.google.com/storage/docs/configuring-default-event-based-hold
- rule_id: gcp.storage.bucket.versioning_enabled
  service: storage
  resource: bucket
  requirement: Versioning Enabled
  scope: storage.bucket.versioning
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure Bucket Versioning is Enabled in GCP Storage
  rationale: Enabling versioning on GCP Storage buckets is crucial for protecting against accidental overwrites or deletions of critical data. It provides the ability to recover previous versions of objects, which helps in mitigating data loss due to human error, application bugs, or malicious attacks. Compliance with data retention policies and regulatory standards often requires maintaining historical data states.
  description: This check verifies that versioning is enabled on GCP Storage buckets. Versioning allows you to preserve, retrieve, and restore every version of every object stored in a bucket, providing an additional layer of data protection. To enable versioning, access the GCP Console, navigate to the desired bucket, and activate versioning under the 'Versioning' section. This can also be done via the 'gsutil versioning set on' command. Remediation involves enabling this feature on all buckets lacking it to ensure data resiliency.
  references:
  - https://cloud.google.com/storage/docs/using-object-versioning
  - https://cloud.google.com/storage/docs/best-practices
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0 - 6.1
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-checklist/
  - https://www.iso.org/iso-27001-information-security.html
- rule_id: gcp.storage.bucket.website_https_only_configured
  service: storage
  resource: bucket
  requirement: Website HTTPS Only Configured
  scope: storage.bucket.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Ensure GCS Buckets Serve Websites Over HTTPS
  rationale: Configuring Google Cloud Storage buckets to serve website content exclusively over HTTPS is crucial to protect data integrity and confidentiality by encrypting data in transit. This prevents man-in-the-middle attacks and eavesdropping, safeguarding sensitive information from unauthorized access. Compliance with security frameworks like PCI-DSS and HIPAA often requires secure transmission of data, ensuring adherence to these standards enhances business reputation and trust.
  description: This rule checks whether Google Cloud Storage buckets configured for website hosting enforce HTTPS-only access. To verify, ensure that the bucket's website configuration mandates HTTPS by redirecting HTTP requests to HTTPS. Remediation involves updating the bucket configuration to include a policy that redirects HTTP requests to HTTPS, thereby ensuring data is securely transmitted. This can typically be done via the GCP Console or using the gsutil command line tool.
  references:
  - https://cloud.google.com/storage/docs/hosting-static-website
  - https://cloud.google.com/storage/docs/encryption
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.pcisecuritystandards.org/
  - https://www.hipaajournal.com/hipaa-compliance-guide/
- rule_id: gcp.storage.notification.data_protection_bucket_cross_account_destinations_restricted
  service: storage
  resource: notification
  requirement: Data Protection Bucket Cross Account Destinations Restricted
  scope: storage.notification.security
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: low
  title: Restrict Cross-Account Destinations for Storage Notifications
  rationale: Allowing cross-account destinations for storage notifications can expose sensitive data to unauthorized entities, leading to data breaches. It is crucial to restrict these destinations to mitigate risks of data leakage and to comply with data protection regulations such as GDPR and CCPA.
  description: This rule checks whether storage notifications are configured to send data to cross-account destinations. To verify, review the notification configurations in your Cloud Storage settings and ensure that destinations are limited to trusted accounts within your organization. Remediation involves updating the notification configuration to exclude cross-account destinations unless absolutely necessary and permitted under your data protection policy.
  references:
  - https://cloud.google.com/storage/docs/configuring-notifications
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/docs/security/best-practices
- rule_id: gcp.storage.notification.data_protection_bucket_destination_encrypted
  service: storage
  resource: notification
  requirement: Data Protection Bucket Destination Encrypted
  scope: storage.notification.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Storage Bucket Notifications Use Encrypted Destinations
  rationale: Encrypting bucket destinations for storage notifications is crucial to protect sensitive data from unauthorized access, especially when notifications are sent to external systems or services. Unencrypted destinations can lead to data breaches, violating compliance requirements such as PCI-DSS and HIPAA, and exposing organizations to financial and reputational damage.
  description: This rule checks that all Google Cloud Storage bucket notifications are configured to use encrypted destinations. To verify, ensure that the destination of the notification, such as a Pub/Sub topic, is encrypted using customer-managed encryption keys (CMEK) or Google-managed encryption keys. Remediation involves enabling encryption for the destination service, ensuring that sensitive data remains protected at rest.
  references:
  - https://cloud.google.com/storage/docs/pubsub-notifications
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.storage.notification.data_protection_bucket_destination_least_privilege
  service: storage
  resource: notification
  requirement: Data Protection Bucket Destination Least Privilege
  scope: storage.notification.least_privilege
  domain: storage_and_database_security
  subcategory: storage_encryption
  severity: high
  title: Ensure Least Privilege for Storage Notification Bucket Destinations
  rationale: Implementing least privilege for bucket destinations in storage notifications is critical to minimize the risk of unauthorized access or data breaches. By restricting permissions, organizations can prevent exploitation of overly permissive roles that could lead to data leakage, non-compliance with regulations like GDPR and PCI-DSS, and potential financial and reputational damage.
  description: This rule checks that storage notification configurations are using the least privilege principle for bucket destinations. It requires verifying that IAM roles associated with bucket notifications do not have excessive permissions beyond what is necessary for their function. Remediation involves reviewing the roles assigned to these bucket destinations and adjusting permissions to align with the principle of least privilege, ensuring that only necessary permissions are granted.
  references:
  - https://cloud.google.com/storage/docs/pubsub-notifications
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform/
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/storage/docs/access-control/iam
- rule_id: gcp.trace.trace.monitoring_access_rbac_least_privilege
  service: trace
  resource: trace
  requirement: Monitoring Access RBAC Least Privilege
  scope: trace.trace.monitoring
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Least Privilege for GCP Trace Monitoring Access
  rationale: Implementing least privilege for monitoring access reduces the attack surface by ensuring that users and services only have permissions necessary for their role. This minimizes the risk of unauthorized data access, potential data breaches, and exposure of sensitive tracing information, which is vital for maintaining compliance with standards like PCI-DSS and ISO 27001.
  description: This rule checks if GCP Trace monitoring access is restricted to the minimum set of permissions required for users and services. It involves reviewing role assignments and permissions related to the trace.trace.monitoring resource to ensure they adhere to the least privilege principle. Remediation involves auditing current roles, removing unnecessary permissions, and assigning predefined roles like 'roles/cloudtrace.agent' only as needed. Verification can be done through the GCP IAM console or using gcloud CLI commands to list and review permissions associated with trace monitoring.
  references:
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://cloud.google.com/trace/docs/iam
  - CIS Google Cloud Platform Foundation Benchmark v1.3.0, Section 1.6
  - https://cloud.google.com/security/compliance
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://www.pcisecuritystandards.org/
- rule_id: gcp.trace.trace.monitoring_retention_days_minimum
  service: trace
  resource: trace
  requirement: Monitoring Retention Days Minimum
  scope: trace.trace.monitoring
  domain: logging_monitoring_and_alerting
  subcategory: audit_logging
  severity: medium
  title: Ensure Minimum Retention of Trace Monitoring Data
  rationale: Maintaining a minimum retention period for trace monitoring data is crucial for incident investigation and compliance with regulatory requirements. Inadequate retention can lead to gaps in data needed for forensic analysis, potentially hindering the detection of security incidents and compliance audits. This ensures that historical data is available to support security and operational analytics, minimizing business risk.
  description: This rule checks that the retention period for trace monitoring data in GCP is set to meet or exceed organizational and compliance requirements. Organizations should configure the minimum retention period based on their specific legal, regulatory, and operational needs. To verify, review the retention settings in the Stackdriver interface and adjust the configuration to extend data retention as necessary. Remediation involves updating the Stackdriver settings to ensure the retention period aligns with best practices and compliance mandates.
  references:
  - https://cloud.google.com/trace/docs/overview
  - https://cloud.google.com/monitoring/settings
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/solutions/best-practices-for-security-and-compliance
- rule_id: gcp.trace.trace.monitoring_store_encrypted
  service: trace
  resource: trace
  requirement: Monitoring Store Encrypted
  scope: trace.trace.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: medium
  title: Ensure Trace Data Encryption at Rest
  rationale: Encryption at rest protects trace data from unauthorized access and mitigates risks associated with data breaches. Encrypting trace data is essential for maintaining customer trust and complying with data protection regulations such as GDPR and HIPAA. Failing to encrypt sensitive data can lead to significant financial and reputational damage in the event of a security incident.
  description: This rule checks whether trace data stored in Google Cloud's monitoring systems is encrypted at rest. Verify that encryption keys are managed and configured properly to protect trace data. To remediate, enable encryption for trace data in the Google Cloud Console or via API by ensuring that all Cloud Trace resources have encryption enabled using either customer-managed encryption keys (CMEK) or Google-managed encryption keys (GMEK).
  references:
  - https://cloud.google.com/trace/docs
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
  - CIS Google Cloud Platform Foundation Benchmark v1.2.0, Section 4.3
  - https://www.nist.gov/cyberframework
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/compliance
- rule_id: gcp.workflows.workflow.incident_approval_steps_required_for_destructive_actions
  service: workflows
  resource: workflow
  requirement: Incident Approval Steps Required For Destructive Actions
  scope: workflows.workflow.security
  domain: compliance_and_governance
  subcategory: security_monitoring
  severity: low
  title: Require Approval Steps for Destructive Workflow Actions
  rationale: Requiring incident approval steps for destructive actions in workflows helps prevent unintended data loss or service disruption. It mitigates the risks of accidental or malicious changes that could compromise system integrity or availability. This control supports compliance with regulations that mandate change management and operational oversight, such as SOC2 and ISO 27001.
  description: This rule checks whether workflows that perform potentially destructive actions, such as data deletion or modification, include mandatory incident approval steps. Ensure workflows have approval mechanisms configured in Google Cloud Workflows by using built-in steps or integrating with external approval systems. To verify, review the workflow definition for approval steps prior to destructive actions, and remediate by adding necessary approval steps in the workflow configuration.
  references:
  - https://cloud.google.com/workflows/docs
  - https://cloud.google.com/security/compliance
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/iam/docs/impersonating-service-accounts
  - https://cloud.google.com/security/best-practices
- rule_id: gcp.workflows.workflow.incident_integrations_least_privilege
  service: workflows
  resource: workflow
  requirement: Incident Integrations Least Privilege
  scope: workflows.workflow.least_privilege
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Workflows Incident Integrations Use Least Privilege
  rationale: Implementing least privilege for workflows in GCP is critical to minimizing potential attack vectors and preventing unauthorized access to sensitive operations. Over-privileged access can lead to data breaches, regulatory non-compliance, and increased risk of lateral movement in case of a compromise. Adhering to least privilege principles supports compliance with frameworks like NIST and ISO 27001, ensuring that permissions are granted based on necessity.
  description: This rule verifies that incident integrations within GCP Workflows adhere to the principle of least privilege by ensuring that only necessary permissions are granted to perform required tasks. Review the IAM roles assigned to workflows and adjust them to ensure they include only permissions essential for their operation. This can be verified by auditing IAM policies associated with workflows and removing any excess permissions. Remediation involves using custom roles or predefined roles that closely match the required permissions without over-provisioning.
  references:
  - https://cloud.google.com/workflows/docs/security-overview
  - https://cloud.google.com/iam/docs/understanding-roles
  - https://www.cisecurity.org/benchmark/google_cloud_computing_platform
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
  - https://www.iso.org/iso-27001-information-security.html
  - https://cloud.google.com/security/best-practices/iam
- rule_id: gcp.workflows.workflow.incident_kms_encryption_enabled
  service: workflows
  resource: workflow
  requirement: Incident KMS Encryption Enabled
  scope: workflows.workflow.encryption
  domain: data_protection_and_privacy
  subcategory: encryption_at_rest
  severity: high
  title: Ensure KMS Encryption is Enabled for GCP Workflows
  rationale: Enabling KMS encryption for workflows protects sensitive data by ensuring that data at rest is encrypted using Google-managed encryption keys. This reduces the risk of unauthorized data access and helps comply with regulatory requirements such as GDPR and HIPAA, which mandate robust data protection mechanisms.
  description: This rule checks if Google Cloud Workflows have Cloud KMS encryption enabled. Verify the configuration by ensuring that each workflow is associated with a KMS key. Remediation involves configuring the workflow to use a Customer-Managed Encryption Key (CMEK) by specifying the KMS key in the workflow's settings. This provides an additional layer of security by allowing key rotation and access control over the encryption keys.
  references:
  - https://cloud.google.com/workflows/docs
  - https://cloud.google.com/security/compliance/cis
  - https://cloud.google.com/kms/docs
  - https://cloud.google.com/security/compliance
  - https://cloud.google.com/security/encryption-at-rest/default-encryption
- rule_id: gcp.workspace.user.mfa_status_configured
  service: workspace
  resource: user
  requirement: MFA Status Configured
  scope: workspace.user.authentication
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure MFA is Configured for All Google Workspace Users
  rationale: Configuring Multi-Factor Authentication (MFA) for Google Workspace users is critical to protect against unauthorized access and account compromise. Without MFA, attackers can easily exploit stolen or weak passwords, leading to potential data breaches and non-compliance with security standards like NIST and PCI-DSS.
  description: This rule checks that all users in Google Workspace have Multi-Factor Authentication (MFA) enabled. Administrators should verify MFA settings in the Google Admin console under Security > 2-Step Verification. If MFA is not enabled for all users, it should be mandated by updating the organization's security settings to require 2-Step Verification. Ensure user education and support for setting up MFA is provided to facilitate smooth adoption.
  references:
  - https://support.google.com/a/answer/175197
  - https://cloud.google.com/security/compliance/cis-benchmarks
  - https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf
  - https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-2-1.pdf
  - https://www.iso.org/standard/54534.html
  - https://csrc.nist.gov/publications/detail/sp/800-63b/final
