metadata:
  csp: K8S
  description: Enriched security and compliance rule IDs for Kubernetes
  total_rules: 649
  format: csp.service.resource.security_check_assertion
  format_version: k8s_enriched_v1
  generated_from: k8s_security_checks_final_list.yaml
  compliance_source: k8s_consolidated_rules_cleaned.csv
  enrichment_date: '2025-12-02'
  domains:
  - logging_and_monitoring
  - workload_security
  - vulnerability_management
  - network_security
  - infrastructure_security
  - business_continuity
  - data_protection
  - identity_and_access_management
  services:
  - scheduler
  - network
  - inventory
  - resource
  - pod_security
  - etcd
  - federation
  - service
  - software
  - persistentvolume
  - monitoring
  - namespace
  - node
  - disaster_recovery
  - cluster
  - event
  - image
  - workload
  - autoscaling
  - pod
  - apiserver
  - controlplane
  - secret
  - kubelet
  - ingress
  - audit
  - general
  - storage
  - certificate
  - configmap
  - rbac
  - admission
  - policy
  - horizontalpodautoscaler
  severity_distribution:
    critical: 12
    high: 156
    medium: 125
    low: 357
  duplicates_removed: 33
  last_deduplication: '20251202_122639'
  duplicates_removed_total: 58
  quality_improvement_date: '2025-12-02'
  quality_improvement_completed: true
rule_ids:
- rule_id: k8s.admission.control.hostipc_restriction
  service: admission
  resource: control
  requirement: Hostipc Restriction
  scope: admission.control.hostipc_restriction
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Restrict HostIPC Namespace Access in Pods
  rationale: Enabling HostIPC allows a pod to share the host's IPC namespace, which can lead to privilege escalation and unauthorized
    inter-process communication. If misconfigured, attackers could exploit this to snoop on or interfere with host-level processes,
    compromising node security and potentially escalating permissions across the cluster.
  description: This rule checks whether the HostIPC field is set to false in pod specifications, preventing pods from sharing
    the host's IPC namespace. A secure configuration ensures that pods cannot access or manipulate host-level processes through
    the IPC namespace, reducing the risk of privilege escalation and maintaining the isolation between pods and the host.
    This control aligns with security best practices outlined in the CIS Kubernetes Benchmark and helps ensure compliance
    with cluster security policies.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - cis_kubernetes_kubernetes_5.2.11_0123
  - cis_kubernetes_kubernetes_5.2.4_0116
- rule_id: k8s.admission.control.hostpath_volumes_restricted
  service: admission
  resource: control
  requirement: Hostpath Volumes Restricted
  scope: admission.control.hostpath_volumes_restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict HostPath Volume Usage in Kubernetes Admission Control
  rationale: Allowing unrestricted HostPath volumes can lead to severe security vulnerabilities as they provide pods with
    direct access to the node's filesystem, potentially allowing for unauthorized data access and privilege escalation. Attackers
    could exploit this to overwrite critical system files or access sensitive information, compromising the integrity and
    confidentiality of the system.
  description: This control checks that HostPath volume usage is restricted through Kubernetes admission control policies.
    It ensures that only explicitly permitted pods can use HostPath volumes, thereby reducing the risk of unauthorized access
    to the host's filesystem. A well-configured policy will specify conditions or namespaces where HostPath volumes are permissible,
    adhering to security best practices and compliance requirements. Validating this control helps enforce a secure boundary
    between containerized applications and the host, mitigating risks associated with privilege escalation and data breaches.
  references:
  - https://kubernetes.io/docs/concepts/storage/volumes/#hostpath
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  compliance:
  - cis_kubernetes_kubernetes_5.2.12_0124
- rule_id: k8s.admission.controller.anomaly_detection_enabled
  service: admission
  resource: controller
  requirement: Anomaly Detection Enabled
  scope: admission.controller.anomaly_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Anomaly Detection in Admission Controllers
  rationale: Without anomaly detection in Kubernetes admission controllers, there is an increased risk of unauthorized or
    malicious changes being applied to the cluster, which can lead to data breaches or service disruptions. Anomaly detection
    helps in identifying and preventing suspicious activities, such as unusual API requests or configuration changes, which
    are common in attack scenarios.
  description: This rule checks whether anomaly detection is enabled in Kubernetes admission controllers. Proper configuration
    involves integrating anomaly detection mechanisms that monitor and alert on unusual patterns or behaviors in API requests
    and other cluster activities. This enhances security by allowing administrators to quickly identify and respond to potential
    threats, ensuring that only legitimate and compliant actions are executed within the cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/
  compliance:
  - soc2_multi_cloud_cc_7_2_0016
- rule_id: k8s.admission.controller.antimalware_policies_enforced
  service: admission
  resource: controller
  requirement: Antimalware Policies Enforced
  scope: admission.controller.antimalware_policies_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Antimalware Policies via Admission Controllers
  rationale: Without enforced antimalware policies, Kubernetes clusters are vulnerable to malicious images and applications,
    potentially leading to unauthorized access, data exfiltration, or service disruption. Attackers can exploit this by deploying
    compromised container images or injecting malware into running applications, jeopardizing cluster integrity and compliance
    with security standards.
  description: This rule checks that the Kubernetes admission controllers are configured to enforce antimalware policies,
    ensuring that only trusted and verified images are deployed. A properly configured admission controller will validate
    metadata and scan images for known vulnerabilities before allowing them into the cluster, thereby reducing the risk of
    malware infiltration. This configuration enhances security by preventing the execution of unverified or compromised code
    and helps in maintaining compliance with standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/#image-security
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook
  - https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/
  compliance:
  - rbi_nbfc_multi_cloud_2.7_0011
  - rbi_nbfc_multi_cloud_5.3_0027
- rule_id: k8s.admission.controller.antivirus_enabled
  service: admission
  resource: controller
  requirement: Antivirus Enabled
  scope: admission.controller.antivirus_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Antivirus on Admission Controllers
  rationale: Without antivirus protection on admission controllers, Kubernetes clusters are vulnerable to malware and malicious
    code injection during resource creation and modification. Attackers can exploit this to gain unauthorized access, cause
    data breaches, or execute arbitrary code within the cluster, impacting confidentiality, integrity, and availability.
  description: This control checks if antivirus software is enabled and properly configured on Kubernetes admission controllers.
    A well-configured antivirus solution helps detect and block malicious payloads during the admission phase, preventing
    harmful resources from entering the cluster. A good configuration involves integrating antivirus solutions that scan for
    known malware signatures and provide real-time protection. This enhances the overall security posture by ensuring that
    any resource created or modified within the cluster is free from malware, thus safeguarding against potential security
    breaches.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/cluster-administration/overview/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - nist_800_171_r2_multi_cloud_3_14_2_3.14.2_Provide_protection_from_malicious_c_0015
- rule_id: k8s.admission.controller.change_control_enabled
  service: admission
  resource: controller
  requirement: Change Control Enabled
  scope: admission.controller.change_control_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Change Control in Kubernetes Admission Controllers
  rationale: Without change control in admission controllers, unauthorized or accidental configuration changes can be introduced,
    leading to potential security vulnerabilities, data breaches, or compliance violations. Attackers could exploit misconfigurations
    to gain unauthorized access or escalate privileges within the cluster.
  description: This control checks that change control mechanisms are active within Kubernetes admission controllers. Proper
    configuration involves ensuring that all changes to admission controller policies are logged, reviewed, and authorized
    before deployment. This reduces the risk of unauthorized changes and helps maintain compliance with security standards
    like the CIS Kubernetes Benchmark. A well-configured change control process prevents unauthorized access by enforcing
    policy validations at the admission level, thereby ensuring that only secure and compliant workloads are admitted into
    the cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/
  - https://kubernetes.io/docs/tasks/administer-cluster/limit-storage-consumption/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-3_0043
  - fedramp_moderate_multi_cloud_CM-3_0105
  - fedramp_moderate_multi_cloud_CM-3_1_0106
  - fedramp_moderate_multi_cloud_CM-3_2_0107
  - fedramp_moderate_multi_cloud_CM-3_4_0108
  - fedramp_moderate_multi_cloud_CM-3_6_0109
- rule_id: k8s.admission.controller.change_management_enabled
  service: admission
  resource: controller
  requirement: Change Management Enabled
  scope: admission.controller.change_management_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Custom Admission Control Webhooks for Change Management
  rationale: Without proper change management enforcement via admission controllers, unauthorized or inadvertent changes can
    occur in the cluster, leading to potential security breaches. Attackers may exploit misconfigured resources or inject
    malicious configurations if changes aren't validated against security policies.
  description: This rule verifies that custom admission control webhooks are configured to enforce change management policies
    within the Kubernetes cluster. It checks for the presence and proper configuration of admission webhooks that validate
    or mutate resources during creation or update. A well-configured admission controller helps ensure that only compliant
    changes are applied to resources, reducing risks of misconfiguration, privilege escalation, and unauthorized access.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook
  - https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/webhooks/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SI-2_0147
  - rbi_bank_multi_cloud_13.1_0006
  - soc2_multi_cloud_cc_3_4_0006
- rule_id: k8s.admission.controller.changes_monitored
  service: admission
  resource: controller
  requirement: Changes Monitored
  scope: admission.controller.changes_monitored
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Monitor and Audit Admission Controller Configuration Changes
  rationale: Without monitoring changes to admission controllers, malicious actors could modify policies to allow unauthorized
    access or escalate privileges within the cluster. This exposure increases the risk of data breaches and non-compliance
    with security standards.
  description: This rule ensures that any configuration changes made to the Kubernetes admission controllers are monitored
    and logged. By validating that audit logs are enabled and properly configured, it helps detect unauthorized modifications
    and policy violations promptly. A good configuration includes enabling audit logging for all admission controller changes,
    ensuring log integrity, and regularly reviewing logs for suspicious activities. This vigilance is crucial for identifying
    potential security incidents early and maintaining compliance with standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - fedramp_moderate_multi_cloud_CA-7_0093
  - fedramp_moderate_multi_cloud_CA-7_1_0094
  - fedramp_moderate_multi_cloud_CA-7_4_0095
  - nist_800_53_rev5_multi_cloud_CA-7-c_0339
  - nist_800_53_rev5_multi_cloud_CA-7-e_0341
- rule_id: k8s.admission.controller.configuration_change_prevention
  service: admission
  resource: controller
  requirement: Configuration Change Prevention
  scope: admission.controller.configuration_change_prevention
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Immutable Admission Controller Configurations
  rationale: Misconfigured admission controllers can lead to unauthorized configuration changes, increasing the risk of privilege
    escalation, data breaches, and compliance violations. Attackers could exploit these vulnerabilities to inject malicious
    configurations or bypass security policies, compromising the cluster's integrity and confidentiality.
  description: This control checks that Kubernetes admission controllers are configured with mechanisms to prevent unauthorized
    changes. A secure configuration involves setting up admission controllers with immutable configurations, ensuring that
    once they are deployed, changes require proper authorization and review. This protects against accidental or malicious
    alterations that could weaken security policies, preventing potential breaches and ensuring compliance with standards
    like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/resource-quotas/
  - https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-d_0400
- rule_id: k8s.admission.controller.configuration_validated
  service: admission
  resource: controller
  requirement: Configuration Validated
  scope: admission.controller.configuration_validated
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Proper Configuration of Admission Controllers
  rationale: Misconfigured admission controllers can lead to unauthorized access, privilege escalation, and potential denial
    of service attacks. Attackers can exploit improperly configured controllers to bypass security policies, inject malicious
    configurations, and compromise cluster integrity.
  description: This control checks that admission controllers in the Kubernetes cluster are configured according to security
    best practices. Proper configuration involves ensuring that admission controllers are enabled, ordered, and parameterized
    correctly to enforce security policies. This includes validating resource requests, ensuring compliance with security
    policies, and preventing the deployment of misconfigured or malicious workloads. A well-configured admission controller
    setup reduces attack surfaces by enforcing strict adherence to organizational security policies before workloads are admitted
    to the cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-b_0398
- rule_id: k8s.admission.controller.image_scan_on_admission
  service: admission
  resource: controller
  requirement: Image Scan On Admission
  scope: admission.controller.image_scan_on_admission
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Image Vulnerability Scanning on Pod Admission
  rationale: Without image scanning on admission, vulnerabilities present in container images can be deployed into the cluster,
    leading to potential exploitation by attackers. This can result in unauthorized access, data breaches, and compromise
    of the entire cluster. Image scanning helps detect known vulnerabilities before they can be exploited.
  description: This control checks that the Kubernetes admission controller is configured to enforce image vulnerability scanning
    before allowing the deployment of pods. A properly configured admission controller will use tools like Clair or Trivy
    to scan container images for known vulnerabilities, ensuring that only secure images are deployed. This reduces the attack
    surface by preventing the deployment of images with critical vulnerabilities, thereby enhancing the overall security posture
    of the cluster.
  references:
  - https://kubernetes.io/docs/concepts/policy/admission-controllers/#validatingadmissionwebhook
  - https://kubernetes.io/docs/concepts/containers/images/#image-security
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-in-one-script
  compliance:
  - fedramp_moderate_multi_cloud_RA-5_0293
  - fedramp_moderate_multi_cloud_RA-5_2_0294
  - fedramp_moderate_multi_cloud_RA-5_3_0295
  - fedramp_moderate_multi_cloud_RA-5_4_0296
  - fedramp_moderate_multi_cloud_RA-5_5_0297
  - fedramp_moderate_multi_cloud_RA-5_8_0298
  - fedramp_moderate_multi_cloud_RA-5_11_0299
  - iso27001_2022_multi_cloud_A.8.7_0094
  - nist_800_171_r2_multi_cloud_3_14_4_3.14.4_Update_malicious_code_protection_me_0017
  - nist_800_53_rev5_multi_cloud_RA-5-a_1018
  - nist_800_53_rev5_multi_cloud_SI-2-d_1345
  - rbi_bank_multi_cloud_10.1_0003
  - rbi_nbfc_multi_cloud_2.1_0005
  - rbi_nbfc_multi_cloud_2.10_0014
- rule_id: k8s.admission.controller.incident_reporting_enabled
  service: admission
  resource: controller
  requirement: Incident Reporting Enabled
  scope: admission.controller.incident_reporting_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable and Configure Audit Logging in Admission Controllers
  rationale: Without proper incident reporting via audit logging in admission controllers, unauthorized or malicious changes
    to workloads and configurations may go undetected, increasing the risk of security breaches. Attackers could exploit this
    to escalate privileges, execute unauthorized operations, or deploy harmful workloads without detection.
  description: This rule checks whether audit logging is enabled and correctly configured in Kubernetes admission controllers.
    Proper configuration involves setting up audit policies that log key events such as changes to resource configurations
    and access attempts. A well-configured audit logging system helps in tracking incidents, enabling forensic analysis in
    the event of a security breach, and ensuring compliance with security standards like CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - nist_800_171_r2_multi_cloud_3_6_2_3.6.2_Track_document_and_report_incidents_0050
  - soc2_multi_cloud_cc_7_4_0018
  - soc2_multi_cloud_cc_7_5_0019
- rule_id: k8s.admission.controller.memory_protection_policies_enforced
  service: admission
  resource: controller
  requirement: Memory Protection Policies Enforced
  scope: admission.controller.memory_protection_policies_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Memory Protection Policies in Admission Controllers
  rationale: Misconfiguring memory protection policies can lead to vulnerabilities such as unauthorized memory access, which
    attackers can exploit to execute malicious code or exfiltrate sensitive data. Proper enforcement is crucial to mitigate
    risks like privilege escalation and maintain system integrity.
  description: This control checks that memory protection policies are enforced within Kubernetes admission controllers. It
    ensures that configurations adhere to security best practices by preventing excessive memory usage and unauthorized memory
    access. A correctly configured admission controller will apply constraints that limit the memory resources a pod can utilize,
    reducing the risk of Denial of Service (DoS) attacks and ensuring stable cluster performance.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-16_1433
- rule_id: k8s.admission.controller.monitoring_enabled
  service: admission
  resource: controller
  requirement: Monitoring Enabled
  scope: admission.controller.monitoring_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable and Monitor Admission Controller for Security Threats
  rationale: Without proper monitoring of the Kubernetes admission controller, malicious activities could go undetected, leading
    to potential unauthorized access, privilege escalation, or denial of service attacks. Attackers may exploit unmonitored
    systems to manipulate workloads or gain access to sensitive resources.
  description: This control verifies that monitoring is enabled on the Kubernetes admission controller. It checks for the
    presence of logging and alerting mechanisms that can detect anomalies and unauthorized attempts to access or modify resources.
    A well-monitored admission controller can quickly identify and respond to suspicious activities, thereby minimizing the
    risk of security breaches. Good configuration involves implementing logging to track requests and responses, integrating
    with a central monitoring system, and setting up alerts for unusual patterns or failures.
  references:
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - nist_800_53_rev5_multi_cloud_CA-7-a_0337
  - nist_800_53_rev5_multi_cloud_CA-7-g_0343
  - nist_800_53_rev5_multi_cloud_SI-4-b_1392
- rule_id: k8s.admission.controller.networkpolicy_changes_logged
  service: admission
  resource: controller
  requirement: Networkpolicy Changes Logged
  scope: admission.controller.networkpolicy_changes_logged
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Log All NetworkPolicy Changes via Admission Controllers
  rationale: Logging NetworkPolicy changes is crucial to detect and prevent potential security breaches such as unauthorized
    network access and data exfiltration. Without proper logging, adversaries can modify network policies to allow unauthorized
    access, leading to lateral movement within the cluster and potential data leaks.
  description: This control verifies that the Kubernetes admission controller is configured to log all changes to NetworkPolicies.
    A secure configuration ensures that every alteration to network policies is recorded, allowing for auditing and monitoring
    of network access controls. This not only helps in identifying malicious changes but also aids in compliance with security
    standards. A well-configured logging mechanism ensures that any unauthorized or suspicious changes are promptly detected
    and addressed, thereby maintaining the integrity and security of the cluster's network configuration.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - nist_800_53_rev5_multi_cloud_CA-7-a_0337
  - nist_800_53_rev5_multi_cloud_CA-7-f_0342
  - nist_800_53_rev5_multi_cloud_PM-31-a_0911
  - nist_800_53_rev5_multi_cloud_PM-31-d_0914
  - nist_800_53_rev5_multi_cloud_PM-31-f_0916
  - nist_800_53_rev5_multi_cloud_SI-2-a_1342
- rule_id: k8s.admission.controller.policy_enforcement_enabled
  service: admission
  resource: controller
  requirement: Policy Enforcement Enabled
  scope: admission.controller.policy_enforcement_enabled
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Pod Security Policies with Admission Controllers
  rationale: Without policy enforcement, Kubernetes clusters are vulnerable to security misconfigurations that could allow
    attackers to escalate privileges, execute unauthorized code, or access sensitive data. Attack vectors include deploying
    malicious containers, privilege escalation through unrestricted capabilities, and bypassing network policies.
  description: This rule verifies that Kubernetes admission controllers are configured to enforce Pod Security Policies (PSPs)
    or their replacements like the Pod Security Admission feature. It ensures that policies are correctly applied to control
    the security context of pods, restricting actions like running as privileged or using host namespaces. Proper configuration
    of admission controllers mitigates risks of privilege escalation and unauthorized access, enhancing the overall security
    posture of the cluster.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-admission/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - hipaa_multi_cloud_164_308_a_1_ii_a_0001
- rule_id: k8s.admission.controller.policy_violation_alerts_enabled
  service: admission
  resource: controller
  requirement: Policy Violation Alerts Enabled
  scope: admission.controller.policy_violation_alerts_enabled
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Enable Policy Violation Alerts in Admission Controllers
  rationale: Without policy violation alerts, unauthorized configurations may go undetected, allowing potential security breaches
    and policy non-compliance. Attackers could exploit these misconfigurations to bypass security policies, leading to data
    exposure or system compromise.
  description: This control verifies that the Kubernetes admission controller is configured to generate alerts for policy
    violations. A properly configured alert system helps in real-time detection and response to unauthorized configuration
    changes and access attempts. Ensuring alerts are enabled and properly configured helps maintain adherence to security
    policies, thereby reducing the risk of security incidents and ensuring compliance with standards such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  compliance:
  - soc2_multi_cloud_cc_4_2_0007
- rule_id: k8s.admission.controller.privilege_escalation_prevention
  service: admission
  resource: controller
  requirement: Privilege Escalation Prevention
  scope: admission.controller.privilege_escalation_prevention
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Prevent Privilege Escalation in Admission Controllers
  rationale: If privilege escalation is not prevented, attackers could exploit vulnerabilities in workloads to gain elevated
    permissions, allowing them to access sensitive data or compromise critical components of the Kubernetes cluster. This
    misconfiguration could lead to unauthorized access, data breaches, and lateral movement within the cluster.
  description: This control checks whether the Kubernetes admission controllers are configured to prevent privilege escalation
    by disallowing containers from running with escalated privileges. A properly configured admission controller should reject
    any pod or container definitions that request privilege escalation. This ensures that even if a container is compromised,
    it cannot gain additional permissions that could be used to exploit other services or the underlying node.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#preventing-privilege-escalation
  - https://kubernetes.io/docs/concepts/security/overview/#security-context
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  compliance:
  - soc2_multi_cloud_cc_3_3_0005
- rule_id: k8s.admission.controller.restrict_user_installed_software
  service: admission
  resource: controller
  requirement: Restrict User Installed Software
  scope: admission.controller.restrict_user_installed_software
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Admission Control to Restrict User-Installed Software
  rationale: Allowing arbitrary software installation by users on Kubernetes clusters can introduce vulnerabilities, enable
    privilege escalation, and compromise the integrity of workloads. By misconfiguring admission controllers, attackers could
    exploit these vulnerabilities to execute unauthorized software, potentially leading to data breaches, denial of service,
    or other malicious activities.
  description: This rule checks the configuration of admission controllers to ensure they are set to restrict the installation
    of user-supplied software. Proper configuration involves setting up policies that prevent unauthorized software from being
    installed on the cluster. This is achieved by using admission controllers like PodSecurityPolicy or Open Policy Agent
    (OPA) to enforce security policies that validate and limit the software users can install. This control helps maintain
    a secure environment by reducing the attack surface and ensuring that only vetted and approved software is deployed within
    the cluster.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - nist_800_171_r2_multi_cloud_3_4_9_3.4.9_Control_and_monitor_user-installed_so_0041
- rule_id: k8s.admission.controller.security_alerts_enabled
  service: admission
  resource: controller
  requirement: Security Alerts Enabled
  scope: admission.controller.security_alerts_enabled
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Enable Security Alerts in Admission Controllers
  rationale: If security alerts are improperly configured or disabled, malicious activities or misconfigurations can go unnoticed,
    leading to potential breaches. Attackers may exploit this by injecting malicious configurations or bypassing security
    policies without detection.
  description: This control checks that the admission controllers in Kubernetes clusters have security alerts enabled. A well-configured
    alert system will notify administrators of any unauthorized access attempts or policy violations, allowing for timely
    remediation. Proper configuration includes setting up alerts for critical security events, ensuring they are monitored
    continuously, and integrating them with incident response processes. This reduces the risk of undetected security incidents
    and aids in compliance with standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/policy/admission-controllers/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - nist_800_171_r2_multi_cloud_3_14_3_3.14.3_Monitor_system_security_alerts_and_0016
  - nist_800_171_r2_multi_cloud_3_14_4_3.14.4_Update_malicious_code_protection_me_0017
  - nist_800_171_r2_multi_cloud_3_1_4_3.1.4_Separate_the_duties_of_individuals_to_0027
- rule_id: k8s.admission.controller.security_event_alerting
  service: admission
  resource: controller
  requirement: Security Event Alerting
  scope: admission.controller.security_event_alerting
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Enable Security Event Alerting for Admission Controllers
  rationale: Without proper security event alerting on admission controllers, unauthorized modifications to Kubernetes cluster
    configurations can go undetected, increasing the risk of privilege escalation and data breaches. Attack vectors include
    malicious actors exploiting vulnerabilities to bypass security policies or inject harmful configurations.
  description: This rule checks that the Kubernetes admission controllers are configured to generate alerts for significant
    security events. It ensures that all modifications to cluster configurations are monitored and alerts are triggered for
    suspicious activities. A properly configured alerting system allows for real-time detection and response to potential
    security incidents, thereby mitigating risks such as unauthorized access and data tampering. Good configuration includes
    integration with centralized logging and monitoring tools that are capable of ingesting and analyzing admission controller
    logs for suspicious activities.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IR-4_0072
  - soc2_multi_cloud_cc_7_3_0017
- rule_id: k8s.admission.controller.security_policy_enabled
  service: admission
  resource: controller
  requirement: Security Policy Enabled
  scope: admission.controller.security_policy_enabled
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enable Pod Security Policies via Admission Controller
  rationale: If Pod Security Policies are not enabled through the admission controller, there is an increased risk of deploying
    pods that can escalate privileges within the cluster. This misconfiguration can allow attackers to exploit vulnerabilities
    by running privileged containers, accessing host resources, or mounting sensitive host paths, exposing the cluster to
    potential attacks and unauthorized access.
  description: This rule checks that the Kubernetes admission controller has Pod Security Policies (PSP) enabled to enforce
    security best practices at the admission stage. A properly configured PSP ensures that pods conform to security policies,
    such as restricting privilege escalation, disallowing host network and volume access, and enforcing resource limits. This
    reduces the attack surface by preventing the deployment of containers with insecure configurations and helps maintain
    compliance with standards like the CIS Kubernetes Benchmark. A correct configuration includes having PSP admission control
    enabled and specific security policies defined and enforced for different workloads.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-2_0035
  - nist_800_171_r2_multi_cloud_3_6_1_3.6.1_Establish_an_operational_incident-han_0049
  - nist_800_53_rev5_multi_cloud_SC-43-b_1322
  - nist_800_53_rev5_multi_cloud_SI-4-a_1391
  - nist_800_53_rev5_multi_cloud_SI-4-c_1393
  - pci_dss_v4_multi_cloud_11.4.2_0170
  - pci_dss_v4_multi_cloud_11.4.3_0171
  - pci_dss_v4_multi_cloud_11.4.4_0172
  - pci_dss_v4_multi_cloud_11.4.5_0173
  - rbi_bank_multi_cloud_14.1_0007
- rule_id: k8s.admission.controller.software_whitelist_enabled
  service: admission
  resource: controller
  requirement: Software Whitelist Enabled
  scope: admission.controller.software_whitelist_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Software Whitelisting in Admission Controllers
  rationale: Without a software whitelist, unauthorized or malicious software can be deployed within the cluster, potentially
    leading to data breaches or service disruptions. Attackers could exploit this by deploying containers with backdoors or
    known vulnerabilities, enabling lateral movement or privilege escalation within the cluster.
  description: This rule checks that the Kubernetes admission controller is configured to enforce a software whitelist, ensuring
    only approved images and software packages are deployed. A properly configured whitelist prevents unauthorized applications
    from running, thereby reducing the risk of security incidents and maintaining compliance with security standards. It involves
    validating the presence of a whitelist policy that specifies allowed images and denies any that are not explicitly listed.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/containers/images/
  compliance:
  - nist_800_171_r2_multi_cloud_3_4_9_3.4.9_Control_and_monitor_user-installed_so_0041
- rule_id: k8s.admission.controller.supplier_service_change_management
  service: admission
  resource: controller
  requirement: Supplier Service Change Management
  scope: admission.controller.supplier_service_change_management
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Change Management on Admission Controllers
  rationale: Without proper change management controls in place, unauthorized or unverified changes to admission controllers
    can introduce vulnerabilities, leading to potential breaches and non-compliance with security standards. Attackers could
    exploit misconfigurations to bypass security policies, escalating privileges or executing malicious operations within
    the cluster.
  description: This rule verifies that a change management process is enforced on Kubernetes admission controllers, ensuring
    that any modifications are reviewed and approved according to security policies. Proper configuration includes documenting
    and controlling changes to admission controllers, which reduces the risk of unauthorized access and ensures compliance
    with industry standards. This control helps maintain a robust security posture by preventing unverified changes that could
    weaken the cluster's defenses.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - iso27001_2022_multi_cloud_A.5.22_0019
- rule_id: k8s.admission.controller.threat_detection_enabled
  service: admission
  resource: controller
  requirement: Threat Detection Enabled
  scope: admission.controller.threat_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Threat Detection in Admission Controllers
  rationale: Without threat detection enabled in Kubernetes admission controllers, malicious activities such as unauthorized
    access, privilege escalation, and resource tampering could go undetected, posing significant risks to cluster integrity
    and data confidentiality. Attackers can exploit misconfigured admission controllers to introduce vulnerable or malicious
    configurations, compromising cluster security.
  description: This rule verifies that threat detection is enabled in Kubernetes admission controllers by checking configurations
    for tools that monitor and log suspicious activities. A well-configured threat detection mechanism analyzes admission
    requests in real-time, alerting administrators to potential security breaches and unauthorized access attempts. By ensuring
    threat detection is active, clusters benefit from enhanced visibility into security events, enabling proactive incident
    response and compliance with security benchmarks like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-admission/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_IR-4-a_0625
- rule_id: k8s.admission.controller.threat_hunting_enabled
  service: admission
  resource: controller
  requirement: Threat Hunting Enabled
  scope: admission.controller.threat_hunting_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Threat Detection in Admission Controllers
  rationale: Without threat detection enabled in admission controllers, Kubernetes clusters may become vulnerable to attacks
    that exploit misconfigurations or unauthorized access attempts. Attack vectors include privilege escalation, execution
    of malicious containers, and unauthorized changes to resources. Ensuring threat detection is active helps identify and
    mitigate these risks before they can be exploited.
  description: This control checks if threat detection mechanisms are enabled in Kubernetes admission controllers. A correctly
    configured admission controller can intercept API requests in real-time and enforce security policies or detect anomalies.
    A good configuration involves enabling specific admission plugins that support threat detection, such as validating and
    mutating webhooks, which help to scrutinize incoming and outgoing traffic for suspicious activities and enforce security
    policies effectively. This enhances the overall security posture by preventing potential breaches and ensuring compliance
    with security standards.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-admission/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do
  compliance:
  - nist_800_53_rev5_multi_cloud_RA-10-a_1029
- rule_id: k8s.admission.controller.vulnerability_scanning_enabled
  service: admission
  resource: controller
  requirement: Vulnerability Scanning Enabled
  scope: admission.controller.vulnerability_scanning_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enable Vulnerability Scanning in Admission Controllers
  rationale: Without vulnerability scanning enabled in admission controllers, Kubernetes clusters are susceptible to deploying
    vulnerable or malicious container images. This increases the risk of exploitation through unpatched vulnerabilities, potentially
    leading to unauthorized access, data breaches, or service disruptions. Attack vectors include the deployment of outdated
    images with known CVEs or the introduction of malicious code into running environments.
  description: This control verifies that vulnerability scanning is actively configured in Kubernetes admission controllers
    to screen container images for known security issues before they are deployed. A good configuration involves integrating
    a security scanner that checks images against a database of known vulnerabilities (CVEs) and enforces policies to block
    images that fail these checks. This helps maintain a secure environment by preventing the deployment of insecure images,
    ensuring compliance with security standards, and reducing the attack surface.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/containers/images/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_RA-5_0115
  - nist_800_53_rev5_multi_cloud_SI-2-d_1345
  - soc2_multi_cloud_cc_7_1_0015
- rule_id: k8s.admission.deny.sensitive_env_variables
  service: admission
  resource: deny
  requirement: Sensitive Env Variables
  scope: admission.deny.sensitive_env_variables
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Use of Sensitive Environment Variables in Pods
  rationale: Exposing sensitive information through environment variables in pods can lead to data breaches and unauthorized
    data access if exploited by attackers. Attack vectors include compromised containers accessing environment variables to
    extract credentials, tokens, or secrets that can be used to escalate privileges or access other systems.
  description: This rule checks if Kubernetes admission controllers are configured to deny pod creation when sensitive environment
    variables are specified. A secure configuration ensures that sensitive information, such as API keys, database passwords,
    or private keys, are not inadvertently exposed in pod specifications. This minimizes the risk of data leakage and unauthorized
    access, enhancing the overall security posture of the Kubernetes environment by enforcing best practices for secret management.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-pod-security-policies
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  compliance:
  - pci_dss_v4_multi_cloud_6.3.2_0080
- rule_id: k8s.admission.mutatingwebhook.secure_webhooks_enforced_enabled
  service: admission
  resource: mutatingwebhook
  requirement: Secure Webhooks Enforced Enabled
  scope: admission.mutatingwebhook.secure_webhooks_enforced_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce TLS in Mutating Admission Webhooks
  rationale: If TLS is not enforced for mutating admission webhooks, sensitive data transmitted between the Kubernetes API
    server and webhooks may be exposed to interception or tampering by unauthorized entities. This vulnerability can lead
    to unauthorized modifications of Kubernetes resources, potentially resulting in privilege escalation or denial of service
    attacks.
  description: This rule checks that all mutating admission webhooks enforce TLS by verifying that the webhook configurations
    specify a 'caBundle' and that the 'url' scheme is 'https'. A secure configuration ensures that communication between the
    API server and webhooks is encrypted, mitigating risks of data interception and ensuring data integrity. Properly configured
    TLS prevents man-in-the-middle attacks and ensures that only authorized endpoints can interact with the webhooks.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#authenticate-apiservers
  - https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/#configure-apiservice
  - https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/webhook/#webhook-security
  compliance: []
- rule_id: k8s.admission.mutatingwebhook.webhook_tls_enforced
  service: admission
  resource: mutatingwebhook
  requirement: Webhook Tls Enforced
  scope: admission.mutatingwebhook.webhook_tls_enforced
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce TLS for MutatingWebhook Admission Controllers
  rationale: If TLS is not enforced for MutatingWebhook admission controllers, data transmitted between the API server and
    webhooks may be susceptible to eavesdropping or man-in-the-middle attacks. This can lead to unauthorized access or tampering
    with requests, compromising the integrity and confidentiality of the Kubernetes cluster.
  description: This rule checks that all MutatingWebhook configurations enforce TLS by ensuring that the 'clientConfig' field
    specifies 'caBundle' or a trusted CA. Proper TLS configuration ensures that communication between the Kubernetes API server
    and the webhook is encrypted, preventing interception and unauthorized modifications of admission requests. This protects
    sensitive data in transit and aligns with security best practices and compliance standards.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#tls-configuration
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#configure-certificates-for-your-webhook
  - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#mutatingwebhookconfiguration-v1-admissionregistration-k8s-io
  compliance: []
- rule_id: k8s.admission.security.incident_alerting_enabled
  service: admission
  resource: security
  requirement: Incident Alerting Enabled
  scope: admission.security.incident_alerting_enabled
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Enable Incident Alerting in Admission Controllers
  rationale: Without incident alerting properly configured in Kubernetes admission controllers, unauthorized access attempts
    or policy violations could go unnoticed, leading to potential security breaches. An attacker could exploit this lack of
    monitoring to gain persistent access or escalate privileges within the cluster.
  description: This control checks that incident alerting is enabled for Kubernetes admission controllers. A properly configured
    alerting system ensures that security events such as unauthorized access attempts or policy breaches trigger alerts, allowing
    for timely response and remediation. This configuration should follow security best practices such as integrating with
    centralized logging and monitoring solutions to enhance visibility and compliance with industry standards like the CIS
    Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - fedramp_moderate_multi_cloud_IR-4_0205
  - fedramp_moderate_multi_cloud_IR-4_1_0206
  - fedramp_moderate_multi_cloud_IR-4_2_0207
  - fedramp_moderate_multi_cloud_IR-4_4_0208
  - fedramp_moderate_multi_cloud_IR-4_6_0209
  - fedramp_moderate_multi_cloud_IR-4_11_0210
- rule_id: k8s.admission.threat.detection_webhook_configured
  service: admission
  resource: threat
  requirement: Detection Webhook Configured
  scope: admission.threat.detection_webhook_configured
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Configure Admission Control Threat Detection Webhooks
  rationale: Improperly configured admission control webhooks can lead to unauthorized access and execution of malicious actions
    within the cluster. Attackers can exploit misconfigurations to bypass security policies, inject malicious workloads, or
    escalate privileges, compromising the integrity and availability of the cluster.
  description: This control checks that Kubernetes admission control webhooks are properly configured to detect and log security
    threats. A well-configured webhook will monitor and validate incoming API requests to the Kubernetes API server, ensuring
    they comply with security policies. Proper configuration involves setting up webhook endpoints with secure communication
    channels, defining clear policy rules, and maintaining audit logs. This helps in early detection of potentially malicious
    activities, reducing the risk of unauthorized access and ensuring compliance with security standards.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - iso27001_2022_multi_cloud_A.5.7_0039
- rule_id: k8s.admission.validatingwebhook.builtin_controllers_enabled_enforced
  service: admission
  resource: validatingwebhook
  requirement: Builtin Controllers Enabled Enforced
  scope: admission.validatingwebhook.builtin_controllers_enabled_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Built-in Admission Controllers in ValidatingWebhooks
  rationale: If built-in admission controllers are not properly enforced in ValidatingWebhooks, Kubernetes clusters may be
    vulnerable to misconfigurations and unauthorized changes. Attackers could exploit these vulnerabilities to escalate privileges,
    deploy malicious workloads, or alter cluster configurations, undermining security and compliance efforts.
  description: This control checks that Kubernetes ValidatingWebhooks have built-in admission controllers enabled and enforced
    according to official security guidelines. Proper enforcement ensures that all API requests are subjected to necessary
    validation checks, reducing the risk of unauthorized or harmful changes. An optimal configuration helps maintain cluster
    integrity, adheres to best practices, and aligns with compliance frameworks such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook
  - https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/webhook/
  - https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.constraints_configured_enabled
  service: admission
  resource: validatingwebhook
  requirement: Constraints Configured Enabled
  scope: admission.validatingwebhook.constraints_configured_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Validating Webhook Configurations Enforce Security Policies
  rationale: If a validating webhook is misconfigured or lacks necessary constraints, it can allow the admission of insecure
    or misconfigured resources into the Kubernetes cluster. This can lead to potential security breaches, such as privilege
    escalation or resource misuse, by permitting operations that should be restricted. By ensuring constraints are properly
    configured and enabled, you mitigate the risk of introducing vulnerabilities through improperly validated configurations.
  description: This control checks that all configured validating webhooks have their constraints properly set and enabled.
    A good configuration ensures that the webhook validates incoming API requests against defined security policies before
    they are persisted in the cluster. This helps prevent the introduction of resources that violate security policies, such
    as pods with elevated privileges or unauthorized network access, thereby maintaining the cluster's integrity and compliance
    with security best practices.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#validating-admission-webhook
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-object-management
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.control_hostpath_volumes_restricted
  service: admission
  resource: validatingwebhook
  requirement: Control Hostpath Volumes Restricted
  scope: admission.validatingwebhook.control_hostpath_volumes_restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict HostPath Volume Usage via Admission Webhooks
  rationale: Improper configuration of HostPath volumes can lead to unauthorized access to the host filesystem, increasing
    the risk of data exfiltration and privilege escalation. By restricting HostPath volume usage, you minimize the attack
    surface and prevent potential exploits where attackers could manipulate or access sensitive data on the host.
  description: This rule checks that admission webhooks are configured to restrict the use of HostPath volumes in Kubernetes
    clusters. Limiting HostPath volumes helps ensure that only authorized applications can access specific directories on
    the host filesystem, reducing the risk of privilege escalations and data breaches. A well-configured admission webhook
    will enforce policies that prevent unauthorized or unintended use of HostPath volumes, aligning with security best practices
    and compliance requirements.
  references:
  - https://kubernetes.io/docs/concepts/storage/volumes/#hostpath
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.controller_configuration_immutable
  service: admission
  resource: validatingwebhook
  requirement: Controller Configuration Immutable
  scope: admission.validatingwebhook.controller_configuration_immutable
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure ValidatingWebhookConfigurations Are Immutable
  rationale: If ValidatingWebhookConfigurations are not immutable, an attacker or a misconfigured application with elevated
    permissions could modify the webhook's behavior, potentially allowing unauthorized access or bypassing critical security
    checks. This could lead to privilege escalation, unauthorized resource access, or denial of service attacks by preventing
    essential security validations.
  description: 'This rule checks that Kubernetes ValidatingWebhookConfigurations are configured to be immutable, meaning their
    configuration cannot be altered after initial creation. An immutable configuration ensures that once the webhook is set
    to enforce certain security policies, those policies cannot be changed without explicit intent. This protects against
    accidental or malicious changes that could weaken security postures, such as disabling critical admission controls that
    validate resources before they are persisted. A secure configuration involves enabling the ''failurePolicy: Fail'' setting
    to ensure requests that cannot be validated are denied, maintaining the integrity and security of the cluster.'
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#validating-admission-webhooks
  - https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/#register-apiservices
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingwebhookconfiguration
  compliance: []
- rule_id: k8s.admission.validatingwebhook.controller_security_patches_enforced
  service: admission
  resource: validatingwebhook
  requirement: Controller Security Patches Enforced
  scope: admission.validatingwebhook.controller_security_patches_enforced
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enforce Security Patch Validation on Admission Controllers
  rationale: Failing to enforce security patches on validating webhooks can expose the Kubernetes cluster to vulnerabilities
    that have already been addressed in updates. These vulnerabilities can be exploited by attackers to gain unauthorized
    access, execute arbitrary code, or disrupt services. Unpatched admission controllers may fail to block malicious or misconfigured
    resources, compromising the entire cluster's security.
  description: This rule verifies that Kubernetes validating webhooks are configured to enforce the application of security
    patches for admission controllers. A correctly configured system should ensure that only patched and verified versions
    of admission controllers are in use, reducing the risk of exploitation of known vulnerabilities. This validation checks
    the configurations of validating webhooks to ensure they are up-to-date with security patches. Proper enforcement helps
    maintain the integrity of the cluster by preventing compromised admission controllers from approving insecure operations
    or configurations.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/setup/release/version-skew-policy/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.pod_security_enforced
  service: admission
  resource: validatingwebhook
  requirement: Pod Security Enforced
  scope: admission.validatingwebhook.pod_security_enforced
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Pod Security Standards via Validating Webhooks
  rationale: Misconfiguration of pod security can lead to unauthorized access, privilege escalation, and data breaches. Attack
    vectors include running privileged containers, accessing host resources, and bypassing network policies. Enforcing pod
    security through validating webhooks ensures that pods comply with security policies before they are admitted, mitigating
    these risks.
  description: This rule checks if pod security standards are enforced via Kubernetes validating webhooks. A validating webhook
    intercepts the pod creation process and ensures that security policies, such as restricting privileged mode or host network
    access, are adhered to. Proper configuration helps prevent unauthorized access, privilege escalation, and ensures compliance
    with industry standards like the CIS Kubernetes Benchmark. A compliant setup involves defining security policies and configuring
    webhooks to reject non-compliant pod specifications.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.policy_testing_enabled
  service: admission
  resource: validatingwebhook
  requirement: Policy Testing Enabled
  scope: admission.validatingwebhook.policy_testing_enabled
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enable Policy Testing in Admission Validating Webhooks
  rationale: Without policy testing enabled in admission validating webhooks, there is a risk of deploying misconfigured or
    malicious workloads into the cluster. Attackers could exploit this by bypassing security policies, potentially escalating
    their privileges or accessing sensitive data. Enabling policy testing helps ensure that all workloads meet predefined
    security criteria before deployment, reducing the surface for such exploits.
  description: This control checks that the admission validating webhooks in the Kubernetes cluster have policy testing enabled.
    A properly configured validating webhook ensures that all resources comply with security policies before they are admitted
    to the cluster. This helps prevent unauthorized or insecure configurations from being deployed, by rejecting requests
    that do not meet the security criteria. For example, it can enforce policies that restrict the use of certain images,
    require specific labels, or limit resource requests and limits. A good configuration includes setting up the webhook to
    evaluate requests against a set of predefined policies and reject those that do not comply, thus maintaining a secure
    cluster environment.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook
  - https://kubernetes.io/docs/tasks/administer-cluster/extend-admission-control/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.security_alerting_enabled
  service: admission
  resource: validatingwebhook
  requirement: Security Alerting Enabled
  scope: admission.validatingwebhook.security_alerting_enabled
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Enable Security Alerting for Admission Validating Webhooks
  rationale: Without proper security alerting for admission validating webhooks, unauthorized changes can go undetected, leading
    to potential security breaches. Attackers could exploit misconfigured webhooks to bypass security policies, leading to
    data leaks or unauthorized access to resources.
  description: This control checks if security alerting is enabled for admission validating webhooks. A well-configured alerting
    system ensures that any unauthorized or unexpected changes to webhook configurations are promptly detected and investigated.
    This involves setting up logging and monitoring to track webhook events and integrating alerts with a centralized monitoring
    system. Proper alerting helps maintain the integrity of admission controls, which are crucial for enforcing security policies
    across the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/tasks/administer-cluster/enabling-audit-logging/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.sensitive_env_vars_denied
  service: admission
  resource: validatingwebhook
  requirement: Sensitive Env Vars Denied
  scope: admission.validatingwebhook.sensitive_env_vars_denied
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Reject Sensitive Environment Variables in Admission Webhooks
  rationale: Exposing sensitive data through environment variables can result in unauthorized access and data breaches. Attackers
    may exploit misconfigured webhooks to extract sensitive information, leading to potential privilege escalation or lateral
    movement within the cluster.
  description: This control ensures that Kubernetes ValidatingAdmissionWebhooks reject pods or resources that attempt to use
    predefined sensitive environment variables. By implementing this control, you prevent the inclusion of sensitive data
    such as passwords, API keys, or tokens in pod specifications. This validation step is crucial for maintaining the confidentiality
    of sensitive information and complying with security best practices. A properly configured webhook will deny any deployment
    that includes such variables, thereby reducing the risk of unauthorized data exposure.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.supplier_change_managed
  service: admission
  resource: validatingwebhook
  requirement: Supplier Change Managed
  scope: admission.validatingwebhook.supplier_change_managed
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure ValidatingWebhookConfiguration Supplier Change is Securely Managed
  rationale: Misconfiguration of the ValidatingWebhookConfiguration can lead to unauthorized changes being applied to the
    cluster, potentially allowing attackers to introduce malicious configurations or bypass security policies. This can result
    in privilege escalation, data breaches, or unauthorized access to cluster resources. Proper management of supplier changes
    ensures that only trusted and verified updates are applied, mitigating these risks.
  description: This rule checks that any changes to the ValidatingWebhookConfiguration supplier are managed securely according
    to best practices. A secure configuration involves using version-controlled and audited webhook configurations, ensuring
    that any updates are authorized and verified by security teams. This helps in maintaining the integrity of the admission
    control process, preventing unauthorized access or modifications to cluster configurations, and aligning with industry
    standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/
  - https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.threat_detection_enabled
  service: admission
  resource: validatingwebhook
  requirement: Threat Detection Enabled
  scope: admission.validatingwebhook.threat_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Threat Detection in Validating Webhooks
  rationale: If threat detection is not enabled in validating webhooks, malicious or misconfigured requests could be admitted
    into the cluster, potentially leading to unauthorized access, privilege escalation, or data exfiltration. Threat detection
    helps identify and mitigate suspicious activities by analyzing requests before they are allowed to modify cluster state.
  description: This control ensures that validating webhooks in Kubernetes are configured with threat detection capabilities.
    It checks that webhooks are set up to inspect API requests for anomalies or malicious patterns before they are processed
    by the API server. A proper configuration includes defining rules and policies that log and potentially block suspicious
    requests. This helps in maintaining audit trails, preventing unauthorized access, and enforcing compliance with security
    standards, such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/webhook/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.user_software_restricted
  service: admission
  resource: validatingwebhook
  requirement: User Software Restricted
  scope: admission.validatingwebhook.user_software_restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Unauthorized Software via ValidatingWebhook
  rationale: If user-provided software is not restricted by validating webhooks, there is a risk of deploying unverified or
    malicious software into the Kubernetes cluster. This can lead to unauthorized access, data breaches, and compromise of
    cluster integrity due to the execution of harmful operations or escalation of privileges.
  description: This control checks that the Kubernetes ValidatingWebhookConfiguration is set up to restrict user software
    that does not meet security policies. A properly configured webhook will validate incoming requests against predefined
    rules, rejecting those that attempt to deploy unsafe or unauthorized software. This ensures only vetted software is admitted,
    reducing the risk of introducing vulnerabilities and ensuring compliance with security guidelines such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#validating-admission-webhooks
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example-podsecuritypolicy
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.admission.validatingwebhook.webhook_validation_enabled
  service: admission
  resource: validatingwebhook
  requirement: Webhook Validation Enabled
  scope: admission.validatingwebhook.webhook_validation_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable and Configure Validating Webhooks for Admission Control
  rationale: If validating webhooks are not properly enabled and configured, malicious actors could exploit improperly validated
    resources to gain unauthorized access or execute unauthorized actions in the cluster. This could lead to privilege escalation,
    data breaches, or service disruptions. Validating webhooks act as a critical security checkpoint by enforcing custom policies
    on incoming resource requests, thereby protecting the cluster from misconfigurations and potential attacks.
  description: This control checks that Kubernetes validating admission webhooks are enabled and properly configured to enforce
    security policies on resource creation and updates. A well-configured validating webhook ensures that only compliant and
    secure configurations are allowed into the cluster. This prevents unsafe configurations, unauthorized resource creation,
    and ultimately helps maintain the integrity and security of the cluster. Proper configuration involves specifying secure
    endpoints, ensuring SSL/TLS encryption for communication, and defining precise webhook rules that align with security
    policies.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#validating-admission-webhooks
  - https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/#register-apiservice-objects
  - https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/webhooks/
  compliance: []
- rule_id: k8s.admission.webhook.authentication_enabled
  service: admission
  resource: webhook
  requirement: Authentication Enabled
  scope: admission.webhook.authentication_enabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Authentication for Admission Webhooks
  rationale: Without proper authentication, admission webhooks are vulnerable to unauthorized requests, which can lead to
    the execution of unauthorized operations or policy violations. Attackers could exploit unauthenticated webhooks to bypass
    security controls, resulting in potential data breaches or service disruptions.
  description: This control checks that Kubernetes admission webhooks have authentication enabled, ensuring that only authenticated
    requests are processed. A properly configured webhook should leverage mechanisms like client certificates or token-based
    authentication to verify the identity of the requestor. This reduces the risk of unauthorized access and ensures that
    only validated and approved configurations are applied, aligning with security best practices and regulatory compliance
    such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#authenticate-apiservers
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-admission/
  compliance:
  - rbi_nbfc_multi_cloud_2.11_0015
- rule_id: k8s.admission.webhook.configured
  service: admission
  resource: webhook
  requirement: Configured
  scope: admission.webhook.configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Admission Webhook Configuration for Enhanced Security
  rationale: Misconfigured admission webhooks can lead to significant security risks, including unauthorized access and privilege
    escalation. Attackers could exploit improperly configured webhooks to bypass security policies, inject malicious resources,
    or perform unauthorized operations within the cluster, compromising the integrity and confidentiality of the Kubernetes
    environment.
  description: This control checks that Kubernetes admission webhooks are properly configured according to security best practices.
    A well-configured webhook ensures that all requests to the API server are subject to validation and mutation policies,
    effectively preventing unauthorized resource creation or modification. This includes ensuring the webhook is registered
    correctly, with secure endpoints using TLS, and that it evaluates critical security policies, such as resource quotas
    and pod security standards. Proper configuration significantly reduces attack surfaces and enforces compliance with industry
    standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/#plugin-proxy
  compliance:
  - fedramp_moderate_multi_cloud_CA-2_0085
  - fedramp_moderate_multi_cloud_CA-2_1_0086
  - fedramp_moderate_multi_cloud_CA-2_2_0087
  - fedramp_moderate_multi_cloud_CA-2_3_0088
  - rbi_bank_multi_cloud_12.1_0005
- rule_id: k8s.admission.webhook.policy_enforced
  service: admission
  resource: webhook
  requirement: Policy Enforced
  scope: admission.webhook.policy_enforced
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Admission Webhook Security Policies
  rationale: Misconfigured admission webhooks may be exploited by attackers to bypass security controls, leading to unauthorized
    resource manipulation or privilege escalation. Proper enforcement is crucial to prevent attack vectors such as injection
    of malicious resources or unauthorized access to sensitive data.
  description: This control checks that admission webhooks enforce security policies by ensuring they are correctly configured
    with fail-open or fail-closed settings, depending on the environment's requirements. Proper configuration ensures that
    policy violations are detected and mitigated before resources are created or modified, reducing the risk of unauthorized
    actions and ensuring compliance with security standards. A well-configured webhook policy enforces checks on resource
    creation and updates, preventing misconfigurations that could lead to security breaches.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/concepts/extend-kubernetes/admission-controllers/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook
  compliance:
  - hipaa_multi_cloud_164_308_a_6_ii_0016
- rule_id: k8s.admission.webhook.security_policy_configured
  service: admission
  resource: webhook
  requirement: Security Policy Configured
  scope: admission.webhook.security_policy_configured
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Ensure Admission Webhooks Enforce Security Policies
  rationale: Without properly configured admission webhooks, Kubernetes clusters are vulnerable to misconfigurations and malicious
    activities. Attackers can exploit these vulnerabilities to execute unauthorized code, escalate privileges, or exfiltrate
    sensitive data. Enforcing security policies through admission webhooks helps mitigate these risks by ensuring that all
    deployed resources adhere to predefined security standards.
  description: This control checks that admission webhooks are configured with security policies that enforce best practices
    and compliance requirements. A well-configured webhook ensures that any incoming API requests are evaluated against security
    policies before they are allowed to persist in the cluster. This reduces the attack surface by preventing the deployment
    of insecure or non-compliant workloads. The configuration should include criteria that align with organizational security
    standards, such as disallowing containers with privileged escalation or enforcing network policies. Proper implementation
    of these policies ensures adherence to the CIS Kubernetes Benchmark and other industry standards.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/tasks/administer-cluster/limit-storage-consumption/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-2_0035
- rule_id: k8s.apiserver.access.logging_enabled
  service: apiserver
  resource: access
  requirement: Logging Enabled
  scope: apiserver.access.logging_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable and Configure API Server Audit Logging
  rationale: Without audit logging enabled on the Kubernetes API server, security incidents may go undetected as there would
    be no record of access and actions taken by users. This lack of visibility can allow malicious activities like unauthorized
    access, privilege escalation, and data exfiltration to occur without detection, increasing the risk of breaches and non-compliance
    with regulatory standards.
  description: This rule checks the configuration of audit logging on the Kubernetes API server to ensure it is enabled and
    properly set up. A good configuration involves specifying an audit policy that captures critical events, including access
    and modification actions. This setup allows security teams to monitor and investigate suspicious activities, thus enhancing
    the detection and response capabilities against potential threats. Proper audit logging is essential for forensic analysis,
    compliance with standards like the CIS Kubernetes Benchmark, and maintaining a robust security posture.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/audit-policy/
  - https://kubernetes.io/docs/tasks/security/audit/
  compliance:
  - nist_800_53_rev5_multi_cloud_PM-31-c_0913
- rule_id: k8s.apiserver.admission.plugin_serviceaccount_check
  service: apiserver
  resource: admission
  requirement: Plugin Serviceaccount Check
  scope: apiserver.admission.plugin_serviceaccount_check
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce ServiceAccount Admission Control in API Server
  rationale: Misconfiguring the ServiceAccount admission plugin can lead to unauthorized access and privilege escalation within
    the Kubernetes cluster. Without proper service account management, attackers may exploit service account tokens to gain
    unauthorized access to cluster resources, potentially leading to data breaches or lateral movement across the cluster.
  description: This rule checks whether the ServiceAccount admission plugin is enabled and properly configured in the Kubernetes
    API server. The ServiceAccount admission plugin ensures that pods are assigned service accounts, which are crucial for
    managing pod permissions and access control. Proper configuration includes verifying that each pod has a service account
    and that tokens are handled securely. This helps maintain the principle of least privilege and restricts unauthorized
    access to sensitive resources.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#serviceaccount
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  compliance:
  - cis_kubernetes_kubernetes_1.2.13_0034
- rule_id: k8s.apiserver.anonymous.auth_disabled
  service: apiserver
  resource: anonymous
  requirement: Auth Disabled
  scope: apiserver.anonymous.auth_disabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Anonymous Authentication on Kube-apiserver
  rationale: Enabling anonymous authentication on the Kube-apiserver allows unauthenticated access, which can lead to unauthorized
    users performing actions on the cluster. This misconfiguration can be exploited by attackers to gain access to sensitive
    data, modify cluster configurations, or disrupt operations. Disabling anonymous authentication is crucial to ensure that
    all API requests are properly authenticated, reducing the risk of unauthorized access and potential breaches.
  description: This rule checks that the Kube-apiserver is configured with anonymous authentication disabled by ensuring the
    `--anonymous-auth` flag is set to `false`. A secure configuration prevents unauthenticated requests, enforcing that all
    interactions with the API server are from verified users. This enhances security by ensuring that access to the cluster
    is controlled and monitored, thereby reducing attack vectors.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#anonymous-requests
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-anonymous-access
  compliance:
  - hipaa_multi_cloud_164_312_d_0029
  - rbi_bank_multi_cloud_12.1_0005
  - soc2_multi_cloud_cc_6_6_0012
- rule_id: k8s.apiserver.audit.logging_enabled
  service: apiserver
  resource: audit
  requirement: Logging Enabled
  scope: apiserver.audit.logging_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable and Configure Audit Logging for K8s API Server
  rationale: Without proper audit logging, malicious activities or unauthorized access to sensitive resources may go undetected.
    Attackers can exploit this lack of visibility to persist in the system, exfiltrate data, or cause disruptions without
    being noticed. Audit logs provide traceability and accountability, which are essential for incident response and forensic
    investigations.
  description: This control checks if audit logging is enabled and properly configured on the Kubernetes API server. A well-configured
    audit logging setup should specify a meaningful audit policy that captures critical events, such as authentication attempts,
    resource modifications, and access to sensitive data. This setup should ensure that logs are stored securely and retained
    for an appropriate period. Implementing audit logging helps in detecting anomalies, monitoring user activities, and meeting
    compliance requirements by providing an evidential trail of operations within the cluster.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-2_0042
  - fedramp_moderate_multi_cloud_CM-2_0101
  - fedramp_moderate_multi_cloud_CM-2_2_0102
  - fedramp_moderate_multi_cloud_CM-2_3_0103
  - fedramp_moderate_multi_cloud_CM-2_7_0104
  - fedramp_moderate_multi_cloud_CP-9_0160
  - fedramp_moderate_multi_cloud_CP-9_1_0161
  - fedramp_moderate_multi_cloud_CP-9_2_0162
  - fedramp_moderate_multi_cloud_CP-9_3_0163
  - fedramp_moderate_multi_cloud_CP-9_5_0164
  - fedramp_moderate_multi_cloud_CP-9_8_0165
  - gdpr_multi_cloud_Article_25_Data_protection_by_design_and_by_defaul_0001
  - gdpr_multi_cloud_Article_32_Security_of_processing_0003
  - hipaa_multi_cloud_164_308_a_1_ii_b_0002
  - hipaa_multi_cloud_164_308_a_1_ii_d_0003
  - hipaa_multi_cloud_164_308_a_5_ii_c_0013
  - iso27001_2022_multi_cloud_A.8.27_0083
  - iso27001_2022_multi_cloud_A.8.9_0096
  - nist_800_53_rev5_multi_cloud_AU-14-a_0289
  - nist_800_53_rev5_multi_cloud_AU-2-b_0224
  - nist_800_53_rev5_multi_cloud_CA-7-b_0338
  - nist_800_53_rev5_multi_cloud_CM-2-a_0366
  - nist_800_53_rev5_multi_cloud_CP-9-d_0512
  - nist_800_53_rev5_multi_cloud_PM-31-b_0912
  - nist_800_53_rev5_multi_cloud_PM-31-d_0914
  - nist_800_53_rev5_multi_cloud_PM-31-e_0915
  - nist_800_53_rev5_multi_cloud_PM-31-f_0916
  - pci_dss_v4_multi_cloud_3.4.1_0039
  - pci_dss_v4_multi_cloud_4.1.1_0058
  - soc2_multi_cloud_cc_3_2_0004
  - soc2_multi_cloud_cc_c_1_1_0022
- rule_id: k8s.apiserver.authentication.enabled
  service: apiserver
  resource: authentication
  requirement: Enabled
  scope: apiserver.authentication.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure API Server Authentication is Enabled
  rationale: If the Kubernetes API server authentication is not enabled, unauthorized users could potentially gain access
    to the cluster, leading to data breaches, resource hijacking, and potential denial of service attacks. Attackers could
    exploit this misconfiguration to execute arbitrary commands, deploy malicious pods, or extract sensitive information from
    the cluster.
  description: This control checks that the Kubernetes API server has authentication mechanisms properly configured and enabled.
    A secure configuration typically involves enabling TokenReview, client certificate authentication, or other authentication
    methods like OpenID Connect (OIDC) or webhook. This ensures that only authenticated users can access the API server, reducing
    the risk of unauthorized access. Proper API server authentication configuration also ensures compliance with security
    frameworks and benchmarks like the CIS Kubernetes Benchmark. Administrators should verify that all authentication methods
    are functioning as expected and audit logs are capturing authentication attempts.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IA-2_0062
  - fedramp_moderate_multi_cloud_IA-2_0170
  - fedramp_moderate_multi_cloud_IA-2_1_0171
  - fedramp_moderate_multi_cloud_IA-2_2_0172
  - fedramp_moderate_multi_cloud_IA-2_5_0173
  - fedramp_moderate_multi_cloud_IA-2_6_0174
  - fedramp_moderate_multi_cloud_IA-2_8_0175
  - fedramp_moderate_multi_cloud_IA-2_12_0176
  - fedramp_moderate_multi_cloud_IA-5_0180
  - fedramp_moderate_multi_cloud_IA-5_1_0181
  - fedramp_moderate_multi_cloud_IA-5_2_0182
  - fedramp_moderate_multi_cloud_IA-5_6_0183
  - fedramp_moderate_multi_cloud_IA-5_7_0184
  - fedramp_moderate_multi_cloud_IA-5_8_0185
  - fedramp_moderate_multi_cloud_IA-5_13_0186
  - fedramp_moderate_multi_cloud_SI-2_0363
  - fedramp_moderate_multi_cloud_SI-2_2_0364
  - fedramp_moderate_multi_cloud_SI-2_3_0365
  - hipaa_multi_cloud_164_312_c_2_0028
  - hipaa_multi_cloud_164_312_d_0029
  - iso27001_2022_multi_cloud_A.8.5_0092
  - iso27001_2022_multi_cloud_A.9.4_0099
  - nist_800_171_r2_multi_cloud_3_5_2_3.5.2_Authenticate_or_verify_the_identiti_0043
  - rbi_nbfc_multi_cloud_2.11_0015
- rule_id: k8s.apiserver.authentication.mfa_enabled
  service: apiserver
  resource: authentication
  requirement: Mfa Enabled
  scope: apiserver.authentication.mfa_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Multi-Factor Authentication for API Server Access
  rationale: Without multi-factor authentication (MFA), the Kubernetes API server is vulnerable to unauthorized access through
    compromised credentials. Attackers could exploit this to gain administrative access, manipulate workloads, or exfiltrate
    sensitive data. MFA adds an additional layer of security by requiring a second form of verification, significantly reducing
    the risk of credential-based attacks.
  description: This rule verifies that the Kubernetes API server is configured to enforce multi-factor authentication (MFA)
    for all access attempts. A correct configuration involves integrating an external identity provider that supports MFA.
    This setup not only fortifies the API server against unauthorized access but also ensures compliance with industry standards
    and regulations. By requiring MFA, organizations can mitigate risks related to stolen or weak passwords while enhancing
    overall security posture.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/concepts/security/overview/#api-server-authentication
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - nist_800_171_r2_multi_cloud_3_3_5_3.3.5_Correlate_audit_record_review_analys_0035
  - nist_800_171_r2_multi_cloud_3_5_3_3.5.3_Use_multifactor_authentication_for_lo_0044
- rule_id: k8s.apiserver.authentication.strong_mechanism_enabled
  service: apiserver
  resource: authentication
  requirement: Strong Mechanism Enabled
  scope: apiserver.authentication.strong_mechanism_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Strong Authentication Mechanisms for API Server
  rationale: If strong authentication mechanisms are not enabled for the Kubernetes API server, it could lead to unauthorized
    access by attackers exploiting weak or improperly configured authentication methods. This can result in compromised cluster
    integrity, unauthorized data access, and the potential for lateral movement within the cluster.
  description: This control checks whether strong authentication mechanisms, such as client certificates, OpenID Connect,
    or webhook token authentication, are enabled for the Kubernetes API server. Proper configuration ensures that only authenticated
    and authorized users can interact with the API server, significantly reducing the risk of unauthorized access. Strong
    mechanisms help enforce secure user identity verification, which is critical for maintaining the security posture of the
    Kubernetes cluster and complying with industry standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#authentication
  - https://kubernetes.io/docs/concepts/security/overview/#namespace-separation
  compliance:
  - nist_800_53_rev5_multi_cloud_MA-4-c_0684
- rule_id: k8s.apiserver.authorization.mode_check
  service: apiserver
  resource: authorization
  requirement: Mode Check
  scope: apiserver.authorization.mode_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce RBAC and Node Authorization Modes
  rationale: Misconfigured authorization modes in the Kubernetes API server can lead to unauthorized access to cluster resources.
    Without proper authorization modes like RBAC, malicious actors could exploit overly permissive access controls, leading
    to privilege escalation and unauthorized data access.
  description: This control checks that the Kubernetes API server is configured to use secure authorization modes, specifically
    Role-Based Access Control (RBAC) and Node authorization. By ensuring these modes are enabled, the cluster implements granular
    access control policies and authenticates node requests effectively. A correct configuration enforces the principle of
    least privilege, reducing the risk of unauthorized access and potential data breaches.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/reference/access-authn-authz/node/
  compliance:
  - cis_kubernetes_kubernetes_1.2.6_0027
  - cis_kubernetes_kubernetes_3.1.14_0207
- rule_id: k8s.apiserver.authorization.mode_rbac
  service: apiserver
  resource: authorization
  requirement: Mode Rbac
  scope: apiserver.authorization.mode_rbac
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Enforce RBAC Authorization Mode on API Server
  rationale: Without RBAC, Kubernetes clusters are susceptible to unauthorized access and privilege escalation. Attackers
    can exploit weak access controls to gain excessive permissions, compromising cluster integrity and data confidentiality.
    RBAC enforces least privilege, mitigating risks of insider threats and external attacks.
  description: This rule checks that the Kubernetes API server is configured to use RBAC (Role-Based Access Control) for authorization.
    A correctly configured RBAC limits user and service account permissions to only what is necessary, adhering to the principle
    of least privilege. This setup helps prevent unauthorized actions within the cluster, ensuring that users can only perform
    operations within their defined roles, thus reducing the attack surface.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#rbac-authorization
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IA-2_0062
  - fedramp_moderate_multi_cloud_IA-2_0170
  - fedramp_moderate_multi_cloud_IA-2_1_0171
  - fedramp_moderate_multi_cloud_IA-2_2_0172
  - fedramp_moderate_multi_cloud_IA-2_5_0173
  - fedramp_moderate_multi_cloud_IA-2_6_0174
  - fedramp_moderate_multi_cloud_IA-2_8_0175
  - fedramp_moderate_multi_cloud_IA-2_12_0176
  - fedramp_moderate_multi_cloud_IA-5_0180
  - fedramp_moderate_multi_cloud_IA-5_1_0181
  - fedramp_moderate_multi_cloud_IA-5_2_0182
  - fedramp_moderate_multi_cloud_IA-5_6_0183
  - fedramp_moderate_multi_cloud_IA-5_7_0184
  - fedramp_moderate_multi_cloud_IA-5_8_0185
  - fedramp_moderate_multi_cloud_IA-5_13_0186
  - fedramp_moderate_multi_cloud_SI-2_0363
  - fedramp_moderate_multi_cloud_SI-2_2_0364
  - fedramp_moderate_multi_cloud_SI-2_3_0365
  - hipaa_multi_cloud_164_312_d_0029
  - iso27001_2022_multi_cloud_A.8.3_0085
  - iso27001_2022_multi_cloud_A.8.30_0086
  - iso27001_2022_multi_cloud_A.8.31_0087
  - iso27001_2022_multi_cloud_A.8.32_0088
  - iso27001_2022_multi_cloud_A.8.33_0089
  - iso27001_2022_multi_cloud_A.8.34_0090
  - iso27001_2022_multi_cloud_A.8.5_0092
  - nist_800_171_r2_multi_cloud_3_5_2_3.5.2_Authenticate_or_verify_the_identiti_0043
  - rbi_bank_multi_cloud_12.1_0005
  - rbi_nbfc_multi_cloud_2.1_0005
  - rbi_nbfc_multi_cloud_2.10_0014
- rule_id: k8s.apiserver.automatic.upgrade_enabled
  service: apiserver
  resource: automatic
  requirement: Upgrade Enabled
  scope: apiserver.automatic.upgrade_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Automatic Upgrades for Kube-Apiserver
  rationale: Automatic upgrades for the kube-apiserver ensure that the system is consistently updated with the latest security
    patches and features, reducing the risk of vulnerabilities being exploited by attackers. Without automatic upgrades, outdated
    versions might expose the apiserver to known vulnerabilities, leading to potential data breaches or unauthorized access.
  description: This control checks whether automatic upgrades are enabled for the kube-apiserver component. Ensuring automatic
    upgrades are enabled helps maintain the security and stability of the Kubernetes control plane by applying the latest
    security patches and updates. A properly configured system minimizes the risk of exploitation from known vulnerabilities,
    ensures compliance with security standards, and reduces the operational overhead of manual upgrades.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/setup/release/version-skew-policy/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SI-2_0147
- rule_id: k8s.apiserver.backup.configured
  service: apiserver
  resource: backup
  requirement: Configured
  scope: apiserver.backup.configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Scheduled Backups of Kubernetes API Server Configuration
  rationale: Without regular backups of the Kubernetes API server configuration, recovery from data corruption or accidental
    deletion can be severely delayed. This increases the risk of prolonged service outages and potential exposure to security
    vulnerabilities if configuration changes cannot be reverted quickly. Attackers could exploit misconfigurations or gain
    unauthorized access, especially if sensitive configurations are lost or mismanaged.
  description: This control checks that a regular backup schedule for the Kubernetes API server configuration is in place
    and properly implemented. A good configuration involves automated, periodic backups stored securely and offsite, with
    encryption enabled to protect data integrity and confidentiality. Such practices help ensure quick recovery from incidents,
    maintain service availability, and protect against data loss, meeting compliance requirements and industry best practices.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/backup-restore/
  compliance:
  - hipaa_multi_cloud_164_308_a_7_ii_b_0019
  - rbi_bank_multi_cloud_6.2_0022
- rule_id: k8s.apiserver.configuration.review_scheduled
  service: apiserver
  resource: configuration
  requirement: Review Scheduled
  scope: apiserver.configuration.review_scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regularly Review and Update Kube-apiserver Configuration
  rationale: Kube-apiserver acts as the central management point for the entire Kubernetes cluster. If its configuration is
    outdated or misconfigured, it could lead to unauthorized access, data breaches, or denial of service attacks. Regular
    reviews ensure that security patches are applied and configurations are aligned with the latest security policies, reducing
    the risk of exploitation.
  description: This control checks that the Kube-apiserver configuration is reviewed and updated regularly. It ensures that
    security settings, such as authentication and authorization configurations, network policies, and audit logging, adhere
    to industry best practices. A well-maintained configuration helps mitigate risks by preventing potential vulnerabilities
    from being exploited and ensuring compliance with standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-2-b_0367
- rule_id: k8s.apiserver.ddos.protection_configured
  service: apiserver
  resource: ddos
  requirement: Protection Configured
  scope: apiserver.ddos.protection_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable API Server Rate Limiting for DDoS Protection
  rationale: Without rate limiting on the API server, a Distributed Denial of Service (DDoS) attack can overwhelm the server,
    leading to service disruptions. Attackers can exploit this by sending excessive requests, which could degrade performance
    and impact availability of the Kubernetes control plane.
  description: This rule checks that the Kubernetes API server has rate limiting configured to mitigate the risk of DDoS attacks.
    A correctly configured rate limiter controls the number of requests allowed per unit of time, preventing resource exhaustion
    and maintaining availability. Proper configuration involves setting flags such as --max-requests-inflight and --max-mutating-requests-inflight
    to reasonable values, thereby ensuring the API server can handle legitimate traffic while protecting against abuse.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/#api-priorities-and-fairness
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#rate-limiting
  compliance:
  - fedramp_moderate_multi_cloud_SC-5_0331
- rule_id: k8s.apiserver.deletion.protection_enabled
  service: apiserver
  resource: deletion
  requirement: Protection Enabled
  scope: apiserver.deletion.protection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Deletion Protection on Kubernetes API Server
  rationale: Misconfigured deletion protection on the Kubernetes API server can lead to accidental or malicious removal of
    critical server components, potentially causing disruptions in service availability and enabling denial-of-service attacks.
    By ensuring deletion protection is enabled, we mitigate the risk of unauthorized deletions that might exploit improper
    access controls or vulnerabilities.
  description: This check verifies that deletion protection is enabled for the Kubernetes API server. A properly configured
    API server with deletion protection ensures that critical resources cannot be easily removed without appropriate permissions
    and review. This security measure helps maintain the integrity and availability of the Kubernetes control plane, preventing
    unauthorized users or malicious insiders from deleting resources that could disrupt cluster operations. The configuration
    aligns with best practices outlined in the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node/
  - https://kubernetes.io/docs/tasks/administer-cluster/protect-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_CP-2-e_0466
- rule_id: k8s.apiserver.encryption.config_check
  service: apiserver
  resource: encryption
  requirement: Config Check
  scope: apiserver.encryption.config_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Encryption Configuration for Secrets at Rest
  rationale: Without encryption configured for secrets at rest, sensitive data stored in etcd is vulnerable to unauthorized
    access and data breaches. Attackers gaining access to etcd can exploit unencrypted secrets, leading to potential information
    disclosure and compromise of the entire Kubernetes cluster. Encrypting secrets mitigates these risks by ensuring that
    even if etcd is accessed, the data remains protected.
  description: This check ensures that the Kubernetes API server is configured to use encryption for secrets at rest. It involves
    validating that the 'EncryptionConfiguration' parameter is properly defined in the API server's configuration file, specifying
    the use of encryption providers such as 'AES-CBC' or 'secretbox'. A correctly configured encryption setup ensures that
    secrets stored in etcd are encrypted and only accessible by authorized components, significantly enhancing the security
    posture of the cluster by protecting sensitive data from unauthorized access and exposure.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/setup/best-practices/enforcing-policies/
  - https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/
  compliance:
  - cis_kubernetes_kubernetes_1.2.32_0136
- rule_id: k8s.apiserver.high.availability_configured
  service: apiserver
  resource: high
  requirement: Availability Configured
  scope: apiserver.high.availability_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure API Server High Availability Configuration
  rationale: A misconfigured API server high availability setup can lead to a single point of failure, making the Kubernetes
    cluster vulnerable to downtime and potential denial-of-service attacks. Ensuring high availability is crucial for maintaining
    the reliability and security of the cluster by distributing the load across multiple instances and preventing service
    disruption.
  description: This rule checks that the Kubernetes API server is configured for high availability, requiring multiple replicas
    distributed across different nodes. A proper high availability setup ensures that if one instance fails, others can continue
    to serve requests. This configuration is crucial for minimizing downtime, preventing service disruption, and ensuring
    continuous access to the Kubernetes control plane, thereby enhancing both the reliability and security posture of the
    cluster.
  references:
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
  - https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node/
  compliance:
  - hipaa_multi_cloud_164_308_a_7_ii_c_0020
- rule_id: k8s.apiserver.https.only
  service: apiserver
  resource: https
  requirement: Only
  scope: apiserver.https.only
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce HTTPS for Kubernetes API Server
  rationale: Misconfiguring the API server to allow non-HTTPS traffic can expose sensitive data to interception and unauthorized
    access. Attackers could exploit this to perform Man-in-the-Middle (MitM) attacks, gain unauthorized access to cluster
    resources, and compromise the entire Kubernetes environment.
  description: This control checks that the Kubernetes API server is configured to accept only HTTPS connections, ensuring
    that all data transmitted between clients and the server is encrypted. A correctly configured API server uses TLS certificates
    to establish secure communication channels, preventing eavesdropping and data tampering. Proper configuration involves
    setting the '--secure-port' flag, ensuring no plain HTTP ports are open, and using valid TLS certificates. This helps
    maintain confidentiality and integrity of the data, aligning with security best practices and compliance requirements.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - fedramp_moderate_multi_cloud_SC-8_0343
  - fedramp_moderate_multi_cloud_SC-8_1_0344
- rule_id: k8s.apiserver.log.backup_configured
  service: apiserver
  resource: log
  requirement: Backup Configured
  scope: apiserver.log.backup_configured
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: medium
  title: Ensure Backup of Kubernetes API Server Logs
  rationale: Failure to configure regular backups for Kubernetes API server logs can lead to loss of crucial audit trails
    and system activity records, hindering incident response and forensic investigations. An attacker exploiting this misconfiguration
    could erase traces of unauthorized access or malicious activities, complicating detection and accountability.
  description: This rule checks whether backups for Kubernetes API server logs are configured and operational. A well-configured
    backup ensures that logs are stored securely offsite, protecting them from local system failures and unauthorized tampering.
    This control helps in maintaining an immutable record of API server activity, which is critical for identifying unusual
    patterns, unauthorized access attempts, and ensuring compliance with industry standards and audit requirements.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/administer-cluster/logging-stack/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-2_0053
  - nist_800_53_rev5_multi_cloud_CP-9-c_0511
- rule_id: k8s.apiserver.log.maxsize_check
  service: apiserver
  resource: log
  requirement: Maxsize Check
  scope: apiserver.log.maxsize_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Max Log Size for kube-apiserver
  rationale: If the maximum size of the kube-apiserver logs is not properly configured, it can lead to excessive disk usage,
    potentially exhausting storage resources and leading to denial of service. Moreover, excessively large log files can become
    difficult to manage, making it harder to audit and detect security incidents. Without proper log size management, attackers
    might exploit this to cover their tracks or create noise to mask malicious activities.
  description: This control checks whether the `--audit-log-maxsize` flag is set for the kube-apiserver, ensuring that log
    files do not exceed a specified size. A properly configured max log size ensures that log rotation occurs, preventing
    logs from growing indefinitely, which could lead to disk exhaustion and hinder effective log analysis. This also aligns
    with best practices as outlined in the CIS Kubernetes Benchmark, helping maintain a secure and manageable Kubernetes environment.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#log-backend
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit-config.v1/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  compliance:
  - cis_kubernetes_kubernetes_1.2.21_0042
- rule_id: k8s.apiserver.policy.captures_metadata
  service: apiserver
  resource: policy
  requirement: Captures Metadata
  scope: apiserver.policy.captures_metadata
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Metadata Capture in API Server Audit Logs
  rationale: Failure to capture metadata in API server audit logs can lead to insufficient tracking of changes and activities
    within the Kubernetes cluster, making it difficult to detect unauthorized actions or anomalies. Attackers could exploit
    this lack of visibility to perform lateral movement or escalate privileges without detection, compromising cluster integrity
    and confidentiality.
  description: This rule checks if the Kubernetes API server is configured to capture metadata in its audit logs. Proper configuration
    involves setting up audit policies that include metadata such as user identity, IP address, and request parameters. Capturing
    detailed metadata helps in creating an audit trail, crucial for forensic analysis, compliance audits, and ensuring accountability.
    It reduces security risks by enabling the detection of unauthorized access and anomalous behavior.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/kubernetes-audit/v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - pci_dss_v4_multi_cloud_9.2.1_0121
  - pci_dss_v4_multi_cloud_9.2.1.1_0122
  - pci_dss_v4_multi_cloud_9.3.1_0126
  - pci_dss_v4_multi_cloud_9.3.1.1_0127
  - pci_dss_v4_multi_cloud_10.1.2_0142
- rule_id: k8s.apiserver.policy.configured
  service: apiserver
  resource: policy
  requirement: Configured
  scope: apiserver.policy.configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure API Server Audit Logging is Enabled
  rationale: Without audit logging enabled, unauthorized access and malicious activities within the Kubernetes cluster may
    go undetected. Audit logs provide a detailed record of interactions with the API server, which is crucial for identifying
    and investigating security incidents, monitoring access patterns, and ensuring compliance with security policies.
  description: This control checks that audit logging is enabled on the Kubernetes API server. Proper configuration includes
    setting appropriate log levels, defining audit policies, and ensuring logs are securely stored and accessible for review.
    Enabling audit logging enhances security by providing visibility into API requests and responses, helping detect anomalies
    and unauthorized access attempts. It is a foundational security measure to monitor and investigate API activity.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-logging
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.k8s.io/
  compliance: []
- rule_id: k8s.apiserver.rate.limiting_enabled
  service: apiserver
  resource: rate
  requirement: Limiting Enabled
  scope: apiserver.rate.limiting_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable API Server Request Rate Limiting
  rationale: If rate limiting on the Kubernetes API server is not properly configured, it may become susceptible to Denial
    of Service (DoS) attacks. These attacks can overwhelm the API server, leading to degraded performance or complete unavailability
    of the cluster's control plane. This can prevent legitimate requests from being fulfilled, potentially impacting application
    availability and security monitoring.
  description: This control checks whether rate limiting is enabled on the Kubernetes API server to ensure that it is configured
    according to security best practices. A proper rate limiting configuration ensures that the API server can handle requests
    efficiently without being overwhelmed by excessive requests, whether intentional or accidental. This helps to mitigate
    the risk of DoS attacks by limiting the number of requests an entity can make within a specified time frame, thus preserving
    the availability and reliability of the Kubernetes control plane.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/#apiserver
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-requests-using-auditing
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-5_0131
  - nist_800_53_rev5_multi_cloud_SC-5-b_1192
- rule_id: k8s.apiserver.secret.authority_validated
  service: apiserver
  resource: secret
  requirement: Authority Validated
  scope: apiserver.secret.authority_validated
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Valid Authentication for API Server Secrets
  rationale: If API server secrets are not properly authenticated, attackers could exploit vulnerabilities to gain unauthorized
    access to sensitive data. This can lead to data breaches, unauthorized data manipulation, and potential privilege escalation
    within the Kubernetes cluster.
  description: This control checks that the Kubernetes API server is configured to validate the authority of requests to access
    secrets. Proper configuration involves ensuring that authentication mechanisms such as RBAC are correctly implemented
    to verify that only authorized entities can access secrets. A secure configuration helps prevent unauthorized access and
    mitigates the risk of sensitive data leaks.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.apiserver.secure.configuration_applied
  service: apiserver
  resource: secure
  requirement: Configuration Applied
  scope: apiserver.secure.configuration_applied
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Secure API Server Configuration
  rationale: A misconfigured API Server can be exploited to bypass authentication and authorization controls, allowing unauthorized
    access to the Kubernetes cluster. Attack vectors include API server exposure to public networks, lack of transport layer
    security (TLS), and improper authentication methods, all of which can lead to data breaches and unauthorized manipulation
    of workloads.
  description: This control checks that the Kubernetes API Server is configured with security best practices. Key configurations
    include enforcing TLS for all communications, enabling secure authentication and authorization mechanisms, and restricting
    access to sensitive API endpoints. Proper configuration minimizes the attack surface and ensures only authenticated and
    authorized entities can interact with the API server, thereby securing the cluster management plane.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-6_0046
  - nist_800_171_r2_multi_cloud_3_4_2_3.4.2_Establish_and_enforce_security_config_0038
- rule_id: k8s.apiserver.secure.flags_configured
  service: apiserver
  resource: secure
  requirement: Flags Configured
  scope: apiserver.secure.flags_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kube-apiserver Secure Flags Are Properly Configured
  rationale: Misconfiguration of kube-apiserver flags can lead to vulnerabilities such as unauthorized access, data breaches,
    and privilege escalation attacks. Properly configured flags ensure that security features like encryption, authentication,
    and authorization are enforced, reducing the risk of exploitation by malicious actors.
  description: This rule checks that the kube-apiserver is configured with secure flags according to best practices. Critical
    flags include '--anonymous-auth=false', '--insecure-port=0', and '--secure-port=443', among others. Proper configuration
    of these flags ensures that the API server does not accept requests from unauthenticated users, disables insecure communication
    channels, and enforces secure communication. This helps in mitigating risks of unauthorized access and data interception,
    ensuring compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-b_0398
- rule_id: k8s.apiserver.server.admission_control_policy_check
  service: apiserver
  resource: server
  requirement: Admission Control Policy Check
  scope: apiserver.server.admission_control_policy_check
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enable and Configure Mandatory Admission Controllers
  rationale: Admission controllers act as gatekeepers for the Kubernetes API, intercepting requests to ensure they meet certain
    security and compliance criteria before resources are created, updated, or deleted. Without them, attackers could exploit
    misconfigurations to escalate privileges, bypass security policies, or deploy insecure workloads. Admission controllers
    can enforce policies such as PodSecurity, resource quotas, and more, providing a critical layer of defense against unauthorized
    or harmful changes.
  description: This rule checks that specific, mandatory admission controllers are enabled and correctly configured in the
    kube-apiserver. Proper configuration of admission controllers like 'PodSecurity', 'ResourceQuota', and 'NodeRestriction'
    ensures that pods adhere to security policies, resources are not over-allocated, and nodes are protected from unauthorized
    scheduling. This setup minimizes the risk of privilege escalation, resource abuse, and policy violations, thus maintaining
    a secure and compliant Kubernetes environment.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/security/overview/#pod-security-standards
  - https://kubernetes.io/docs/tasks/administer-cluster/enforce-quota/
  compliance:
  - cis_kubernetes_kubernetes_3.1.8_0201
- rule_id: k8s.apiserver.server.admission_control_service_account_check
  service: apiserver
  resource: server
  requirement: Admission Control Service Account Check
  scope: apiserver.server.admission_control_service_account_check
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Service Account Admission Control
  rationale: Without proper admission control for service accounts, unauthorized or rogue services can impersonate existing
    accounts, potentially gaining access to sensitive resources. This elevates the risk of privilege escalation and unauthorized
    data access, compromising the cluster's security integrity.
  description: This control ensures that the Kubernetes API server has the admission control plugin 'ServiceAccount' enabled.
    When properly configured, it verifies that all pods are associated with a valid service account, preventing the execution
    of pods with default or no account settings. This mitigates risks associated with default permissions and unauthorized
    resource access, aligning with security best practices and compliance standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#serviceaccount
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  compliance:
  - cis_kubernetes_kubernetes_1.1.28_0147
- rule_id: k8s.apiserver.server.admission_controller_image_policy_webhook_configured
  service: apiserver
  resource: server
  requirement: Admission Controller Image Policy Webhook Configured
  scope: apiserver.server.admission_controller_image_policy_webhook_configured
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Image Policy with Admission Controller Webhook
  rationale: Without an Image Policy Webhook, there is a risk of running unauthorized or potentially malicious container images.
    Attackers could exploit this to deploy vulnerable or compromised images, leading to supply chain attacks or privilege
    escalation within the cluster. Configuring this ensures that only vetted and compliant images are allowed, reducing the
    attack surface and maintaining cluster integrity.
  description: This control checks that the Kubernetes API server has the Image Policy Webhook admission controller configured.
    It validates that the webhook is active and properly set to enforce image policies, ensuring that only images meeting
    specific security criteria are allowed to run. Proper configuration helps prevent the deployment of unsafe images, thereby
    enforcing compliance with security policies and reducing risks associated with untrusted container images.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook
  - https://kubernetes.io/docs/concepts/containers/images/#image-policies
  - https://kubernetes.io/docs/tasks/administer-cluster/admission-controllers/
  compliance:
  - cis_kubernetes_kubernetes_1.6.8_0177
- rule_id: k8s.apiserver.server.admission_plugin_namespace_lifecycle_enabled
  service: apiserver
  resource: server
  requirement: Admission Plugin Namespace Lifecycle Enabled
  scope: apiserver.server.admission_plugin_namespace_lifecycle_enabled
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Namespace Lifecycle Admission Control
  rationale: If the Namespace Lifecycle admission control is not enabled, namespaces can be deleted while they still have
    active resources, leading to orphaned resources that may not be properly managed or secured. This can result in unauthorized
    access and resource leakage, potentially becoming an attack vector for privilege escalation and denial-of-service attacks.
  description: This control checks whether the Namespace Lifecycle admission plugin is enabled on the Kubernetes API server.
    A valid configuration ensures that namespaces cannot be deleted if they contain any resources, preventing the accidental
    or malicious removal of namespaces with active workloads. This reduces the risk of resource mismanagement and ensures
    that resources are properly cleaned up, thereby maintaining a secure and stable cluster environment.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespace-lifecycle
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  - https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/
  compliance:
  - cis_kubernetes_kubernetes_1.2.14_0035
  - cis_kubernetes_kubernetes_3.1.9_0202
- rule_id: k8s.apiserver.server.admission_plugins_always_pull_images_set
  service: apiserver
  resource: server
  requirement: Admission Plugins Always Pull Images Set
  scope: apiserver.server.admission_plugins_always_pull_images_set
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce AlwaysPullImages Admission Controller
  rationale: Without AlwaysPullImages, Kubernetes might run images from the local cache instead of pulling the latest version
    from the registry. This can lead to running outdated or compromised images, increasing the risk of vulnerabilities and
    allowing potential attacks such as executing unverified code or exploiting known vulnerabilities in older image versions.
  description: This control checks whether the AlwaysPullImages admission plugin is enabled in the Kubernetes API server configuration.
    When enabled, this setting ensures that the kubelet always pulls the image from the registry when starting containers,
    which helps maintain up-to-date deployments and prevents the execution of stale or unauthorized images. This is crucial
    for environments where image updates are frequent or security patches are rapidly deployed. Enforcing this policy mitigates
    risks associated with cached images and ensures compliance with security best practices.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages
  - https://kubernetes.io/docs/concepts/containers/images/#updating-images
  - https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  compliance:
  - cis_kubernetes_kubernetes_1.2.11_0032
- rule_id: k8s.apiserver.server.admission_plugins_check
  service: apiserver
  resource: server
  requirement: Admission Plugins Check
  scope: apiserver.server.admission_plugins_check
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Mandatory Admission Plugins in Kube-Apiserver
  rationale: Admission plugins in Kubernetes control and validate requests before they persist in the cluster. Misconfiguring
    them can lead to unauthorized resource creation, privilege escalation, and bypass of security policies. Attackers could
    exploit these misconfigurations to deploy malicious workloads or access sensitive resources.
  description: This check ensures that the Kubernetes API server has critical admission plugins such as 'NamespaceLifecycle',
    'NodeRestriction', and 'PodSecurityPolicy' enabled. These plugins help enforce namespace access controls, restrict node
    operations, and apply security policies to pods. Proper configuration reduces the risk of unauthorized access and enforces
    compliance with security policies, thereby enhancing overall cluster security.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#builtin-admission-controllers
  compliance:
  - cis_kubernetes_kubernetes_1.2.10_0031
- rule_id: k8s.apiserver.server.admission_plugins_event_rate_limit_set
  service: apiserver
  resource: server
  requirement: Admission Plugins Event Rate Limit Set
  scope: apiserver.server.admission_plugins_event_rate_limit_set
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Event Rate Limiting in Admission Control
  rationale: Without proper event rate limiting, the Kubernetes API server is vulnerable to denial-of-service attacks where
    malicious actors can overwhelm the server with a high volume of requests. This can lead to degraded performance, service
    outages, or even system crashes, compromising the overall security and availability of the cluster.
  description: This control ensures that the Kubernetes API server has the EventRateLimit admission plugin enabled and properly
    configured. It checks for the presence of rate limiting settings within the admission controller configuration. A well-configured
    event rate limit prevents excessive API server load by capping the number of requests that can be processed. This contributes
    to maintaining service stability, protecting against DoS attacks, and complying with security standards like the CIS Kubernetes
    Benchmark. A good configuration involves setting appropriate limits based on cluster size and traffic patterns.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#eventratelimit
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/tasks/administer-cluster/rate-limit/
  compliance:
  - cis_kubernetes_kubernetes_1.2.9_0030
- rule_id: k8s.apiserver.server.admission_plugins_node_restriction_check
  service: apiserver
  resource: server
  requirement: Admission Plugins Node Restriction Check
  scope: apiserver.server.admission_plugins_node_restriction_check
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Ensure NodeRestriction Admission Plugin is Enabled
  rationale: If the NodeRestriction admission plugin is not enabled, nodes could obtain unauthorized access to resources by
    impersonating other nodes or service accounts, leading to potential data breaches and privilege escalation attacks.
  description: This rule checks that the NodeRestriction admission plugin is configured on the Kubernetes API server. The
    NodeRestriction plugin limits the nodes' ability to modify or create resources to only those that are bound to the node
    itself, reducing the risk of a node impersonating another node or accessing unauthorized resources. Proper configuration
    ensures nodes cannot access or modify resources beyond their intended scope, adhering to the principle of least privilege.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#admission-controllers
  compliance:
  - cis_kubernetes_kubernetes_1.2.15_0036
- rule_id: k8s.apiserver.server.admission_plugins_podsecuritypolicy_check
  service: apiserver
  resource: server
  requirement: Admission Plugins Podsecuritypolicy Check
  scope: apiserver.server.admission_plugins_podsecuritypolicy_check
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce PodSecurityPolicy via Admission Controller
  rationale: Without PodSecurityPolicy enforcement, malicious users can deploy pods with elevated privileges, access host
    resources, or execute arbitrary code, leading to potential data breaches or cluster compromise. Ensuring PodSecurityPolicy
    is enforced prevents privilege escalation and maintains cluster isolation.
  description: This rule checks if the PodSecurityPolicy admission controller is enabled in the Kubernetes API server configuration.
    A well-configured PodSecurityPolicy defines a set of conditions that a pod must meet before it is allowed to run. This
    includes restrictions on running as root, using privileged containers, accessing sensitive host resources, and more. Proper
    configuration helps mitigate risks associated with privilege escalation and unauthorized resource access, thereby enhancing
    overall cluster security.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#admission-controllers
  compliance:
  - cis_kubernetes_kubernetes_1.1.25_0144
- rule_id: k8s.apiserver.server.admission_plugins_security_context_deny_check
  service: apiserver
  resource: server
  requirement: Admission Plugins Security Context Deny Check
  scope: apiserver.server.admission_plugins_security_context_deny_check
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Security Context Constraints with Admission Plugins
  rationale: Without proper configuration of the Security Context Deny admission plugin, Kubernetes could allow pods to run
    with elevated privileges or access sensitive host resources, increasing the risk of privilege escalation and unauthorized
    access. An attacker could exploit misconfigured security contexts to gain access to the host system or compromise other
    pods.
  description: This rule checks if the Kubernetes API server is configured with the Security Context Deny admission plugin,
    which prevents pods from being created with insecure security contexts. A valid configuration denies requests that specify
    disallowed security contexts, such as running containers as root or using the host's network namespace. By enforcing strict
    security policies at the admission level, it ensures that all workloads adhere to security best practices, reducing the
    risk of privilege escalation and maintaining cluster integrity.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#securitycontextdeny
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_1.2.12_0033
- rule_id: k8s.apiserver.server.args_profiling_disabled
  service: apiserver
  resource: server
  requirement: Args Profiling Disabled
  scope: apiserver.server.args_profiling_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Profiling in Kube-apiserver to Prevent Information Leakage
  rationale: Enabling profiling in the kube-apiserver can inadvertently expose sensitive information about the server's performance
    and internal state, which can be exploited by attackers to gain insights into potential vulnerabilities or orchestrate
    denial-of-service attacks. Disabling profiling minimizes the risk of unauthorized access to profiling data, thus maintaining
    the confidentiality and integrity of the Kubernetes control plane.
  description: This rule checks that the '--profiling' argument in the kube-apiserver is set to 'false'. Proper configuration
    ensures that profiling endpoints, which can expose sensitive information, are disabled. By preventing profiling data from
    being exposed, this control helps protect the Kubernetes API server from being targeted by attackers looking to gather
    reconnaissance data or exploit detailed server performance insights. This is aligned with security best practices and
    is necessary for compliance with the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-access-to-the-kubernetes-api
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#api-server-metrics
  compliance:
  - cis_kubernetes_kubernetes_1.2.17_0038
- rule_id: k8s.apiserver.server.audit_log_maxage_compliance_check
  service: apiserver
  resource: server
  requirement: Audit Log Maxage Compliance Check
  scope: apiserver.server.audit_log_maxage_compliance_check
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enforce Audit Log MaxAge for Kubernetes API Server
  rationale: Without properly configuring the audit log max age in Kubernetes API Server, logs may be retained longer than
    necessary, increasing the risk of exposure to unauthorized access or data leakage. An attacker could exploit old logs
    to gain insights into past system activities and potential vulnerabilities. Ensuring that logs are rotated and removed
    in a timely manner mitigates these risks and helps in maintaining a minimal data retention policy.
  description: This control verifies that the Kubernetes API Server's audit logging configuration includes a defined maximum
    age (MaxAge) for log retention. A proper configuration ensures that audit logs are retained only for a necessary period,
    thereby reducing the storage of potentially sensitive historical data. A compliant setup will have a 'MaxAge' parameter
    set to a reasonable number of days, ensuring logs are rotated and old data is purged. This practice not only enhances
    data protection but also aligns with compliance standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/kube-apiserver-config.v1/
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  compliance:
  - cis_kubernetes_kubernetes_1.2.19_0040
- rule_id: k8s.apiserver.server.audit_log_maxbackup_check
  service: apiserver
  resource: server
  requirement: Audit Log Maxbackup Check
  scope: apiserver.server.audit_log_maxbackup_check
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Configure Audit Log MaxBackup in Kube-Apiserver
  rationale: Improper configuration of the audit-log-maxbackup setting in the kube-apiserver can lead to excessive storage
    usage or loss of audit logs, which are crucial for post-incident investigations and ensuring compliance with security
    policies. Without proper backups, critical security events may be missed, hindering the ability to detect and respond
    to unauthorized access or configuration changes.
  description: This rule checks that the `--audit-log-maxbackup` parameter is correctly set in the kube-apiserver configuration.
    The parameter determines the maximum number of audit log files to retain, which is essential for maintaining a historical
    record of API server interactions. A properly configured maxbackup ensures that audit logs are preserved for a sufficient
    duration to support forensic investigations and compliance audits, while also preventing disk space exhaustion. The recommended
    configuration aligns with security best practices and industry standards, such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/audit/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  compliance:
  - cis_kubernetes_kubernetes_3.1.11_0204
  - cis_kubernetes_kubernetes_3.1.12_0205
- rule_id: k8s.apiserver.server.audit_log_maxbackup_retent
  service: apiserver
  resource: server
  requirement: Audit Log Maxbackup Retent
  scope: apiserver.server.audit_log_maxbackup_retent
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Ensure Apiserver Audit Log MaxBackup is Configured
  rationale: If the apiserver's audit log max backup is not properly configured, excessive log data can accumulate, leading
    to potential disk space exhaustion. This can disrupt critical logging processes, hindering the ability to detect and investigate
    malicious activities effectively. Proper configuration mitigates risks of log tampering and ensures that audit trails
    are preserved for compliance and forensic analysis.
  description: This control checks that the 'audit-log-maxbackup' parameter for the Kubernetes apiserver is set to an appropriate
    value. This parameter determines how many backup audit logs are retained, ensuring that historical audit data is available
    for a sufficient period. A well-configured audit log retention policy helps maintain a secure logging environment, aids
    in compliance with security benchmarks like CIS, and assists in retaining evidence for security investigations.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-config.v1/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  compliance:
  - cis_kubernetes_kubernetes_1.2.20_0041
- rule_id: k8s.apiserver.server.audit_log_path_check
  service: apiserver
  resource: server
  requirement: Audit Log Path Check
  scope: apiserver.server.audit_log_path_check
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Ensure Audit Log Path is Configured for Kube-Apiserver
  rationale: Without a properly configured audit log path, Kubernetes cannot effectively capture and store audit logs, which
    are critical for forensic investigations, compliance audits, and detecting unauthorized actions. Misconfiguration may
    lead to missing logs, making it difficult to trace actions performed within the cluster, thereby increasing the risk of
    undetected malicious activity or compliance violations.
  description: This control checks that the Kubernetes API server is configured with a valid audit log path, ensuring that
    all audit events are logged and stored securely. A correctly set audit log path enables the capture of all pertinent API
    server requests and responses, facilitating complete visibility into cluster operations. This helps in identifying unauthorized
    access attempts, changes to cluster resources, and ensures adherence to compliance frameworks like CIS Kubernetes Benchmark.
    A good configuration includes specifying a non-default, secure location for storing audit logs, with appropriate access
    controls to prevent tampering.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/audit-policy/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  compliance:
  - cis_kubernetes_kubernetes_1.2.18_0039
- rule_id: k8s.apiserver.server.audit_policy_file_configured
  service: apiserver
  resource: server
  requirement: Audit Policy File Configured
  scope: apiserver.server.audit_policy_file_configured
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Configure Kubernetes API Server Audit Policy File
  rationale: Misconfiguration of the audit policy file can lead to inadequate logging and monitoring, making it difficult
    to detect unauthorized access and activities. Without proper audit logging, attackers can exploit vulnerabilities unnoticed,
    leading to data breaches and non-compliance with regulatory standards.
  description: This rule verifies that the Kubernetes API Server has an audit policy file configured according to best practices.
    It checks for the presence and correct configuration of the audit policy file, which defines how audit logs are generated
    and processed. A well-configured audit policy file helps in tracking access and changes to the cluster, identifying potential
    security incidents, and ensuring compliance with industry standards.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-7_0039
  - nist_800_171_r2_multi_cloud_3_11_2_3.11.2_Scan_for_vulnerabilities_in_organiz_0001
  - nist_800_171_r2_multi_cloud_3_1_12_3.1.12_Monitor_and_control_remote_access_s_0021
  - nist_800_53_rev5_multi_cloud_CA-7-a_0337
  - nist_800_53_rev5_multi_cloud_PM-31-a_0911
  - nist_800_53_rev5_multi_cloud_SI-2-a_1342
  - pci_dss_v4_multi_cloud_8.4.1_0113
  - pci_dss_v4_multi_cloud_8.5.1_0116
  - pci_dss_v4_multi_cloud_8.6.1_0117
  - pci_dss_v4_multi_cloud_9.1.1_0119
  - pci_dss_v4_multi_cloud_10.1.1_0141
  - cis_kubernetes_kubernetes_3.2.1_0072
- rule_id: k8s.apiserver.server.audit_policy_verification
  service: apiserver
  resource: server
  requirement: Audit Policy Verification
  scope: apiserver.server.audit_policy_verification
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enforce Comprehensive Audit Policy in API Server
  rationale: Without a properly configured audit policy, malicious activities could go undetected, leading to potential breaches.
    Attackers could exploit this to perform unauthorized actions without leaving a trace. An effective audit policy helps
    in tracking access and modification events, crucial for identifying and responding to suspicious activities and ensuring
    accountability.
  description: This control checks that the Kubernetes API server has a comprehensive audit policy configured. It ensures
    that all relevant API server requests are logged, capturing sufficient detail to enable effective monitoring and incident
    response. A well-configured audit policy helps security teams detect unauthorized access, potential misconfigurations,
    and policy violations. The policy should cover a range of events including authentication, authorization, and resource
    modifications, thereby improving visibility and aiding compliance with standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-logging
  compliance:
  - cis_kubernetes_kubernetes_3.2.2_0073
- rule_id: k8s.apiserver.server.authorization_mode_includes_node
  service: apiserver
  resource: server
  requirement: Authorization Mode Includes Node
  scope: apiserver.server.authorization_mode_includes_node
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure API Server Authorization Mode Includes Node
  rationale: If the 'Node' authorization mode is not enabled, unauthorized users or compromised nodes may gain unnecessary
    access to sensitive API requests meant for nodes, leading to potential privilege escalation or data breaches. This mode
    restricts nodes to only access resources necessary for their operation, mitigating the risk of lateral movement and maintaining
    cluster security by ensuring only legitimate node-level access.
  description: This rule checks that the Kubernetes API server is configured with 'Node' as one of its authorization modes.
    Proper configuration ensures that nodes can only perform operations on resources that are specifically allowed, thereby
    adhering to the principle of least privilege. This reduces the attack surface by limiting node access strictly to its
    required resources, preventing unauthorized access, and enhancing overall cluster security.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/#node
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-cloud-metadata-api-access
  compliance:
  - cis_kubernetes_kubernetes_1.2.7_0028
- rule_id: k8s.apiserver.server.automatic_upgrade_enabled
  service: apiserver
  resource: server
  requirement: Automatic Upgrade Enabled
  scope: apiserver.server.automatic_upgrade_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Automatic Updates for Kube-Apiserver
  rationale: Without automatic updates, the kube-apiserver may run outdated versions that are susceptible to known vulnerabilities,
    leading to potential compromise of the cluster. Attackers often exploit outdated software to gain unauthorized access
    or execute malicious code. Keeping the kube-apiserver updated helps mitigate these risks by automatically applying security
    patches and updates.
  description: This rule checks that automatic updates are enabled for the kube-apiserver. Proper configuration ensures that
    the server will automatically receive and apply critical security updates and patches. An up-to-date kube-apiserver reduces
    the attack surface by mitigating known vulnerabilities and exploits, thus enhancing the overall security posture of the
    Kubernetes cluster. A good configuration involves setting up update mechanisms that align with the cluster's operational
    policies while ensuring minimal disruption.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.apiserver.server.client_ca_configured
  service: apiserver
  resource: server
  requirement: Client Ca Configured
  scope: apiserver.server.client_ca_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure API Server Client Certificate Authentication is Enabled
  rationale: If the API server is not configured to verify client certificates using a Certificate Authority (CA), unauthorized
    users could gain access to the Kubernetes cluster by bypassing identity verification. This misconfiguration increases
    the risk of privilege escalation, unauthorized data access, and potential denial-of-service attacks. Additionally, it
    may lead to non-compliance with security standards and best practices.
  description: This control checks that the Kubernetes API server is configured with a valid client Certificate Authority
    (CA) bundle using the `--client-ca-file` flag. Proper configuration ensures that all client requests to the API server
    are authenticated through valid client certificates, thereby preventing unauthorized access. A correctly configured client
    CA enhances security by ensuring that only trusted clients can communicate with the API server, mitigating risks of impersonation
    and unauthorized actions within the cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-certificates
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  compliance: []
- rule_id: k8s.apiserver.server.client_ca_file_configuration_check
  service: apiserver
  resource: server
  requirement: Client Ca File Configuration Check
  scope: apiserver.server.client_ca_file_configuration_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Client CA File is Configured for API Server Authentication
  rationale: If the Client CA file is not properly configured on the Kubernetes API Server, there is a risk of unauthorized
    access due to the inability to authenticate client certificates. This misconfiguration can lead to potential man-in-the-middle
    attacks or unauthorized data access, compromising the integrity and confidentiality of the Kubernetes cluster.
  description: This control checks that the Kubernetes API Server is configured with a valid Client CA file, which is essential
    for authenticating client certificates. Proper configuration ensures that only clients with valid certificates, signed
    by a trusted Certificate Authority, can communicate with the API Server. This setup helps prevent unauthorized access
    and ensures that only legitimate users and services can perform actions within the cluster, thus maintaining the security
    boundary of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/administer-cluster/certificates/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - cis_kubernetes_kubernetes_1.1.30_0149
- rule_id: k8s.apiserver.server.client_ca_file_presence_check
  service: apiserver
  resource: server
  requirement: Client Ca File Presence Check
  scope: apiserver.server.client_ca_file_presence_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure API Server Has a Configured Client CA File
  rationale: An improperly configured or missing Client CA file allows malicious users to spoof identities and gain unauthorized
    access to the Kubernetes cluster by bypassing client certificate authentication. This can lead to data breaches, service
    disruptions, and unauthorized resource modifications.
  description: This rule checks if the Kubernetes API server has a valid Client Certificate Authority (CA) file configured.
    The Client CA file is essential for authenticating client credentials that attempt to communicate with the API server.
    Proper configuration ensures that only clients with valid certificates, issued by trusted authorities, can access the
    cluster's API. This reduces the risk of unauthorized access and helps maintain the security of the cluster by enforcing
    strong client authentication.
  references:
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-certificates
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/concepts/cluster-administration/certificates/
  compliance:
  - cis_kubernetes_kubernetes_1.2.27_0048
- rule_id: k8s.apiserver.server.configuration_review_scheduled
  service: apiserver
  resource: server
  requirement: Configuration Review Scheduled
  scope: apiserver.server.configuration_review_scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Schedule Regular Configuration Reviews for Kube-Apiserver
  rationale: Misconfigured kube-apiserver settings can lead to vulnerabilities such as unauthorized access, data breaches,
    and privilege escalation attacks. Regularly reviewing configurations helps detect and rectify misconfigurations, reducing
    the risk of security incidents.
  description: This control checks whether a regular review schedule is in place for the kube-apiserver configurations. A
    good configuration review process involves periodic assessment and validation against security best practices, ensuring
    settings align with security policies and regulations. This helps in identifying and mitigating potential security risks
    proactively, maintaining an optimal security posture and compliance with standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/overview/components/#kube-apiserver
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.apiserver.server.ddos_protection_configured
  service: apiserver
  resource: server
  requirement: Ddos Protection Configured
  scope: apiserver.server.ddos_protection_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Configure API Server for DDoS Mitigation
  rationale: Without proper DDoS protection, the Kubernetes API server is vulnerable to volumetric attacks, which can exhaust
    resources and lead to service disruptions. Attackers can exploit this to degrade cluster performance or render it unavailable,
    impacting application uptime and potentially leading to unauthorized access if the server is overwhelmed.
  description: This control checks that DDoS mitigation configurations, such as rate limiting and resource quotas, are applied
    to the Kubernetes API server. Proper configuration ensures that the server can handle spikes in traffic without degradation.
    This involves setting appropriate flags and using network policies to limit request rates and sources. Implementing these
    configurations helps maintain availability and reliability, safeguarding the API server from being overwhelmed by malicious
    traffic.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/#api-server-protections
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  compliance: []
- rule_id: k8s.apiserver.server.deny_service_external_ips_not_set
  service: apiserver
  resource: server
  requirement: Deny Service External Ips Not Set
  scope: apiserver.server.deny_service_external_ips_not_set
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Restrict Use of External IPs in Services
  rationale: Allowing services to use external IPs can expose them to the internet, increasing the risk of unauthorized access
    and potential attacks such as DDoS. Misconfiguration could lead to sensitive services being accessible from outside the
    cluster, bypassing network security controls and exposing them to vulnerabilities.
  description: This control ensures that services within the Kubernetes cluster do not have external IPs unless explicitly
    required. It checks for the absence of external IPs in service configurations, which helps to prevent accidental exposure
    of internal services to the public internet. By limiting the use of external IPs, the attack surface is reduced, enhancing
    the security posture of the cluster. A properly configured service should only use ClusterIP or NodePort types unless
    there is a justified need for an external IP, which should be meticulously documented and reviewed.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
  - https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - cis_kubernetes_kubernetes_1.2.3_0024
- rule_id: k8s.apiserver.server.encryption_provider_config_check
  service: apiserver
  resource: server
  requirement: Encryption Provider Config Check
  scope: apiserver.server.encryption_provider_config_check
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure Encryption at Rest is Configured in Kube-Apiserver
  rationale: Failure to configure encryption at rest for data in Kubernetes can lead to data breaches, where sensitive data
    may be accessed by unauthorized parties. Attack vectors include unauthorized access to etcd, where Kubernetes stores all
    its data, including secrets and configuration details. Proper encryption mitigates these risks by ensuring that data remains
    unreadable without appropriate decryption keys.
  description: This rule checks if the Kubernetes API server has an encryption provider configuration set, ensuring that data
    stored in etcd is encrypted at rest. A proper configuration involves specifying an encryption provider and ensuring that
    all sensitive data types, such as Secrets, ConfigMaps, and other resources, are encrypted. This helps protect against
    data exfiltration and unauthorized access, maintaining compliance with standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/setup/best-practices/enforcing-policies/
  compliance:
  - cis_kubernetes_kubernetes_1.2.33_0213
- rule_id: k8s.apiserver.server.encryption_provider_verification
  service: apiserver
  resource: server
  requirement: Encryption Provider Verification
  scope: apiserver.server.encryption_provider_verification
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Enforce Encryption at Rest for Kubernetes Secrets
  rationale: Without encryption at rest, sensitive data stored in Kubernetes Secrets is vulnerable to unauthorized access
    if the etcd database is compromised. Attackers gaining access to etcd could retrieve plaintext secrets, leading to potential
    data breaches and violations of compliance requirements.
  description: This control verifies that the Kubernetes API server is configured to use an encryption provider for encrypting
    Secrets at rest in etcd. A properly configured encryption provider ensures that all sensitive data stored as Kubernetes
    Secrets are encrypted before being persisted. This configuration involves specifying encryption providers in the kube-apiserver
    manifest file, typically using a combination of encryption techniques like AES-CBC or AES-GCM. By ensuring encryption
    at rest, this control mitigates risks associated with data exposure and enhances overall cluster security posture.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-config-file/
  compliance:
  - cis_kubernetes_kubernetes_1.2.30_0051
- rule_id: k8s.apiserver.server.etcd_cafile_tls_verification
  service: apiserver
  resource: server
  requirement: Etcd Cafile Tls Verification
  scope: apiserver.server.etcd_cafile_tls_verification
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enable Etcd CA File TLS Verification in API Server
  rationale: Without proper TLS verification of the etcd CA file, the Kubernetes API server could connect to an untrusted
    etcd instance. This could lead to a man-in-the-middle attack where an attacker intercepts or modifies sensitive cluster
    data stored in etcd, compromising the integrity and confidentiality of the entire Kubernetes environment.
  description: This rule checks that the `--etcd-cafile` flag is set on the Kubernetes API server to ensure TLS connections
    to etcd are verified with the specified Certificate Authority (CA) file. Proper configuration requires the etcd CA file
    to be present and correctly referenced, ensuring that all communication between the API server and etcd is encrypted and
    authenticated. This minimizes the risk of unauthorized data access and helps maintain compliance with security benchmarks
    by protecting data in transit.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-cluster-communication
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-certificates
  compliance:
  - cis_kubernetes_kubernetes_1.2.28_0049
- rule_id: k8s.apiserver.server.etcd_cafile_validity_check
  service: apiserver
  resource: server
  requirement: Etcd Cafile Validity Check
  scope: apiserver.server.etcd_cafile_validity_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Validity of Etcd CA File in Kube-Apiserver Configuration
  rationale: Invalid or improperly configured CA files could lead to unauthorized access and data breaches. By ensuring the
    validity of the Etcd CA file, you protect the integrity and confidentiality of the data stored in etcd, which is a critical
    component in Kubernetes infrastructure. Without proper CA file validation, attackers could execute man-in-the-middle attacks,
    intercepting or altering communication between the kube-apiserver and etcd.
  description: This check verifies that the kube-apiserver is configured with a valid CA file for etcd communication. A valid
    CA file ensures that the kube-apiserver only trusts secure and authorized etcd nodes, mitigating potential security threats
    like unauthorized access and data tampering. Proper configuration includes checking the file's presence, its correct path,
    and ensuring it has not expired. This practice aligns with the CIS Kubernetes Benchmark and is essential for maintaining
    secure communication channels within the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/
  compliance:
  - cis_kubernetes_kubernetes_1.1.31_0150
- rule_id: k8s.apiserver.server.etcd_tls_configured
  service: apiserver
  resource: server
  requirement: Etcd Tls Configured
  scope: apiserver.server.etcd_tls_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure etcd Communication Uses TLS Encryption
  rationale: Without TLS encryption between the Kubernetes API server and etcd, sensitive data transmitted, including secrets,
    could be intercepted by unauthorized parties. An attacker with network access could perform man-in-the-middle attacks,
    leading to data breaches or unauthorized modifications. TLS ensures that the data is encrypted in transit, thereby protecting
    against such attack vectors.
  description: This rule checks that the Kubernetes API server is configured to use TLS for communication with the etcd datastore.
    A secure configuration involves specifying TLS certificates and keys for both the API server and etcd in their respective
    configuration files. Proper TLS setup ensures that all data exchanges between these components are encrypted, maintaining
    the confidentiality and integrity of the data. A misconfiguration could lead to exposure of sensitive information and
    non-compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-components
  - https://kubernetes.io/docs/setup/best-practices/certificates/#etcd-peer-and-client-certificates
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#etcd-security
  compliance: []
- rule_id: k8s.apiserver.server.high_availability_enabled
  service: apiserver
  resource: server
  requirement: High Availability Enabled
  scope: apiserver.server.high_availability_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes API Server High Availability Configuration
  rationale: Without high availability, a single point of failure in the API server can disrupt the entire cluster's operations,
    potentially exposing it to denial-of-service attacks. High availability ensures redundancy and resilience, mitigating
    risks of service disruption and unauthorized access during server downtimes or attacks.
  description: This check ensures that the Kubernetes API server is configured in a high availability (HA) setup. A proper
    HA configuration typically involves running multiple instances of the API server in a load-balanced environment, preventing
    a single instance failure from affecting cluster operations. This setup enhances security by maintaining continuous availability
    and reducing the risk of downtime, which could otherwise lead to vulnerabilities being exploited during periods when the
    cluster is unresponsive.
  references:
  - https://kubernetes.io/docs/setup/best-practices/multiple-zones/
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
  - https://kubernetes.io/docs/tasks/administer-cluster/highly-available-control-plane/
  compliance: []
- rule_id: k8s.apiserver.server.https_only_configured
  service: apiserver
  resource: server
  requirement: Https Only Configured
  scope: apiserver.server.https_only_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce HTTPS for Kubernetes API Server Communication
  rationale: Misconfiguring the Kubernetes API Server to allow non-HTTPS traffic exposes sensitive data to potential interception
    and man-in-the-middle (MITM) attacks. Attackers could exploit this to gain unauthorized access to the cluster, extract
    sensitive configuration or operational data, and compromise the integrity of the Kubernetes environment.
  description: This control checks that the Kubernetes API Server is configured to accept only HTTPS connections. Proper configuration
    includes ensuring TLS certificates are correctly set up and used. By enforcing HTTPS, data in transit between the API
    Server and clients (such as kubectl, other cluster components, or external services) is encrypted, reducing the risk of
    eavesdropping and data tampering. A secure setup involves configuring the kube-apiserver with the '--tls-cert-file' and
    '--tls-private-key-file' flags to specify valid TLS certificates for secure communication.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance: []
- rule_id: k8s.apiserver.server.image_policy_webhook_enabled
  service: apiserver
  resource: server
  requirement: Image Policy Webhook Enabled
  scope: apiserver.server.image_policy_webhook_enabled
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enable and Configure ImagePolicyWebhook for Secure Image Admission
  rationale: Without the ImagePolicyWebhook enabled, the Kubernetes cluster is vulnerable to running unverified or potentially
    malicious container images. This can lead to security breaches such as running compromised images that might contain malware
    or backdoors, potentially jeopardizing the integrity and confidentiality of the cluster and its applications. Enabling
    this webhook ensures that only images that meet predefined security policies are deployed, mitigating risks associated
    with image provenance and integrity.
  description: This rule checks that the ImagePolicyWebhook is enabled on the Kubernetes API server. A correctly configured
    ImagePolicyWebhook acts as an admission controller that intercepts image deployment requests, verifying them against a
    policy before allowing them to run. This ensures that only images from trusted sources are deployed, and restricts unauthorized
    or potentially harmful images. A good configuration involves setting up the webhook to reference a policy that defines
    allowed image sources, image signatures, and other compliance requirements. This control is critical for maintaining a
    secure and compliant container environment.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook
  - https://kubernetes.io/docs/concepts/containers/images/#image-security
  - https://kubernetes.io/docs/tasks/administer-cluster/secure-cluster/
  compliance:
  - cis_kubernetes_kubernetes_5.5.1_0130
- rule_id: k8s.apiserver.server.insecure_port_disabled
  service: apiserver
  resource: server
  requirement: Insecure Port Disabled
  scope: apiserver.server.insecure_port_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: critical
  title: Disable Kube-apiserver Insecure Port
  rationale: Leaving the kube-apiserver's insecure port enabled poses a significant security risk as it bypasses authentication
    and encryption, allowing unauthorized access to the Kubernetes API. Attackers could exploit this to gain control over
    the entire cluster, leading to data breaches, service disruption, or unauthorized resource usage.
  description: This control checks that the kube-apiserver has its insecure port disabled by verifying that the `--insecure-port=0`
    flag is set. A correctly configured kube-apiserver uses secure communication protocols such as HTTPS to ensure all API
    requests are authenticated and encrypted, thus protecting sensitive data and cluster operations from unauthorized access
    and interception.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  compliance:
  - cis_kubernetes_kubernetes_1.2.16_0037
  - cis_kubernetes_kubernetes_3.1.5_0198
- rule_id: k8s.apiserver.server.kubelet_auth_certificates_configured
  service: apiserver
  resource: server
  requirement: Kubelet Auth Certificates Configured
  scope: apiserver.server.kubelet_auth_certificates_configured
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Kubelet Authentication with Certificates is Configured
  rationale: Without proper kubelet authentication, unauthorized users could gain access to kubelet APIs, potentially leading
    to unauthorized workload management and node resource exploitation. Attackers might exploit this to escalate privileges,
    deploy malicious containers, or extract sensitive data from pods.
  description: This control checks if the Kubernetes apiserver is configured to authenticate kubelets using client certificates.
    A valid configuration ensures that only authenticated kubelets can communicate with the apiserver, thereby reducing the
    risk of unauthorized access. Proper configuration involves setting up the apiserver with the '--kubelet-client-certificate'
    and '--kubelet-client-key' flags, which specify the client certificate and key to use when connecting to the kubelet.
    This setup helps ensure that only verified kubelets can execute operations, thus enhancing the overall security of the
    cluster.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-kubelet
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  compliance:
  - cis_kubernetes_kubernetes_1.1.22_0141
  - cis_kubernetes_kubernetes_1.1.23_0142
  - cis_kubernetes_kubernetes_1.2.5_0026
- rule_id: k8s.apiserver.server.kubelet_client_cert_key_verification
  service: apiserver
  resource: server
  requirement: Kubelet Client Cert Key Verification
  scope: apiserver.server.kubelet_client_cert_key_verification
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce Kubelet Client Certificate Key Verification on API Server
  rationale: Without proper verification of client certificates, malicious actors could impersonate kubelets, gaining unauthorized
    access to sensitive cluster operations. This could lead to data breaches, unauthorized workload management, and cluster
    disruption.
  description: This rule verifies that the Kubernetes API server is configured to enforce client certificate key verification
    for kubelet connections. It checks the presence and correct configuration of the '--kubelet-client-certificate' and '--kubelet-client-key'
    flags. Proper configuration ensures that only authenticated and authorized kubelets can communicate with the API server,
    mitigating spoofing attacks and ensuring encrypted communication.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-kubelet
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-certificates
  compliance:
  - cis_kubernetes_kubernetes_1.2.4_0025
- rule_id: k8s.apiserver.server.pod_anonymous_auth_disabled
  service: apiserver
  resource: server
  requirement: Pod Anonymous Auth Disabled
  scope: apiserver.server.pod_anonymous_auth_disabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: critical
  title: Disable Anonymous Authentication to API Server
  rationale: Anonymous authentication allows unauthenticated requests to the Kubernetes API server, which poses a significant
    security risk. Attackers can exploit this to gain unauthorized access, perform reconnaissance, or execute malicious actions
    without accountability. Disabling anonymous authentication mitigates this risk by ensuring that only authenticated and
    authorized users can interact with the cluster, thereby reducing the potential attack surface.
  description: This check ensures that anonymous authentication is disabled in the Kubernetes API server configuration. A
    secure setup demands the '--anonymous-auth=false' flag to be set in the API server startup parameters. This prevents any
    unauthenticated requests from being accepted by the API server, thereby enforcing strict access control protocols. Proper
    configuration ensures robust security by ensuring that all requests are traceable to an authenticated user or service
    account, which is crucial for auditing and compliance with security standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/#api-server-ports-and-ips
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/concepts/security/overview/#controlling-access-to-the-kube-api-server
  compliance:
  - cis_kubernetes_kubernetes_1.2.1_0022
- rule_id: k8s.apiserver.server.pod_authorization_mode_rbac_enabled
  service: apiserver
  resource: server
  requirement: Pod Authorization Mode Rbac Enabled
  scope: apiserver.server.pod_authorization_mode_rbac_enabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enable RBAC for Pod Authorization in Kubernetes API Server
  rationale: Without Role-Based Access Control (RBAC) enabled for pod authorization, there is a heightened risk of unauthorized
    users gaining access to Kubernetes resources. This can lead to privilege escalation attacks where attackers exploit elevated
    permissions to compromise the cluster. RBAC provides granular access control, ensuring only authorized users and services
    can access or modify resources.
  description: This rule ensures that the Kubernetes API server is configured with the RBAC authorization mode for pods. It
    checks the API server configuration to verify that the '--authorization-mode' flag includes 'RBAC'. Proper configuration
    of RBAC limits access to resources based on user roles and permissions, reducing the risk of unauthorized access and actions
    within the cluster. This helps in adhering to security best practices and compliance standards like the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - cis_kubernetes_kubernetes_1.2.8_0029
- rule_id: k8s.apiserver.server.rate_limiting_enabled
  service: apiserver
  resource: server
  requirement: Rate Limiting Enabled
  scope: apiserver.server.rate_limiting_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable and Configure Rate Limiting on Kubernetes API Server
  rationale: Without rate limiting, the Kubernetes API server is vulnerable to Denial of Service (DoS) attacks, where a malicious
    actor could overwhelm the server with requests, leading to degraded performance or a complete outage. Properly configured
    rate limiting helps mitigate the risk of resource exhaustion, ensuring availability and responsiveness of the API server
    under load, and helps prevent unauthorized access attempts by limiting the number of requests from potentially malicious
    sources.
  description: This control checks whether rate limiting is enabled and properly configured on the Kubernetes API server.
    A valid configuration includes setting request limits based on client IP or user, ensuring that excessive requests are
    throttled or rejected. This validation involves examining the API server's configuration flags such as '--max-requests-inflight'
    and '--max-mutating-requests-inflight'. Proper configuration reduces the risk of DoS attacks by controlling the load on
    the server, thus maintaining the availability and performance of the Kubernetes control plane.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#rate-limiting
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node/#apiserver
  compliance: []
- rule_id: k8s.apiserver.server.request_timeout_check
  service: apiserver
  resource: server
  requirement: Request Timeout Check
  scope: apiserver.server.request_timeout_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Configure API Server Request Timeout to Prevent Denial of Service
  rationale: Improperly configured request timeouts can lead to Denial of Service (DoS) attacks by allowing excessive allocation
    of resources to long-running requests. Attackers could exploit this by sending numerous requests that occupy server resources,
    potentially leading to exhaustion and service disruption.
  description: This rule checks if the Kubernetes API server has a request timeout configured that adheres to security best
    practices. A well-configured timeout ensures that requests are processed within a reasonable period, freeing resources
    and preventing potential DoS attacks. The Kubernetes API server should have the `--request-timeout` flag set to a value
    that balances allowing legitimate operations while mitigating resource exhaustion risks. An optimal configuration reduces
    the attack surface and aligns with compliance standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/setup/best-practices/node-conformance/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_1.2.22_0043
- rule_id: k8s.apiserver.server.secure_configuration_enforced
  service: apiserver
  resource: server
  requirement: Secure Configuration Enforced
  scope: apiserver.server.secure_configuration_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure API Server Secure Configuration Parameters
  rationale: If the API server is not properly secured, it can be vulnerable to unauthorized access and attacks such as privilege
    escalation, data breaches, and denial of service. Misconfigurations can lead to exposure of sensitive data and control
    over cluster operations, making it critical to ensure all configurations adhere to security best practices.
  description: This rule checks the Kubernetes API server's configuration to ensure that security best practices are enforced.
    It validates settings such as authentication mechanisms, access control policies, and network policies. A secure configuration
    should enforce TLS encryption, use RBAC for access control, and disable insecure ports. These measures protect the API
    server from unauthorized access and potential exploits, maintaining the integrity and availability of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-communications
  compliance: []
- rule_id: k8s.apiserver.server.secure_port_not_zero
  service: apiserver
  resource: server
  requirement: Secure Port Not Zero
  scope: apiserver.server.secure_port_not_zero
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Non-Zero Secure Port for API Server
  rationale: If the secure port for the Kubernetes API server is set to zero, it disables secure communication, increasing
    the risk of man-in-the-middle attacks and unauthorized access. This misconfiguration could allow attackers to intercept
    sensitive data or issue unauthorized commands, compromising the entire Kubernetes cluster.
  description: This rule checks that the secure port for the Kubernetes API server is configured to a non-zero value, ensuring
    that secure HTTPS communication is enabled. A correctly configured secure port enhances security by encrypting data in
    transit and authenticating clients, thereby minimizing the risk of data breaches and unauthorized access. The default
    secure port is 6443, which should be specified to comply with security best practices and industry benchmarks like the
    CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  compliance:
  - cis_kubernetes_kubernetes_3.1.6_0199
- rule_id: k8s.apiserver.server.service_account_key_file_check
  service: apiserver
  resource: server
  requirement: Service Account Key File Check
  scope: apiserver.server.service_account_key_file_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Service Account Key Rotation and Management
  rationale: Misconfigured service account keys can lead to unauthorized access to the Kubernetes API, allowing attackers
    to perform malicious actions such as data exfiltration or privilege escalation. Without proper key management and rotation,
    the risk of compromised credentials increases, making it easier for attackers to exploit the cluster.
  description: This control checks that the Kubernetes API server is configured with a valid and secure service account key
    file. It ensures that the key used for signing service account tokens is properly managed and rotated regularly. A well-configured
    key file minimizes the risk of token forgery and unauthorized API access, thus protecting the cluster from potential security
    breaches.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  compliance:
  - cis_kubernetes_kubernetes_1.2.23_0044
  - cis_kubernetes_kubernetes_1.2.24_0045
  - cis_kubernetes_kubernetes_3.1.16_0209
  - cis_kubernetes_kubernetes_3.1.17_0210
- rule_id: k8s.apiserver.server.tls_cert_file_check
  service: apiserver
  resource: server
  requirement: Tls Cert File Check
  scope: apiserver.server.tls_cert_file_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure API Server TLS Certificate is Properly Configured
  rationale: If the TLS certificate for the Kubernetes API server is misconfigured or absent, sensitive communication can
    be exposed to man-in-the-middle (MITM) attacks. This could allow attackers to intercept, modify, or spoof API requests,
    leading to potential data breaches and unauthorized access to cluster resources.
  description: This control checks that the Kubernetes API server is configured with a valid TLS certificate. Proper TLS configuration
    ensures that all communication to the API server is encrypted, protecting the integrity and confidentiality of data in
    transit. A good configuration includes using certificates signed by a trusted Certificate Authority (CA) with appropriate
    expiration dates and ensuring that the API server's `--tls-cert-file` and `--tls-private-key-file` flags are correctly
    set. This mitigates risks associated with eavesdropping and tampering by ensuring encrypted communication channels.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-the-kubernetes-api-server
  - https://kubernetes.io/docs/concepts/cluster-administration/certificates/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  compliance:
  - cis_kubernetes_kubernetes_1.2.26_0047
- rule_id: k8s.apiserver.server.tls_certificates_configured
  service: apiserver
  resource: server
  requirement: Tls Certificates Configured
  scope: apiserver.server.tls_certificates_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure TLS Certificates Are Properly Configured for Kube-apiserver
  rationale: Misconfigured TLS certificates on the kube-apiserver could allow attackers to intercept, modify, or eavesdrop
    on sensitive data in transit, potentially leading to data breaches or man-in-the-middle attacks. This can compromise the
    confidentiality, integrity, and authenticity of communication between clients and the Kubernetes API server.
  description: This control checks that the kube-apiserver is configured with valid and correctly implemented TLS certificates.
    Proper configuration includes ensuring that the certificates are signed by a trusted certificate authority, have not expired,
    and match the server's hostname. This helps to encrypt data in transit, protecting against unauthorized access and eavesdropping,
    thereby ensuring compliance with security best practices and standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - cis_kubernetes_kubernetes_1.1.29_0148
  - nist_800_53_rev5_multi_cloud_CA-9-b_0350
- rule_id: k8s.apiserver.server.tls_cipher_suites_strong_check
  service: apiserver
  resource: server
  requirement: Tls Cipher Suites Strong Check
  scope: apiserver.server.tls_cipher_suites_strong_check
  domain: network_security
  subcategory: network_configuration
  severity: high
  title: Enforce Strong TLS Cipher Suites for API Server
  rationale: Using weak or deprecated TLS cipher suites in the API server can expose Kubernetes clusters to man-in-the-middle
    (MITM) attacks, eavesdropping, and other cryptographic vulnerabilities. Strong cipher suites ensure data integrity, confidentiality,
    and protection against unauthorized access.
  description: This control verifies that the Kubernetes API server is configured to use only strong TLS cipher suites, which
    are resistant to known cryptographic attacks. A correctly configured TLS setup includes cipher suites that support forward
    secrecy and modern encryption standards, reducing the risk of data breaches and ensuring regulatory compliance with security
    frameworks such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-strong-tls-ciphers
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  compliance:
  - cis_kubernetes_kubernetes_1.2.31_0135
  - cis_kubernetes_kubernetes_1.2.34_0214
- rule_id: k8s.apiserver.server.token_auth_file_not_set
  service: apiserver
  resource: server
  requirement: Token Auth File Not Set
  scope: apiserver.server.token_auth_file_not_set
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Token-Based Authentication for API Server
  rationale: Token-based authentication in Kubernetes can expose the API server to unauthorized access if tokens are leaked
    or improperly managed. Such vulnerabilities might allow attackers to gain access to sensitive cluster resources and escalate
    privileges. Disabling token-based authentication reduces the attack surface by relying on more secure methods such as
    client certificates or OAuth2.
  description: This rule checks if the Kubernetes API server is configured without a static token authentication file by ensuring
    the '--token-auth-file' flag is not set. Properly disabling token-based authentication enhances security by preventing
    the use of static tokens, which are difficult to rotate and manage securely. Instead, it encourages the use of dynamic
    and more secure authentication methods, thereby reducing the risk of unauthorized access and ensuring compliance with
    best security practices.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - cis_kubernetes_kubernetes_1.2.2_0023
  - cis_kubernetes_kubernetes_3.1.15_0208
- rule_id: k8s.apiserver.server.verification_configured
  service: apiserver
  resource: server
  requirement: Verification Configured
  scope: apiserver.server.verification_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable and Configure API Server Request Verification
  rationale: Misconfigured or disabled request verification in the Kubernetes API server can expose the cluster to unauthorized
    access and man-in-the-middle attacks. Without proper verification, an attacker can intercept or forge API requests, leading
    to potential data breaches or unauthorized operations within the cluster.
  description: This rule checks that the Kubernetes API server has request verification mechanisms properly configured, including
    TLS certificates and client authentication. A secure configuration ensures that the API server only accepts requests from
    authenticated and authorized users, which mitigates risks such as unauthorized access and impersonation attacks. Proper
    verification includes setting up valid server certificates and enabling mutual TLS (mTLS) to authenticate clients.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance: []
- rule_id: k8s.apiserver.service.account_lookup_enabled
  service: apiserver
  resource: service
  requirement: Account Lookup Enabled
  scope: apiserver.service.account_lookup_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Service Account Lookup is Enabled in API Server
  rationale: If service account lookup is disabled, the API server might fail to authenticate service accounts properly, allowing
    unauthorized access to resources. This misconfiguration can be exploited through privilege escalation attacks or unauthorized
    resource access, compromising the entire cluster's security.
  description: This rule checks whether the Kubernetes API server is configured with the '--service-account-lookup' flag set
    to 'true'. A properly configured API server ensures that service account tokens are validated against the configured authentication
    method. This reduces the risk of unauthorized access by ensuring that service accounts are legitimate and have appropriate
    permissions, thus enhancing the overall security of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.apiserver.ssh.access_disabled
  service: apiserver
  resource: ssh
  requirement: Access Disabled
  scope: apiserver.ssh.access_disabled
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Disable SSH Access to Kubernetes API Server
  rationale: Allowing SSH access to the Kubernetes API server poses significant security risks, including unauthorized access
    and potential control over the cluster. Attackers could exploit SSH access to execute arbitrary commands, exfiltrate sensitive
    data, and disrupt cluster operations. Disabling SSH access mitigates these risks by enforcing strict access control and
    reducing the attack surface.
  description: This control checks that SSH access to the Kubernetes API server is disabled. Proper configuration involves
    ensuring that no SSH keys or SSH services are running on the API server. SSH access should be replaced with secure, audited
    API calls and role-based access control (RBAC) to manage and monitor interactions with the API server. This enhances security
    by preventing unauthorized command execution and data manipulation, maintaining compliance with security best practices
    and benchmarks such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - rbi_bank_multi_cloud_9.1_0026
- rule_id: k8s.apiserver.tls.client_certificates_enabled
  service: apiserver
  resource: tls
  requirement: Client Certificates Enabled
  scope: apiserver.tls.client_certificates_enabled
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enable Client Certificate Authentication for API Server
  rationale: Without client certificate authentication, the Kubernetes API server is vulnerable to unauthorized access and
    man-in-the-middle attacks. This misconfiguration can allow attackers to impersonate legitimate users or services, potentially
    leading to data breaches and unauthorized actions within the cluster.
  description: This rule checks that the Kubernetes API server is configured to require client certificates for authentication.
    A valid configuration includes enabling the `--client-ca-file` flag with a path to a valid CA certificate file. This setup
    ensures that only clients with valid certificates issued by the CA can interact with the API server, thus preventing unauthorized
    access and enhancing the security of API communications.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-communication
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance: []
- rule_id: k8s.apiserver.tls.enabled
  service: apiserver
  resource: tls
  requirement: Enabled
  scope: apiserver.tls.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure TLS is Enabled and Properly Configured for API Server
  rationale: Without TLS enabled, the API server's communication could be intercepted or tampered with by unauthorized entities,
    leading to data breaches or malicious activities. Improper TLS configuration can leave the Kubernetes control plane vulnerable
    to man-in-the-middle attacks, where adversaries could gain access to sensitive administrative operations.
  description: This control checks whether the Kubernetes API server is configured to use TLS for encrypting data in transit.
    It ensures that a valid TLS certificate is in place and that the API server is not vulnerable to known TLS-based attacks.
    Proper TLS configuration helps maintain confidentiality and integrity of the data exchanged between clients and the server,
    mitigating the risk of eavesdropping and unauthorized access. A secure setup includes using strong cipher suites and ensuring
    the certificate is issued by a trusted CA.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-cluster-communications
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-9_0040
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-13_0136
  - fedramp_moderate_multi_cloud_CA-9_0099
  - fedramp_moderate_multi_cloud_SC-13_0348
  - iso27001_2022_multi_cloud_A.10.1_0001
  - nist_800_171_r2_multi_cloud_3_11_3_3.11.3_Remediate_vulnerabilities_in_accord_0002
  - nist_800_171_r2_multi_cloud_3_13_15_3.13.15_Protect_the_authenticity_of_commu_0006
  - nist_800_171_r2_multi_cloud_3_1_1_3.1.1_Limit_system_access_to_authorized_use_0020
  - nist_800_171_r2_multi_cloud_3_1_13_3.1.13_Employ_cryptographic_mechanisms_to_0022
  - nist_800_53_rev5_multi_cloud_MA-4-c_0684
  - nist_800_53_rev5_multi_cloud_PM-17-b_0876
  - nist_800_53_rev5_multi_cloud_SC-13-a_1248
  - pci_dss_v4_multi_cloud_4.1.2_0059
  - pci_dss_v4_multi_cloud_9.4.1.2_0132
  - rbi_bank_multi_cloud_12.1_0005
  - rbi_nbfc_multi_cloud_2.11_0015
- rule_id: k8s.audit.change.management_logging_enabled
  service: audit
  resource: change
  requirement: Management Logging Enabled
  scope: audit.change.management_logging_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable and Configure Kubernetes Audit Logs for Management Changes
  rationale: Without proper audit logging, unauthorized or accidental changes to Kubernetes configurations may go undetected,
    increasing the risk of configuration drift, privilege escalation, and data breaches. Attackers can exploit this lack of
    oversight to introduce malicious changes or maintain persistence within the cluster without being noticed.
  description: This control checks that Kubernetes audit logging is enabled and specifically configured to capture management-level
    changes, such as those to API server configurations, RBAC policies, and network policies. A properly configured audit
    log will include who made the change, what resources were affected, and the time of the change. This information is crucial
    for forensic investigations and incident response. Ensuring audit logs are enabled helps maintain accountability and traceability,
    thereby reducing the risk of unauthorized access and ensuring compliance with security standards like PCI DSS and SOC
    2.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - pci_dss_v4_multi_cloud_2.2.1_0020
  - soc2_multi_cloud_cc_8_1_0020
- rule_id: k8s.audit.configuration.change_detection_enabled
  service: audit
  resource: configuration
  requirement: Change Detection Enabled
  scope: audit.configuration.change_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Change Detection in Kubernetes Audit Logs
  rationale: Without change detection in audit logs, unauthorized or malicious modifications to the cluster configuration
    may go unnoticed, leading to potential security breaches. Attackers could exploit this by altering configurations to escalate
    privileges, exfiltrate data, or disrupt services. Change detection is crucial for timely identification and response to
    suspicious activities.
  description: This rule checks if change detection is enabled in the Kubernetes audit logging configuration. A good configuration
    involves setting 'auditPolicy' to capture and record changes to critical resources and configurations. This helps in tracking
    modifications, thereby reducing the risk of unauthorized changes and ensuring accountability. Properly configured audit
    logs are essential for forensic analysis, compliance with security standards, and maintaining a secure environment.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  compliance:
  - soc2_multi_cloud_cc_7_1_0015
- rule_id: k8s.audit.configuration.change_logged
  service: audit
  resource: configuration
  requirement: Change Logged
  scope: audit.configuration.change_logged
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Ensure Audit Logs Capture Configuration Changes
  rationale: Failure to log configuration changes can allow unauthorized modifications to go undetected, potentially leading
    to security breaches. Attackers may exploit this by altering configurations to weaken security postures, establish persistence,
    or disrupt services without leaving a trace. Timely detection of such changes is crucial to prevent or mitigate attacks
    like privilege escalation and data exfiltration.
  description: This rule checks that the Kubernetes audit logs are configured to capture all changes to system configurations.
    A properly configured audit log should track who performed the change, what was changed, and when. Such logging is critical
    for forensic analysis, accountability, and compliance with regulatory standards. Ensuring that configuration changes are
    logged helps in detecting and responding to unauthorized access attempts and maintaining a robust security framework.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/administer-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-3_0043
  - nist_800_171_r2_multi_cloud_3_4_1_3.4.1_Establish_and_maintain_baseline_confi_0037
  - nist_800_53_rev5_multi_cloud_CM-6-d_0400
- rule_id: k8s.audit.incident.detection_enabled
  service: audit
  resource: incident
  requirement: Detection Enabled
  scope: audit.incident.detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable and Configure Kubernetes Audit Logging for Incident Detection
  rationale: Without proper audit logging, it is difficult to detect and respond to security incidents within a Kubernetes
    cluster. Misconfigured or disabled audit logs can allow unauthorized activities to go unnoticed, leading to potential
    data breaches or system compromises. Attackers often exploit the lack of visibility to perform lateral movements or privilege
    escalations undetected.
  description: This rule checks that Kubernetes audit logging is enabled and properly configured to capture sufficient details
    about API activities. A good configuration includes setting the audit policy to capture relevant events, such as authentication
    failures, role changes, and resource access attempts. This helps security teams detect suspicious activities and respond
    to potential threats promptly, ensuring compliance with industry standards like CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - fedramp_moderate_multi_cloud_IR-4_0205
  - fedramp_moderate_multi_cloud_IR-4_1_0206
  - fedramp_moderate_multi_cloud_IR-4_2_0207
  - fedramp_moderate_multi_cloud_IR-4_4_0208
  - fedramp_moderate_multi_cloud_IR-4_6_0209
  - fedramp_moderate_multi_cloud_IR-4_11_0210
  - nist_800_53_rev5_multi_cloud_IR-4-a_0625
- rule_id: k8s.audit.incident.notification_configured
  service: audit
  resource: incident
  requirement: Notification Configured
  scope: audit.incident.notification_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Configure Notifications for Audit Incidents
  rationale: Without properly configured notifications for audit incidents, security teams may not be promptly alerted to
    potential security breaches or misconfigurations. This delay in response could allow attackers to exploit system vulnerabilities,
    leading to data breaches, unauthorized access, and compliance violations.
  description: This control checks that Kubernetes audit logs are configured to send notifications for specific incidents,
    such as unauthorized access attempts or policy violations. It ensures that alerts are sent to appropriate monitoring and
    response systems. A good configuration includes specifying the events that trigger notifications and the channels through
    which alerts are sent (e.g., email, Slack, SIEM systems). Properly configured notifications enable timely incident response,
    reducing the window of opportunity for attackers and aiding in compliance with security standards.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-backends
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy
  compliance:
  - rbi_nbfc_multi_cloud_3.3_0019
  - rbi_nbfc_multi_cloud_7.1_0033
- rule_id: k8s.audit.incident.response_admission_controller_enabled
  service: audit
  resource: incident
  requirement: Response Admission Controller Enabled
  scope: audit.incident.response_admission_controller_enabled
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enable and Configure Dynamic Admission Control
  rationale: Without proper admission control, Kubernetes clusters are vulnerable to unauthorized resource configurations,
    leading to potential security breaches. Attackers could exploit misconfigurations to escalate privileges, deploy malicious
    workloads, or exfiltrate sensitive data. Admission controllers intercept API requests before they are persisted, providing
    a critical security checkpoint.
  description: This rule checks if the Kubernetes cluster has dynamic admission controllers enabled and properly configured.
    It ensures that admission webhooks are set up to enforce security policies at runtime, thereby preventing policy violations
    and unauthorized access. A well-configured admission controller can validate and mutate API requests according to security
    policies, reducing the attack surface and ensuring compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - rbi_nbfc_multi_cloud_3.3_0019
  - rbi_nbfc_multi_cloud_7.1_0033
- rule_id: k8s.audit.incident.response_plan_configured
  service: audit
  resource: incident
  requirement: Response Plan Configured
  scope: audit.incident.response_plan_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Incident Response Plan is Configured for Kubernetes Audit Logs
  rationale: Without a properly configured incident response plan for Kubernetes audit logs, organizations might fail to detect
    or respond to security incidents promptly, leading to prolonged exposure to threats and potential data breaches. Misconfiguration
    may allow attackers to exploit unknown vulnerabilities or unauthorized access attempts without quick mitigation.
  description: This control checks that a defined incident response plan is in place for handling Kubernetes audit logs. It
    ensures that there are procedures for identifying, analyzing, and responding to security incidents identified in audit
    logs. A well-configured plan includes automated alerts, incident categorization, and escalation procedures, which help
    in minimizing the impact of security events and ensuring compliance with industry standards.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IR-4_0072
  - fedramp_moderate_multi_cloud_IR-4_0205
  - fedramp_moderate_multi_cloud_IR-4_11_0210
  - fedramp_moderate_multi_cloud_IR-4_1_0206
  - fedramp_moderate_multi_cloud_IR-4_2_0207
  - fedramp_moderate_multi_cloud_IR-4_4_0208
  - fedramp_moderate_multi_cloud_IR-4_6_0209
  - nist_800_53_rev5_multi_cloud_IR-4-a_0625
  - rbi_nbfc_multi_cloud_3.3_0019
  - rbi_nbfc_multi_cloud_7.1_0033
- rule_id: k8s.audit.log.access_control_enforced
  service: audit
  resource: log
  requirement: Access Control Enforced
  scope: audit.log.access_control_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce RBAC on Kubernetes Audit Logs
  rationale: Without enforcing RBAC on Kubernetes audit logs, unauthorized users could gain access to sensitive log data,
    which may include information about cluster activities, attempted security breaches, and configuration details. This exposure
    can lead to insider threats, data exfiltration, and compliance violations. Attackers could use this information to strategize
    further attacks or evade detection.
  description: This rule checks that Role-Based Access Control (RBAC) is properly configured to restrict access to Kubernetes
    audit logs. A secure configuration ensures that only authorized personnel can access and manage audit logs, which include
    sensitive operational data. Proper access control helps prevent unauthorized access, maintains the integrity of audit
    data, and supports compliance with security best practices and regulatory requirements. A good configuration involves
    defining roles and permissions that limit log access to only those with a legitimate need.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#security-audit
  compliance:
  - fedramp_moderate_multi_cloud_AU-9_0075
  - fedramp_moderate_multi_cloud_AU-9_2_0076
  - fedramp_moderate_multi_cloud_AU-9_3_0077
  - fedramp_moderate_multi_cloud_AU-9_4_0078
- rule_id: k8s.audit.log.access_review
  service: audit
  resource: log
  requirement: Access Review
  scope: audit.log.access_review
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Ensure Audit Logs Include Access Review Events for Compliance
  rationale: Misconfigured audit logs can lead to undetected unauthorized access and privilege escalation, allowing attackers
    to exploit vulnerabilities without detection. Ensuring access review events are logged helps track and audit access requests
    and modifications, supporting the identification of potential security incidents and ensuring accountability.
  description: This rule checks that Kubernetes audit logs are configured to capture access review events, which include any
    actions taken by users or service accounts that request access to resources. A properly configured audit log should include
    records of who accessed what resources and when. This configuration helps in identifying anomalies, supports forensic
    investigations, and ensures adherence to compliance frameworks like PCI-DSS. A good configuration involves setting an
    appropriate audit policy that captures 'create', 'update', 'delete', and 'get' operations on sensitive resources.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - pci_dss_v4_multi_cloud_8.2.7_0105
- rule_id: k8s.audit.log.alerts_configured
  service: audit
  resource: log
  requirement: Alerts Configured
  scope: audit.log.alerts_configured
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Configure Alerts for Kubernetes Audit Logs
  rationale: Without proper alerting on audit logs, malicious activities such as unauthorized access attempts or privilege
    escalations can go unnoticed, increasing the risk of data breaches or system compromise. Attackers may exploit this by
    performing lateral movement within the cluster or exfiltrating sensitive data without triggering any immediate response.
  description: This rule checks that alerts are configured on Kubernetes audit logs to ensure real-time monitoring and response
    to suspicious activities. A well-configured alerting system should notify the relevant security teams of any anomalous
    behavior, such as failed login attempts, unauthorized resource access, or changes to critical configurations. This aids
    in proactive threat detection and incident response, aligning with best practices and compliance with frameworks like
    the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/security/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-6_0028
- rule_id: k8s.audit.log.analysis_configured
  service: audit
  resource: log
  requirement: Analysis Configured
  scope: audit.log.analysis_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes Audit Logs Are Analyzed for Security Threats
  rationale: Without proper analysis of Kubernetes audit logs, potential security breaches may go undetected. Attack vectors
    such as unauthorized access attempts, privilege escalation, or configuration changes could occur unnoticed, leading to
    severe security incidents. Log analysis can help identify anomalies and patterns indicative of malicious activities, thus
    enabling timely responses and mitigation strategies.
  description: This rule checks that Kubernetes clusters have audit log analysis configured to continuously monitor and scrutinize
    events. A properly configured log analysis system should parse and evaluate log data to uncover suspicious activities.
    Good configuration involves setting up log collectors and analysis tools that can alert on predefined security events,
    ensuring that logs are not only collected but also actively analyzed. This helps in early detection of threats and compliance
    with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-6_0028
- rule_id: k8s.audit.log.analysis_mechanism
  service: audit
  resource: log
  requirement: Analysis Mechanism
  scope: audit.log.analysis_mechanism
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Implement and Configure Audit Log Analysis Mechanisms
  rationale: Without a properly configured audit log analysis mechanism, Kubernetes clusters are vulnerable to undetected
    malicious activities. Attackers could exploit this to perform unauthorized actions without leaving traces, leading to
    potential data breaches and compliance violations.
  description: This control checks that an analysis mechanism is configured for Kubernetes audit logs, ensuring that logs
    are monitored and reviewed for suspicious activities. A good configuration includes automated log analysis tools that
    detect anomalies, alert administrators, and retain logs for an appropriate duration. This helps in early detection of
    unauthorized access, supports forensic investigations, and ensures compliance with security standards.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-logs
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - fedramp_moderate_multi_cloud_AU-6_0065
  - fedramp_moderate_multi_cloud_AU-6_1_0066
  - fedramp_moderate_multi_cloud_AU-6_3_0067
  - fedramp_moderate_multi_cloud_AU-6_4_0068
  - fedramp_moderate_multi_cloud_AU-6_5_0069
  - fedramp_moderate_multi_cloud_AU-6_6_0070
  - fedramp_moderate_multi_cloud_AU-6_7_0071
- rule_id: k8s.audit.log.anomaly_detection_configured
  service: audit
  resource: log
  requirement: Anomaly Detection Configured
  scope: audit.log.anomaly_detection_configured
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Anomaly Detection in Kubernetes Audit Logs
  rationale: Without anomaly detection in audit logs, deviations from normal operations may go unnoticed, allowing potential
    security breaches such as unauthorized access or privilege escalation to occur undetected. Anomaly detection helps identify
    suspicious activities by analyzing patterns in the logs that could indicate a compromised system.
  description: This control checks that anomaly detection is configured within the Kubernetes audit logging framework. Proper
    configuration involves setting up tools and processes to monitor logs for irregularities, such as unusually high access
    rates or attempts to access restricted resources. This helps in early detection of security incidents, enabling timely
    responses to mitigate potential threats and ensuring compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-backends
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-4-d_1394
- rule_id: k8s.audit.log.authentication_failures
  service: audit
  resource: log
  requirement: Authentication Failures
  scope: audit.log.authentication_failures
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Monitor and Alert on Authentication Failures in Kubernetes Audit Logs
  rationale: Failure to properly monitor authentication failures can result in undetected unauthorized access attempts. Such
    attempts can be indicative of an ongoing attack or misconfiguration that could lead to a breach of sensitive data or disruption
    of services. Attackers often exploit weak authentication mechanisms to gain access, and being alerted to repeated failed
    attempts is critical to identifying and mitigating these threats promptly.
  description: This control ensures that Kubernetes audit logs are configured to capture and alert on authentication failures.
    Proper configuration includes setting up audit policies that log failed authentication attempts and integrating these
    logs with a monitoring and alerting system. This setup helps security teams quickly identify and respond to unauthorized
    access attempts, reducing the risk of a successful breach. A well-configured audit log should include details like the
    source IP, user identity, and timestamp of the failed attempts, providing context for further investigation.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - hipaa_multi_cloud_164_308_a_5_ii_c_0013
- rule_id: k8s.audit.log.backup_configured
  service: audit
  resource: log
  requirement: Backup Configured
  scope: audit.log.backup_configured
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: medium
  title: Ensure Backup of Kubernetes Audit Logs
  rationale: Without properly configured backups of Kubernetes audit logs, critical security events may be lost, hindering
    incident response and forensic investigations. This can lead to undetected breaches, compliance violations, and a failure
    to understand the scope of security incidents. Attackers could exploit this by tampering with logs to cover their tracks,
    thus undermining the integrity of the system.
  description: This control verifies that Kubernetes audit logs are backed up in accordance with defined security best practices.
    A good configuration involves automated, regular backups to a secure, immutable storage location, ensuring logs are available
    for at least the duration specified by compliance mandates. This setup enhances security by ensuring log integrity and
    availability, supporting audit trails, and facilitating compliance with standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/extend-kubernetes/audit/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-9_0059
- rule_id: k8s.audit.log.creation_deletion_events_enabled
  service: audit
  resource: log
  requirement: Creation Deletion Events Enabled
  scope: audit.log.creation_deletion_events_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Audit Logging for Resource Creation and Deletion
  rationale: Without logging creation and deletion events, it becomes challenging to track unauthorized changes or resource
    manipulations that could lead to security breaches. Attackers could create or delete resources to disrupt services or
    cover their tracks, and the absence of logs would hinder forensic investigations and breach detection.
  description: This rule checks if the Kubernetes audit logging configuration captures events related to resource creation
    and deletion. Proper audit logging ensures that all actions affecting the state of the cluster are recorded, which is
    critical for detecting unauthorized activities and maintaining a secure environment. A good configuration involves setting
    up audit policies to log 'create', 'delete', and 'deletecollection' verbs for important resources. This helps in monitoring
    unauthorized access, ensuring compliance with security standards, and facilitating incident response.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-backends
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/#audit-logs
  compliance:
  - pci_dss_v4_multi_cloud_10.2.1.7_0150
- rule_id: k8s.audit.log.enabled
  service: audit
  resource: log
  requirement: Enabled
  scope: audit.log.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes Audit Logging is Enabled and Configured
  rationale: Without properly enabled and configured audit logging, Kubernetes environments are vulnerable to undetected unauthorized
    activities, making it difficult to identify and respond to security incidents. Lack of audit logs can hinder forensic
    investigations and compliance with regulatory requirements, potentially leading to undetected breaches and non-compliance
    penalties.
  description: This control checks if Kubernetes audit logging is enabled and properly configured. A well-configured audit
    log captures all relevant events, such as access attempts and changes to cluster configurations, which are crucial for
    detecting suspicious activities and unauthorized access. Proper configuration should include setting the audit log level
    to capture the necessary events, specifying the output format, and securing log storage. Implementing this control helps
    in maintaining an actionable audit trail, enhances threat detection capabilities, and supports compliance with security
    standards.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-2_0024
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-3_0025
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-6_0028
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-8_0030
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-9_0031
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-12_0033
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-2_0035
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-7_0039
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-6_0046
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-9_0049
  - canada_pbmm_moderate_multi_cloud_CCCS_SI-4_0149
  - fedramp_moderate_multi_cloud_AU-2_0058
  - fedramp_moderate_multi_cloud_AU-3_0059
  - fedramp_moderate_multi_cloud_AU-3_1_0060
  - fedramp_moderate_multi_cloud_AU-6_0065
  - fedramp_moderate_multi_cloud_AU-6_1_0066
  - fedramp_moderate_multi_cloud_AU-6_3_0067
  - fedramp_moderate_multi_cloud_AU-6_4_0068
  - fedramp_moderate_multi_cloud_AU-6_5_0069
  - fedramp_moderate_multi_cloud_AU-6_6_0070
  - fedramp_moderate_multi_cloud_AU-6_7_0071
  - fedramp_moderate_multi_cloud_AU-8_0074
  - fedramp_moderate_multi_cloud_AU-12_0081
  - fedramp_moderate_multi_cloud_AU-12_1_0082
  - fedramp_moderate_multi_cloud_AU-12_3_0083
  - fedramp_moderate_multi_cloud_CA-2_0085
  - fedramp_moderate_multi_cloud_CA-2_1_0086
  - fedramp_moderate_multi_cloud_CA-2_2_0087
  - fedramp_moderate_multi_cloud_CA-2_3_0088
  - fedramp_moderate_multi_cloud_CA-7_0093
  - fedramp_moderate_multi_cloud_CA-7_1_0094
  - fedramp_moderate_multi_cloud_CA-7_4_0095
  - fedramp_moderate_multi_cloud_CM-6_0116
  - fedramp_moderate_multi_cloud_CM-6_1_0117
  - fedramp_moderate_multi_cloud_CM-6_2_0118
  - fedramp_moderate_multi_cloud_CM-9_0128
  - fedramp_moderate_multi_cloud_SI-2_0363
  - fedramp_moderate_multi_cloud_SI-2_2_0364
  - fedramp_moderate_multi_cloud_SI-2_3_0365
  - fedramp_moderate_multi_cloud_SI-4_0367
  - fedramp_moderate_multi_cloud_SI-4_1_0368
  - fedramp_moderate_multi_cloud_SI-4_2_0369
  - fedramp_moderate_multi_cloud_SI-4_4_0370
  - fedramp_moderate_multi_cloud_SI-4_5_0371
  - fedramp_moderate_multi_cloud_SI-4_10_0372
  - fedramp_moderate_multi_cloud_SI-4_11_0373
  - fedramp_moderate_multi_cloud_SI-4_12_0374
  - fedramp_moderate_multi_cloud_SI-4_14_0375
  - fedramp_moderate_multi_cloud_SI-4_16_0376
  - fedramp_moderate_multi_cloud_SI-4_18_0377
  - fedramp_moderate_multi_cloud_SI-4_19_0378
  - fedramp_moderate_multi_cloud_SI-4_20_0379
  - fedramp_moderate_multi_cloud_SI-4_22_0380
  - fedramp_moderate_multi_cloud_SI-4_23_0381
  - gdpr_multi_cloud_Article_30_Records_of_processing_activities_0002
  - hipaa_multi_cloud_164_308_a_1_ii_a_0001
  - hipaa_multi_cloud_164_308_a_1_ii_d_0003
  - hipaa_multi_cloud_164_308_a_6_i_0015
  - hipaa_multi_cloud_164_308_a_6_ii_0016
  - hipaa_multi_cloud_164_308_a_8_0021
  - hipaa_multi_cloud_164_312_b_0026
  - hipaa_multi_cloud_164_312_e_2_i_0031
  - iso27001_2022_multi_cloud_A.12.4_0002
  - iso27001_2022_multi_cloud_A.8.16_0071
  - nist_800_171_r2_multi_cloud_3_11_2_3.11.2_Scan_for_vulnerabilities_in_organiz_0001
  - nist_800_171_r2_multi_cloud_3_14_3_3.14.3_Monitor_system_security_alerts_and_0016
  - nist_800_171_r2_multi_cloud_3_1_12_3.1.12_Monitor_and_control_remote_access_s_0021
  - nist_800_171_r2_multi_cloud_3_1_4_3.1.4_Separate_the_duties_of_individuals_to_0027
  - nist_800_171_r2_multi_cloud_3_3_2_3.3.2_Ensure_that_the_actions_of_individual_0032
  - nist_800_171_r2_multi_cloud_3_3_3_3.3.3_Review_and_update_logged_events_0033
  - nist_800_171_r2_multi_cloud_3_3_4_3.3.4_Alert_in_the_event_of_an_audit_loggin_0034
  - nist_800_171_r2_multi_cloud_3_3_8_3.3.8_Protect_audit_information_and_audit_l_0036
  - nist_800_171_r2_multi_cloud_3_6_1_3.6.1_Establish_an_operational_incident-han_0049
  - nist_800_171_r2_multi_cloud_3_6_2_3.6.2_Track_document_and_report_incidents_0050
  - nist_800_53_rev5_multi_cloud_AC-2-g_0023
  - nist_800_53_rev5_multi_cloud_AC-16-b_0135
  - nist_800_53_rev5_multi_cloud_AU-2-b_0224
  - nist_800_53_rev5_multi_cloud_AU-3-a_0231
  - nist_800_53_rev5_multi_cloud_AU-3-b_0232
  - nist_800_53_rev5_multi_cloud_AU-3-c_0233
  - nist_800_53_rev5_multi_cloud_AU-3-d_0234
  - nist_800_53_rev5_multi_cloud_AU-3-e_0235
  - nist_800_53_rev5_multi_cloud_AU-3-f_0236
  - nist_800_53_rev5_multi_cloud_AU-8-b_0258
  - nist_800_53_rev5_multi_cloud_AU-9-a_0266
  - nist_800_53_rev5_multi_cloud_AU-12-a_0278
  - nist_800_53_rev5_multi_cloud_AU-12-c_0280
  - nist_800_53_rev5_multi_cloud_AU-14-a_0289
  - nist_800_53_rev5_multi_cloud_AU-14-b_0290
  - nist_800_53_rev5_multi_cloud_CA-2-d_0303
  - nist_800_53_rev5_multi_cloud_CA-7-a_0337
  - nist_800_53_rev5_multi_cloud_CA-7-b_0338
  - nist_800_53_rev5_multi_cloud_CA-7-c_0339
  - nist_800_53_rev5_multi_cloud_CA-7-d_0340
  - nist_800_53_rev5_multi_cloud_CA-7-e_0341
  - nist_800_53_rev5_multi_cloud_CA-7-f_0342
  - nist_800_53_rev5_multi_cloud_CA-7-g_0343
  - nist_800_53_rev5_multi_cloud_CM-6-a_0397
  - nist_800_53_rev5_multi_cloud_CM-9-b_0428
  - nist_800_53_rev5_multi_cloud_PM-11-b_0867
  - nist_800_53_rev5_multi_cloud_PM-31-a_0911
  - nist_800_53_rev5_multi_cloud_PM-31-b_0912
  - nist_800_53_rev5_multi_cloud_PM-31-c_0913
  - nist_800_53_rev5_multi_cloud_PM-31-d_0914
  - nist_800_53_rev5_multi_cloud_PM-31-e_0915
  - nist_800_53_rev5_multi_cloud_PM-31-f_0916
  - nist_800_53_rev5_multi_cloud_SC-43-b_1322
  - nist_800_53_rev5_multi_cloud_SI-2-a_1342
  - nist_800_53_rev5_multi_cloud_SI-4-a_1391
  - nist_800_53_rev5_multi_cloud_SI-4-b_1392
  - nist_800_53_rev5_multi_cloud_SI-4-c_1393
  - nist_800_53_rev5_multi_cloud_SI-4-d_1394
  - pci_dss_v4_multi_cloud_3.4.2_0040
  - pci_dss_v4_multi_cloud_8.1.1_0099
  - pci_dss_v4_multi_cloud_8.2.1_0101
  - pci_dss_v4_multi_cloud_8.2.7_0105
  - pci_dss_v4_multi_cloud_8.4.1_0113
  - pci_dss_v4_multi_cloud_8.5.1_0116
  - pci_dss_v4_multi_cloud_8.6.1_0117
  - pci_dss_v4_multi_cloud_9.1.1_0119
  - pci_dss_v4_multi_cloud_9.2.1_0121
  - pci_dss_v4_multi_cloud_9.2.1.1_0122
  - pci_dss_v4_multi_cloud_9.3.1_0126
  - pci_dss_v4_multi_cloud_9.3.1.1_0127
  - pci_dss_v4_multi_cloud_10.1.1_0141
  - pci_dss_v4_multi_cloud_10.1.2_0142
  - pci_dss_v4_multi_cloud_10.2.1_0143
  - pci_dss_v4_multi_cloud_10.2.1.1_0144
  - pci_dss_v4_multi_cloud_10.2.1.2_0145
  - pci_dss_v4_multi_cloud_10.2.1.3_0146
  - pci_dss_v4_multi_cloud_10.2.1.4_0147
  - pci_dss_v4_multi_cloud_10.2.1.5_0148
  - pci_dss_v4_multi_cloud_10.2.1.6_0149
  - pci_dss_v4_multi_cloud_10.3.1_0151
  - pci_dss_v4_multi_cloud_10.3.2_0152
  - pci_dss_v4_multi_cloud_10.3.3_0153
  - pci_dss_v4_multi_cloud_10.3.4_0154
  - pci_dss_v4_multi_cloud_10.4.1.1_0155
  - pci_dss_v4_multi_cloud_10.5.1_0157
  - pci_dss_v4_multi_cloud_10.7.2_0160
  - pci_dss_v4_multi_cloud_10.7.3_0161
  - pci_dss_v4_multi_cloud_11.4.2_0170
  - pci_dss_v4_multi_cloud_11.4.3_0171
  - pci_dss_v4_multi_cloud_11.4.4_0172
  - pci_dss_v4_multi_cloud_11.4.5_0173
  - pci_dss_v4_multi_cloud_11.5.1_0174
  - pci_dss_v4_multi_cloud_11.5.1.1_0175
  - pci_dss_v4_multi_cloud_11.5.2_0176
  - rbi_bank_multi_cloud_13.1_0006
  - rbi_bank_multi_cloud_14.1_0007
  - rbi_bank_multi_cloud_15.1_0008
  - rbi_bank_multi_cloud_15.2_0009
  - rbi_bank_multi_cloud_5.1_0019
  - rbi_bank_multi_cloud_5.2_0020
  - rbi_nbfc_multi_cloud_3.1_0017
  - rbi_nbfc_multi_cloud_3.5_0021
  - rbi_nbfc_multi_cloud_6.3_0031
  - rbi_nbfc_multi_cloud_7.5_0037
  - soc2_multi_cloud_cc_2_1_0002
  - soc2_multi_cloud_cc_3_1_0003
  - soc2_multi_cloud_cc_3_3_0005
  - soc2_multi_cloud_cc_3_4_0006
  - soc2_multi_cloud_cc_4_2_0007
  - soc2_multi_cloud_cc_5_2_0008
  - soc2_multi_cloud_cc_7_2_0016
  - soc2_multi_cloud_cc_7_3_0017
  - soc2_multi_cloud_cc_7_4_0018
  - soc2_multi_cloud_cc_7_5_0019
  - soc2_multi_cloud_cc_a_1_1_0021
- rule_id: k8s.audit.log.failure_alert_configured
  service: audit
  resource: log
  requirement: Failure Alert Configured
  scope: audit.log.failure_alert_configured
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Configure Audit Log Failure Alerts
  rationale: Without failure alerts configured for Kubernetes audit logs, potential security breaches or misconfigurations
    may go unnoticed, allowing malicious activities to persist unchecked. Attackers can exploit this lack of visibility to
    perform unauthorized actions, such as privilege escalation or data exfiltration, without detection.
  description: This rule checks whether Kubernetes audit logs have failure alerts properly configured. A well-configured alert
    system ensures that any issues with logging, such as failures to write logs or unexpected log entries, are promptly reported
    to administrators. This proactive measure enables timely investigation and response, reducing the window of opportunity
    for attackers and ensuring compliance with security standards and regulations.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_171_r2_multi_cloud_3_3_4_3.3.4_Alert_in_the_event_of_an_audit_loggin_0034
- rule_id: k8s.audit.log.file_validation_enabled
  service: audit
  resource: log
  requirement: File Validation Enabled
  scope: audit.log.file_validation_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable and Verify Kubernetes Audit Log File Integrity
  rationale: Without file validation, audit logs can be tampered with by malicious actors to cover tracks or introduce misleading
    information, undermining the integrity of security investigations and compliance reporting. File integrity validation
    ensures that any unauthorized changes to audit logs are detected, preventing attackers from hiding their activities.
  description: This control checks that Kubernetes audit logs have file integrity validation enabled, ensuring that logs cannot
    be altered without detection. Proper configuration involves setting up mechanisms such as checksums or hashes to verify
    the authenticity and integrity of log files. This helps maintain trust in audit logs during security audits and investigations,
    ensuring they reflect accurate and untampered records of system activities.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#log-backend
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-annotations
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SI-4_0149
  - fedramp_moderate_multi_cloud_AU-9_0075
  - fedramp_moderate_multi_cloud_AU-9_2_0076
  - fedramp_moderate_multi_cloud_AU-9_3_0077
  - fedramp_moderate_multi_cloud_AU-9_4_0078
  - hipaa_multi_cloud_164_312_c_2_0028
  - nist_800_53_rev5_multi_cloud_PM-17-b_0876
  - rbi_bank_multi_cloud_15.1_0008
  - rbi_bank_multi_cloud_5.1_0019
  - rbi_nbfc_multi_cloud_3.1_0017
  - rbi_nbfc_multi_cloud_6.3_0031
- rule_id: k8s.audit.log.format_json
  service: audit
  resource: log
  requirement: Format Json
  scope: audit.log.format_json
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce JSON Format for Kubernetes Audit Logs
  rationale: If Kubernetes audit logs are not formatted in JSON, it can lead to inadequate parsing and analysis capabilities,
    making it difficult to detect and respond to security incidents. JSON format ensures structured data that can be easily
    integrated with logging and monitoring tools, enabling timely detection of unauthorized access attempts or configuration
    changes.
  description: This control verifies that Kubernetes audit logs are configured to use JSON format. Checking for JSON format
    ensures that logs are structured and machine-readable, facilitating integration with SIEM systems for real-time monitoring
    and alerting. Properly formatted logs enhance the ability to perform forensic analysis, correlate events, and comply with
    security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-3_0025
- rule_id: k8s.audit.log.high_severity_event_detection
  service: audit
  resource: log
  requirement: High Severity Event Detection
  scope: audit.log.high_severity_event_detection
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Detect and Alert on High Severity Events in Audit Logs
  rationale: Misconfigured audit logging can lead to undetected malicious activities, such as privilege escalation or unauthorized
    access, compromising the entire Kubernetes cluster. High severity events often indicate critical issues that could be
    exploited by attackers to gain control or exfiltrate sensitive data.
  description: This control ensures that Kubernetes audit logs are configured to detect and alert on high severity events.
    It validates the presence of rules that capture critical actions like access to sensitive resources or unauthorized modifications.
    Proper configuration includes setting appropriate audit policies and integrating with alerting systems to promptly respond
    to potential threats. This helps in early detection of compromise and aids in forensic analysis during security incidents.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IR-4_0072
- rule_id: k8s.audit.log.integrity_enabled
  service: audit
  resource: log
  requirement: Integrity Enabled
  scope: audit.log.integrity_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Integrity Verification for Kubernetes Audit Logs
  rationale: Without integrity verification, Kubernetes audit logs are vulnerable to tampering, which can mask unauthorized
    activities, hinder forensic investigations, and lead to non-compliance with regulatory standards. Attackers may alter
    logs to hide traces of their actions, making it crucial to ensure logs are protected against unauthorized modifications.
  description: This control checks that integrity verification is enabled for Kubernetes audit logs. Proper configuration
    involves using mechanisms like checksums or digital signatures to ensure that any unauthorized changes to the logs are
    detectable. Ensuring audit log integrity helps maintain a reliable trail of events for security monitoring and compliance
    audits, thereby reducing the risk of undetected security breaches.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  compliance:
  - iso27001_2022_multi_cloud_A.12.4_0002
  - pci_dss_v4_multi_cloud_10.5.1_0157
- rule_id: k8s.audit.log.monitoring_configured
  service: audit
  resource: log
  requirement: Monitoring Configured
  scope: audit.log.monitoring_configured
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Ensure Audit Log Monitoring is Enabled and Configured
  rationale: Failure to monitor Kubernetes audit logs can lead to undetected security breaches and unauthorized access. Attackers
    may exploit misconfigurations or vulnerabilities without leaving a trace, making it difficult to respond to incidents.
    Proper log monitoring aids in the detection of anomalies and potential threats in real-time, enabling prompt response
    and mitigation.
  description: This rule checks that Kubernetes audit logs are actively monitored and configured according to security best
    practices. A well-configured audit logging system captures all significant events, such as API calls, access attempts,
    and configuration changes. It ensures that logs are stored securely and analyzed continuously for suspicious activities.
    This helps in early detection of security incidents, compliance with regulatory standards, and maintaining the integrity
    of the cluster.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit-log-policy/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-2-a_1342
- rule_id: k8s.audit.log.network_policy_changes
  service: audit
  resource: log
  requirement: Network Policy Changes
  scope: audit.log.network_policy_changes
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Monitor and Audit Network Policy Modifications
  rationale: Failure to monitor and audit network policy changes can lead to undetected security policy misconfigurations
    or unauthorized modifications, potentially exposing sensitive services to unauthorized access and lateral movement within
    the cluster. Attackers could exploit such vulnerabilities to escalate privileges or exfiltrate data.
  description: This control ensures that any changes to Kubernetes Network Policies are logged in the audit logs. It checks
    that audit policies are configured to capture modifications to Network Policy resources, which are crucial for defining
    and enforcing inter-pod traffic rules. By capturing these changes, administrators can detect and respond to unauthorized
    or unintended modifications, ensuring that network segmentation and security policies remain effective. Properly configured
    audit logs help maintain visibility into network policy changes, thereby enhancing the detection of potential security
    incidents and maintaining compliance with security best practices.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/audit/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SI-2_0147
- rule_id: k8s.audit.log.policies_documented
  service: audit
  resource: log
  requirement: Policies Documented
  scope: audit.log.policies_documented
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes Audit Log Policies are Explicitly Defined
  rationale: Without explicitly defined audit log policies, there is a risk of incomplete or inadequate logging, which can
    lead to undetected malicious activities such as privilege escalation or unauthorized access. Attackers could exploit these
    gaps to cover their tracks, making it difficult to conduct forensic investigations and comply with regulatory requirements.
  description: This control verifies that Kubernetes audit log policies are explicitly defined and configured according to
    security best practices. A well-configured audit policy ensures comprehensive logging of critical events such as access
    to sensitive resources and changes to cluster state. This helps in early detection of potential security incidents and
    supports compliance with standards like CIS Kubernetes Benchmark. Proper audit logging configuration involves specifying
    what events are logged, where logs are stored, and how they are protected against tampering.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/setup/best-practices/audit/
  compliance:
  - iso27001_2022_multi_cloud_A.5.1_0005
  - iso27001_2022_multi_cloud_A.5.10_0006
  - iso27001_2022_multi_cloud_A.5.11_0007
  - iso27001_2022_multi_cloud_A.5.12_0008
  - iso27001_2022_multi_cloud_A.5.13_0009
  - iso27001_2022_multi_cloud_A.5.14_0010
  - iso27001_2022_multi_cloud_A.5.16_0012
  - iso27001_2022_multi_cloud_A.5.17_0013
- rule_id: k8s.audit.log.protection_mechanism_configured
  service: audit
  resource: log
  requirement: Protection Mechanism Configured
  scope: audit.log.protection_mechanism_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Audit Logs Are Protected with Access Controls
  rationale: Without proper protection mechanisms, Kubernetes audit logs are vulnerable to unauthorized access and tampering,
    which can lead to exposure of sensitive information and difficulty in forensic investigations. Attackers could exploit
    these vulnerabilities to cover their tracks or gain insights into system operations.
  description: This check ensures that Kubernetes audit logs have adequate protection mechanisms in place, such as encryption
    and access controls. A well-configured audit log should be encrypted both in transit and at rest, and access should be
    restricted to authorized personnel only. By validating these configurations, the control helps prevent unauthorized access,
    protects sensitive data integrity, and aids in compliance with security standards and regulations. Properly protected
    audit logs also facilitate reliable forensic analysis in the case of a security incident.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/setup/best-practices/certificates/#configure-audit-logging
  compliance:
  - fedramp_moderate_multi_cloud_AU-9_0075
  - fedramp_moderate_multi_cloud_AU-9_2_0076
  - fedramp_moderate_multi_cloud_AU-9_3_0077
  - fedramp_moderate_multi_cloud_AU-9_4_0078
- rule_id: k8s.audit.log.retention_configured
  service: audit
  resource: log
  requirement: Retention Configured
  scope: audit.log.retention_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Audit Log Retention Policies
  rationale: Without proper retention policies, audit logs may be lost or overwritten, hindering forensic investigations and
    compliance checks. Attackers may exploit this by covering their tracks or causing denial of service by filling storage.
    Ensuring retention helps in tracking unauthorized access and malicious activities.
  description: This check ensures that Kubernetes audit logs have a defined retention policy, specifying the duration logs
    are retained before deletion. A well-configured retention policy helps maintain a history of events, crucial for auditing
    and identifying security incidents. Proper retention aligns with compliance mandates and prevents the inadvertent loss
    of critical security data.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/reference/config-api/kubernetes-audit/v1/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-2_0024
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-3_0025
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-6_0028
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-9_0031
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-12_0033
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-7_0039
  - canada_pbmm_moderate_multi_cloud_CCCS_SI-4_0149
  - fedramp_moderate_multi_cloud_AU-2_0058
  - fedramp_moderate_multi_cloud_AU-3_0059
  - fedramp_moderate_multi_cloud_AU-3_1_0060
  - fedramp_moderate_multi_cloud_AU-6_0065
  - fedramp_moderate_multi_cloud_AU-6_1_0066
  - fedramp_moderate_multi_cloud_AU-6_3_0067
  - fedramp_moderate_multi_cloud_AU-6_4_0068
  - fedramp_moderate_multi_cloud_AU-6_5_0069
  - fedramp_moderate_multi_cloud_AU-6_6_0070
  - fedramp_moderate_multi_cloud_AU-6_7_0071
  - fedramp_moderate_multi_cloud_AU-8_0074
  - fedramp_moderate_multi_cloud_AU-12_0081
  - fedramp_moderate_multi_cloud_AU-12_1_0082
  - fedramp_moderate_multi_cloud_AU-12_3_0083
  - fedramp_moderate_multi_cloud_CA-7_0093
  - fedramp_moderate_multi_cloud_CA-7_1_0094
  - fedramp_moderate_multi_cloud_CA-7_4_0095
  - fedramp_moderate_multi_cloud_CM-9_0128
  - fedramp_moderate_multi_cloud_SI-4_0367
  - fedramp_moderate_multi_cloud_SI-4_1_0368
  - fedramp_moderate_multi_cloud_SI-4_2_0369
  - fedramp_moderate_multi_cloud_SI-4_4_0370
  - fedramp_moderate_multi_cloud_SI-4_5_0371
  - fedramp_moderate_multi_cloud_SI-4_10_0372
  - fedramp_moderate_multi_cloud_SI-4_11_0373
  - fedramp_moderate_multi_cloud_SI-4_12_0374
  - fedramp_moderate_multi_cloud_SI-4_14_0375
  - fedramp_moderate_multi_cloud_SI-4_16_0376
  - fedramp_moderate_multi_cloud_SI-4_18_0377
  - fedramp_moderate_multi_cloud_SI-4_19_0378
  - fedramp_moderate_multi_cloud_SI-4_20_0379
  - fedramp_moderate_multi_cloud_SI-4_22_0380
  - fedramp_moderate_multi_cloud_SI-4_23_0381
  - gdpr_multi_cloud_Article_30_Records_of_processing_activities_0002
  - hipaa_multi_cloud_164_308_a_1_ii_d_0003
  - hipaa_multi_cloud_164_308_a_6_ii_0016
  - hipaa_multi_cloud_164_312_b_0026
  - iso27001_2022_multi_cloud_A.12.4_0002
  - iso27001_2022_multi_cloud_A.8.16_0071
  - nist_800_171_r2_multi_cloud_3_11_2_3.11.2_Scan_for_vulnerabilities_in_organiz_0001
  - nist_800_171_r2_multi_cloud_3_1_12_3.1.12_Monitor_and_control_remote_access_s_0021
  - nist_800_171_r2_multi_cloud_3_3_2_3.3.2_Ensure_that_the_actions_of_individual_0032
  - nist_800_171_r2_multi_cloud_3_3_3_3.3.3_Review_and_update_logged_events_0033
  - nist_800_171_r2_multi_cloud_3_3_4_3.3.4_Alert_in_the_event_of_an_audit_loggin_0034
  - nist_800_171_r2_multi_cloud_3_6_1_3.6.1_Establish_an_operational_incident-han_0049
  - nist_800_171_r2_multi_cloud_3_6_2_3.6.2_Track_document_and_report_incidents_0050
  - nist_800_53_rev5_multi_cloud_AU-2-b_0224
  - nist_800_53_rev5_multi_cloud_AU-3-a_0231
  - nist_800_53_rev5_multi_cloud_AU-3-b_0232
  - nist_800_53_rev5_multi_cloud_AU-3-c_0233
  - nist_800_53_rev5_multi_cloud_AU-3-d_0234
  - nist_800_53_rev5_multi_cloud_AU-3-e_0235
  - nist_800_53_rev5_multi_cloud_AU-3-f_0236
  - nist_800_53_rev5_multi_cloud_AU-8-b_0258
  - nist_800_53_rev5_multi_cloud_AU-9-a_0266
  - nist_800_53_rev5_multi_cloud_AU-12-a_0278
  - nist_800_53_rev5_multi_cloud_AU-12-c_0280
  - nist_800_53_rev5_multi_cloud_AU-14-a_0289
  - nist_800_53_rev5_multi_cloud_AU-14-b_0290
  - nist_800_53_rev5_multi_cloud_CA-7-b_0338
  - nist_800_53_rev5_multi_cloud_CA-7-c_0339
  - nist_800_53_rev5_multi_cloud_CA-7-d_0340
  - nist_800_53_rev5_multi_cloud_CA-7-e_0341
  - nist_800_53_rev5_multi_cloud_CA-7-g_0343
  - nist_800_53_rev5_multi_cloud_CM-6-a_0397
  - nist_800_53_rev5_multi_cloud_CM-9-b_0428
  - nist_800_53_rev5_multi_cloud_PM-31-a_0911
  - nist_800_53_rev5_multi_cloud_PM-31-b_0912
  - nist_800_53_rev5_multi_cloud_PM-31-c_0913
  - nist_800_53_rev5_multi_cloud_PM-31-d_0914
  - nist_800_53_rev5_multi_cloud_PM-31-e_0915
  - nist_800_53_rev5_multi_cloud_PM-31-f_0916
  - nist_800_53_rev5_multi_cloud_SI-4-c_1393
  - pci_dss_v4_multi_cloud_3.1.1_0032
  - pci_dss_v4_multi_cloud_3.3.1.1_0034
  - pci_dss_v4_multi_cloud_3.4.2_0040
  - pci_dss_v4_multi_cloud_8.2.1_0101
  - pci_dss_v4_multi_cloud_8.4.1_0113
  - pci_dss_v4_multi_cloud_8.5.1_0116
  - pci_dss_v4_multi_cloud_8.6.1_0117
  - pci_dss_v4_multi_cloud_9.1.1_0119
  - pci_dss_v4_multi_cloud_9.2.1_0121
  - pci_dss_v4_multi_cloud_9.2.1.1_0122
  - pci_dss_v4_multi_cloud_9.3.1_0126
  - pci_dss_v4_multi_cloud_9.3.1.1_0127
  - pci_dss_v4_multi_cloud_10.1.1_0141
  - pci_dss_v4_multi_cloud_10.1.2_0142
  - pci_dss_v4_multi_cloud_10.2.1_0143
  - pci_dss_v4_multi_cloud_10.2.1.1_0144
  - pci_dss_v4_multi_cloud_10.2.1.2_0145
  - pci_dss_v4_multi_cloud_10.2.1.3_0146
  - pci_dss_v4_multi_cloud_10.2.1.4_0147
  - pci_dss_v4_multi_cloud_10.2.1.5_0148
  - pci_dss_v4_multi_cloud_10.2.1.6_0149
  - pci_dss_v4_multi_cloud_10.2.1.7_0150
  - pci_dss_v4_multi_cloud_10.3.1_0151
  - pci_dss_v4_multi_cloud_10.3.2_0152
  - pci_dss_v4_multi_cloud_10.3.3_0153
  - pci_dss_v4_multi_cloud_10.3.4_0154
  - pci_dss_v4_multi_cloud_10.4.1.1_0155
  - pci_dss_v4_multi_cloud_10.5.1_0157
  - pci_dss_v4_multi_cloud_11.4.4_0172
  - pci_dss_v4_multi_cloud_11.5.1_0174
  - pci_dss_v4_multi_cloud_11.5.1.1_0175
  - pci_dss_v4_multi_cloud_11.5.2_0176
  - rbi_bank_multi_cloud_13.1_0006
  - rbi_bank_multi_cloud_14.1_0007
  - rbi_bank_multi_cloud_15.1_0008
  - rbi_bank_multi_cloud_15.2_0009
  - rbi_bank_multi_cloud_5.1_0019
  - rbi_bank_multi_cloud_5.2_0020
  - rbi_nbfc_multi_cloud_3.1_0017
  - rbi_nbfc_multi_cloud_3.5_0021
  - rbi_nbfc_multi_cloud_6.3_0031
  - rbi_nbfc_multi_cloud_7.5_0037
  - soc2_multi_cloud_cc_2_1_0002
  - soc2_multi_cloud_cc_4_2_0007
  - soc2_multi_cloud_cc_5_2_0008
  - soc2_multi_cloud_cc_7_2_0016
  - soc2_multi_cloud_cc_7_3_0017
  - soc2_multi_cloud_cc_7_4_0018
- rule_id: k8s.audit.log.retention_days_specified
  service: audit
  resource: log
  requirement: Retention Days Specified
  scope: audit.log.retention_days_specified
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Specify Retention Period for Kubernetes Audit Logs
  rationale: Without a specified retention period for audit logs, critical security events may be lost over time, hindering
    incident response and forensic investigations. Attackers could exploit this by deliberately performing actions that go
    unnoticed due to the lack of historical audit data. Specifying retention days ensures that logs are retained for a sufficient
    period to detect suspicious activities and comply with regulatory requirements.
  description: This control checks that a retention period is specified for Kubernetes audit logs. A well-configured retention
    policy ensures that audit logs are kept for a defined period, allowing for adequate monitoring and analysis of system
    activities. Proper configuration involves setting a specific number of days for which logs should be retained, balancing
    between storage costs and the need for historical data. This helps in maintaining an audit trail, facilitating compliance
    with standards like the CIS Kubernetes Benchmark, and enhancing the overall security posture by ensuring logs are available
    for audits and investigations.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - nist_800_53_rev5_multi_cloud_PM-21-b_0885
- rule_id: k8s.audit.log.timestamps_configured
  service: audit
  resource: log
  requirement: Timestamps Configured
  scope: audit.log.timestamps_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Audit Logs Include Timestamps
  rationale: Without timestamps, correlating and analyzing audit logs becomes difficult, impeding the detection and response
    to unauthorized access or configuration changes. Timestamps are crucial for forensic investigations and maintaining chronological
    order in security audits. Attackers may exploit systems with inadequate logging to obscure their actions and persistence.
  description: This rule checks that Kubernetes audit logs are configured to include precise timestamps for each logged event.
    A proper configuration mandates that each log entry is timestamped, enabling accurate event correlation and sequence analysis.
    This helps in quickly identifying breaches or misconfigurations, ensuring that incident response teams have the necessary
    information to act. Proper timestamp configuration aligns with security best practices and regulatory standards such as
    CIS Kubernetes Benchmark, promoting a robust security posture.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-annotations
  compliance:
  - fedramp_moderate_multi_cloud_AU-8_0074
- rule_id: k8s.audit.networkpolicy.changes_logged
  service: audit
  resource: networkpolicy
  requirement: Changes Logged
  scope: audit.networkpolicy.changes_logged
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Log NetworkPolicy Configuration Changes
  rationale: Failure to log changes to NetworkPolicy configurations can lead to undetected security breaches, as malicious
    actors may alter policies to allow unauthorized traffic. Without proper logging, it becomes difficult to trace unauthorized
    access or changes, potentially leading to data breaches or compliance violations.
  description: This control checks that all changes to Kubernetes NetworkPolicy resources are being logged. Proper logging
    involves capturing create, update, and delete operations on NetworkPolicies. This enables monitoring of policy changes
    that could otherwise expose the cluster to unauthorized network traffic. Ensuring that these logs are collected and monitored
    helps detect and respond to policy misconfigurations and unauthorized access attempts, thereby enhancing the security
    posture and ensuring compliance with security standards.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/audit/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-14-b_0290
  - nist_800_53_rev5_multi_cloud_CA-7-f_0342
- rule_id: k8s.audit.policy.capture_user_identity
  service: audit
  resource: policy
  requirement: Capture User Identity
  scope: audit.policy.capture_user_identity
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Ensure User Identity is Captured in Audit Logs
  rationale: Capturing user identity in Kubernetes audit logs is critical to trace actions back to individual users. Without
    this, it becomes challenging to detect and investigate unauthorized access or malicious activities, as attackers may exploit
    the lack of traceability to cover their tracks.
  description: This rule checks that the Kubernetes audit policy is configured to capture user identity information for all
    API server requests. A well-configured audit policy logs 'user' and 'userGroups' fields, ensuring that every action within
    the cluster can be attributed to a specific user or service account. This is essential for accountability, incident response,
    and compliance with standards like CIS Kubernetes Benchmark. Properly capturing user identity aids in forensic analysis,
    helping to understand the scope and impact of security incidents.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-3-f_0236
- rule_id: k8s.audit.policy.captures_all_requests
  service: audit
  resource: policy
  requirement: Captures All Requests
  scope: audit.policy.captures_all_requests
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Comprehensive Audit Logging for All API Requests
  rationale: Without comprehensive audit logging, unauthorized or malicious activities may go undetected, as there won't be
    a complete record of actions taken in the cluster. This can lead to unnoticed elevation of privileges, unauthorized data
    access, or exploitation attempts. Capturing all API requests ensures that any anomalous or suspicious activities can be
    traced and investigated, thus reducing the risk of security breaches and aiding in incident response.
  description: This control checks if the Kubernetes audit policy is configured to log all API requests, including metadata
    and request body. A well-configured audit policy includes rules that capture all request and response data for critical
    operations, such as changes to RBAC, creation/deletion of resources, and access to sensitive data. Ensuring comprehensive
    logging helps in forensic analysis and compliance with security frameworks like CIS Kubernetes Benchmark. Proper audit
    logging allows for tracking of user actions, detection of unauthorized access, and compliance with regulatory requirements.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-annotations
  compliance:
  - fedramp_moderate_multi_cloud_AU-3_0059
  - fedramp_moderate_multi_cloud_AU-3_1_0060
- rule_id: k8s.audit.policy.captures_metadata
  service: audit
  resource: policy
  requirement: Captures Metadata
  scope: audit.policy.captures_metadata
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Audit Policy Captures Metadata Events
  rationale: Failing to capture metadata events in Kubernetes audit logs can result in undetected unauthorized access or changes
    to the cluster. Without these logs, it becomes challenging to perform forensic analysis, trace the source of an attack,
    or understand the sequence of events leading to a security incident. Attackers might exploit this lack of visibility to
    escalate privileges or exfiltrate data without leaving a trace.
  description: This control checks that the Kubernetes audit policy is configured to capture metadata events, such as who
    performed an action, when it was performed, and what resources were affected. A properly configured audit policy ensures
    that all important activities are logged, providing a comprehensive audit trail. This configuration helps security teams
    to monitor for suspicious activities, enforce compliance with regulatory standards, and quickly respond to potential security
    breaches by providing detailed logs for analysis.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-annotations
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-2_0024
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-9_0031
  - canada_pbmm_moderate_multi_cloud_CCCS_SI-4_0149
  - fedramp_moderate_multi_cloud_AU-3_0059
  - fedramp_moderate_multi_cloud_AU-3_1_0060
  - fedramp_moderate_multi_cloud_SI-4_0367
  - fedramp_moderate_multi_cloud_SI-4_1_0368
  - fedramp_moderate_multi_cloud_SI-4_2_0369
  - fedramp_moderate_multi_cloud_SI-4_4_0370
  - fedramp_moderate_multi_cloud_SI-4_5_0371
  - fedramp_moderate_multi_cloud_SI-4_10_0372
  - fedramp_moderate_multi_cloud_SI-4_11_0373
  - fedramp_moderate_multi_cloud_SI-4_12_0374
  - fedramp_moderate_multi_cloud_SI-4_14_0375
  - fedramp_moderate_multi_cloud_SI-4_16_0376
  - fedramp_moderate_multi_cloud_SI-4_18_0377
  - fedramp_moderate_multi_cloud_SI-4_19_0378
  - fedramp_moderate_multi_cloud_SI-4_20_0379
  - fedramp_moderate_multi_cloud_SI-4_22_0380
  - fedramp_moderate_multi_cloud_SI-4_23_0381
  - gdpr_multi_cloud_Article_30_Records_of_processing_activities_0002
  - hipaa_multi_cloud_164_308_a_1_ii_d_0003
  - hipaa_multi_cloud_164_308_a_6_ii_0016
  - hipaa_multi_cloud_164_308_a_8_0021
  - iso27001_2022_multi_cloud_A.12.4_0002
  - iso27001_2022_multi_cloud_A.8.16_0071
  - nist_800_171_r2_multi_cloud_3_3_2_3.3.2_Ensure_that_the_actions_of_individual_0032
  - nist_800_171_r2_multi_cloud_3_3_3_3.3.3_Review_and_update_logged_events_0033
  - nist_800_53_rev5_multi_cloud_AC-16-b_0135
  - nist_800_53_rev5_multi_cloud_AU-2-b_0224
  - nist_800_53_rev5_multi_cloud_AU-12-c_0280
  - nist_800_53_rev5_multi_cloud_AU-14-a_0289
  - nist_800_53_rev5_multi_cloud_AU-14-b_0290
  - nist_800_53_rev5_multi_cloud_CA-2-d_0303
  - pci_dss_v4_multi_cloud_8.1.1_0099
  - pci_dss_v4_multi_cloud_10.3.3_0153
  - pci_dss_v4_multi_cloud_10.3.4_0154
  - pci_dss_v4_multi_cloud_10.4.1.1_0155
  - pci_dss_v4_multi_cloud_10.5.1_0157
  - pci_dss_v4_multi_cloud_11.4.3_0171
  - pci_dss_v4_multi_cloud_11.5.2_0176
  - rbi_nbfc_multi_cloud_3.5_0021
  - soc2_multi_cloud_cc_2_1_0002
- rule_id: k8s.audit.policy.captures_privileged_actions
  service: audit
  resource: policy
  requirement: Captures Privileged Actions
  scope: audit.policy.captures_privileged_actions
  domain: identity_and_access_management
  subcategory: access_control
  severity: critical
  title: Audit Privileged Kubernetes API Actions
  rationale: Failure to accurately capture privileged actions in Kubernetes can lead to undetected unauthorized access and
    privilege escalation. Attackers may exploit misconfigured audit policies to perform malicious activities without leaving
    a trace, potentially compromising sensitive data and system integrity.
  description: This control verifies that the Kubernetes audit policy is configured to specifically capture actions performed
    with elevated privileges. It ensures that all access and modifications by users with admin-level privileges are logged.
    A well-configured audit policy will include capturing 'create', 'update', 'delete', and 'exec' actions on critical resources
    such as secrets, pods, and nodes. This monitoring helps in detecting and responding to unauthorized or anomalous activities,
    enhancing the overall security posture.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/administer-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  compliance:
  - nist_800_171_r2_multi_cloud_3_1_7_3.1.7_Prevent_non-privileged_users_from_exe_0030
- rule_id: k8s.audit.policy.captures_security_events
  service: audit
  resource: policy
  requirement: Captures Security Events
  scope: audit.policy.captures_security_events
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Comprehensive Security Event Auditing in Kubernetes
  rationale: Without a properly configured audit policy, Kubernetes clusters are vulnerable to undetected security breaches,
    unauthorized access attempts, and data exfiltration. Attackers could exploit this to move laterally within the cluster,
    alter configurations, or gain access to sensitive data without leaving a trace. Comprehensive auditing helps in detecting
    such malicious activities and maintaining accountability.
  description: This check ensures that the Kubernetes audit policy is configured to capture all relevant security events,
    such as changes to RBAC settings, pod and container lifecycle events, and authentication and authorization requests. A
    robust audit policy should be in place to log these events, which can then be analyzed to detect anomalies, investigate
    incidents, and fulfill compliance requirements. Proper configuration includes defining audit policy rules with appropriate
    levels such as 'Metadata', 'Request', 'RequestResponse', and ensuring logs are securely stored and monitored.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/#audit-logging
  compliance:
  - hipaa_multi_cloud_164_308_a_6_i_0015
  - pci_dss_v4_multi_cloud_11.4.2_0170
- rule_id: k8s.audit.policy.captures_timestamps
  service: audit
  resource: policy
  requirement: Captures Timestamps
  scope: audit.policy.captures_timestamps
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Audit Logs Capture Precise Timestamps
  rationale: Capturing precise timestamps in audit logs is essential for effective incident response and forensic analysis.
    Without accurate timestamps, correlating events across systems becomes challenging, which can hinder the identification
    of security incidents, delay response actions, and obscure the timeline of unauthorized activities.
  description: This control checks that the Kubernetes audit policy is configured to capture precise timestamps for every
    recorded event. A correctly configured audit policy includes timestamps in ISO 8601 format, ensuring that each event log
    is time-stamped accurately. This precision is crucial for tracing unauthorized access attempts, monitoring system changes,
    and ensuring accountability. Proper timestamping aids in compliance with industry standards and provides a solid foundation
    for maintaining a secure and auditable Kubernetes environment.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-8-b_0258
- rule_id: k8s.audit.policy.captures_unauthorized_access
  service: audit
  resource: policy
  requirement: Captures Unauthorized Access
  scope: audit.policy.captures_unauthorized_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Ensure Comprehensive Audit Logging for Unauthorized Access
  rationale: Without proper audit logging, unauthorized access attempts may go undetected, leaving the cluster vulnerable
    to attacks such as privilege escalation or data exfiltration. Comprehensive logging is crucial for detecting, responding
    to, and investigating security incidents, helping to prevent or mitigate potential breaches.
  description: This control verifies that the Kubernetes audit policy is configured to capture unauthorized access attempts
    comprehensively. The audit policy should include rules that log access denials and authentication failures, ensuring that
    any unauthorized access attempts are recorded. A well-configured audit policy helps in forensic analysis and compliance
    with security standards by providing necessary evidence in case of an incident. Best practices include defining clear
    audit policy rules that capture relevant events and ensure logs are retained and protected.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/security/authentication/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-4-b_1392
- rule_id: k8s.audit.policy.captures_user_access
  service: audit
  resource: policy
  requirement: Captures User Access
  scope: audit.policy.captures_user_access
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Ensure Kubernetes Audit Logs Capture All User Access Events
  rationale: Without comprehensive audit logging of user access, malicious activities can go undetected, allowing unauthorized
    access or changes to the cluster. Misconfigured audit policies can lead to incomplete logging, making it difficult to
    trace user actions, investigate incidents, and comply with regulatory standards. Attackers could exploit these gaps to
    perform privilege escalation or data exfiltration without detection.
  description: This control checks that the Kubernetes audit policy is configured to capture all user access events, including
    successful and failed API requests. A well-configured audit policy should log details such as user identity, source IP
    address, and request path, ensuring a comprehensive record of user interactions with the cluster. This helps in detecting
    and investigating security incidents, enforcing accountability, and meeting compliance requirements such as PCI DSS. Proper
    audit logging aids in monitoring unauthorized access attempts and supports forensic analysis by providing a detailed activity
    trail.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-logging
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/#audit-logging
  compliance:
  - pci_dss_v4_multi_cloud_10.2.1.1_0144
- rule_id: k8s.audit.policy.change_detection_enabled
  service: audit
  resource: policy
  requirement: Change Detection Enabled
  scope: audit.policy.change_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Change Detection in Kubernetes Audit Policies
  rationale: Without change detection, unauthorized modifications to configurations can go unnoticed, leading to potential
    data breaches or service disruptions. Attackers can exploit this to escalate privileges or persist within the cluster
    undetected.
  description: This control checks if Kubernetes audit policies are configured to detect changes in cluster configurations.
    A well-configured audit policy should include rules that log any modifications to critical resources, such as roles, role
    bindings, and cluster roles. This helps in early detection of unauthorized changes, aiding in rapid incident response
    and forensic investigations. Compliance with this control ensures that any unauthorized or accidental modifications are
    logged and can be traced back to their source, thereby enhancing overall security posture.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-annotations
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/#audit-logs
  compliance:
  - pci_dss_v4_multi_cloud_8.1.2_0100
- rule_id: k8s.audit.policy.change_events_logged
  service: audit
  resource: policy
  requirement: Change Events Logged
  scope: audit.policy.change_events_logged
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Ensure Audit Logging of Kubernetes API Changes
  rationale: Failure to log API change events can lead to undetected unauthorized modifications, allowing attackers to alter
    configurations or escalate privileges without notice. Logging these events is crucial for forensic analysis and accountability.
    Without proper logging, it becomes challenging to track changes, identify potential breaches, and comply with regulatory
    standards that require audit trails.
  description: This control verifies that the Kubernetes audit policy is configured to log events related to changes in the
    API, such as modifications to resources, role bindings, and configurations. A comprehensive audit policy should include
    rules that specifically capture create, update, delete, and patch operations on critical resources. Proper logging helps
    detect unauthorized actions, supports incident response efforts, and ensures compliance with standards like the CIS Kubernetes
    Benchmark. It also aids in identifying misconfigurations and potential security breaches by providing a clear history
    of changes.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-annotations
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#log-admission-control
  compliance:
  - pci_dss_v4_multi_cloud_2.2.1_0020
- rule_id: k8s.audit.policy.change_logging_enabled
  service: audit
  resource: policy
  requirement: Change Logging Enabled
  scope: audit.policy.change_logging_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable and Configure Kubernetes Audit Policy for Change Logging
  rationale: Failure to enable detailed change logging in Kubernetes audit policies can lead to undetected unauthorized changes,
    which can be exploited by attackers to escalate privileges or compromise the cluster. Without proper logging, tracing
    malicious activities becomes challenging, hindering incident response and forensic analysis.
  description: This control checks that the Kubernetes audit policy is configured to log all significant changes, including
    modifications to RBAC roles, cluster configurations, and API server interactions. A well-configured audit policy ensures
    all critical changes are recorded with sufficient detail, such as the 'metadata' and 'responseBody', to provide insights
    into who made changes, what changes were made, and when. This logging supports compliance with security standards and
    helps in identifying and responding to suspicious activities promptly.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-annotations
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - pci_dss_v4_multi_cloud_8.1.2_0100
- rule_id: k8s.audit.policy.configured
  service: audit
  resource: policy
  requirement: Configured
  scope: audit.policy.configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Audit Policy is Properly Configured
  rationale: An improperly configured audit policy in Kubernetes can lead to insufficient logging of critical events, making
    it difficult to detect and respond to security incidents. This can allow attackers to exploit vulnerabilities without
    detection and evade accountability. Proper audit configuration helps in tracing unauthorized access and modifications,
    which is crucial for incident response and forensic analysis.
  description: This control checks whether the Kubernetes audit policy is configured according to security best practices.
    It ensures that audit logs capture all necessary events, including authentication and authorization attempts, resource
    access, and system modifications. A well-configured audit policy helps in maintaining a comprehensive security monitoring
    system, aiding in the detection of anomalous activities and ensuring compliance with security standards such as the CIS
    Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/security/overview/#audit-logging
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-9_0049
- rule_id: k8s.audit.policy.custom_rules_defined
  service: audit
  resource: policy
  requirement: Custom Rules Defined
  scope: audit.policy.custom_rules_defined
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Define Custom Audit Policy Rules
  rationale: Without properly configured custom audit policy rules, Kubernetes environments are at risk of unauthorized changes
    and access going undetected. This can lead to security breaches via privilege escalation and unauthorized resource access.
  description: This control checks the presence and configuration of custom audit policy rules in Kubernetes clusters. A well-defined
    audit policy should capture critical events, such as changes to RBAC roles or access to sensitive resources, and log them
    for analysis. Implementing a tailored audit policy helps detect and respond to malicious activity, ensuring compliance
    with security standards and enhancing the overall security posture.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/security/audit/
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-12-c_0280
- rule_id: k8s.audit.policy.event_location_captured
  service: audit
  resource: policy
  requirement: Event Location Captured
  scope: audit.policy.event_location_captured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Audit Logs Capture Event Source Location
  rationale: If the event source location is not captured in audit logs, it becomes difficult to trace the origin of security
    incidents, potentially allowing unauthorized activities to go undetected. Attackers could exploit this gap to obscure
    their activities, making it challenging to identify and mitigate threats effectively. Capturing the location of events
    helps in identifying patterns of malicious behavior and supports forensic investigations.
  description: This control verifies that the Kubernetes audit policy is configured to capture the source location of events.
    A well-configured audit log should include details such as the user, the action performed, the resource affected, and
    the location from where the action was initiated. Properly capturing this information enhances the traceability of actions
    within the cluster, facilitates compliance with security regulations, and strengthens the overall security posture by
    providing necessary data for detecting and investigating security incidents.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-3-c_0233
- rule_id: k8s.audit.policy.event_outcomes_captured
  service: audit
  resource: policy
  requirement: Event Outcomes Captured
  scope: audit.policy.event_outcomes_captured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Audit Policy Logs Event Outcomes
  rationale: Failure to capture event outcomes in Kubernetes audit logs can lead to insufficient visibility into cluster activities,
    making it difficult to detect unauthorized access or malicious actions. Attackers could exploit this by performing actions
    without being detected, potentially leading to data breaches or unauthorized changes.
  description: This control checks that the Kubernetes audit policy is configured to log the outcome of events, including
    success and failure states. Properly capturing event outcomes in audit logs ensures comprehensive visibility into all
    operations within the cluster. This enables security teams to monitor for unauthorized activities, investigate incidents
    effectively, and comply with industry standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/audit-policy/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-backend
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-3-e_0235
- rule_id: k8s.audit.policy.event_timestamp_captured
  service: audit
  resource: policy
  requirement: Event Timestamp Captured
  scope: audit.policy.event_timestamp_captured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Accurate Event Timestamps in Kubernetes Audit Logs
  rationale: Accurate timestamps in audit logs are crucial for incident investigation and forensic analysis. If timestamps
    are misconfigured or missing, it can lead to difficulties in correlating events, identifying unauthorized activities,
    and establishing timelines of security incidents. Attackers might exploit this by obfuscating their tracks or creating
    confusion during incident response.
  description: This control checks that the Kubernetes audit policy is configured to capture accurate event timestamps. A
    properly configured audit policy includes a 'stage' field with precise timestamps for each event, ensuring that all actions
    within the cluster are logged with a reliable time reference. This configuration aids in compliance with security standards
    and regulatory requirements by providing a clear and traceable record of cluster activities, thereby enhancing the detection
    of anomalies and unauthorized access.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-3-b_0232
- rule_id: k8s.audit.policy.event_types_captured
  service: audit
  resource: policy
  requirement: Event Types Captured
  scope: audit.policy.event_types_captured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Capture All Relevant Event Types in Audit Policy
  rationale: Failure to capture all relevant event types in the Kubernetes audit logs can lead to missing critical security
    events, making it difficult to detect unauthorized access attempts, policy violations, or other malicious activities.
    An attacker could exploit this misconfiguration to perform actions without being detected, increasing the risk of a successful
    attack on the cluster.
  description: This control checks that the Kubernetes audit policy is configured to capture all necessary event types, such
    as 'RequestReceived', 'ResponseStarted', 'ResponseComplete', and 'Panic'. A well-configured audit policy ensures comprehensive
    logging, allowing for effective monitoring and forensic analysis of actions taken within the cluster. Capturing all relevant
    events helps in fulfilling compliance requirements, identifying suspicious activities, and ensuring accountability.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy
  - https://kubernetes.io/docs/reference/config-api/kubernetes-audit.v1/
  - https://kubernetes.io/docs/reference/access-authn-authz/auditing/
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-3-a_0231
- rule_id: k8s.audit.policy.metadata_capture
  service: audit
  resource: policy
  requirement: Metadata Capture
  scope: audit.policy.metadata_capture
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Comprehensive Metadata Capture in Audit Policies
  rationale: Failure to configure comprehensive metadata capture in Kubernetes audit policies can result in incomplete audit
    logs, making it difficult to trace unauthorized access or configuration changes. Attackers could exploit this by masking
    their activities, leading to potential data breaches or system compromise without detection.
  description: This rule checks that the Kubernetes audit policy is set to capture comprehensive metadata for all relevant
    events, including user identities, request parameters, and response statuses. A well-configured audit policy ensures that
    all actions taken within the cluster are logged with sufficient detail, allowing for effective monitoring, incident response,
    and forensic analysis. This helps detect unauthorized access, configuration changes, and potential exploits, thereby enhancing
    the security posture of the cluster.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  - https://kubernetes.io/docs/concepts/policy/audit-logging/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-3_0025
  - fedramp_moderate_multi_cloud_AU-6_0065
  - fedramp_moderate_multi_cloud_AU-6_1_0066
  - fedramp_moderate_multi_cloud_AU-6_3_0067
  - fedramp_moderate_multi_cloud_AU-6_4_0068
  - fedramp_moderate_multi_cloud_AU-6_5_0069
  - fedramp_moderate_multi_cloud_AU-6_6_0070
  - fedramp_moderate_multi_cloud_AU-6_7_0071
  - nist_800_53_rev5_multi_cloud_AU-12-a_0278
- rule_id: k8s.audit.policy.review_scheduled
  service: audit
  resource: policy
  requirement: Review Scheduled
  scope: audit.policy.review_scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Schedule Regular Reviews of Kubernetes Audit Policies
  rationale: If Kubernetes audit policies are not regularly reviewed and updated, it may lead to insufficient logging of critical
    events, allowing potential security incidents to go undetected. Attackers could exploit this by conducting malicious activities
    without being noticed, as outdated audit configurations might miss logging unauthorized access or changes.
  description: This rule checks that Kubernetes audit policies are configured to be reviewed and updated regularly. A well-scheduled
    review process ensures that audit policies remain effective and aligned with evolving security threats and compliance
    requirements. Properly configured audit policies help in identifying unauthorized access attempts, privilege escalations,
    and other suspicious activities by ensuring all relevant actions are logged. Regularly reviewing these policies ensures
    that the audit logs are comprehensive and the configurations reflect the current security posture and compliance needs.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/audit/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_171_r2_multi_cloud_3_3_3_3.3.3_Review_and_update_logged_events_0033
- rule_id: k8s.audit.policy.source_capture_enabled
  service: audit
  resource: policy
  requirement: Source Capture Enabled
  scope: audit.policy.source_capture_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Source IP Capture in Kubernetes Audit Policy
  rationale: Without source IP capture in audit logs, it becomes challenging to trace unauthorized access attempts or identify
    the origin of malicious activities. This can lead to undetected breaches or inadequate response to security incidents,
    as source information is crucial for forensic analysis and incident response.
  description: This control checks if the Kubernetes audit policy is configured to capture the source IP address of requests.
    Proper configuration ensures that each API request's origin is logged, aiding in monitoring, detecting, and analyzing
    unauthorized or suspicious activities. A well-configured audit policy with source capture helps organizations adhere to
    security best practices and regulatory requirements, thereby reducing the risk of undetected intrusion and enhancing incident
    response capabilities.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_53_rev5_multi_cloud_AU-3-d_0234
- rule_id: k8s.audit.policy.system_level_event_logging
  service: audit
  resource: policy
  requirement: System Level Event Logging
  scope: audit.policy.system_level_event_logging
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable Detailed System Level Event Logging in Kubernetes Audit Policy
  rationale: Without detailed system level event logging, detecting unauthorized access and abnormal behavior becomes challenging.
    Attackers can exploit this lack of visibility to perform actions unnoticed, such as privilege escalation or data exfiltration.
    Comprehensive logging is critical for forensic analysis and timely incident response.
  description: This control verifies that the Kubernetes audit policy is configured to log detailed system level events, including
    access to sensitive resources, changes to configurations, and administrative actions. A well-configured audit policy should
    capture events such as 'create', 'update', 'delete', and 'patch' for all significant API resources. This logging helps
    in identifying suspicious activities, providing a trail for security investigations, and ensuring compliance with security
    standards like CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - pci_dss_v4_multi_cloud_10.2.1.7_0150
- rule_id: k8s.audit.policy.timestamps_included
  service: audit
  resource: policy
  requirement: Timestamps Included
  scope: audit.policy.timestamps_included
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Audit Logs Include Precise Timestamps
  rationale: Without precise timestamps in audit logs, it becomes challenging to accurately reconstruct events, detect anomalies,
    or correlate security incidents across systems. This can hinder incident response efforts and leave the system vulnerable
    to undetected malicious activities. Attackers could exploit this lack of traceability to obscure their actions within
    the cluster.
  description: This rule verifies that Kubernetes audit logs are configured to include precise timestamps for each recorded
    event. A well-configured audit policy should capture the metadata necessary to track and analyze user and system activity
    over time. Including timestamps in audit logs provides a chronological order to the events, which is crucial for forensic
    analysis, troubleshooting, and compliance with security standards. This configuration reduces the risk of undetected security
    breaches and aids in maintaining a secure cluster environment.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-annotations
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AU-8_0030
- rule_id: k8s.audit.risk.analysis_policy_configured
  service: audit
  resource: risk
  requirement: Analysis Policy Configured
  scope: audit.risk.analysis_policy_configured
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Audit Policy for Security Compliance
  rationale: Without a properly configured audit policy, malicious activities could go undetected, leading to potential unauthorized
    access and data breaches. Attackers may exploit unmonitored actions to escalate privileges or exfiltrate sensitive data.
    A comprehensive audit trail is crucial for identifying suspicious behavior and ensuring accountability.
  description: This control checks whether a Kubernetes audit policy is configured to capture critical events. A well-defined
    audit policy specifies which events get logged, such as access to sensitive resources and configuration changes, ensuring
    that any deviations from expected behavior are detected promptly. By monitoring these events, organizations can reduce
    security risks, respond quickly to incidents, and demonstrate compliance with standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - hipaa_multi_cloud_164_308_a_1_ii_a_0001
- rule_id: k8s.audit.software.installation_logging_enabled
  service: audit
  resource: software
  requirement: Installation Logging Enabled
  scope: audit.software.installation_logging_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable Audit Logging for Software Installation Events
  rationale: Without audit logging for software installation events, unauthorized software changes may go undetected, increasing
    the risk of introducing vulnerabilities or malware. Attackers could exploit this lack of visibility to install malicious
    software without triggering alerts, compromising the integrity and security of the Kubernetes environment.
  description: This control checks that audit logging is enabled and properly configured to capture software installation
    events within the Kubernetes environment. Properly configured audit logs should record all software installation attempts,
    capturing details such as who initiated the installation, what was installed, and when it occurred. This information is
    crucial for forensic investigations and ensuring compliance with security standards. It helps detect unauthorized changes
    and potential security breaches, thereby enhancing the overall security posture.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_171_r2_multi_cloud_3_4_9_3.4.9_Control_and_monitor_user-installed_so_0041
- rule_id: k8s.audit.supplier.service_changes_logged
  service: audit
  resource: supplier
  requirement: Service Changes Logged
  scope: audit.supplier.service_changes_logged
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Log and Monitor All Kubernetes API Server Requests
  rationale: If API server requests are not logged, unauthorized changes can occur without detection, allowing attackers to
    modify resources or escalate privileges unnoticed. This can lead to data breaches, service disruptions, or compliance
    violations.
  description: This rule ensures that all Kubernetes API server requests, especially those involving changes to cluster resources,
    are logged. It validates that audit logging is enabled with a focus on capturing create, update, and delete operations.
    Proper logging provides visibility into cluster activities, aiding in the detection of suspicious actions and facilitating
    forensic investigations. A good configuration includes specifying audit policies that capture and store logs in a secure,
    tamper-proof location, ensuring that all relevant events are monitored and retained according to compliance requirements.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#audit-logging
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  compliance:
  - iso27001_2022_multi_cloud_A.5.22_0019
- rule_id: k8s.audit.threat.detection_enabled
  service: audit
  resource: threat
  requirement: Detection Enabled
  scope: audit.threat.detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable and Configure Kubernetes Audit Logging for Threat Detection
  rationale: Failure to enable and properly configure Kubernetes audit logging can result in undetected security breaches,
    as audit logs are essential for identifying unauthorized access and malicious activities. Without comprehensive logging,
    attackers can exploit vulnerabilities or misconfigurations within the cluster without being detected, leading to potential
    data breaches or disruption of services.
  description: This control ensures that Kubernetes auditing is enabled and configured accurately to capture security-relevant
    activities within the cluster. It validates that audit logs are being generated for key actions, such as authentication
    events, administrative operations, and access attempts to sensitive resources. A properly configured audit logging mechanism
    allows for monitoring and forensic investigation, helping to identify and mitigate potential threats. Good configuration
    includes specifying a comprehensive audit policy, securely storing logs, and integrating them with a monitoring solution
    to alert on suspicious activities.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/security/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - iso27001_2022_multi_cloud_A.5.7_0039
  - nist_800_53_rev5_multi_cloud_RA-10-a_1029
- rule_id: k8s.audit.user.activity_logging_enabled
  service: audit
  resource: user
  requirement: Activity Logging Enabled
  scope: audit.user.activity_logging_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable and Configure Kubernetes Audit Logging for User Activities
  rationale: Without proper audit logging, malicious activities or unauthorized access attempts by users may go undetected,
    leading to potential breaches. Attackers could exploit this lack of visibility to escalate privileges or extract sensitive
    data without leaving a trace.
  description: This rule checks if audit logging for user activities is enabled and properly configured in Kubernetes. A well-configured
    audit logging setup captures detailed records of user actions, including access to resources and modification events.
    This helps in identifying suspicious activities, supports forensic investigations, and ensures accountability. A proper
    configuration involves defining an audit policy that specifies which events to log and ensuring logs are securely stored
    and monitored.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - pci_dss_v4_multi_cloud_8.2.6_0104
- rule_id: k8s.autoscaling.configured.for_workloads
  service: autoscaling
  resource: configured
  requirement: For Workloads
  scope: autoscaling.configured.for_workloads
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Horizontal Pod Autoscaler is Configured for Workloads
  rationale: Without properly configured autoscaling, workloads may become vulnerable to denial-of-service attacks due to
    resource exhaustion. Misconfigured scaling can also lead to inefficiencies that increase costs or reduce application availability,
    making the system susceptible to failure under load or attack.
  description: This rule checks that the Horizontal Pod Autoscaler (HPA) is configured for Kubernetes workloads according
    to security and operational best practices. Proper configuration of HPA ensures workloads can scale based on demand, reducing
    the risk of resource exhaustion and ensuring availability during peak loads. This involves verifying the correct metric
    targets are set, such as CPU and memory utilization, and ensuring that scaling policies are defined to prevent rapid scaling
    that could destabilize systems.
  references:
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#horizontalpodautoscaler-v2beta1-autoscaling
  compliance:
  - soc2_multi_cloud_a1_1_0024
- rule_id: k8s.certificate.expiration.check
  service: certificate
  resource: expiration
  requirement: Check
  scope: certificate.expiration.check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Timely Renewal of Kubernetes Certificates
  rationale: Expired Kubernetes certificates can lead to service outages and security vulnerabilities, as they are no longer
    valid for authenticating entities within the cluster. Attackers may exploit expired certificates to intercept or manipulate
    traffic, potentially leading to unauthorized access and data breaches. Ensuring certificates are renewed before expiration
    mitigates these risks and maintains a secure and functional cluster environment.
  description: This control checks that all Kubernetes certificates are configured to be monitored for expiration and timely
    renewal. It validates that automated processes are in place to renew certificates before they expire. A well-configured
    certificate management system reduces the risk of service disruptions and unauthorized access, thereby enhancing security
    posture and ensuring compliance with industry standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/
  compliance:
  - pci_dss_v4_multi_cloud_3.6.1.1_0045
  - pci_dss_v4_multi_cloud_4.1.1_0058
- rule_id: k8s.certificate.pod.expiration_configured
  service: certificate
  resource: pod
  requirement: Expiration Configured
  scope: certificate.pod.expiration_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pod Certificate Expiration Is Configured
  rationale: Configuring certificate expiration for pod communications is crucial as it limits the lifespan of credentials.
    Without expiration, certificates can be exploited indefinitely if compromised, allowing attackers to maintain unauthorized
    access to cluster resources. This misconfiguration increases the risk of man-in-the-middle attacks and data breaches.
  description: This control checks that certificates used by Kubernetes pods have an expiration date set, ensuring that these
    credentials are not valid indefinitely. Proper configuration involves setting a reasonable expiration period that balances
    operational requirements with security. By enforcing certificate expiration, organizations can automatically rotate credentials,
    reducing the risk of credential exposure and ensuring that any compromised credentials have a limited lifespan. This approach
    aligns with best practices such as the CIS Kubernetes Benchmark, which recommend regular certificate renewal to maintain
    security posture.
  references:
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance: []
- rule_id: k8s.cluster.autoscaler.enabled
  service: cluster
  resource: autoscaler
  requirement: Enabled
  scope: cluster.autoscaler.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes Cluster Autoscaler is Enabled and Configured Correctly
  rationale: An improperly configured or disabled Kubernetes Cluster Autoscaler can lead to inefficient resource management,
    resulting in service disruption, increased attack surface due to over-provisioned resources, and potential denial of service.
    Enabling and properly configuring the autoscaler ensures that the cluster scales in response to workload demands, minimizing
    unused resources and reducing the risk of attacks exploiting idle resources.
  description: This control checks if the Kubernetes Cluster Autoscaler is enabled and configured according to best practices.
    A correctly configured autoscaler dynamically adjusts the number of nodes in a cluster based on workload demand, ensuring
    efficient resource utilization and cost management. This reduces the risk of security incidents by limiting the resources
    available for potential exploitation and ensuring that scaling decisions are made securely. Proper configuration includes
    setting appropriate scaling policies and limits that align with the organization's security requirements and workload
    characteristics.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-6_1193
  - soc2_multi_cloud_a1_1_0024
- rule_id: k8s.cluster.backup.and_restore_procedures_documented
  service: cluster
  resource: backup
  requirement: And Restore Procedures Documented
  scope: cluster.backup.and_restore_procedures_documented
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Documented Backup and Restore Procedures for Kubernetes Clusters
  rationale: Without documented backup and restore procedures, a Kubernetes cluster is vulnerable to data loss and prolonged
    downtime in the event of a disaster or malicious attack. This can lead to a loss of critical data and compromise the availability
    of applications running within the cluster. Documented procedures ensure that backups are consistently performed and can
    be reliably restored, minimizing recovery time and maintaining business continuity.
  description: This rule checks that backup and restore procedures for Kubernetes clusters are thoroughly documented. Proper
    documentation includes a detailed plan for regular backups, the steps required to restore from a backup, and testing of
    these procedures to ensure effectiveness. By having these procedures documented, organizations can quickly respond to
    incidents, reduce the risk of data loss, and ensure compliance with standards that require data integrity and availability.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#backing-up-etcd
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#restoring-a-cluster
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-2_0053
- rule_id: k8s.cluster.backup.and_restore_tested
  service: cluster
  resource: backup
  requirement: And Restore Tested
  scope: cluster.backup.and_restore_tested
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regularly Test Kubernetes Cluster Backup and Restore Procedures
  rationale: Failure to regularly test backup and restore procedures can lead to data loss and extended downtime during recovery
    from incidents such as cyber attacks, accidental deletions, or system failures. Without verified backup and restore processes,
    clusters are vulnerable to prolonged disruptions, risking data integrity and availability, which could be exploited by
    attackers to cause operational chaos or data breaches.
  description: This rule checks that Kubernetes clusters have backup and restore procedures that are not only in place but
    are also regularly tested. A successful configuration ensures that all critical data and configurations are recoverable
    and that the restoration process is efficient and reliable. By simulating failure scenarios and validating restoration
    steps, organizations can identify and rectify potential weaknesses in their backup strategies, thus bolstering their disaster
    recovery capabilities and compliance with regulatory demands such as HIPAA.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#backup
  - https://kubernetes.io/docs/tasks/administer-cluster/disaster-recovery/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#etcd-backup-and-restore
  compliance:
  - hipaa_multi_cloud_164_308_a_7_i_0017
  - hipaa_multi_cloud_164_308_a_7_ii_a_0018
  - hipaa_multi_cloud_164_308_a_7_ii_b_0019
  - nist_800_53_rev5_multi_cloud_CP-2-a_0462
  - rbi_bank_multi_cloud_6.2_0022
- rule_id: k8s.cluster.cluster.autoscaler_enabled
  service: cluster
  resource: cluster
  requirement: Autoscaler Enabled
  scope: cluster.cluster.autoscaler_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Cluster Autoscaler is Enabled and Configured
  rationale: Without an enabled and properly configured cluster autoscaler, a Kubernetes cluster may waste resources or become
    under-provisioned, leading to potential denial of service. An improperly scaled cluster can be vulnerable to resource
    exhaustion attacks, where an attacker may attempt to induce excessive load to degrade service or cause outages.
  description: This control checks that the Kubernetes cluster has the autoscaler enabled and properly configured to adjust
    the number of nodes according to the workload demand. A well-configured autoscaler helps maintain optimal resource utilization,
    ensuring that the cluster can handle varying loads without manual intervention. This reduces the risk of resource exhaustion
    and improves the cluster's resilience against load-based attacks.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaler
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/
  compliance: []
- rule_id: k8s.cluster.cluster.component_network_isolation_enforced
  service: cluster
  resource: cluster
  requirement: Component Network Isolation Enforced
  scope: cluster.cluster.component_network_isolation_enforced
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Enforce Network Policies for Component Isolation
  rationale: Without network policies, Kubernetes components might inadvertently communicate, exposing the cluster to potential
    lateral movement by attackers. Misconfigured network isolation can lead to unauthorized access to sensitive components
    and data, increasing the risk of data breaches and denial-of-service attacks.
  description: This rule verifies that network policies are configured to enforce isolation between Kubernetes components,
    ensuring that only specified traffic is allowed. Proper network isolation prevents unauthorized communication between
    pods and services, mitigating risks such as data exfiltration and privilege escalation. A well-configured network policy
    will define ingress and egress rules that limit traffic to only what is necessary for application functionality, adhering
    to the principle of least privilege.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/
  compliance: []
- rule_id: k8s.cluster.daemonset.proxy_configuration_secured_enabled
  service: cluster
  resource: daemonset
  requirement: Proxy Configuration Secured Enabled
  scope: cluster.daemonset.proxy_configuration_secured_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Secure Proxy Configuration in DaemonSets
  rationale: Improperly configured proxy settings in DaemonSets can expose sensitive data and increase the risk of man-in-the-middle
    attacks. Attackers could intercept or alter traffic, leading to data breaches or unauthorized access. Secure proxy configuration
    is essential to ensure data integrity and confidentiality when routing traffic through proxies.
  description: This rule checks that all DaemonSets in the Kubernetes cluster have secure proxy configurations. It verifies
    that the proxy settings include encryption and authentication mechanisms that comply with security best practices. A secure
    configuration ensures that all communication routed through the proxy is protected against interception and tampering,
    thereby maintaining the integrity and confidentiality of data. Properly configured proxies also help comply with security
    standards and benchmarks.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/
  compliance: []
- rule_id: k8s.cluster.disaster.recovery_plan_documented
  service: cluster
  resource: disaster
  requirement: Recovery Plan Documented
  scope: cluster.disaster.recovery_plan_documented
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: low
  title: Document and Maintain Kubernetes Disaster Recovery Plan
  rationale: A poorly documented or non-existent disaster recovery plan can lead to prolonged downtime and data loss in the
    event of a catastrophic failure. Attackers can exploit such gaps to disrupt operations further or access sensitive data
    during chaotic recovery efforts. Proper documentation ensures quick recovery and mitigates the impact of attacks such
    as ransomware or distributed denial-of-service (DDoS).
  description: This control verifies the existence and regular maintenance of a documented disaster recovery plan specific
    to Kubernetes clusters. A well-documented plan includes detailed procedures for backup and restoration, testing protocols,
    and designated roles and responsibilities. This ensures that in the event of a disaster, all team members can quickly
    and effectively restore the cluster to its operational state, minimizing downtime and data loss. Proper documentation
    and regular testing of the recovery plan help in adhering to industry standards and regulatory requirements.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/disaster-recovery/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/
  - https://kubernetes.io/docs/concepts/architecture/cloud-controller/
  compliance:
  - hipaa_multi_cloud_164_308_a_7_ii_c_0020
- rule_id: k8s.cluster.etcd.backup_encryption_enabled_enforced
  service: cluster
  resource: etcd
  requirement: Backup Encryption Enabled Enforced
  scope: cluster.etcd.backup_encryption_enabled_enforced
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure Etcd Backup Encryption is Enabled
  rationale: Etcd stores all cluster data, including secrets and configuration details. If backup encryption is misconfigured,
    sensitive data may be exposed during backup processes, making it vulnerable to unauthorized access or interception by
    attackers who exploit unsecured backups.
  description: This control checks whether encryption is enabled for etcd backups in a Kubernetes cluster. A properly configured
    backup encryption ensures that the data is encrypted at rest, protecting it from unauthorized access and potential data
    breaches. A good configuration involves using strong encryption standards and regularly verifying that encryption is consistently
    applied. By enforcing encryption on etcd backups, the risk of data exposure is minimized, thereby enhancing the overall
    security posture of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/concepts/secret/
  compliance: []
- rule_id: k8s.cluster.etcd.etcd_peer_tls_enabled_enforced
  service: cluster
  resource: etcd
  requirement: Etcd Peer Tls Enabled Enforced
  scope: cluster.etcd.etcd_peer_tls_enabled_enforced
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce TLS for Etcd Peer Communication
  rationale: Etcd is the key-value store for Kubernetes, holding all cluster data. If TLS is not enforced for peer communication,
    it exposes the cluster to man-in-the-middle attacks, where an attacker could intercept and manipulate sensitive data.
    This misconfiguration can lead to unauthorized data access, integrity issues, and potential cluster compromise.
  description: This control checks that TLS is enforced for communication between etcd peers in a Kubernetes cluster. A correctly
    configured etcd will have peer certificates and keys specified in its configuration, ensuring all peer-to-peer traffic
    is encrypted. This prevents eavesdropping and tampering by unauthorized parties, thus maintaining data confidentiality
    and integrity within the cluster.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-etcd
  - https://etcd.io/docs/v3.5/op-guide/security/#data-encryption
  - https://kubernetes.io/docs/setup/best-practices/certificates/#configure-certificates-for-etcd-components
  compliance: []
- rule_id: k8s.cluster.high.availability_configured
  service: cluster
  resource: high
  requirement: Availability Configured
  scope: cluster.high.availability_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure High Availability for Kubernetes Control Plane
  rationale: High availability (HA) configurations mitigate the risk of single points of failure in the Kubernetes control
    plane, which could lead to cluster downtime. Without HA, a failure in a control plane component could disrupt critical
    services and workloads, potentially exposing the cluster to security vulnerabilities and operational risks.
  description: This rule checks that the Kubernetes control plane is configured for high availability by ensuring multiple
    instances of critical components like etcd, the API server, controller manager, and scheduler are distributed across different
    nodes. This setup increases resilience against failures and supports continuous operation. Proper high availability configuration
    ensures that the cluster can sustain node or component failures without service interruption, thereby maintaining security
    and operational integrity.
  references:
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
  - https://kubernetes.io/docs/tasks/administer-cluster/highly-available-control-plane/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-6_1193
- rule_id: k8s.cluster.kubelet.kubelet_authentication_enabled_enforced
  service: cluster
  resource: kubelet
  requirement: Kubelet Authentication Enabled Enforced
  scope: cluster.kubelet.kubelet_authentication_enabled_enforced
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Kubelet Authentication to Prevent Unauthorized Access
  rationale: Without proper kubelet authentication, unauthorized entities could gain access to the kubelet API, allowing them
    to execute arbitrary commands on the nodes, retrieve sensitive information, or disrupt workloads. This misconfiguration
    could lead to privilege escalation, data breaches, and denial of service attacks.
  description: "This control checks if kubelet authentication is enabled on all nodes in the Kubernetes cluster. Proper configuration\
    \ requires setting up authentication mechanisms like client certificates, bearer tokens, or authentication plugins. Enforcing\
    \ kubelet authentication ensures that only authorized users or services can interact with the kubelet API, thereby reducing\
    \ the risk of unauthorized access that could exploit the node\u2019s resources or data."
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/architecture/nodes/#kubelet
  compliance: []
- rule_id: k8s.cluster.namespace.high_availability_enabled
  service: cluster
  resource: namespace
  requirement: High Availability Enabled
  scope: cluster.namespace.high_availability_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Namespace Quotas and Limits for High Availability
  rationale: Without properly configured quotas and limits, a namespace can consume excessive resources, leading to instability
    or downtime of critical services. This can be exploited by attackers to cause Denial of Service (DoS) by overloading the
    cluster, affecting its availability and reliability.
  description: This rule checks that each namespace has resource quotas and limit ranges configured to ensure high availability
    of applications within the cluster. A well-defined quota and limit range configuration prevents any single namespace from
    monopolizing cluster resources, thus maintaining balance and availability across all applications. Proper configuration
    includes setting CPU, memory, and storage limits which help mitigate risks such as resource exhaustion attacks.
  references:
  - https://kubernetes.io/docs/concepts/policy/resource-quotas/
  - https://kubernetes.io/docs/concepts/policy/limit-range/
  - https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/
  compliance: []
- rule_id: k8s.cluster.namespace.plane_patch_enforced
  service: cluster
  resource: namespace
  requirement: Plane Patch Enforced
  scope: cluster.namespace.plane_patch_enforced
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure Namespace-Level Security Policies are Up-to-Date
  rationale: Outdated or unpatched namespace configurations can expose clusters to vulnerabilities, allowing attackers to
    exploit known security weaknesses. Attack vectors include privilege escalation, data exfiltration, or denial of service
    attacks by exploiting unpatched APIs or components.
  description: This rule checks that all namespaces within the Kubernetes cluster have the latest security patches applied
    to their associated resources, such as network policies and RBAC roles. By ensuring namespaces are configured with the
    latest security updates, clusters are protected against known vulnerabilities and unauthorized access is minimized. A
    compliant configuration follows Kubernetes best practices and industry standards, reducing the attack surface and aligning
    with the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#upgrading-a-cluster
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.cluster.namespace.review_scheduled
  service: cluster
  resource: namespace
  requirement: Review Scheduled
  scope: cluster.namespace.review_scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regular Namespace Security Review Scheduling
  rationale: Without regular reviews of namespace configurations, there's a risk of unnoticed security misconfigurations that
    attackers can exploit. These might include overly permissive access controls or outdated policies that could lead to unauthorized
    access or privilege escalation.
  description: This check ensures that a schedule is set for regular reviews of all namespace configurations in the Kubernetes
    cluster. Proper scheduling allows for timely identification and remediation of security issues, such as misconfigured
    network policies or improper resource quotas. A well-configured review schedule ensures namespaces comply with security
    standards and reduces the risk of breaches by maintaining updated policies.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  - https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/
  - https://kubernetes.io/docs/concepts/policy/resource-quotas/
  compliance: []
- rule_id: k8s.cluster.namespace.supported_version_configured
  service: cluster
  resource: namespace
  requirement: Supported Version Configured
  scope: cluster.namespace.supported_version_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Namespace Uses Supported Kubernetes API Versions
  rationale: Using unsupported API versions in a Kubernetes cluster can lead to vulnerabilities due to unpatched security
    flaws and compatibility issues. Attackers might exploit deprecated features or unmaintained APIs to gain unauthorized
    access or escalate privileges within the cluster. Ensuring namespaces are configured with supported API versions is crucial
    to mitigate these risks and maintain a secure and stable environment.
  description: This control checks that all namespaces in the Kubernetes cluster are configured to use supported API versions.
    It validates against the list of deprecated and removed API versions as per the official Kubernetes deprecation policy.
    Proper configuration ensures that namespaces leverage up-to-date and secure API functionalities, reducing exposure to
    known vulnerabilities and ensuring compatibility with other Kubernetes components. This practice aligns with security
    best practices and compliance frameworks that emphasize using supported software versions.
  references:
  - https://kubernetes.io/docs/reference/using-api/deprecation-policy/
  - https://kubernetes.io/docs/setup/release/version-skew-policy/
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  compliance: []
- rule_id: k8s.cluster.namespace.version_supported_configured
  service: cluster
  resource: namespace
  requirement: Version Supported Configured
  scope: cluster.namespace.version_supported_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Namespace Uses Supported Kubernetes API Versions
  rationale: Using unsupported Kubernetes API versions in namespaces exposes clusters to security vulnerabilities and potential
    compatibility issues. Unsupported versions may contain unpatched security flaws that attackers can exploit, leading to
    unauthorized access or data breaches. Keeping API versions updated mitigates these risks and ensures compatibility with
    security features and policies.
  description: This control checks whether namespaces within a Kubernetes cluster are configured to use supported API versions.
    A valid configuration ensures that namespaces are not relying on deprecated or obsolete API versions, which might lack
    critical security updates. This verification helps maintain a secure environment by aligning with current Kubernetes security
    practices and reducing the attack surface. A well-configured namespace with up-to-date API versions ensures adherence
    to industry standards, such as the CIS Kubernetes Benchmark, and minimizes vulnerabilities.
  references:
  - https://kubernetes.io/docs/reference/using-api/deprecation-policy/
  - https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance: []
- rule_id: k8s.cluster.node.tpm_verification_enabled_enforced
  service: cluster
  resource: node
  requirement: Tpm Verification Enabled Enforced
  scope: cluster.node.tpm_verification_enabled_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce TPM Verification on Cluster Nodes
  rationale: Without TPM verification, nodes may be susceptible to unauthorized access and tampering, such as firmware attacks
    or unauthorized hardware modifications. This can lead to potential data breaches and compromise of the entire cluster.
    Enforcing TPM verification helps ensure that only trusted hardware can join the cluster, mitigating risks associated with
    physical and firmware attacks.
  description: This control checks whether Trusted Platform Module (TPM) verification is enabled and enforced on all Kubernetes
    cluster nodes. Proper TPM configuration ensures that nodes can prove their integrity and trustworthiness before becoming
    part of the cluster. Verification is achieved by validating that nodes are equipped with TPM hardware and configured to
    use it for attestation. A correct setup prevents nodes with potentially compromised hardware from joining the cluster,
    thereby protecting against attacks that could exploit compromised hardware to gain unauthorized access or control.
  references:
  - https://kubernetes.io/docs/concepts/architecture/nodes/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-certificates
  compliance: []
- rule_id: k8s.cluster.pod.apiserver_insecure_bind_address_configured
  service: cluster
  resource: pod
  requirement: Apiserver Insecure Bind Address Configured
  scope: cluster.pod.apiserver_insecure_bind_address_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Insecure Bind Address for API Server
  rationale: Exposing the API server on an insecure bind address can lead to unauthorized access and data interception, as
    it allows unauthenticated, unencrypted connections. Attackers could exploit this to capture sensitive information or execute
    unauthorized commands on the cluster.
  description: This rule checks that the Kubernetes API server is not configured to bind to an insecure address, such as 0.0.0.0
    without TLS. A secure configuration ensures the API server only listens on secured endpoints, enforcing encrypted communication
    channels and authentication. This reduces the risk of man-in-the-middle attacks and unauthorized access, thus maintaining
    the integrity and confidentiality of the cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  compliance: []
- rule_id: k8s.cluster.pod.apiserver_profiling_disabled
  service: cluster
  resource: pod
  requirement: Apiserver Profiling Disabled
  scope: cluster.pod.apiserver_profiling_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Profiling in Kubernetes API Server
  rationale: Enabling profiling in the Kubernetes API server can expose sensitive performance metrics and debugging information
    that may be exploited by attackers to gain insights into the cluster's operations. This can lead to information leakage
    or serve as a precursor to targeted attacks. Profiling should be disabled to minimize the attack surface and protect against
    unauthorized access to potentially sensitive information.
  description: This control checks that the Kubernetes API server does not have profiling enabled. Profiling can be disabled
    by setting the `--profiling` flag to `false` in the API server configuration. Disabling profiling enhances security by
    preventing unauthorized users from accessing detailed performance data, which could be used to infer internal states or
    identify potential vulnerabilities. Properly configured, this setting aligns with security best practices and helps protect
    the cluster's core components from misuse.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-visibility-of-metrics
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance: []
- rule_id: k8s.cluster.pod.plane_auto_upgrade_enabled
  service: cluster
  resource: pod
  requirement: Plane Auto Upgrade Enabled
  scope: cluster.pod.plane_auto_upgrade_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Control Plane's Auto-Upgrades
  rationale: Without automated upgrades, Kubernetes control planes might run outdated versions, exposing clusters to known
    vulnerabilities and exploits. Attackers can leverage these weaknesses to gain unauthorized access or disrupt services.
    Ensuring that control planes are auto-upgraded reduces the risk of exploitation by keeping the environment up-to-date
    with security patches and improvements.
  description: This rule checks whether the Kubernetes control plane is configured to perform automatic upgrades. A control
    plane that is not set to auto-upgrade may miss critical security patches and updates, increasing the risk of vulnerabilities.
    By enabling auto-upgrades, organizations ensure that their control plane is always running the latest, most secure version,
    thus helping to maintain the integrity and security of the cluster. Good configuration involves setting the control plane
    to automatically upgrade to the latest stable versions as they become available.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/upgrade-cluster/
  - https://kubernetes.io/releases/version-skew-policy/
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/
  compliance: []
- rule_id: k8s.cluster.pod.scheduler_profiling_disabled
  service: cluster
  resource: pod
  requirement: Scheduler Profiling Disabled
  scope: cluster.pod.scheduler_profiling_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Scheduler Profiling in Kubernetes Clusters
  rationale: Leaving scheduler profiling enabled can expose internal metrics and operational details about the Kubernetes
    scheduler, which may be exploited by attackers to perform reconnaissance and craft targeted attacks. By disabling scheduler
    profiling, the attack surface is reduced, minimizing the risk of unauthorized data exposure and enhancing the overall
    security posture of the cluster.
  description: This control checks that the profiling feature in the Kubernetes scheduler is disabled. Profiling is primarily
    used for debugging and can reveal sensitive operational information. A secure configuration ensures that the '--profiling'
    flag is set to 'false' in the scheduler configuration, preventing the exposure of profiling endpoints. This aligns with
    best security practices by reducing unnecessary information exposure and potential attack vectors.
  references:
  - https://kubernetes.io/docs/concepts/overview/components/#kube-scheduler
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-profiling-access
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/
  compliance:
  - cis_kubernetes_kubernetes_1.4.1_0059
- rule_id: k8s.cluster.resource.monitoring_enabled
  service: cluster
  resource: resource
  requirement: Monitoring Enabled
  scope: cluster.resource.monitoring_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable and Configure Kubernetes Cluster Resource Monitoring
  rationale: Without proper monitoring, anomalies and unauthorized activities within the Kubernetes cluster might go undetected,
    increasing the risk of data breaches and insider threats. Monitoring is essential for timely detection of threats, auditing
    cluster activities, and ensuring compliance with security policies.
  description: This control checks that Kubernetes cluster resource monitoring is enabled and properly configured according
    to security best practices. A comprehensive monitoring setup includes capturing logs and metrics for resource usage, network
    activities, and access patterns. This facilitates early detection of suspicious activities, aids in forensic analysis,
    and supports compliance with standards such as the CIS Kubernetes Benchmark. A well-configured monitoring system helps
    in identifying unusual patterns that could indicate a security incident, thus enabling proactive threat mitigation.
  references:
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - rbi_nbfc_multi_cloud_3.2_0018
  - rbi_nbfc_multi_cloud_6.4_0032
- rule_id: k8s.cluster.secret.enabled
  service: cluster
  resource: secret
  requirement: Enabled
  scope: cluster.secret.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Secrets Are Encrypted at Rest
  rationale: If secrets are not encrypted at rest, they are vulnerable to unauthorized access and data breaches. Attackers
    who gain access to the etcd database could extract sensitive information, leading to potential exposure of confidential
    data or gaining unauthorized access to resources within the cluster.
  description: This control checks that Kubernetes Secrets are encrypted at rest using a strong encryption mechanism. Proper
    configuration involves enabling encryption providers and configuring encryption keys in the kube-apiserver. Encryption
    at rest ensures that even if an attacker gains access to the etcd database, the secrets remain protected. The good configuration
    involves setting up the EncryptionConfiguration file and verifying that the kube-apiserver is correctly referencing it.
    This not only mitigates the risk of data exposure but also aligns with industry best practices and compliance requirements.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.cluster.server.anonymous_access_disabled_enforced
  service: cluster
  resource: server
  requirement: Anonymous Access Disabled Enforced
  scope: cluster.server.anonymous_access_disabled_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: critical
  title: Disable Anonymous Access to Kubernetes API Server
  rationale: If anonymous access is enabled, unauthenticated users can interact with the Kubernetes API server, potentially
    allowing attackers to gain unauthorized access to cluster resources. This can lead to data breaches, privilege escalation,
    and denial of service attacks. Disabling anonymous access ensures that only authenticated and authorized users can perform
    actions on the cluster.
  description: This control checks that the Kubernetes API server is configured to disable anonymous access by setting the
    '--anonymous-auth=false' flag. A secure configuration ensures that all API requests are authenticated, reducing the risk
    of unauthorized access and protecting sensitive data and resources within the cluster. Proper configuration aligns with
    security best practices and compliance standards, mitigating potential attack vectors such as unauthorized API requests.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#anonymous-requests
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-anonymous-access
  - https://kubernetes.io/docs/concepts/security/overview/#controlling-access-to-the-kubernetes-api
  compliance: []
- rule_id: k8s.cluster.server.audit_logging_enabled_enforced
  service: cluster
  resource: server
  requirement: Audit Logging Enabled Enforced
  scope: cluster.server.audit_logging_enabled_enforced
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enforce Kubernetes Audit Logging on API Server
  rationale: Without audit logging, malicious activities such as privilege escalation and unauthorized resource access may
    go undetected, increasing the risk of data breaches. Audit logs provide critical insights into API server activities,
    enabling security teams to detect anomalies and trace unauthorized actions back to their source.
  description: This rule checks that audit logging is enabled and properly configured on the Kubernetes API server. A good
    configuration involves specifying an appropriate audit policy file, which dictates which events are logged and at what
    level of detail. Proper audit logging helps in tracking access to the cluster, monitoring administrative actions, and
    maintaining a history of configuration changes, thereby significantly enhancing security by facilitating incident detection
    and response.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/setup/best-practices/audit-policy/
  compliance: []
- rule_id: k8s.cluster.service.account_credentials_enabled
  service: cluster
  resource: service
  requirement: Account Credentials Enabled
  scope: cluster.service.account_credentials_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Service Account Tokens Are Not Auto-Mounted
  rationale: Service accounts in Kubernetes automatically mount their tokens into pods by default, which can be exploited
    if a pod is compromised. Attackers can use these tokens to escalate privileges, access sensitive data, or interfere with
    cluster operations. By preventing the automatic mounting of service account tokens, you reduce the risk of token theft
    and unauthorized access within the cluster.
  description: 'This rule checks whether the Kubernetes configuration disables the automatic mounting of service account tokens
    in pods, unless explicitly needed. A proper configuration involves setting `automountServiceAccountToken: false` in the
    pod or service account specification. Ensuring that service account tokens are not auto-mounted by default minimizes exposure
    to unnecessary credentials, helping to protect the cluster from potential privilege escalation and unauthorized access
    activities.'
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#automatically-mounting-api-credentials-for-a-service-account
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
  compliance: []
- rule_id: k8s.cluster.supported.version
  service: cluster
  resource: supported
  requirement: Version
  scope: cluster.supported.version
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes Clusters Run Supported and Secure Versions
  rationale: Running outdated or unsupported Kubernetes versions exposes the cluster to known vulnerabilities and exploits,
    as these versions no longer receive security patches. Attackers can exploit these vulnerabilities to gain unauthorized
    access, disrupt services, or exfiltrate data. Keeping the cluster up-to-date mitigates these risks and ensures alignment
    with security best practices.
  description: This control checks that the Kubernetes cluster is running a version that is currently supported by the Kubernetes
    community, which typically includes the last three minor releases. A supported version ensures that the cluster receives
    timely security patches and updates, reducing the risk of exploitation from known vulnerabilities. A good configuration
    is one where the cluster version is regularly reviewed and upgraded as newer, secure versions become available. This practice
    aids in maintaining the integrity and confidentiality of the cluster environments.
  references:
  - https://kubernetes.io/releases/version-skew-policy/
  - https://kubernetes.io/docs/setup/release/version-skew-policy/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
  compliance:
  - pci_dss_v4_multi_cloud_6.2.4_0079
- rule_id: k8s.cluster.version.supported
  service: cluster
  resource: version
  requirement: Supported
  scope: cluster.version.supported
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes Cluster Uses Supported Versions
  rationale: Using an unsupported Kubernetes version increases the risk of exposure to unpatched vulnerabilities, which can
    be exploited by attackers to gain unauthorized access or disrupt services. Attack vectors include known security flaws
    that aren't addressed after a version goes out of support, potentially leading to data breaches or service outages.
  description: This control checks that the Kubernetes cluster is running a version that is actively supported by the Kubernetes
    community or your cloud provider. Supported versions receive security updates and patches, reducing the risk of exploitation
    from known vulnerabilities. A secure configuration ensures that the cluster is protected against common threats, adheres
    to compliance requirements, and benefits from performance and feature improvements included in newer releases.
  references:
  - https://kubernetes.io/releases/version-skew-policy/#supported-versions
  - https://kubernetes.io/releases/
  - https://kubernetes.io/docs/setup/release/version-skew-policy/
  compliance:
  - pci_dss_v4_multi_cloud_6.2.1_0075
  - pci_dss_v4_multi_cloud_6.2.2_0076
  - pci_dss_v4_multi_cloud_6.2.3_0077
  - pci_dss_v4_multi_cloud_6.2.3.1_0078
- rule_id: k8s.cluster.version.up_to_date
  service: cluster
  resource: version
  requirement: Up To Date
  scope: cluster.version.up_to_date
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes Cluster Version is Current
  rationale: Running an outdated Kubernetes version can expose clusters to security vulnerabilities that have been addressed
    in newer releases. Attackers may exploit known vulnerabilities to gain unauthorized access, execute arbitrary code, or
    disrupt services. Keeping the cluster version up to date mitigates these risks by incorporating the latest security enhancements
    and patches.
  description: This control checks if the Kubernetes cluster is running the latest stable version or a version that is still
    supported by the community. Up-to-date versions include security patches and improvements, reducing the risk of exploitation
    through known vulnerabilities. A good configuration ensures that the cluster's version is not end-of-life and has all
    critical security updates applied. Regular updates help maintain compliance with security benchmarks, such as the CIS
    Kubernetes Benchmark, and support a robust security posture.
  references:
  - https://kubernetes.io/releases/
  - https://kubernetes.io/docs/setup/release/version-skew-policy/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-2-c_1344
- rule_id: k8s.configmap.baseline.policies_applied
  service: configmap
  resource: baseline
  requirement: Policies Applied
  scope: configmap.baseline.policies_applied
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce ConfigMap Policy Compliance for Security Baselines
  rationale: ConfigMaps, if improperly configured, can expose sensitive data and configurations, leading to potential unauthorized
    access or data leakage. Attackers may exploit misconfigured ConfigMaps to inject malicious configurations or to gain insights
    into the cluster's operational state, facilitating further attacks.
  description: This rule checks that all ConfigMaps adhere to defined security policies that align with best practices for
    secure configuration management. It ensures that any ConfigMap used within the cluster is compliant with established baseline
    security policies, reducing the risk of misconfiguration that could lead to unauthorized access. An ideal configuration
    involves strict access controls and policy validation that prevents unapproved changes, thereby maintaining cluster integrity
    and compliance with security frameworks.
  references:
  - https://kubernetes.io/docs/concepts/configuration/configmap/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - nist_800_171_r2_multi_cloud_3_4_1_3.4.1_Establish_and_maintain_baseline_confi_0037
- rule_id: k8s.configmap.configmap.baseline_policies_enforced
  service: configmap
  resource: configmap
  requirement: Baseline Policies Enforced
  scope: configmap.configmap.baseline_policies_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce ConfigMap Baseline Security Policies
  rationale: Misconfigured ConfigMaps can lead to unauthorized access to sensitive configuration data, potentially exposing
    the application to security threats such as data leaks, unauthorized modifications, and privilege escalation. By ensuring
    baseline security policies are enforced, you can mitigate risks associated with insecure data handling and ensure compliance
    with security best practices.
  description: This control verifies that ConfigMaps adhere to defined baseline security policies. It checks for secure access
    configurations, such as restricting ConfigMap access to only necessary namespaces and roles, ensuring that data within
    ConfigMaps is encrypted in transit, and validating that sensitive information is not stored in plaintext. Proper enforcement
    of these policies reduces the risk of unauthorized data exposure and manipulation, and aligns with industry standards
    like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/configmap/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.configmap.configmap.management_tool_integration_configured
  service: configmap
  resource: configmap
  requirement: Management Tool Integration Configured
  scope: configmap.configmap.management_tool_integration_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure ConfigMap Integration with Configuration Management Tools
  rationale: Without proper integration with configuration management tools, ConfigMaps may be prone to unauthorized changes
    or misconfigurations, potentially leading to configuration drift, security vulnerabilities, or non-compliance with security
    policies. Attackers may exploit improperly managed ConfigMaps to inject malicious configurations or exfiltrate sensitive
    data.
  description: This control checks whether a Kubernetes ConfigMap is integrated with an external configuration management
    tool, such as Helm or Ansible, ensuring that changes to ConfigMaps are tracked, audited, and compliant with security policies.
    Proper integration ensures that ConfigMaps are consistently managed and reduces the risk of unauthorized modifications.
    A well-configured integration allows for automated compliance checks and alerts, maintaining the integrity and security
    of the deployment.
  references:
  - https://kubernetes.io/docs/tasks/configmap-secret/managing-with-kubectl/
  - https://kubernetes.io/docs/concepts/configuration/configmap/
  - https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/
  compliance: []
- rule_id: k8s.configmap.management.tool_integration
  service: configmap
  resource: management
  requirement: Tool Integration
  scope: configmap.management.tool_integration
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Integrate Configuration Management Tools with Kubernetes ConfigMaps
  rationale: Without proper integration of configuration management tools, there is a heightened risk of misconfigurations
    and unauthorized changes to ConfigMaps. This can lead to exposure of sensitive data or service disruptions due to incorrect
    configurations. Attack vectors include unauthorized access and manipulation of ConfigMaps, which can lead to privilege
    escalation or service compromise.
  description: This rule checks whether Kubernetes ConfigMaps are managed and monitored using established configuration management
    tools that enforce best practices and security policies. A good configuration should ensure that only authorized tools
    and users can modify ConfigMaps, and all changes are logged and auditable. This helps in maintaining a secure environment
    by preventing unauthorized changes, reducing the risk of data leaks, and ensuring compliance with security standards.
  references:
  - https://kubernetes.io/docs/concepts/configuration/configmap/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/
  - https://kubernetes.io/docs/tasks/administer-cluster/enforce-standards-namespace/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-c_0399
- rule_id: k8s.controlplane.controller.manager_args_profiling_disabled
  service: controlplane
  resource: controller
  requirement: Manager Args Profiling Disabled
  scope: controlplane.controller.manager_args_profiling_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Profiling in Controller Manager
  rationale: If profiling is enabled in the Kubernetes Controller Manager, it can expose sensitive system performance data
    which could be exploited by attackers to understand system behavior and identify potential vulnerabilities. This exposure
    could lead to unauthorized access or denial of service attacks, compromising the security and availability of the cluster.
  description: This rule checks that the profiling feature is disabled in the Kubernetes Controller Manager by verifying that
    the '--profiling' argument is set to 'false'. Disabling profiling prevents unauthorized users from accessing detailed
    performance metrics and system data that could be used to craft attacks. Ensuring profiling is disabled helps in maintaining
    a minimal attack surface and aligns with security best practices, reducing the risk of information disclosure.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
  - https://kubernetes.io/docs/concepts/architecture/controller/
  compliance:
  - cis_kubernetes_kubernetes_1.3.2_0053
- rule_id: k8s.controlplane.controller.manager_bind_address_localhost_check
  service: controlplane
  resource: controller
  requirement: Manager Bind Address Localhost Check
  scope: controlplane.controller.manager_bind_address_localhost_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Bind Controller Manager to Localhost for Enhanced Security
  rationale: Binding the Kubernetes Controller Manager to localhost reduces the attack surface by limiting remote access to
    this critical component. If improperly configured, an attacker could gain unauthorized access to the control plane, potentially
    leading to cluster compromise, data exposure, or disruption of workloads.
  description: This check ensures that the Kubernetes Controller Manager is configured to bind only to the localhost interface.
    By doing so, it restricts access to the controller manager to the local machine, preventing remote exploitation of vulnerabilities.
    A correctly configured bind address enhances security by ensuring that only trusted, local system processes can interact
    with the Controller Manager, thereby complying with the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/architecture/controller/
  compliance:
  - cis_kubernetes_kubernetes_1.3.7_0058
- rule_id: k8s.controlplane.controller.manager_root_ca_file_verification
  service: controlplane
  resource: controller
  requirement: Manager Root Ca File Verification
  scope: controlplane.controller.manager_root_ca_file_verification
  domain: infrastructure_security
  subcategory: configuration_management
  severity: critical
  title: Verify Root CA File for Controller Manager
  rationale: A misconfigured root CA file in the Controller Manager can lead to vulnerabilities where malicious actors intercept
    or manipulate data between Kubernetes components. This configuration ensures that the Controller Manager uses a trusted
    CA, preventing man-in-the-middle attacks and unauthorized access to the Kubernetes API server. Without proper verification,
    attackers could impersonate services or eavesdrop on sensitive data.
  description: This control checks that the Kubernetes Controller Manager is configured with a valid and trusted root CA file
    for communication with the Kubernetes API server. A correct configuration involves specifying a CA file that includes
    trusted CA certificates, ensuring that any communication with the API server is secure and authenticated. This setup helps
    prevent unauthorized entities from gaining access to Kubernetes infrastructure by ensuring that all communication is encrypted
    and verified against a trusted certificate authority.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_1.3.5_0056
- rule_id: k8s.controlplane.controller.manager_rotate_kubelet_server_certificate_enabled
  service: controlplane
  resource: controller
  requirement: Manager Rotate Kubelet Server Certificate Enabled
  scope: controlplane.controller.manager_rotate_kubelet_server_certificate_enabled
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enable Kubelet Server Certificate Rotation
  rationale: Without enabling kubelet server certificate rotation, Kubernetes nodes are at risk of using stale or compromised
    certificates, which could be exploited by attackers to perform man-in-the-middle attacks, intercepting or altering data
    communicated between nodes and the control plane. This misconfiguration increases the risk of unauthorized access and
    data breaches.
  description: This rule checks that the Kubernetes controller manager is configured with --rotate-kubelet-server-certificate
    flag set to true. This ensures that the kubelet server certificates are automatically rotated, enhancing security by minimizing
    the risk of certificate compromise and maintaining up-to-date encryption credentials. Proper certificate rotation is a
    key aspect of securing communication channels and preventing unauthorized access to the cluster.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
  - https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/
  compliance:
  - cis_kubernetes_kubernetes_1.3.6_0057
- rule_id: k8s.controlplane.controller.manager_service_account_private_key_file_check
  service: controlplane
  resource: controller
  requirement: Manager Service Account Private Key File Check
  scope: controlplane.controller.manager_service_account_private_key_file_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Secure Configuration of Controller Manager Service Account Key File
  rationale: If the service account private key file for the Kubernetes controller manager is improperly configured, it could
    lead to unauthorized access to sensitive service account credentials. These credentials can be used by attackers to impersonate
    system components, allowing them to escalate privileges, access sensitive data, or disrupt operations. Ensuring the file
    is properly secured mitigates these risks.
  description: This check verifies that the Kubernetes controller manager's service account private key file is securely configured.
    A good configuration ensures that file permissions are restricted to the necessary users and processes, preventing unauthorized
    access. Specifically, the file should be readable only by the controller manager process, ensuring that attackers cannot
    leverage these keys to impersonate system components or escalate privileges. This aligns with best practices and compliance
    standards, reducing the risk of credential compromise.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - cis_kubernetes_kubernetes_1.3.3_0054
  - cis_kubernetes_kubernetes_1.3.4_0055
- rule_id: k8s.controlplane.controller.manager_terminated_pod_gc_threshold_check
  service: controlplane
  resource: controller
  requirement: Manager Terminated Pod Gc Threshold Check
  scope: controlplane.controller.manager_terminated_pod_gc_threshold_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Configure Terminated Pod Garbage Collection Threshold
  rationale: If the terminated pod garbage collection threshold is misconfigured, it can lead to excessive accumulation of
    terminated pods. This can increase resource usage on the control plane, potentially leading to denial-of-service conditions.
    An attacker could exploit this by repeatedly creating and terminating pods to exhaust system resources, thereby affecting
    cluster stability and availability.
  description: This rule checks that the 'terminated-pod-gc-threshold' parameter in the Kubernetes controller manager is set
    to a reasonable value. This parameter controls the maximum number of terminated pods that the garbage collector retains.
    A well-configured threshold prevents resource exhaustion by ensuring that terminated pods are cleaned up promptly, thus
    maintaining system stability and performance. Proper configuration aids in mitigating denial-of-service attacks by ensuring
    resources are not unnecessarily consumed by stale resources.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
  - https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/
  - https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/
  compliance:
  - cis_kubernetes_kubernetes_1.3.1_0052
- rule_id: k8s.controlplane.plane.auto_upgrade_enabled
  service: controlplane
  resource: plane
  requirement: Auto Upgrade Enabled
  scope: controlplane.plane.auto_upgrade_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Auto-Upgrades for Kubernetes Control Plane
  rationale: Without auto-upgrades, the Kubernetes control plane can fall behind on critical security patches and updates,
    leaving it vulnerable to known vulnerabilities. Attackers can exploit these vulnerabilities to gain unauthorized access,
    cause denial-of-service, or escalate privileges within the cluster.
  description: This control checks that auto-upgrade is enabled for the Kubernetes control plane, ensuring that the cluster
    receives the latest security patches and feature updates automatically. A properly configured auto-upgrade mechanism minimizes
    the window of exposure to vulnerabilities by promptly applying updates, thus maintaining the integrity, availability,
    and confidentiality of the cluster. It ensures compliance with security best practices and reduces the risk of attacks
    exploiting outdated software.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/
  - https://kubernetes.io/docs/concepts/cluster-administration/cluster-management/
  compliance:
  - rbi_bank_multi_cloud_21.2_0013
- rule_id: k8s.controlplane.plane.patch_compliance
  service: controlplane
  resource: plane
  requirement: Patch Compliance
  scope: controlplane.plane.patch_compliance
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure Control Plane Nodes are Regularly Patched
  rationale: Failure to regularly patch control plane nodes can lead to vulnerabilities being exploited by attackers, potentially
    resulting in unauthorized access, data breaches, and service disruptions. Unpatched systems are often targeted by attackers
    using known exploits to escalate privileges or execute arbitrary code.
  description: This rule checks that all control plane nodes in a Kubernetes cluster are up-to-date with the latest security
    patches. A good configuration includes enabling automatic updates or implementing a regular update schedule to promptly
    address known vulnerabilities. Keeping control plane nodes patched ensures they are protected against exploits targeting
    Kubernetes components and reduces the risk of compromise.
  references:
  - https://kubernetes.io/docs/setup/best-practices/cluster-large/#control-plane-node-configuration
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/
  compliance:
  - rbi_nbfc_multi_cloud_2.6_0010
  - rbi_nbfc_multi_cloud_5.2_0026
- rule_id: k8s.controlplane.proxy.metrics_bind_address_localhost
  service: controlplane
  resource: proxy
  requirement: Metrics Bind Address Localhost
  scope: controlplane.proxy.metrics_bind_address_localhost
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Metrics Endpoint to Localhost for Control Plane Proxy
  rationale: Exposing the metrics endpoint on non-local interfaces can allow unauthorized access to sensitive data. This misconfiguration
    can lead to information leakage and potential exploitation by attackers who can gather insights into cluster performance
    and state, which might be used to orchestrate further attacks.
  description: This rule checks that the Kubernetes control plane proxy's metrics endpoint is bound to the localhost interface.
    By ensuring the metrics are only accessible locally, you prevent unauthorized external access to sensitive operational
    data. A correct configuration involves setting the metrics bind address to '127.0.0.1'. This helps protect against unauthorized
    access and potential data leakage, aligning with security best practices and compliance standards such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  - https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_4.3.1_0099
- rule_id: k8s.controlplane.scheduler.profiling_disabled
  service: controlplane
  resource: scheduler
  requirement: Profiling Disabled
  scope: controlplane.scheduler.profiling_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Profiling on Kubernetes Scheduler
  rationale: Enabling profiling on the Kubernetes scheduler exposes endpoints that could be exploited by an attacker to gather
    sensitive performance data. This could lead to information disclosure vulnerabilities, where an attacker could gain insights
    into the system's performance characteristics and operational behavior, potentially aiding in further attacks or system
    disruptions.
  description: This check ensures that the profiling feature on the Kubernetes scheduler is disabled. Profiling is a tool
    primarily used for debugging and performance tuning, which, if inadvertently left enabled in a production environment,
    can expose internal metrics and system details. By validating that profiling is disabled, this control helps in minimizing
    the attack surface and reducing the risk of unauthorized access to sensitive information. A secure configuration is achieved
    by setting the '--profiling' flag to 'false' in the scheduler's component configuration.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/
  compliance: []
- rule_id: k8s.controlplane.server.audit_log_maxage_check
  service: controlplane
  resource: server
  requirement: Audit Log Maxage Check
  scope: controlplane.server.audit_log_maxage_check
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Ensure Audit Log MaxAge is Properly Configured
  rationale: Improper configuration of audit log retention can lead to insufficient logging data for forensic analysis in
    the event of a security incident. Attackers could exploit this by erasing traces of unauthorized access or other malicious
    activities if logs are not retained long enough for detection. Maintaining an adequate log retention period is crucial
    for compliance with security policies and regulatory requirements.
  description: This control checks that the 'maxAge' parameter for Kubernetes audit logs is configured to retain log entries
    for an appropriate duration, typically in line with organizational policies or regulatory requirements. A correctly configured
    'maxAge' ensures that audit logs are available for a sufficient period to facilitate thorough investigation and compliance
    audits. Proper configuration helps in tracking and identifying unauthorized access attempts, thereby enhancing the overall
    security posture of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance: []
- rule_id: k8s.controlplane.server.service_account_key_file_set
  service: controlplane
  resource: server
  requirement: Service Account Key File Set
  scope: controlplane.server.service_account_key_file_set
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Service Account Key Files Are Configured
  rationale: If the service account key files are not properly configured on the Kubernetes control plane server, it can lead
    to unauthorized access and potential privilege escalation. Attackers could exploit this misconfiguration to impersonate
    service accounts, gain unauthorized access to resources, or disrupt operations, compromising the cluster's security.
  description: This rule checks that the Kubernetes control plane server has the service account key file (`--service-account-key-file`)
    properly set. A correctly configured key file is crucial for the secure signing of service account tokens, which are used
    for authentication and authorization in Kubernetes. By ensuring that this key file is set and correctly configured, you
    reduce the risk of token forgery and unauthorized access, maintaining the integrity and security of the cluster.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/#service-account-token-secrets
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.disaster_recovery.recovery.plan_documented
  service: disaster_recovery
  resource: recovery
  requirement: Plan Documented
  scope: disaster_recovery.recovery.plan_documented
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Documented Disaster Recovery Plan for Kubernetes
  rationale: Without a documented disaster recovery plan, Kubernetes environments are vulnerable to prolonged downtime and
    data loss in the event of a failure. Attackers could exploit this lack of preparedness by orchestrating disruptions that
    take advantage of slow recovery processes, leading to potential breaches of availability and loss of critical data. This
    is particularly crucial for meeting compliance requirements and ensuring business continuity.
  description: This rule checks that a detailed disaster recovery plan for Kubernetes is documented and maintained. An effective
    plan should outline specific procedures for backup, recovery, and failover, ensuring that Kubernetes workloads can be
    restored quickly and securely after a disruption. Proper documentation helps coordinate response efforts, minimizes downtime,
    and ensures consistency in recovery operations. Additionally, it helps in auditing processes to meet compliance with industry
    standards like CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/disaster-recovery/
  - https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#disaster-recovery
  - https://kubernetes.io/docs/concepts/cluster-administration/backups/
  compliance:
  - hipaa_multi_cloud_164_308_a_7_ii_b_0019
  - rbi_bank_multi_cloud_6.2_0022
- rule_id: k8s.etcd.auto.tls_check
  service: etcd
  resource: auto
  requirement: Tls Check
  scope: etcd.auto.tls_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure etcd Has TLS Encryption Enabled
  rationale: Without TLS encryption, data transmitted between the etcd server and its clients can be intercepted and read
    by malicious actors. This could lead to unauthorized access to sensitive cluster data, including secrets and configuration
    details, thereby facilitating potential man-in-the-middle attacks or data breaches.
  description: This control checks that TLS is enabled for etcd communications, ensuring that all data in transit is encrypted
    and secured from eavesdropping. A properly configured TLS setup involves having both certificates and keys correctly installed
    and configured on etcd endpoints. This configuration mitigates the risk of data interception and unauthorized access,
    maintaining the confidentiality and integrity of the data stored in etcd.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-cluster-communication
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#secure-communication
  compliance:
  - cis_kubernetes_kubernetes_1.5.3_0163
- rule_id: k8s.etcd.backup.and_restore_tested
  service: etcd
  resource: backup
  requirement: And Restore Tested
  scope: etcd.backup.and_restore_tested
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regular Testing of ETCD Backup and Restore Procedures
  rationale: Without regular testing of etcd backup and restore procedures, a Kubernetes cluster may face significant downtime
    and data loss in the event of a failure. This can lead to service disruption, potential data breaches, and failure to
    meet compliance requirements. Attackers could exploit untested backup systems by corrupting or deleting data, knowing
    that restoration processes are ineffective.
  description: This control verifies that backup and restore procedures for etcd are tested regularly to ensure data recovery
    capabilities are intact. A valid configuration involves automated or manual testing of backup integrity and restoration
    processes. Confirming these procedures are operational helps mitigate risks of data loss, ensures business continuity,
    and maintains compliance with industry standards. Effective testing demonstrates that the system can recover from failures
    or attacks, preserving cluster state and functionality.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#etcd-backup-and-restore
  - https://kubernetes.io/docs/concepts/cluster-administration/backup-restore/
  compliance:
  - nist_800_53_rev5_multi_cloud_CP-2-e_0466
- rule_id: k8s.etcd.backup.configured
  service: etcd
  resource: backup
  requirement: Configured
  scope: etcd.backup.configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Scheduled and Secured etcd Backups
  rationale: etcd is the primary data store for Kubernetes, containing all cluster state and configuration. If not backed
    up properly, data loss can lead to extensive downtime and potential data breaches. Attack vectors include data corruption,
    accidental deletion, or a successful attack on etcd leading to the loss of critical configuration and secrets.
  description: This control checks that etcd backups are scheduled regularly and stored securely. A good configuration includes
    automated backups, encrypted backup data, and access controls to ensure that only authorized users can access the backup
    files. This helps protect against data loss from hardware failures, administrative errors, and malicious activities, thereby
    maintaining the integrity and availability of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/etcd-backup-restore/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-2_0053
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-6_0056
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-9_0059
  - fedramp_moderate_multi_cloud_CM-3_0105
  - fedramp_moderate_multi_cloud_CM-3_1_0106
  - fedramp_moderate_multi_cloud_CM-3_2_0107
  - fedramp_moderate_multi_cloud_CM-3_4_0108
  - fedramp_moderate_multi_cloud_CM-3_6_0109
  - fedramp_moderate_multi_cloud_CP-6_0146
  - fedramp_moderate_multi_cloud_CP-6_1_0147
  - fedramp_moderate_multi_cloud_CP-6_2_0148
  - fedramp_moderate_multi_cloud_CP-6_3_0149
  - fedramp_moderate_multi_cloud_CP-9_0160
  - fedramp_moderate_multi_cloud_CP-9_1_0161
  - fedramp_moderate_multi_cloud_CP-9_2_0162
  - fedramp_moderate_multi_cloud_CP-9_3_0163
  - fedramp_moderate_multi_cloud_CP-9_5_0164
  - fedramp_moderate_multi_cloud_CP-9_8_0165
  - hipaa_multi_cloud_164_308_a_7_i_0017
  - hipaa_multi_cloud_164_308_a_7_ii_a_0018
  - hipaa_multi_cloud_164_308_a_7_ii_b_0019
  - hipaa_multi_cloud_164_308_a_7_ii_c_0020
  - nist_800_53_rev5_multi_cloud_CP-2-a_0462
  - nist_800_53_rev5_multi_cloud_CP-6-a_0485
  - nist_800_53_rev5_multi_cloud_CP-9-a_0509
  - nist_800_53_rev5_multi_cloud_CP-9-b_0510
  - nist_800_53_rev5_multi_cloud_CP-9-c_0511
  - nist_800_53_rev5_multi_cloud_CP-9-d_0512
  - pci_dss_v4_multi_cloud_3.1.1_0032
  - pci_dss_v4_multi_cloud_3.1.2_0033
  - pci_dss_v4_multi_cloud_3.3.1.1_0034
  - pci_dss_v4_multi_cloud_3.3.1.2_0035
  - pci_dss_v4_multi_cloud_10.3.3_0153
  - pci_dss_v4_multi_cloud_11.3.1.2_0167
  - rbi_bank_multi_cloud_11.1_0004
  - rbi_bank_multi_cloud_16.1_0010
  - rbi_bank_multi_cloud_6.1_0021
  - rbi_bank_multi_cloud_6.2_0022
  - rbi_nbfc_multi_cloud_3.4_0020
  - rbi_nbfc_multi_cloud_7.2_0034
  - rbi_nbfc_multi_cloud_7.3_0035
  - rbi_nbfc_multi_cloud_7.5_0037
  - soc2_multi_cloud_cc_7_5_0019
  - soc2_multi_cloud_cc_a_1_1_0021
  - soc2_multi_cloud_a1_2_0025
- rule_id: k8s.etcd.backup.encryption_enabled
  service: etcd
  resource: backup
  requirement: Encryption Enabled
  scope: etcd.backup.encryption_enabled
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Enable Encryption for etcd Backups
  rationale: Without encryption, etcd backups are vulnerable to unauthorized access and data breaches. Attackers can exploit
    unencrypted backups to gain access to sensitive cluster data, including secrets, configuration details, and authentication
    credentials, which can lead to privilege escalation and cluster compromise.
  description: This rule checks that encryption is enabled for etcd backups to protect sensitive data at rest. A good configuration
    ensures that backup files are encrypted using strong encryption algorithms. This reduces the risk of data leaks in case
    of unauthorized access to backup storage, and complies with security frameworks like the CIS Kubernetes Benchmark. Validating
    encryption settings involves ensuring that backup processes use encryption tools or integrated encryption features provided
    by backup solutions.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-2_0042
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-6_0046
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-9_0049
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-6_0056
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-9_0059
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-12_0135
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-13_0136
  - fedramp_moderate_multi_cloud_CM-2_0101
  - fedramp_moderate_multi_cloud_CM-2_2_0102
  - fedramp_moderate_multi_cloud_CM-2_3_0103
  - fedramp_moderate_multi_cloud_CM-2_7_0104
  - fedramp_moderate_multi_cloud_CM-6_0116
  - fedramp_moderate_multi_cloud_CM-6_1_0117
  - fedramp_moderate_multi_cloud_CM-6_2_0118
  - fedramp_moderate_multi_cloud_CM-9_0128
  - fedramp_moderate_multi_cloud_CP-9_0160
  - fedramp_moderate_multi_cloud_CP-9_1_0161
  - fedramp_moderate_multi_cloud_CP-9_2_0162
  - fedramp_moderate_multi_cloud_CP-9_3_0163
  - fedramp_moderate_multi_cloud_CP-9_5_0164
  - fedramp_moderate_multi_cloud_CP-9_8_0165
  - fedramp_moderate_multi_cloud_SC-12_0346
  - fedramp_moderate_multi_cloud_SC-12_1_0347
  - fedramp_moderate_multi_cloud_SC-13_0348
  - gdpr_multi_cloud_Article_30_Records_of_processing_activities_0002
  - gdpr_multi_cloud_Article_32_Security_of_processing_0003
  - hipaa_multi_cloud_164_312_c_1_0027
  - hipaa_multi_cloud_164_312_c_2_0028
  - hipaa_multi_cloud_164_312_e_2_i_0031
  - hipaa_multi_cloud_164_312_e_2_ii_0032
  - iso27001_2022_multi_cloud_A.10.1_0001
  - iso27001_2022_multi_cloud_A.8.24_0080
  - nist_800_171_r2_multi_cloud_3_3_8_3.3.8_Protect_audit_information_and_audit_l_0036
  - nist_800_171_r2_multi_cloud_3_5_10_3.5.10_Store_and_transmit_only_cryptograph_0042
  - nist_800_53_rev5_multi_cloud_CM-2-a_0366
  - nist_800_53_rev5_multi_cloud_CM-6-a_0397
  - nist_800_53_rev5_multi_cloud_CM-9-b_0428
  - nist_800_53_rev5_multi_cloud_CP-9-d_0512
  - nist_800_53_rev5_multi_cloud_IA-5-g_0578
  - nist_800_53_rev5_multi_cloud_PM-11-b_0867
  - nist_800_53_rev5_multi_cloud_SC-13-a_1248
  - pci_dss_v4_multi_cloud_3.4.1_0039
  - pci_dss_v4_multi_cloud_3.4.2_0040
  - pci_dss_v4_multi_cloud_3.5.1.2_0042
  - pci_dss_v4_multi_cloud_3.5.1.3_0043
  - pci_dss_v4_multi_cloud_3.5.1.3_0043
  - pci_dss_v4_multi_cloud_3.6.1.4_0048
  - pci_dss_v4_multi_cloud_8.2.1_0101
  - rbi_bank_multi_cloud_11.1_0004
  - rbi_bank_multi_cloud_18.1_0012
  - rbi_bank_multi_cloud_3.3_0016
  - rbi_bank_multi_cloud_8.1_0024
  - rbi_nbfc_multi_cloud_2.9_0013
  - rbi_nbfc_multi_cloud_6.1_0029
  - soc2_multi_cloud_cc_6_7_0013
  - soc2_multi_cloud_cc_c_1_1_0022
  - soc2_multi_cloud_a1_2_0025
- rule_id: k8s.etcd.backup.retention_policy_set
  service: etcd
  resource: backup
  requirement: Retention Policy Set
  scope: etcd.backup.retention_policy_set
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Configure and Enforce Etcd Backup Retention Policies
  rationale: Without a proper retention policy for etcd backups, sensitive data may be retained longer than necessary, increasing
    the risk of data breaches or unauthorized access. Attackers could exploit improperly managed backups to access historical
    data, leading to potential data exfiltration or unauthorized insights into the cluster's state.
  description: This control checks that etcd backups have a configured retention policy, ensuring that backups are retained
    only for a necessary period. A well-defined retention policy helps limit the exposure of sensitive data, minimizes storage
    costs, and reduces the risk of data leakage by ensuring old backups are securely deleted. Implementing this policy allows
    organizations to maintain compliance with security standards and regulatory requirements, such as GDPR or HIPAA, by managing
    data lifecycle effectively.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backup-etcd
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/
  compliance:
  - rbi_bank_multi_cloud_16.1_0010
  - rbi_bank_multi_cloud_6.1_0021
- rule_id: k8s.etcd.backup.scheduled
  service: etcd
  resource: backup
  requirement: Scheduled
  scope: etcd.backup.scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Scheduled Backups for etcd Data
  rationale: Failure to schedule regular backups for etcd can result in data loss during critical failures or attacks. Misconfigured
    or absent backups expose the cluster to risks of data unavailability, integrity issues, and potential breaches from malicious
    actors aiming to disrupt or manipulate essential Kubernetes data stored in etcd.
  description: This control checks that etcd data is backed up on a scheduled basis as per security best practices. A valid
    configuration includes automated, regular backups with secure storage. This ensures data recovery in case of corruption,
    accidental deletion, or compromise, thus maintaining the cluster's operational integrity and compliance with security
    standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-backup-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/
  compliance:
  - nist_800_53_rev5_multi_cloud_CP-2-e_0466
- rule_id: k8s.etcd.ca.uniqueness_check
  service: etcd
  resource: ca
  requirement: Uniqueness Check
  scope: etcd.ca.uniqueness_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Unique CA Certificates for etcd
  rationale: If CA certificates in etcd are not unique, it could allow attackers to intercept and alter sensitive data between
    Kubernetes components, leveraging man-in-the-middle attacks. Unique certificates ensure that each connection is verified,
    maintaining the integrity and confidentiality of data within the cluster.
  description: This control checks that each etcd instance is using a unique CA certificate. A properly configured unique
    CA certificate ensures that data transmission between etcd and its clients is secure, mitigating risks of unauthorized
    data access or tampering. This configuration aligns with security best practices and compliance standards such as the
    CIS Kubernetes Benchmark, which mandates secure communication between cluster components.
  references:
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  compliance:
  - cis_kubernetes_kubernetes_1.5.9_0169
- rule_id: k8s.etcd.cluster.state_backup_scheduled
  service: etcd
  resource: cluster
  requirement: State Backup Scheduled
  scope: etcd.cluster.state_backup_scheduled
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: medium
  title: Ensure Scheduled Backups for etcd Cluster State
  rationale: Failure to schedule regular etcd backups can lead to catastrophic data loss in the event of a cluster failure,
    corruption, or malicious attack. As etcd stores all cluster state data, including secrets and configuration details, its
    loss can disrupt operations and expose sensitive information. Regular backups ensure data integrity and continuity by
    allowing recovery from failures or security incidents.
  description: This control verifies that the etcd cluster state is configured to perform regular, automated backups. A valid
    configuration includes scheduling periodic snapshots and securely storing them. This practice mitigates risks of data
    loss from hardware failures, accidental deletions, or ransomware attacks that could compromise the cluster state. Ensuring
    that backup mechanisms are in place and tested aligns with best practices and compliance requirements.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-backup-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-backup-etcd/#automated-etcd-backup
  - https://kubernetes.io/docs/reference/command-line-tools-reference/etcdctl/
  compliance:
  - nist_800_53_rev5_multi_cloud_CP-9-c_0511
- rule_id: k8s.etcd.encryption.at_rest_enabled
  service: etcd
  resource: encryption
  requirement: At Rest Enabled
  scope: etcd.encryption.at_rest_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Encryption for Data at Rest in etcd
  rationale: Without encryption at rest, sensitive data stored in etcd, such as secrets and configuration, is vulnerable to
    unauthorized access if the underlying storage is compromised. Attackers with access to the etcd data can extract secrets
    and compromise the entire Kubernetes cluster, leading to potential data breaches and compliance violations.
  description: This control checks that encryption is enabled for data stored at rest in etcd, ensuring that sensitive information
    is not exposed to unauthorized users. To comply, etcd must be configured with encryption keys to encrypt data before it's
    written to disk. A proper configuration involves setting the encryption provider in etcd's configuration file and verifying
    that all data is encrypted. This reduces the risk of exposing sensitive information if the storage system is accessed
    by unauthorized parties, thus maintaining the confidentiality and integrity of cluster data.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-etcd-encryption/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-28_0144
  - fedramp_moderate_multi_cloud_SC-28_0357
  - fedramp_moderate_multi_cloud_SC-28_1_0358
- rule_id: k8s.etcd.encryption.enabled
  service: etcd
  resource: encryption
  requirement: Enabled
  scope: etcd.encryption.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Data Encryption for etcd in Kubernetes
  rationale: Without encryption, data stored in etcd is vulnerable to unauthorized access and exfiltration. Attackers gaining
    access to etcd can intercept and manipulate sensitive data, including secrets and cluster state information, which can
    lead to privilege escalation and compromise of the entire Kubernetes cluster.
  description: This control checks that encryption is enabled for data stored in etcd, which is critical for protecting sensitive
    information such as API server secrets. Enabling etcd encryption involves configuring the Kubernetes API server with an
    encryption config file, specifying the encryption providers (e.g., AES-CBC, Secretbox) and the resources to be encrypted.
    Proper configuration ensures the confidentiality of data at rest, reducing the risk of data breaches and unauthorized
    access.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
  - https://kubernetes.io/docs/setup/best-practices/certificates/#etcd
  compliance: []
- rule_id: k8s.etcd.etcd.apiserver_ca_configured
  service: etcd
  resource: etcd
  requirement: Apiserver Ca Configured
  scope: etcd.etcd.apiserver_ca_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Etcd Server Uses a Secure Apiserver CA Configuration
  rationale: If the Etcd server is not configured with a secure Apiserver CA, it may be vulnerable to man-in-the-middle attacks,
    unauthorized access, and data breaches. Attackers could intercept or modify data in transit between the Kubernetes API
    server and Etcd, leading to potential leakage or tampering of sensitive cluster state information.
  description: This rule checks that the Etcd server is configured to use a trusted Certificate Authority (CA) for its interactions
    with the Kubernetes API server. A properly configured Apiserver CA ensures that the communication between the API server
    and Etcd is encrypted and authenticated, preventing unauthorized access and data tampering. A secure configuration includes
    using a valid and trusted CA certificate, ensuring that both the API server and Etcd trust the same CA. This helps in
    maintaining data integrity and confidentiality, as well as protecting the cluster state from unauthorized modifications.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#ensure-etcd-cluster-is-secure
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-certificates
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/#etcd
  compliance: []
- rule_id: k8s.etcd.etcd.auto_tls_configured
  service: etcd
  resource: etcd
  requirement: Auto Tls Configured
  scope: etcd.etcd.auto_tls_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enable Auto TLS for etcd Communication
  rationale: Without Auto TLS, etcd communication may be susceptible to man-in-the-middle attacks, where an attacker could
    intercept and alter data between the etcd nodes and clients. This configuration ensures encryption of data in transit,
    preventing unauthorized access and maintaining confidentiality and integrity of the cluster's state data.
  description: This rule checks that etcd is configured with Auto TLS, which automatically generates and rotates certificates
    for secure communication between etcd nodes and clients. A proper Auto TLS configuration ensures that all data exchanged
    is encrypted, mitigating the risk of eavesdropping or data tampering by unauthorized entities. This is critical to protect
    against unauthorized access to sensitive cluster data and to comply with security best practices such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-etcd
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/#etcd
  compliance: []
- rule_id: k8s.etcd.etcd.ca_uniqueness_configured
  service: etcd
  resource: etcd
  requirement: Ca Uniqueness Configured
  scope: etcd.etcd.ca_uniqueness_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Unique CA Certificates for etcd in Kubernetes
  rationale: Using unique CA certificates for each etcd instance mitigates the risk of man-in-the-middle attacks and unauthorized
    access. If CA certificates are not unique, an attacker with access to one etcd instance could potentially use the same
    credentials to access others, compromising the integrity and confidentiality of the cluster's data.
  description: This rule checks that each etcd instance in a Kubernetes cluster is configured with a unique Certificate Authority
    (CA) certificate. A properly configured CA ensures that each instance can be individually authenticated, reducing the
    risk of unauthorized access. This practice is aligned with industry security standards, preventing potential breaches
    and ensuring that etcd data remains secure and isolated. Configuration should follow the CIS Kubernetes Benchmark for
    securing etcd, which includes issuing distinct certificates for each instance.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/
  compliance: []
- rule_id: k8s.etcd.etcd.encryption_at_rest_enabled
  service: etcd
  resource: etcd
  requirement: Encryption At Rest Enabled
  scope: etcd.etcd.encryption_at_rest_enabled
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Enable Encryption at Rest for etcd Data
  rationale: Without encryption at rest, sensitive data stored in etcd is vulnerable to unauthorized access and data breaches.
    Attackers who gain access to the etcd data directory may directly read sensitive information, such as service accounts,
    secrets, and config maps, which can lead to privilege escalation and exploitation of the cluster.
  description: This check ensures that encryption at rest is enabled for etcd, which involves configuring etcd to use a secure
    encryption provider to encrypt data before it is written to disk. A correctly configured encryption at rest setup requires
    setting up an encryption configuration file and specifying the encryption providers and keys. This setup protects sensitive
    data from unauthorized access, even if the data store is compromised, and aligns with best practices and compliance requirements
    such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/reference/config-api/kube-apiserver-config.v1/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance: []
- rule_id: k8s.etcd.etcd.patch_enforced
  service: etcd
  resource: etcd
  requirement: Patch Enforced
  scope: etcd.etcd.patch_enforced
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure etcd is Regularly Patched and Upgraded
  rationale: Unpatched etcd instances can be vulnerable to known exploits and vulnerabilities, which might lead to unauthorized
    access or data corruption. Attackers could exploit these vulnerabilities to gain control over Kubernetes API operations
    or access sensitive data stored in etcd. Regular patching minimizes these risks by ensuring that security vulnerabilities
    are addressed promptly.
  description: This control checks if the etcd database, which is critical for Kubernetes cluster operation, is up-to-date
    with the latest patches and upgrades. A good configuration involves implementing a regular patch management process to
    ensure etcd runs the latest stable version. This helps prevent potential exploits and maintains the integrity and availability
    of the Kubernetes control plane. It also aligns with best practices outlined in the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-etcd
  - https://kubernetes.io/docs/setup/best-practices/cluster-large/#etcd
  compliance: []
- rule_id: k8s.etcd.etcd.peer_client_cert_auth_in_transit_enabled
  service: etcd
  resource: etcd
  requirement: Peer Client Cert Auth In Transit Enabled
  scope: etcd.etcd.peer_client_cert_auth_in_transit_enabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enable Peer Client Certificate Authentication for Etcd
  rationale: If peer client certificate authentication is not enabled for etcd, unauthorized nodes could potentially communicate
    with the etcd cluster, leading to data breaches or cluster compromise. Enabling this authentication mechanism ensures
    that only authorized and authenticated nodes can participate in etcd peer-to-peer communication, mitigating risks such
    as man-in-the-middle attacks and unauthorized data access.
  description: This control checks whether peer client certificate authentication is enabled for etcd, ensuring that all communication
    between etcd nodes is authenticated using TLS certificates. A correct configuration involves setting 'peer-client-cert-auth'
    to 'true' in the etcd configuration, which requires all etcd peers to present valid client certificates for mutual TLS
    authentication. This measure enhances security by preventing unauthorized access to etcd data and maintaining the integrity
    and confidentiality of node-to-node communication within the etcd cluster.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#configure-tls-for-etcd
  - https://kubernetes.io/docs/setup/best-practices/certificates/#etcd-peer-certificates
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/#etcd
  compliance:
  - cis_kubernetes_kubernetes_2.5_0065
- rule_id: k8s.etcd.etcd.pod_tls_encryption_configuration_configured
  service: etcd
  resource: etcd
  requirement: Pod Tls Encryption Configuration Configured
  scope: etcd.etcd.pod_tls_encryption_configuration_configured
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure etcd Pod TLS Encryption is Properly Configured
  rationale: If etcd Pod TLS encryption is misconfigured, sensitive data in transit between etcd nodes and clients is vulnerable
    to interception and unauthorized access. Attackers could exploit this to perform man-in-the-middle attacks, compromising
    the integrity and confidentiality of cluster data.
  description: This rule checks that TLS encryption is properly configured for etcd pods, ensuring that all communication
    between etcd clients and servers is encrypted using TLS. Proper configuration involves setting up valid certificates and
    keys for etcd pods. This reduces the risk of data breaches by protecting against eavesdropping and tampering, and helps
    maintain compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#encryption-at-rest
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-etcd
  - https://kubernetes.io/docs/setup/best-practices/certificates/#etcd
  compliance: []
- rule_id: k8s.etcd.patch.compliance
  service: etcd
  resource: patch
  requirement: Compliance
  scope: etcd.patch.compliance
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure etcd Patches Are Up-to-Date for Compliance
  rationale: Failing to keep etcd patched can expose Kubernetes clusters to vulnerabilities, potentially leading to data breaches
    or unauthorized access. Attackers may exploit known vulnerabilities in unpatched etcd versions to access sensitive cluster
    data or disrupt cluster operations, which is critical for maintaining regulatory compliance and a strong security posture.
  description: This control checks that the etcd component of a Kubernetes cluster is running the latest security patches.
    A well-maintained etcd ensures that known vulnerabilities are addressed, reducing the risk of exploitation. The validation
    process involves verifying the etcd version against the latest available stable release and ensuring that configurations
    align with security best practices. Regular patching of etcd helps in safeguarding the integrity and availability of critical
    cluster data and is essential for meeting compliance requirements such as those outlined by the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/setup/best-practices/cluster-large/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/
  compliance:
  - rbi_nbfc_multi_cloud_2.6_0010
  - rbi_nbfc_multi_cloud_5.2_0026
- rule_id: k8s.etcd.peer.client_cert_auth_in_transit_enabled
  service: etcd
  resource: peer
  requirement: Client Cert Auth In Transit Enabled
  scope: etcd.peer.client_cert_auth_in_transit_enabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enable Client Certificate Authentication for ETCD Peer Communication
  rationale: Without client certificate authentication for etcd peer communication, the cluster is vulnerable to man-in-the-middle
    attacks and unauthorized access, as attackers could intercept or impersonate etcd peer connections. This misconfiguration
    can lead to data breaches and compromise the integrity of the cluster's state data.
  description: This control checks that etcd peer communication has client certificate authentication enabled, ensuring that
    only authenticated peers can connect. A valid configuration requires that etcd peers use mutual TLS (mTLS) to authenticate
    each other, which involves setting flags such as `--peer-cert-file`, `--peer-key-file`, and `--peer-client-cert-auth`
    to true. This setup helps in maintaining secure and trusted communication channels between etcd peers, protecting the
    cluster from unauthorized access and potential data leakage.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/
  compliance: []
- rule_id: k8s.etcd.peer.tls_configuration_check
  service: etcd
  resource: peer
  requirement: Tls Configuration Check
  scope: etcd.peer.tls_configuration_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure etcd Peer-to-Peer TLS Encryption
  rationale: Failure to configure TLS for etcd peer communication can expose sensitive data to interception and unauthorized
    access. Without encryption, attackers could perform man-in-the-middle attacks, leading to data breaches and compromised
    cluster integrity.
  description: This check ensures that TLS encryption is enabled and properly configured for etcd peer-to-peer communication.
    It validates the presence of necessary certificates and configuration settings in etcd. Proper TLS configuration encrypts
    data in transit, protecting it from eavesdropping and tampering, and fulfills compliance with security standards like
    the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-communication
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/#etcd
  compliance:
  - cis_kubernetes_kubernetes_1.5.4_0164
- rule_id: k8s.etcd.peer.tls_in_transit_configuration
  service: etcd
  resource: peer
  requirement: Tls In Transit Configuration
  scope: etcd.peer.tls_in_transit_configuration
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure TLS for etcd Peer Communication
  rationale: Without TLS encryption, communication between etcd peers is susceptible to interception and man-in-the-middle
    attacks, allowing attackers to eavesdrop on or manipulate the data being exchanged. This poses a significant risk to the
    integrity and confidentiality of cluster data.
  description: This control verifies that etcd peer-to-peer communication is secured using Transport Layer Security (TLS).
    It checks for the presence of valid TLS certificates and ensures that they are correctly configured to encrypt data in
    transit. Proper TLS configuration mitigates risks of data breaches and unauthorized access to sensitive information within
    the etcd cluster, aligning with best practices and compliance requirements like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#configure-tls-certificates-for-etcd
  - https://kubernetes.io/docs/concepts/cluster-administration/certificates/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_2.4_0064
- rule_id: k8s.etcd.pod.auto_tls_check
  service: etcd
  resource: pod
  requirement: Auto Tls Check
  scope: etcd.pod.auto_tls_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enable Auto TLS for Etcd Pods
  rationale: Without Auto TLS, data transmitted between etcd clients and servers is vulnerable to interception and man-in-the-middle
    attacks, as the data is not encrypted. This poses a significant risk of data breaches, especially if sensitive information
    is being stored in etcd. Proper TLS configuration helps ensure that the communication channels are secure and that only
    authorized parties can access the transmitted data.
  description: This check verifies that Etcd pods have Auto TLS enabled, which automatically generates client and server certificates
    to encrypt communications. A correct configuration ensures that all data in transit between etcd nodes and clients is
    encrypted, protecting against unauthorized interception. This helps maintain data confidentiality and integrity, significantly
    reducing the risk of eavesdropping or data tampering, and aligns with CIS Kubernetes Benchmark standards for secure communication.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_2.3_0063
- rule_id: k8s.etcd.pod.client_cert_auth_enabled
  service: etcd
  resource: pod
  requirement: Client Cert Auth Enabled
  scope: etcd.pod.client_cert_auth_enabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enable Client Certificate Authentication for etcd
  rationale: Without client certificate authentication, etcd is vulnerable to unauthorized access, as any client can establish
    a connection without proving its identity. This exposes the cluster to potential data exfiltration and configuration tampering,
    as attackers could read or modify sensitive cluster data stored in etcd.
  description: This control checks if client certificate authentication is enabled for the etcd pod. It validates the 'client-cert-auth'
    flag in the etcd configuration. A secure setup ensures that only clients with valid certificates, issued by a trusted
    Certificate Authority, can communicate with etcd. This prevents unauthorized access, ensuring that only trusted entities
    can read or write cluster state data, thus maintaining data integrity and confidentiality.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#enable-client-certificate-authentication
  - https://kubernetes.io/docs/setup/best-practices/certificates/#etcd
  - https://kubernetes.io/docs/concepts/cluster-administration/certificates/
  compliance:
  - cis_kubernetes_kubernetes_2.2_0062
  - cis_kubernetes_kubernetes_1.5.2_0162
- rule_id: k8s.etcd.pod.peer_auto_tls_disabled
  service: etcd
  resource: pod
  requirement: Peer Auto Tls Disabled
  scope: etcd.pod.peer_auto_tls_disabled
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Disable Automatic TLS for etcd Peer Connections
  rationale: Disabling peer auto TLS in etcd prevents the use of automatically generated certificates, which may not be adequately
    secured or trusted. Misconfiguration can lead to unauthorized peer connections, allowing attackers to intercept or alter
    data. This control mitigates the risk of man-in-the-middle attacks and ensures only trusted nodes can communicate within
    the etcd cluster.
  description: This rule checks that the etcd pod is configured to disable automatic TLS for peer connections. A secure setup
    requires manually specifying trusted certificates and keys for peer communication, thus ensuring that all nodes are using
    validated and secure credentials. Proper configuration helps prevent unauthorized access and data tampering, maintaining
    the integrity and confidentiality of the etcd data store.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-etcd
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  compliance:
  - cis_kubernetes_kubernetes_2.6_0066
- rule_id: k8s.etcd.pod.tls_encryption_configuration_check
  service: etcd
  resource: pod
  requirement: Tls Encryption Configuration Check
  scope: etcd.pod.tls_encryption_configuration_check
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure TLS Encryption for etcd Communication
  rationale: Without TLS encryption, data transmitted between Kubernetes components and etcd can be intercepted and potentially
    modified by adversaries. This exposes sensitive cluster configuration data and secrets, leading to risks such as man-in-the-middle
    attacks and data breaches.
  description: This control checks that the etcd pods in a Kubernetes cluster are configured to use TLS for all client and
    peer communication. Proper TLS configuration involves setting up x.509 certificates for mutual authentication and encrypting
    data in transit. This practice ensures that data exchanged with etcd is secure from eavesdropping and tampering, meeting
    compliance with security standards such as CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#etcd-security
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - cis_kubernetes_kubernetes_2.1_0061
- rule_id: k8s.etcd.secret.certificate_configured
  service: etcd
  resource: secret
  requirement: Certificate Configured
  scope: etcd.secret.certificate_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure TLS Certificates are Configured for etcd Secrets
  rationale: Without proper TLS certificates, communication between the Kubernetes API server and etcd could be susceptible
    to man-in-the-middle attacks, as well as unauthorized data interception. This control mitigates the risk of sensitive
    information being compromised during transmission.
  description: This check verifies that TLS certificates are correctly configured for etcd, ensuring that all data in transit
    is encrypted. Proper TLS setup includes ensuring that certificates are valid, correctly signed, and their usage is properly
    specified. This helps prevent unauthorized access and data breaches, maintaining the integrity and confidentiality of
    the cluster's state data.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#securing-communication
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#etcd-recommended-host-practices
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-certificates
  compliance: []
- rule_id: k8s.etcd.server.ca_check
  service: etcd
  resource: server
  requirement: Ca Check
  scope: etcd.server.ca_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure etcd Server Uses Valid CA Certificates
  rationale: Misconfigured or invalid CA certificates on the etcd server can lead to unauthorized access, data breaches, and
    man-in-the-middle attacks. By ensuring that the etcd server is properly using valid CA certificates, you mitigate the
    risk of rogue nodes joining the etcd cluster, protect sensitive data within etcd, and ensure that communication is secure
    and trusted.
  description: This control checks that the Kubernetes etcd server is configured to use valid Certificate Authority (CA) certificates
    for authentication and encryption. A properly configured etcd server should have a CA file specified in its configuration,
    ensuring that all incoming connections are verified against a trusted CA. Validating this configuration helps ensure that
    only legitimate, authenticated clients and peers can access the etcd server, thus maintaining the integrity and confidentiality
    of the data stored in etcd.
  references:
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#securing-etcd
  compliance:
  - cis_kubernetes_kubernetes_2.7_0067
- rule_id: k8s.etcd.snapshot.retention_policy_configured
  service: etcd
  resource: snapshot
  requirement: Retention Policy Configured
  scope: etcd.snapshot.retention_policy_configured
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Configure Retention Policy for etcd Snapshots
  rationale: Without a properly configured retention policy for etcd snapshots, sensitive data could be stored indefinitely,
    increasing the risk of unauthorized access and potential data breaches. Misconfiguration can lead to excessive storage
    use and make it difficult to manage and secure older snapshots, which may contain vulnerabilities over time.
  description: This control checks whether a retention policy is configured for etcd snapshots in a Kubernetes cluster. A
    good configuration limits the number of retained snapshots to a practical number, ensuring that outdated snapshots are
    automatically deleted. This helps in managing storage efficiently and reduces the risk of exposing sensitive data in obsolete
    snapshots. Additionally, it ensures that the cluster complies with industry standards, mitigating the risk of data exposure
    and supporting regulatory compliance.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#etcd-backups
  - https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#back-up-data-from-the-node
  - https://kubernetes.io/docs/setup/best-practices/certificates/#etcd
  compliance:
  - soc2_multi_cloud_cc_c_1_2_0023
- rule_id: k8s.etcd.snapshot.schedule_defined
  service: etcd
  resource: snapshot
  requirement: Schedule Defined
  scope: etcd.snapshot.schedule_defined
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Scheduled ETCD Snapshots for Disaster Recovery
  rationale: Without a scheduled snapshot, etcd data may be lost or corrupted due to unforeseen events like node failures
    or attacks, compromising the integrity and availability of the Kubernetes cluster. This increases the risk of prolonged
    downtime and data loss, making recovery difficult and potentially leading to unauthorized access or data breaches.
  description: This control checks that etcd snapshots are scheduled at regular intervals to ensure that data can be restored
    quickly in the event of data corruption or loss. A well-configured snapshot schedule allows for timely recovery from incidents,
    maintains data integrity, and ensures compliance with disaster recovery policies. This reduces the risk of prolonged outages
    and helps maintain cluster security by ensuring that only authorized, verified data is restored.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/etcd-backup-restore/
  - https://kubernetes.io/docs/setup/best-practices/certificates/#etcd
  compliance:
  - rbi_bank_multi_cloud_16.1_0010
  - rbi_bank_multi_cloud_6.1_0021
- rule_id: k8s.etcd.tls.enabled
  service: etcd
  resource: tls
  requirement: Enabled
  scope: etcd.tls.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce TLS Encryption for etcd Communication
  rationale: Without TLS encryption, data transmitted between the Kubernetes API server and the etcd database is susceptible
    to interception and unauthorized access. Attackers could exploit this vulnerability to capture sensitive data, manipulate
    cluster state, or perform man-in-the-middle attacks. Enforcing TLS ensures data integrity and confidentiality, crucial
    for preventing these security breaches.
  description: This rule verifies that TLS is enabled for all communications between the Kubernetes API server and the etcd
    database. Proper configuration involves setting up mutual TLS authentication to ensure that both parties are authenticated
    and data is encrypted during transit. This reduces the risk of unauthorized access to sensitive cluster data, mitigates
    potential data leaks, and aligns with compliance requirements such as the CIS Kubernetes Benchmark. A well-implemented
    TLS setup includes generating and configuring certificates for both the client and server, which are then used to establish
    secure connections.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#securing-etcd
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-certificates
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-28_0144
  - fedramp_moderate_multi_cloud_SC-8_0343
  - fedramp_moderate_multi_cloud_SC-8_1_0344
  - fedramp_moderate_multi_cloud_SC-28_0357
  - fedramp_moderate_multi_cloud_SC-28_1_0358
  - nist_800_171_r2_multi_cloud_3_11_3_3.11.3_Remediate_vulnerabilities_in_accord_0002
  - nist_800_171_r2_multi_cloud_3_13_15_3.13.15_Protect_the_authenticity_of_commu_0006
  - nist_800_171_r2_multi_cloud_3_1_1_3.1.1_Limit_system_access_to_authorized_use_0020
  - nist_800_171_r2_multi_cloud_3_1_13_3.1.13_Employ_cryptographic_mechanisms_to_0022
  - pci_dss_v4_multi_cloud_3.5.1.3_0043
  - pci_dss_v4_multi_cloud_4.1.2_0059
  - pci_dss_v4_multi_cloud_4.1.4_0060
  - pci_dss_v4_multi_cloud_9.4.1.2_0132
  - rbi_bank_multi_cloud_3.3_0016
  - rbi_nbfc_multi_cloud_2.9_0013
  - rbi_nbfc_multi_cloud_6.1_0029
- rule_id: k8s.etcd.tls.encryption_in_transit_check
  service: etcd
  resource: tls
  requirement: Encryption In Transit Check
  scope: etcd.tls.encryption_in_transit_check
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure Encryption for etcd Communication
  rationale: Without encryption in transit for etcd, sensitive data including configuration details, secrets, and access credentials
    are susceptible to interception by unauthorized entities. This could lead to data breaches and unauthorized access to
    the cluster, as attackers can eavesdrop on traffic between the etcd server and its clients.
  description: "This control checks that Transport Layer Security (TLS) is enabled for all communications to and from the\
    \ etcd server. Proper configuration involves ensuring that the etcd server and clients are configured with valid certificates\
    \ and keys, and that mutual TLS authentication is enforced. This secures the data in transit against man-in-the-middle\
    \ attacks and unauthorized data access, thereby maintaining the confidentiality and integrity of the cluster\u2019s state\
    \ data."
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-cluster-communication
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/setup/best-practices/certificates/#all-about-authentication
  compliance:
  - cis_kubernetes_kubernetes_1.5.1_0161
- rule_id: k8s.etcd.unique.ca_certificate_check
  service: etcd
  resource: unique
  requirement: Ca Certificate Check
  scope: etcd.unique.ca_certificate_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure Unique CA Certificates for etcd Communication
  rationale: Misconfiguring CA certificates in etcd can expose the cluster to man-in-the-middle (MITM) attacks. An attacker
    could intercept and manipulate data between etcd and Kubernetes components, leading to potential data breaches and unauthorized
    access. Proper certificate validation ensures that etcd communication is secure and trusted.
  description: This rule checks that etcd is configured with unique CA certificates for secure communication. It verifies
    that the CA certificate used is not shared with other components, and that it is correctly configured to validate authenticity.
    A properly configured CA certificate ensures encrypted and authenticated communication between etcd and other cluster
    components, reducing the risk of data interception and ensuring compliance with security benchmarks like the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#etcd-security
  - https://kubernetes.io/docs/setup/best-practices/certificates/#etcd-peer-and-client-certificates
  - https://kubernetes.io/docs/concepts/cluster-administration/certificates/
  compliance:
  - cis_kubernetes_kubernetes_2.8_0068
- rule_id: k8s.event.alerting.system_configured
  service: event
  resource: alerting
  requirement: System Configured
  scope: event.alerting.system_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Configure Event Alerting for Security Compliance
  rationale: Without proper configuration of Kubernetes event alerting, critical security incidents may go undetected, allowing
    potential attackers to exploit vulnerabilities unnoticed. This could lead to unauthorized access, data breaches, or service
    disruptions. Ensuring event alerting is properly set up helps in timely detection and response to anomalies or malicious
    activities.
  description: This control checks whether Kubernetes event alerting is configured according to security best practices. It
    verifies that alerts are generated for key security events, such as unauthorized access attempts or configuration changes.
    A well-configured alerting system helps in quickly identifying and responding to potential security incidents, maintaining
    system integrity, and ensuring compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/events/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-runasnonroot/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_171_r2_multi_cloud_3_14_3_3.14.3_Monitor_system_security_alerts_and_0016
  - nist_800_171_r2_multi_cloud_3_1_4_3.1.4_Separate_the_duties_of_individuals_to_0027
- rule_id: k8s.federation.api.server_tls_configuration_check
  service: federation
  resource: api
  requirement: Server Tls Configuration Check
  scope: federation.api.server_tls_configuration_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce TLS for Kubernetes Federation API Server
  rationale: Without proper TLS configuration, data transmitted between clients and the Kubernetes Federation API server can
    be intercepted or tampered with by malicious actors. This exposes sensitive information and increases the risk of man-in-the-middle
    attacks, where attackers can eavesdrop or modify communications. Ensuring TLS is enforced protects data integrity and
    confidentiality, meeting compliance requirements and safeguarding against unauthorized access.
  description: This check verifies that the Kubernetes Federation API server has TLS configured to encrypt data in transit.
    It ensures that the server presents a valid TLS certificate and uses secure ciphers, preventing unauthorized access and
    data breaches. Proper TLS setup involves using certificates issued by a trusted Certificate Authority (CA) and configuring
    the API server to accept only secure connections. This enhances security by ensuring that communications between the client
    and server are encrypted, protecting against interception and tampering.
  references:
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/
  compliance:
  - cis_kubernetes_kubernetes_3.1.19_0212
- rule_id: k8s.federation.server.audit_log_path_check
  service: federation
  resource: server
  requirement: Audit Log Path Check
  scope: federation.server.audit_log_path_check
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Ensure Federation Server Audit Logs Have Defined Path
  rationale: Without a properly configured audit log path, audit logs might not be generated or stored correctly, leading
    to potential loss of critical security incident data. Attackers could exploit this misconfiguration to perform malicious
    activities without detection, compromising the system's integrity and violating compliance requirements.
  description: This control checks that the Kubernetes federation server's audit logging feature is configured with a specific
    and secure log file path. A correctly configured audit log path ensures that all operations are logged and accessible
    for review, which is crucial for detecting unauthorized access and activities. Proper audit logging aids in forensic investigations
    and compliance with security benchmarks such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - cis_kubernetes_kubernetes_3.1.10_0203
- rule_id: k8s.federation.server.basic_auth_disabled
  service: federation
  resource: server
  requirement: Basic Auth Disabled
  scope: federation.server.basic_auth_disabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Basic Authentication on Kubernetes Federation Server
  rationale: Basic Authentication is inherently insecure as it involves sending credentials in plaintext. If misconfigured,
    attackers can intercept these credentials via man-in-the-middle attacks or network traffic sniffing, leading to unauthorized
    access to the federation server. Disabling Basic Auth helps prevent these attack vectors and mitigates the risk of credential
    theft.
  description: This control checks that Basic Authentication is disabled on the Kubernetes federation server. A secure configuration
    requires using stronger authentication methods like token-based or certificate-based authentication, which provide better
    encryption and security. By ensuring Basic Auth is disabled, the control enhances the security posture by reducing the
    attack surface and aligning with best practices and compliance benchmarks such as CIS Kubernetes. This includes verifying
    configuration settings in the federation control plane to ensure no basic-auth flags are enabled.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#x509-client-certs
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-strong-authentication
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - cis_kubernetes_kubernetes_3.1.2_0070
- rule_id: k8s.federation.server.etcd_tls_certificates_configured
  service: federation
  resource: server
  requirement: Etcd Tls Certificates Configured
  scope: federation.server.etcd_tls_certificates_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Configure TLS Certificates for Etcd in Federation Server
  rationale: If etcd traffic is not encrypted using TLS, sensitive data such as secrets and configurations can be intercepted
    by attackers. This configuration helps prevent man-in-the-middle attacks and unauthorized access, which can lead to data
    breaches and compromise of the Kubernetes cluster.
  description: This rule checks if TLS certificates are correctly configured for the etcd server used in the Kubernetes federation.
    Proper TLS configuration ensures that all communications to and from the etcd server are encrypted, thereby protecting
    data in transit. A good configuration includes using strong encryption protocols and regularly rotating certificates.
    This enhances security by safeguarding sensitive data against interception and ensures compliance with security standards
    like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-communications
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - cis_kubernetes_kubernetes_3.1.18_0211
- rule_id: k8s.federation.server.insecure_allow_any_token_disabled
  service: federation
  resource: server
  requirement: Insecure Allow Any Token Disabled
  scope: federation.server.insecure_allow_any_token_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Insecure Token Authentication in Federation Server
  rationale: Allowing any token to authenticate against the Kubernetes federation server poses a significant security risk.
    It could enable unauthorized entities to gain access to the cluster, potentially leading to data breaches, escalation
    of privileges, and disruption of services. Attackers could exploit this misconfiguration to impersonate legitimate users
    and execute arbitrary commands or exfiltrate sensitive information.
  description: This rule verifies that the 'insecure-allow-any-token' flag is disabled on the Kubernetes federation server.
    A securely configured federation server should not accept any token without proper verification. Disabling this setting
    ensures that only valid and authorized tokens can interact with the API server, thereby strengthening the cluster's authentication
    mechanism. This configuration protects against unauthorized access and aligns with security best practices as outlined
    in the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/federation/
  compliance:
  - cis_kubernetes_kubernetes_3.1.3_0071
- rule_id: k8s.federation.server.insecure_bind_address_check
  service: federation
  resource: server
  requirement: Insecure Bind Address Check
  scope: federation.server.insecure_bind_address_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Secure Federation Server Bind Address
  rationale: If the federation server is configured with an insecure bind address, it may expose sensitive API endpoints to
    unauthorized networks, increasing the risk of unauthorized access and potential data breaches. Attackers could exploit
    misconfigured network settings to intercept or manipulate federation traffic, compromising the integrity, confidentiality,
    and availability of the federated clusters.
  description: This control checks that the Kubernetes federation server does not use an insecure bind address, such as 0.0.0.0,
    which exposes the service to all network interfaces. A secure configuration binds the server to a specific, trusted network
    interface, minimizing exposure to hostile networks. Proper configuration ensures that only authorized networks can access
    the federation server, reducing the attack surface and aligning with best practices for securing Kubernetes environments.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-access-to-the-kubernetes-api
  - https://kubernetes.io/docs/concepts/architecture/master-node-communication/
  - https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
  compliance:
  - cis_kubernetes_kubernetes_3.1.4_0197
- rule_id: k8s.federation.server.profiling_disabled
  service: federation
  resource: server
  requirement: Profiling Disabled
  scope: federation.server.profiling_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Profiling on Kubernetes Federation Server
  rationale: Profiling features in Kubernetes Federation Server can expose sensitive information about the server's internals,
    which attackers could exploit to gain unauthorized insights or manipulate system behavior. Disabling profiling mitigates
    the risk of information leakage, reduces the attack surface, and prevents potential exploitation of profiling endpoints
    by malicious actors.
  description: This control ensures that the Kubernetes Federation Server's profiling capabilities are disabled. Profiling
    is often used for performance tuning but can inadvertently expose detailed information about the server's operations and
    resource usage. By validating that profiling is disabled, we protect against unauthorized access to profiling data, thereby
    enhancing the overall security posture and ensuring compliance with the CIS Kubernetes Benchmark. A secure configuration
    involves setting the `--profiling=false` flag in the server's configuration.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - cis_kubernetes_kubernetes_3.1.7_0200
- rule_id: k8s.general.resource.secrets_as_env_vars_check
  service: general
  resource: resource
  requirement: Secrets As Env Vars Check
  scope: general.resource.secrets_as_env_vars_check
  domain: data_protection
  subcategory: secrets_management
  severity: high
  title: Avoid Using Secrets as Environment Variables
  rationale: Using environment variables to pass secrets in Kubernetes can lead to accidental exposure of sensitive information.
    Environment variables are easily accessible by anyone with access to the container, providing a potential attack vector
    for privilege escalation or data exfiltration. This misconfiguration can lead to secrets being inadvertently logged or
    exposed in debugging sessions, increasing the risk of unauthorized access.
  description: This control checks that Kubernetes does not use environment variables to manage secrets. Instead, it recommends
    using Kubernetes Secrets mounted as volumes, which are specifically designed to store and manage sensitive data securely.
    This approach minimizes the risk of exposing secrets by reducing their surface area. Proper configuration ensures that
    secrets are only accessible to authorized pods and are not inadvertently exposed, helping maintain compliance with security
    standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables
  - https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod
  - https://kubernetes.io/docs/concepts/security/overview/#protecting-data
  compliance:
  - cis_kubernetes_kubernetes_5.4.1_0128
- rule_id: k8s.horizontalpodautoscaler.pod.configured_for_workloads_configured
  service: horizontalpodautoscaler
  resource: pod
  requirement: Configured For Workloads Configured
  scope: horizontalpodautoscaler.pod.configured_for_workloads_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Horizontal Pod Autoscaler Configures Pods for Optimal Workload Management
  rationale: Improper configuration of the Horizontal Pod Autoscaler (HPA) can lead to inefficient resource utilization and
    potential service disruptions. This misconfiguration can expose pods to denial-of-service attacks by not scaling adequately
    under heavy load or by over-provisioning resources, which can lead to unnecessary costs and reduced availability of other
    services.
  description: This control checks if the Horizontal Pod Autoscaler is properly configured to manage workloads efficiently
    and securely. Specifically, it ensures that the HPA is set to scale pods based on accurate and relevant metrics such as
    CPU and memory usage. Proper configuration allows the cluster to respond dynamically to workload demands, thereby optimizing
    resource usage, maintaining performance under load, and reducing the risk of resource exhaustion and denial-of-service
    conditions.
  references:
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#horizontalpodautoscaler-v2beta2-autoscaling
  compliance: []
- rule_id: k8s.image.latest.tag_avoidance
  service: image
  resource: latest
  requirement: Tag Avoidance
  scope: image.latest.tag_avoidance
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Avoid Using 'latest' Tag for Container Images
  rationale: Using the 'latest' tag for container images poses a security risk as it can lead to unpredictable deployments.
    If an image is pushed with the 'latest' tag, it may inadvertently propagate changes to production environments without
    proper testing, leading to potential application instability or introduction of vulnerabilities. Attackers could exploit
    these changes if they introduce unpatched or vulnerable software components.
  description: This rule checks that container images do not use the 'latest' tag, which can be mutable and lead to non-deterministic
    deployments. Best practices dictate specifying explicit version tags to ensure that the exact tested and verified image
    is deployed, thus minimizing security risks associated with unexpected changes. Ensuring that images are versioned helps
    maintain consistency, traceability, and auditability, reducing the risk of deploying unverified code.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/#image-tags
  - https://kubernetes.io/docs/concepts/configuration/overview/#container-images
  - https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  compliance:
  - rbi_nbfc_multi_cloud_2.6_0010
  - rbi_nbfc_multi_cloud_5.2_0026
- rule_id: k8s.image.latest.tag_prohibited
  service: image
  resource: latest
  requirement: Tag Prohibited
  scope: image.latest.tag_prohibited
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disallow 'latest' Tag in Container Images
  rationale: Using the 'latest' tag in container images introduces unpredictability into the deployment process, as it can
    lead to unintentional updates that may introduce vulnerabilities or break application functionality. Attackers could exploit
    this by introducing malicious changes into container images under the 'latest' tag, leading to potential security breaches.
  description: This rule checks that container images in Kubernetes deployments do not use the 'latest' tag. Instead, images
    should be tagged with specific version numbers. By disallowing 'latest', you ensure that deployments use immutable and
    explicitly versioned images, reducing the risk of unexpected changes and making it easier to audit and roll back to known
    good states. This practice enhances security by ensuring consistent and predictable deployments, aligning with best practices
    for secure software supply chains.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/
  - https://kubernetes.io/docs/concepts/configuration/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - rbi_bank_multi_cloud_10.1_0003
- rule_id: k8s.image.pod.container_scanning_enabled_enforced
  service: image
  resource: pod
  requirement: Container Scanning Enabled Enforced
  scope: image.pod.container_scanning_enabled_enforced
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enforce Automated Container Image Vulnerability Scanning
  rationale: Without enforced container image scanning, vulnerabilities within images can go undetected, leading to potential
    exploits such as privilege escalation or data breaches. Attackers can leverage known vulnerabilities in containers to
    gain unauthorized access or cause denial of service, threatening the security posture of the Kubernetes cluster.
  description: This control checks that all container images deployed in Kubernetes pods are subject to automated vulnerability
    scanning. A good configuration ensures that only images that have been scanned and deemed free of critical vulnerabilities
    are allowed to run. This process helps in identifying security flaws early, preventing the use of compromised images,
    and maintaining compliance with security standards. Practically, this can be enforced using admission controllers such
    as Open Policy Agent or specific tools integrated into the CI/CD pipeline.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/concepts/containers/images/
  - https://kubernetes.io/docs/concepts/policy/resource-quotas/
  compliance: []
- rule_id: k8s.image.pod.image_provenance_verified_enforced
  service: image
  resource: pod
  requirement: Image Provenance Verified Enforced
  scope: image.pod.image_provenance_verified_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Image Provenance Verification in Pods
  rationale: If image provenance verification is not enforced, there is a risk of deploying untrusted or malicious images,
    which can lead to unauthorized access, data breaches, or execution of harmful code. Attackers may exploit vulnerabilities
    in unverified images to gain control over Kubernetes resources, leading to potential compromise of the entire cluster.
  description: This control checks whether the image provenance verification is enforced for all container images used in
    Kubernetes pods. It ensures that images are sourced from trusted registries and have not been tampered with during transit.
    A good configuration involves using admission controllers to automatically verify image signatures before allowing pod
    deployment. By enforcing image provenance, organizations can enhance the security of their supply chain, reduce the attack
    surface, and comply with security best practices.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.image.pod.latest_tag_avoidance_configured
  service: image
  resource: pod
  requirement: Latest Tag Avoidance Configured
  scope: image.pod.latest_tag_avoidance_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Avoid Using 'latest' Tag for Container Images
  rationale: Using the 'latest' tag for container images can lead to unpredictable deployments and makes it difficult to manage
    versions and ensure consistency across environments. This can introduce security vulnerabilities, as the image tagged
    'latest' may change unexpectedly, potentially introducing unvetted changes or vulnerabilities. Attackers can exploit these
    inconsistencies to introduce malicious code or exploit newly exposed vulnerabilities.
  description: This control checks that Kubernetes pods do not use the 'latest' tag for container images. Best practice is
    to use specific version tags to ensure that the exact same image is used across different environments and deployments.
    By avoiding the 'latest' tag, organizations can ensure a more predictable and secure software supply chain, reducing the
    risk of deploying untested or vulnerable image versions. This practice aligns with security best practices and helps maintain
    a stable and secure deployment process.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/#updating-images
  - https://kubernetes.io/docs/concepts/configuration/overview/#container-images
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#specifying-a-container-image
  compliance: []
- rule_id: k8s.image.pull.policy_always
  service: image
  resource: pull
  requirement: Policy Always
  scope: image.pull.policy_always
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce ImagePullPolicy Always for Immutable Deployments
  rationale: Using an 'Always' image pull policy mitigates the risk of running outdated or compromised images by ensuring
    that the latest version of an image is always retrieved from the registry. This is crucial to prevent the exploitation
    of known vulnerabilities that might exist in cached images, reducing the attack surface in Kubernetes clusters. Attack
    vectors include the execution of vulnerable, outdated images that might allow unauthorized access or privilege escalation.
  description: This rule checks that all Kubernetes Pods have their imagePullPolicy set to 'Always'. A valid configuration
    ensures that every Pod startup retrieves the most recent image from the registry, bypassing any local cache. This practice
    protects against running deprecated or potentially vulnerable images and complies with security best practices. By enforcing
    this policy, administrators ensure that only the most current, vetted images are deployed, thereby enhancing the overall
    security posture.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/#updating-images
  - https://kubernetes.io/docs/concepts/configuration/overview/#container-images
  - https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  compliance:
  - fedramp_moderate_multi_cloud_RA-5_0293
  - fedramp_moderate_multi_cloud_RA-5_2_0294
  - fedramp_moderate_multi_cloud_RA-5_3_0295
  - fedramp_moderate_multi_cloud_RA-5_4_0296
  - fedramp_moderate_multi_cloud_RA-5_5_0297
  - fedramp_moderate_multi_cloud_RA-5_8_0298
  - fedramp_moderate_multi_cloud_RA-5_11_0299
  - rbi_bank_multi_cloud_10.1_0003
- rule_id: k8s.image.scan.on_admission
  service: image
  resource: scan
  requirement: On Admission
  scope: image.scan.on_admission
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Image Vulnerability Scanning on Admission
  rationale: Without mandatory image scanning on admission, Kubernetes clusters risk deploying vulnerable container images
    that could be exploited by attackers. This oversight may lead to privilege escalation, data breaches, or denial-of-service
    attacks, thereby compromising cluster security. Scanning images on admission helps identify vulnerabilities before they
    are deployed, reducing the attack surface and ensuring compliance with security policies.
  description: This control checks if an admission controller is configured to enforce image vulnerability scanning for all
    images being deployed into the Kubernetes cluster. A good configuration involves integrating tools like Open Policy Agent
    (OPA) or Kubernetes' own admission controllers to automatically reject images with known vulnerabilities or that do not
    meet security criteria. Implementing this control enhances security by ensuring that only vetted images are allowed to
    run, thus preventing potential exploitation of known vulnerabilities within container images.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/containers/images/
  compliance: []
- rule_id: k8s.image.vulnerability.scanning_enabled
  service: image
  resource: vulnerability
  requirement: Scanning Enabled
  scope: image.vulnerability.scanning_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enable Automated Image Vulnerability Scanning
  rationale: If container images are not scanned for vulnerabilities, they may contain known security flaws that can be exploited
    by attackers. This can lead to unauthorized access, data breaches, or service disruptions. Regular image scanning helps
    in identifying and mitigating vulnerabilities before they can be exploited in a production environment.
  description: This control checks if automated vulnerability scanning is enabled for container images used in Kubernetes.
    A good configuration involves integrating a security tool that automatically scans images for vulnerabilities upon creation
    or update, ensuring that only compliant images are deployed. This reduces the attack surface by preventing images with
    known vulnerabilities from being used in production, and supports compliance with industry standards such as CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/
  - https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_RA-5_0115
  - fedramp_moderate_multi_cloud_RA-5_0293
  - fedramp_moderate_multi_cloud_RA-5_2_0294
  - fedramp_moderate_multi_cloud_RA-5_3_0295
  - fedramp_moderate_multi_cloud_RA-5_4_0296
  - fedramp_moderate_multi_cloud_RA-5_5_0297
  - fedramp_moderate_multi_cloud_RA-5_8_0298
  - fedramp_moderate_multi_cloud_RA-5_11_0299
  - hipaa_multi_cloud_164_308_a_5_ii_b_0012
  - iso27001_2022_multi_cloud_A.5.19_0015
  - iso27001_2022_multi_cloud_A.8.7_0094
  - nist_800_171_r2_multi_cloud_3_14_2_3.14.2_Provide_protection_from_malicious_c_0015
  - nist_800_171_r2_multi_cloud_3_14_4_3.14.4_Update_malicious_code_protection_me_0017
  - nist_800_53_rev5_multi_cloud_RA-5-a_1018
  - nist_800_53_rev5_multi_cloud_SI-2-c_1344
  - pci_dss_v4_multi_cloud_6.2.1_0075
  - pci_dss_v4_multi_cloud_6.2.3_0077
  - pci_dss_v4_multi_cloud_6.2.4_0079
  - rbi_bank_multi_cloud_10.1_0003
  - rbi_bank_multi_cloud_21.2_0013
  - rbi_nbfc_multi_cloud_2.7_0011
  - rbi_nbfc_multi_cloud_5.3_0027
  - soc2_multi_cloud_cc_3_2_0004
  - soc2_multi_cloud_cc_6_8_0014
  - soc2_multi_cloud_cc_7_1_0015
- rule_id: k8s.ingress.certificate.expiration_check
  service: ingress
  resource: certificate
  requirement: Expiration Check
  scope: ingress.certificate.expiration_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Ingress Certificate Expiration Monitoring
  rationale: Failure to monitor and renew ingress certificates before expiration can lead to service disruptions and potential
    security breaches. Expired certificates may cause data exposure to man-in-the-middle attacks, as secure communication
    channels become vulnerable when certificates are not renewed on time.
  description: This rule checks that all Kubernetes ingress certificates are monitored for expiration and are renewed before
    they expire. A valid certificate ensures encrypted communication between clients and services, maintaining confidentiality
    and integrity of data in transit. By ensuring certificates are current, the risk of unauthorized access due to expired
    encryption mechanisms is minimized. This control is critical for maintaining compliance with security best practices and
    regulations like PCI DSS.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/certificates/
  compliance:
  - pci_dss_v4_multi_cloud_2.3.1_0027
  - pci_dss_v4_multi_cloud_2.3.2_0028
  - pci_dss_v4_multi_cloud_5.2.3.1_0068
- rule_id: k8s.ingress.controller.ssl_protocols_check
  service: ingress
  resource: controller
  requirement: Ssl Protocols Check
  scope: ingress.controller.ssl_protocols_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce Secure SSL Protocols in Ingress Controllers
  rationale: Using outdated or insecure SSL/TLS protocols in ingress controllers exposes applications to vulnerabilities such
    as man-in-the-middle attacks, data interception, and protocol downgrade attacks. Ensuring only secure protocols are enabled
    protects sensitive data in transit and complies with security standards.
  description: This rule checks that the ingress controllers in a Kubernetes cluster are configured to use only secure SSL/TLS
    protocols, such as TLS 1.2 or higher. This involves verifying the `ssl-protocols` setting in ingress controller configurations
    to ensure deprecated protocols like SSLv3 and TLS 1.0 are not in use. Proper configuration prevents the use of weak encryption
    algorithms, reduces the risk of eavesdropping, and ensures compliance with standards such as PCI DSS and the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  - https://kubernetes.io/docs/tasks/traffic-management/ingress/nginx/#tls
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - pci_dss_v4_multi_cloud_2.3.2_0028
- rule_id: k8s.ingress.controller.tls_enforced
  service: ingress
  resource: controller
  requirement: Tls Enforced
  scope: ingress.controller.tls_enforced
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce TLS in Kubernetes Ingress Controllers
  rationale: Without TLS enforced on ingress controllers, data transmitted between clients and services is vulnerable to interception
    and tampering by attackers, leading to potential data breaches and MITM attacks. This poses a significant security risk,
    especially when handling sensitive information, and can result in non-compliance with standards like PCI DSS.
  description: This rule checks that TLS is configured and enforced on all Kubernetes ingress controllers. A correct configuration
    includes specifying TLS certificates and ensuring all HTTP traffic is redirected to HTTPS. This practice ensures that
    data in transit is encrypted, maintaining confidentiality and integrity, and aligns with industry standards for data protection.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/tasks/traffic-management/ingress/ingress-tls/
  - https://kubernetes.io/docs/concepts/security/overview/#protecting-cluster-components
  compliance:
  - pci_dss_v4_multi_cloud_4.1.1_0058
  - pci_dss_v4_multi_cloud_4.1.4_0060
  - pci_dss_v4_multi_cloud_4.2.1.1_0062
- rule_id: k8s.ingress.controller.tls_protocols_configured
  service: ingress
  resource: controller
  requirement: Tls Protocols Configured
  scope: ingress.controller.tls_protocols_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce Secure TLS Protocols in Ingress Controllers
  rationale: If TLS protocols in Kubernetes ingress controllers are not properly configured, it exposes the cluster to vulnerabilities
    such as man-in-the-middle attacks, data leakage, and unauthorized data access. Attackers could exploit weak or deprecated
    TLS protocols to intercept or tamper with data in transit, undermining both data integrity and confidentiality.
  description: This control checks that the Kubernetes ingress controllers are configured to use strong, secure TLS protocols
    and ciphers, specifically prioritizing TLS 1.2 and above while disabling deprecated versions such as TLS 1.0 and 1.1.
    Ensuring proper TLS configuration mitigates risks of outdated encryption methods, enhances protection against eavesdropping
    and data breaches, and aligns with stringent compliance standards like PCI DSS and CIS Kubernetes Benchmark. A secure
    configuration also includes the use of robust ciphers and ensuring certificate validity.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  - https://kubernetes.io/docs/concepts/security/overview/#transport-security
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  compliance:
  - pci_dss_v4_multi_cloud_2.3.1_0027
  - pci_dss_v4_multi_cloud_5.2.3.1_0068
- rule_id: k8s.ingress.controller.waf_enabled
  service: ingress
  resource: controller
  requirement: Waf Enabled
  scope: ingress.controller.waf_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Web Application Firewall in Ingress Controller
  rationale: A misconfigured or absent Web Application Firewall (WAF) in the Kubernetes Ingress Controller exposes your applications
    to various web-based attacks such as SQL injection, cross-site scripting (XSS), and remote code execution. Attackers can
    exploit these vulnerabilities to gain unauthorized access, steal sensitive information, or disrupt services. By ensuring
    that a WAF is enabled and correctly configured, you add an essential layer of security that scrutinizes incoming HTTP
    requests and blocks malicious traffic, thereby reducing the attack surface.
  description: This control checks whether a Web Application Firewall (WAF) is enabled and properly configured in the Kubernetes
    Ingress Controller. A correctly configured WAF inspects HTTP requests against a set of predefined security rules to detect
    and block harmful web traffic. A robust WAF configuration includes custom rules tailored to the application's specific
    needs, logging and monitoring of suspicious activities, and regular updates to security rules. Proper WAF configuration
    helps to mitigate risks from common web vulnerabilities, ensuring compliance with security standards such as the CIS Kubernetes
    Benchmark and PCI DSS requirements.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - pci_dss_v4_multi_cloud_1.1.1_0001
  - pci_dss_v4_multi_cloud_1.1.2_0002
  - pci_dss_v4_multi_cloud_8.3.9_0111
  - pci_dss_v4_multi_cloud_8.3.11_0112
  - pci_dss_v4_multi_cloud_11.1.1_0162
  - pci_dss_v4_multi_cloud_11.1.2_0163
  - pci_dss_v4_multi_cloud_11.2.1_0164
  - pci_dss_v4_multi_cloud_11.2.2_0165
- rule_id: k8s.ingress.dos.protection_enabled
  service: ingress
  resource: dos
  requirement: Protection Enabled
  scope: ingress.dos.protection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Ingress DoS Protection for Mitigating Traffic Floods
  rationale: Ingress DoS protection is crucial as it mitigates the risk of Denial of Service attacks that can overwhelm Kubernetes
    clusters with excessive traffic, causing service disruption, resource exhaustion, and potential financial losses. Without
    proper protection, attackers can exploit ingress points to flood services with malicious requests, leading to downtime
    and degraded performance.
  description: This rule ensures that Denial of Service (DoS) protection is enabled on Kubernetes ingress resources. It checks
    for configurations such as rate limiting and IP whitelisting that are crucial in mitigating excessive traffic floods.
    Properly configured ingress DoS protections can prevent unauthorized traffic spikes and maintain the availability and
    reliability of applications by limiting the impact of potential DoS attacks.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  - https://kubernetes.io/docs/concepts/security/overview/#network-policies
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-5-a_1191
- rule_id: k8s.ingress.ingress.certificate_expiration_configured
  service: ingress
  resource: ingress
  requirement: Certificate Expiration Configured
  scope: ingress.ingress.certificate_expiration_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure Ingress Certificates Have Expiration Dates Set
  rationale: Failing to configure expiration dates on ingress certificates increases the risk of using outdated, compromised,
    or insecure certificates. Attackers could exploit expired certificates to impersonate services, intercept sensitive data,
    or conduct man-in-the-middle attacks, compromising data integrity and confidentiality.
  description: This rule checks that all TLS certificates used in Kubernetes ingress resources have defined expiration dates.
    A valid configuration includes certificates that are regularly rotated and monitored for expiration. Proper expiration
    settings ensure that certificates are updated before they become invalid, reducing the risk of service disruptions and
    enhancing the security of data transmitted over the network.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/certificates/
  compliance: []
- rule_id: k8s.ingress.ingress.dos_protection_enabled
  service: ingress
  resource: ingress
  requirement: Dos Protection Enabled
  scope: ingress.ingress.dos_protection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable DoS Protection for Kubernetes Ingress
  rationale: Without DoS protection, Kubernetes Ingress resources are vulnerable to Denial of Service attacks, which can overwhelm
    applications, degrade performance, and lead to outages. Attackers might exploit this by sending a large volume of requests
    to exhaust system resources, potentially causing service downtime and impacting availability.
  description: This control verifies that Kubernetes Ingress resources have Denial of Service (DoS) protection mechanisms
    enabled. Proper configuration involves setting rate limiting and connection limits to mitigate the risk of DoS attacks.
    Ensuring these settings are in place helps maintain service availability and protects against resource exhaustion, aligning
    with best security practices.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  - https://kubernetes.io/docs/tasks/administer-cluster/limitrange/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.ingress.ingress.tls_enforced
  service: ingress
  resource: ingress
  requirement: Tls Enforced
  scope: ingress.ingress.tls_enforced
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce TLS on Kubernetes Ingress Resources
  rationale: Failing to enforce TLS on Ingress resources exposes data in transit to eavesdropping and man-in-the-middle attacks.
    This misconfiguration can also lead to sensitive information leakage and non-compliance with security standards, increasing
    the risk of data breaches and regulatory fines.
  description: This rule checks that all Kubernetes Ingress resources have TLS configured properly by verifying the presence
    of a valid TLS section within the Ingress specification. A correctly configured TLS ensures that data transmitted between
    clients and services is encrypted, maintaining confidentiality and integrity. Implementing TLS on Ingress resources reduces
    the risk of unauthorized data access and strengthens the overall security of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/tasks/traffic-management/ingress/ingress-tls/
  - https://kubernetes.io/docs/concepts/security/overview/#node-to-node-and-pod-to-pod-communications
  compliance: []
- rule_id: k8s.ingress.no.public_access
  service: ingress
  resource: 'no'
  requirement: Public Access
  scope: ingress.no.public_access
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict Public Access to Kubernetes Ingress Resources
  rationale: Exposing Kubernetes ingress resources to the public internet without proper controls can lead to unauthorized
    access and potential data breaches. Attackers can exploit misconfigured ingress resources to access internal services,
    leading to information leakage, service disruption, or further lateral attacks within the cluster.
  description: This check ensures that Kubernetes ingress resources are not publicly accessible by default. It validates that
    ingress resources are configured with appropriate host whitelisting, HTTPS/TLS encryption, and network policies to restrict
    access to trusted sources only. By limiting public exposure, this control helps mitigate the risk of unauthorized access
    and protects sensitive applications and data from external threats.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  - https://kubernetes.io/docs/concepts/security/overview/#protecting-cluster-components-from-unwanted-access
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-25_1277
- rule_id: k8s.ingress.tls.certificate_expiration_check
  service: ingress
  resource: tls
  requirement: Certificate Expiration Check
  scope: ingress.tls.certificate_expiration_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure Ingress TLS Certificates Are Valid and Not Expired
  rationale: TLS certificates that are expired or invalid can expose applications to man-in-the-middle attacks, unauthorized
    access, and data breaches as they fail to establish a secure communication channel. Attackers can exploit expired certificates
    to impersonate services, leading to data interception and compromise. Proper certificate management is crucial for maintaining
    data integrity and confidentiality.
  description: This rule checks that all TLS certificates used in Kubernetes Ingress resources are currently valid and not
    expired. It ensures that certificates used for encrypting traffic between clients and services are not outdated, which
    could otherwise lead to failed secure connections or security vulnerabilities. A valid TLS configuration helps ensure
    encrypted data transmission, protects against eavesdropping, and complies with security frameworks like the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/security/certificates/
  compliance:
  - pci_dss_v4_multi_cloud_3.6.1.1_0045
- rule_id: k8s.ingress.tls.enabled
  service: ingress
  resource: tls
  requirement: Enabled
  scope: ingress.tls.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce TLS for Ingress Resources
  rationale: Ingress resources without TLS enabled are vulnerable to man-in-the-middle attacks, where an attacker could intercept
    and alter the data being transmitted. This could lead to data breaches, unauthorized access, and potentially compromised
    integrity of communications. Enforcing TLS helps ensure that data transmitted between clients and the Kubernetes cluster
    is encrypted and secure.
  description: This rule checks that all Kubernetes Ingress resources are configured to use TLS. A valid TLS configuration
    should include a properly defined 'tls' section in the Ingress resource, specifying the secret containing the TLS certificate
    and key. This helps protect data in transit by encrypting communications between the client and the ingress controller,
    reducing the risk of eavesdropping and data tampering.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/tasks/traffic-management/ingress/ingress-tls/
  - https://kubernetes.io/docs/concepts/security/overview/#transport-security
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-17_0013
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-9_0040
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-7_0132
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-8_0133
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-13_0136
  - fedramp_moderate_multi_cloud_AC-17_0034
  - fedramp_moderate_multi_cloud_AC-17_1_0035
  - fedramp_moderate_multi_cloud_AC-17_2_0036
  - fedramp_moderate_multi_cloud_AC-17_3_0037
  - fedramp_moderate_multi_cloud_AC-17_4_0038
  - fedramp_moderate_multi_cloud_CA-9_0099
  - fedramp_moderate_multi_cloud_SC-7_0332
  - fedramp_moderate_multi_cloud_SC-7_3_0333
  - fedramp_moderate_multi_cloud_SC-7_4_0334
  - fedramp_moderate_multi_cloud_SC-7_5_0335
  - fedramp_moderate_multi_cloud_SC-7_7_0336
  - fedramp_moderate_multi_cloud_SC-7_8_0337
  - fedramp_moderate_multi_cloud_SC-7_10_0338
  - fedramp_moderate_multi_cloud_SC-7_12_0339
  - fedramp_moderate_multi_cloud_SC-7_18_0340
  - fedramp_moderate_multi_cloud_SC-7_20_0341
  - fedramp_moderate_multi_cloud_SC-7_21_0342
  - fedramp_moderate_multi_cloud_SC-8_0343
  - fedramp_moderate_multi_cloud_SC-8_1_0344
  - fedramp_moderate_multi_cloud_SC-13_0348
  - hipaa_multi_cloud_164_312_e_1_0030
  - hipaa_multi_cloud_164_312_e_2_ii_0032
  - iso27001_2022_multi_cloud_A.10.1_0001
  - iso27001_2022_multi_cloud_A.8.1_0064
  - iso27001_2022_multi_cloud_A.8.10_0065
  - iso27001_2022_multi_cloud_A.8.11_0066
  - iso27001_2022_multi_cloud_A.8.12_0067
  - iso27001_2022_multi_cloud_A.8.13_0068
  - iso27001_2022_multi_cloud_A.8.14_0069
  - iso27001_2022_multi_cloud_A.8.15_0070
  - iso27001_2022_multi_cloud_A.8.17_0072
  - iso27001_2022_multi_cloud_A.8.18_0073
  - iso27001_2022_multi_cloud_A.8.19_0074
  - iso27001_2022_multi_cloud_A.8.24_0080
  - nist_800_171_r2_multi_cloud_3_11_3_3.11.3_Remediate_vulnerabilities_in_accord_0002
  - nist_800_171_r2_multi_cloud_3_13_15_3.13.15_Protect_the_authenticity_of_commu_0006
  - nist_800_171_r2_multi_cloud_3_1_1_3.1.1_Limit_system_access_to_authorized_use_0020
  - nist_800_171_r2_multi_cloud_3_1_13_3.1.13_Employ_cryptographic_mechanisms_to_0022
  - nist_800_171_r2_multi_cloud_3_1_14_3.1.14_Route_remote_access_via_managed_acc_0023
  - nist_800_53_rev5_multi_cloud_AC-17-b_0152
  - nist_800_53_rev5_multi_cloud_CA-9-b_0350
  - nist_800_53_rev5_multi_cloud_PM-11-b_0867
  - nist_800_53_rev5_multi_cloud_PM-17-b_0876
  - nist_800_53_rev5_multi_cloud_SC-7-a_1224
  - nist_800_53_rev5_multi_cloud_SC-7-b_1225
  - nist_800_53_rev5_multi_cloud_SC-7-c_1226
  - nist_800_53_rev5_multi_cloud_SC-13-a_1248
  - pci_dss_v4_multi_cloud_1.1.1_0001
  - pci_dss_v4_multi_cloud_1.1.2_0002
  - pci_dss_v4_multi_cloud_1.2.1_0003
  - pci_dss_v4_multi_cloud_1.2.5_0005
  - pci_dss_v4_multi_cloud_1.2.6_0006
  - pci_dss_v4_multi_cloud_1.2.7_0007
  - pci_dss_v4_multi_cloud_1.2.8_0008
  - pci_dss_v4_multi_cloud_1.3.1_0009
  - pci_dss_v4_multi_cloud_1.3.2_0010
  - pci_dss_v4_multi_cloud_2.2.2_0021
  - pci_dss_v4_multi_cloud_2.2.6_0025
  - pci_dss_v4_multi_cloud_2.2.7_0026
  - pci_dss_v4_multi_cloud_2.3.1_0027
  - pci_dss_v4_multi_cloud_2.3.2_0028
  - pci_dss_v4_multi_cloud_4.1.2_0059
  - pci_dss_v4_multi_cloud_4.2.1.1_0062
  - pci_dss_v4_multi_cloud_5.2.3.1_0068
  - pci_dss_v4_multi_cloud_7.2.1_0091
  - pci_dss_v4_multi_cloud_7.2.2_0092
  - pci_dss_v4_multi_cloud_7.2.3_0093
  - pci_dss_v4_multi_cloud_7.2.5_0094
  - pci_dss_v4_multi_cloud_7.2.5.1_0095
  - pci_dss_v4_multi_cloud_8.3.9_0111
  - pci_dss_v4_multi_cloud_8.3.11_0112
  - pci_dss_v4_multi_cloud_9.2.4_0125
  - pci_dss_v4_multi_cloud_9.3.2_0128
  - pci_dss_v4_multi_cloud_9.4.1.1_0131
  - pci_dss_v4_multi_cloud_9.4.1.2_0132
  - pci_dss_v4_multi_cloud_11.1.1_0162
  - pci_dss_v4_multi_cloud_11.1.2_0163
  - pci_dss_v4_multi_cloud_11.2.1_0164
  - pci_dss_v4_multi_cloud_11.2.2_0165
  - pci_dss_v4_multi_cloud_11.3.1_0166
  - pci_dss_v4_multi_cloud_11.3.2_0168
  - pci_dss_v4_multi_cloud_11.3.2.1_0169
  - pci_dss_v4_multi_cloud_12.1.2_0178
  - pci_dss_v4_multi_cloud_12.1.3_0179
  - pci_dss_v4_multi_cloud_12.1.4_0180
  - pci_dss_v4_multi_cloud_12.5.1_0186
  - pci_dss_v4_multi_cloud_12.5.3_0187
  - pci_dss_v4_multi_cloud_12.6.1_0188
  - pci_dss_v4_multi_cloud_12.6.2_0189
  - pci_dss_v4_multi_cloud_12.6.3_0190
  - pci_dss_v4_multi_cloud_12.6.3.1_0191
  - pci_dss_v4_multi_cloud_12.7.1_0192
  - pci_dss_v4_multi_cloud_12.8.1_0193
  - pci_dss_v4_multi_cloud_12.8.2_0194
  - pci_dss_v4_multi_cloud_12.8.3_0195
  - pci_dss_v4_multi_cloud_12.8.4_0196
  - pci_dss_v4_multi_cloud_12.8.5_0197
  - pci_dss_v4_multi_cloud_12.10.3_0200
  - pci_dss_v4_multi_cloud_12.10.4_0201
  - pci_dss_v4_multi_cloud_12.10.4.1_0202
  - pci_dss_v4_multi_cloud_12.10.5_0203
  - pci_dss_v4_multi_cloud_12.10.6_0204
  - pci_dss_v4_multi_cloud_12.10.7_0205
  - rbi_nbfc_multi_cloud_2.11_0015
  - soc2_multi_cloud_cc_6_6_0012
  - soc2_multi_cloud_cc_6_7_0013
- rule_id: k8s.ingress.tls.enforced
  service: ingress
  resource: tls
  requirement: Enforced
  scope: ingress.tls.enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce TLS in Ingress Resources
  rationale: Without enforcing TLS, data transmitted between clients and services may be exposed to eavesdropping or tampering
    by attackers. This can lead to the interception of sensitive information or unauthorized access to services, especially
    in scenarios where ingress resources expose services to the internet.
  description: This control checks that all Kubernetes ingress resources have TLS configurations properly enforced. A valid
    TLS configuration requires specifying the `tls` field in ingress resources with correctly set `hosts` and `secretName`
    attributes. By ensuring TLS is enforced, data integrity and confidentiality between clients and services are maintained,
    thus protecting against man-in-the-middle attacks and ensuring compliance with security best practices.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/tasks/traffic-management/ingress/secure-ingress/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.inventory.review.scheduled
  service: inventory
  resource: review
  requirement: Scheduled
  scope: inventory.review.scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regularly Scheduled Kubernetes Resource Inventory Review
  rationale: Without regular inventory reviews, misconfigurations or unauthorized changes in Kubernetes resources could go
    unnoticed, increasing the risk of security vulnerabilities and compliance issues. Attackers could exploit these unnoticed
    changes to gain unauthorized access or escalate privileges within the cluster.
  description: This control checks that a schedule is in place for reviewing the entire Kubernetes resource inventory, including
    deployments, services, and configurations. Regular reviews help identify unauthorized changes, detect configuration drift,
    and ensure compliance with security policies. A well-configured schedule ensures these reviews are conducted consistently,
    reducing the risk of exposure to vulnerabilities and maintaining security posture.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/
  - https://kubernetes.io/docs/concepts/overview/components/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-8-b_0425
- rule_id: k8s.kubelet.anonymous.auth_disabled
  service: kubelet
  resource: anonymous
  requirement: Auth Disabled
  scope: kubelet.anonymous.auth_disabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Anonymous Authentication for Kubelet
  rationale: Enabling anonymous authentication for the kubelet allows unauthenticated users to access kubelet APIs, which
    could lead to unauthorized access to sensitive node-level operations. This misconfiguration could be exploited to gain
    unauthorized access to node resources, execute arbitrary code, or disrupt cluster operations.
  description: This control checks that the kubelet is configured with anonymous authentication set to false. A secure configuration
    ensures that all requests to the kubelet are authenticated, preventing unauthorized users from executing commands or accessing
    sensitive information. Disabling anonymous access strengthens the security posture by ensuring that only authenticated
    and authorized users can interact with the kubelet API, mitigating potential unauthorized access and privilege escalation
    attacks.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-kubelet-usage
  - https://kubernetes.io/docs/setup/best-practices/authentication/
  compliance:
  - cis_kubernetes_kubernetes_2.1.2_0179
- rule_id: k8s.kubelet.authorization.mode_not_always_allow
  service: kubelet
  resource: authorization
  requirement: Mode Not Always Allow
  scope: kubelet.authorization.mode_not_always_allow
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubelet Authorization Mode is Set to Webhook or Node
  rationale: If the kubelet authorization mode is set to 'AlwaysAllow', any request made to the kubelet API is automatically
    authorized, leading to potential unauthorized access. This can expose sensitive operations on nodes, such as executing
    commands, retrieving logs, or accessing confidential data, thereby increasing the risk of malicious activities and data
    breaches.
  description: This rule checks the kubelet configuration to ensure that the authorization mode is not set to 'AlwaysAllow'.
    Instead, it should be configured to 'Webhook' or 'Node' to enforce proper authorization checks. 'Webhook' mode allows
    for external authorization services to validate requests, while 'Node' restricts authorization based on node identity,
    both reducing the risk of unauthorized access. Properly configuring the kubelet authorization mode enhances the security
    posture by ensuring only authorized requests are processed, thus mitigating risks associated with unauthorized access
    and operations.
  references:
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_4.2.2_0085
- rule_id: k8s.kubelet.configmap.file_permissions_check
  service: kubelet
  resource: configmap
  requirement: File Permissions Check
  scope: kubelet.configmap.file_permissions_check
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Ensure Kubelet ConfigMap Files Have Secure Permissions
  rationale: Improper file permissions on Kubelet ConfigMap files can lead to unauthorized access or modification by malicious
    actors. This misconfiguration could allow an attacker to read sensitive information or inject malicious content, facilitating
    data breaches or privilege escalation within the Kubernetes cluster.
  description: This control checks that the Kubelet's ConfigMap files have permissions set to restrict access to only necessary
    users and processes. Specifically, it validates that these files are not globally readable or writable, adhering to the
    principle of least privilege. Ensuring proper file permissions helps mitigate risks of unauthorized access or tampering,
    thereby protecting sensitive data and maintaining cluster integrity.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  compliance:
  - cis_kubernetes_kubernetes_4.1.9_0082
- rule_id: k8s.kubelet.read.only_port_disabled
  service: kubelet
  resource: read
  requirement: Only Port Disabled
  scope: kubelet.read.only_port_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Kubelet Read-Only Port
  rationale: The kubelet read-only port, by default, exposes sensitive endpoints without authentication, allowing potential
    attackers to retrieve cluster information and perform reconnaissance. Disabling this port mitigates the risk of unauthorized
    access to kubelet endpoints, thereby reducing the attack surface and preventing data leakage or malicious actions against
    the cluster.
  description: This control checks that the kubelet's read-only port (default 10255) is disabled. A correct configuration
    involves setting the `--read-only-port=0` flag in the kubelet configuration. By disabling the read-only port, unnecessary
    access to kubelet metrics and potentially sensitive data are blocked, enhancing the security posture of the Kubernetes
    nodes. This configuration aligns with best practices for minimizing exposed services and helps ensure compliance with
    security benchmarks such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#check-kubelet-configuration
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-network-access
  compliance:
  - cis_kubernetes_kubernetes_4.2.4_0087
- rule_id: k8s.kubelet.seccomp.default_enabled
  service: kubelet
  resource: seccomp
  requirement: Default Enabled
  scope: kubelet.seccomp.default_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Default Seccomp Profile for Kubelet
  rationale: Seccomp provides an additional layer of security by restricting system calls that containers can make, reducing
    the attack surface. Without a default seccomp profile, containers may have unrestricted access to system calls, potentially
    allowing attackers to exploit vulnerabilities in the Linux kernel and gain unauthorized access or escalate privileges.
  description: This rule checks if the default seccomp profile is enabled in the kubelet configuration. A correctly configured
    default seccomp profile restricts the system calls available to containers, mitigating the risk of kernel exploitation
    and maintaining compliance with security benchmarks. The configuration is deemed secure when the `--seccomp-profile-root`
    is set, and the default profile is applied to containers unless explicitly overridden. This helps enforce policy consistency
    and reduces the likelihood of configuration drift that might lead to security vulnerabilities.
  references:
  - https://kubernetes.io/docs/tutorials/clusters/seccomp/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-pod-security-standards
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
  compliance:
  - cis_kubernetes_kubernetes_4.2.14_0097
- rule_id: k8s.monitoring.alerts.configured
  service: monitoring
  resource: alerts
  requirement: Configured
  scope: monitoring.alerts.configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Comprehensive Monitoring Alerts in Kubernetes
  rationale: Without properly configured monitoring alerts, anomalous activities and security breaches can go undetected in
    a Kubernetes environment. This increases the risk of prolonged unauthorized access and data breaches, as attackers might
    exploit vulnerabilities without notice. Ensuring alerts are configured helps in early detection and response to potential
    threats, significantly reducing the attack surface.
  description: This control checks that monitoring alerts in Kubernetes are configured to cover critical events such as unauthorized
    access attempts, privilege escalation, and resource usage anomalies. A well-configured alert system should include thresholds
    and triggers for unusual activities, ensuring that security teams are notified promptly. This reduces the risk of security
    incidents by enabling proactive measures and compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/
  - https://kubernetes.io/docs/tasks/debug/debug-application/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-monitoring/
  compliance:
  - rbi_bank_multi_cloud_15.2_0009
  - rbi_bank_multi_cloud_5.2_0020
- rule_id: k8s.monitoring.daemonset.centralized_logging_enabled
  service: monitoring
  resource: daemonset
  requirement: Centralized Logging Enabled
  scope: monitoring.daemonset.centralized_logging_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable Centralized Logging for Monitoring DaemonSets
  rationale: Without centralized logging, security incidents can go unnoticed, as logs dispersed across nodes are difficult
    to aggregate and analyze. This can lead to delayed detection of unauthorized access, anomalies, or breaches, allowing
    attackers to persist undetected in the system.
  description: This control verifies that all monitoring DaemonSets in the Kubernetes cluster are configured to send logs
    to a centralized logging system. A proper setup ensures that logs are collected in real-time and securely stored for analysis
    and compliance purposes. This configuration helps in detecting suspicious activities, auditing system access, and providing
    insights into potential security incidents.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/architecture/monitoring/
  compliance: []
- rule_id: k8s.monitoring.deployment.anomaly_detection_enabled
  service: monitoring
  resource: deployment
  requirement: Anomaly Detection Enabled
  scope: monitoring.deployment.anomaly_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable and Configure Anomaly Detection in Monitoring Deployments
  rationale: Without anomaly detection enabled in monitoring deployments, unusual patterns indicating potential security breaches,
    such as unauthorized access attempts or resource misuse, may go unnoticed. This could lead to undetected attacks, data
    exfiltration, or service disruptions, especially in dynamic environments like Kubernetes where changes happen rapidly.
  description: Checks that anomaly detection is enabled and properly configured within Kubernetes monitoring deployments.
    Proper configuration involves setting thresholds and alerts for unusual activities such as unexpected traffic spikes or
    abnormal resource usage patterns. By doing so, it assists in early detection of potential security threats, enabling timely
    responses to mitigate risks. This proactive approach helps maintain the integrity and availability of the cluster and
    aligns with security best practices.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.monitoring.deployment.automated_containment_enabled
  service: monitoring
  resource: deployment
  requirement: Automated Containment Enabled
  scope: monitoring.deployment.automated_containment_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Automated Containment in Monitoring Deployments
  rationale: Failing to enable automated containment can lead to delayed response to security incidents, allowing attackers
    to exploit vulnerabilities or escalate privileges within the cluster. Automated containment quickly isolates compromised
    components, minimizing potential damage and preventing lateral movement within the infrastructure.
  description: This control checks whether automated containment mechanisms, such as pod security policies or network policies,
    are enabled in Kubernetes monitoring deployments. Proper configuration involves setting up alerts and automated actions
    that restrict compromised resources, preventing unauthorized access and data exfiltration. This helps in reducing the
    attack surface and ensures prompt incident response, aligning with best practices and policies like the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.monitoring.deployment.forensic_data_collection_enabled
  service: monitoring
  resource: deployment
  requirement: Forensic Data Collection Enabled
  scope: monitoring.deployment.forensic_data_collection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Forensic Data Collection in Monitoring Deployments
  rationale: Without forensic data collection, detecting and investigating security incidents becomes difficult, leading to
    prolonged exposure and potential damage. Misconfiguration might allow attackers to erase traces of their activities, hindering
    incident response and analysis.
  description: This rule checks that forensic data collection is enabled in Kubernetes monitoring deployments. Proper configuration
    involves setting up logging and monitoring tools to capture detailed logs of system activities and changes. By ensuring
    forensic data collection is active, organizations can quickly detect security incidents, track unauthorized access, and
    comply with security standards. Good configuration includes using tools like Fluentd or Prometheus to gather logs and
    metrics, ensuring that logs are stored securely, and regularly reviewing and analyzing collected data to identify anomalies.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
  compliance: []
- rule_id: k8s.monitoring.deployment.incident_forensics_enabled
  service: monitoring
  resource: deployment
  requirement: Incident Forensics Enabled
  scope: monitoring.deployment.incident_forensics_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Incident Forensics is Enabled in Monitoring Deployments
  rationale: Without incident forensics enabled, it becomes challenging to trace and analyze security incidents, potentially
    allowing attackers to obscure their activities and persist undetected. This can lead to prolonged breaches and failure
    to mitigate damage effectively.
  description: This check verifies that incident forensics are enabled in Kubernetes monitoring deployments by ensuring logging,
    monitoring, and audit trails are properly configured. A well-configured setup includes enabling audit logs, using tools
    like Falco for real-time anomaly detection, and ensuring logs are stored securely and are accessible for analysis. Properly
    configured incident forensics help in identifying unauthorized access, tracing attack vectors, and facilitating compliance
    with security standards.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/
  compliance: []
- rule_id: k8s.monitoring.deployment.playbooks_automated_enabled
  service: monitoring
  resource: deployment
  requirement: Playbooks Automated Enabled
  scope: monitoring.deployment.playbooks_automated_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Automated Playbooks for Monitoring Deployments
  rationale: Automating playbooks in Kubernetes monitoring deployments mitigates the risk of human error and ensures consistent
    application of security configurations. Without automation, there is a higher likelihood of misconfiguration, which can
    be exploited by attackers to gain unauthorized access or disrupt services.
  description: This control checks if automated playbooks are enabled for Kubernetes monitoring deployments. Automated playbooks
    ensure that security configurations are consistently applied across all monitoring components, reducing the risk of manual
    errors and non-compliance with security policies. A proper setup involves using Infrastructure as Code (IaC) tools like
    Ansible or Terraform to manage configurations, ensuring they are version-controlled and auditable. This approach helps
    in maintaining a secure and reliable monitoring setup, preventing potential breaches and ensuring alignment with industry
    best practices and standards.
  references:
  - https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/
  - https://kubernetes.io/docs/concepts/configuration/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/
  compliance: []
- rule_id: k8s.monitoring.deployment.proactive_hunting_enabled
  service: monitoring
  resource: deployment
  requirement: Proactive Hunting Enabled
  scope: monitoring.deployment.proactive_hunting_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Proactive Threat Hunting in Monitoring Deployments
  rationale: Without proactive threat hunting, malicious activities within the cluster may go undetected, leading to potential
    data breaches and unauthorized access. Attack vectors such as privilege escalation, lateral movement within the network,
    and exploitation of vulnerabilities can be identified and mitigated early with proactive monitoring.
  description: This rule checks that proactive threat hunting is enabled and configured in Kubernetes monitoring deployments.
    A correct setup involves enabling anomaly detection, behavioral analysis, and continuous threat intelligence updates.
    Proactive monitoring helps quickly identify suspicious activities and potential threats, enhancing the security posture
    and ensuring compliance with best practices such as those outlined in the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/security/
  compliance: []
- rule_id: k8s.monitoring.persistentvolume.retention_policies_enforced_enabled
  service: monitoring
  resource: persistentvolume
  requirement: Retention Policies Enforced Enabled
  scope: monitoring.persistentvolume.retention_policies_enforced_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Retention Policies on PersistentVolumes for Data Security
  rationale: Without enforced retention policies on PersistentVolumes, sensitive data may be retained longer than necessary,
    increasing the risk of unauthorized access or data breaches. Attackers could exploit this to obtain outdated but sensitive
    information, leading to potential compliance violations and data leaks.
  description: This rule verifies that retention policies are configured on PersistentVolumes to automatically delete or archive
    data that is no longer needed. Proper retention policies help in minimizing data exposure by ensuring that data is stored
    only for as long as it is required. This configuration mitigates risks of unauthorized access by ensuring data is not
    kept beyond its useful lifecycle, thereby supporting compliance with data protection standards and reducing storage costs.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://kubernetes.io/docs/concepts/storage/volume-snapshot-classes/
  - https://kubernetes.io/docs/tasks/administer-cluster/pvc-storage-policy/
  compliance: []
- rule_id: k8s.monitoring.pod.alerting_system_configured
  service: monitoring
  resource: pod
  requirement: Alerting System Configured
  scope: monitoring.pod.alerting_system_configured
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Ensure Alerting System is Configured for Monitoring Pods
  rationale: Without an alerting system, critical security events and anomalies in Kubernetes monitoring pods can go unnoticed,
    leaving the cluster vulnerable to breaches. Misconfigured or absent alerting systems can delay the detection of unauthorized
    access or misconfigurations, increasing the risk of data loss or corruption.
  description: This control verifies that an alerting system is properly configured for all Kubernetes monitoring pods. It
    ensures that alerts are set up to notify administrators of suspicious activities or configuration changes. A properly
    configured alerting system should be capable of detecting and reporting on key security events, such as unauthorized access
    attempts, resource spikes indicative of a denial-of-service attack, or configuration drifts. This setup helps maintain
    a proactive security stance by enabling timely responses to potential threats.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance: []
- rule_id: k8s.monitoring.prometheus.alerts_configured
  service: monitoring
  resource: prometheus
  requirement: Alerts Configured
  scope: monitoring.prometheus.alerts_configured
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Ensure Prometheus Alerts Are Configured Properly
  rationale: Without properly configured alerts in Prometheus, security incidents may go unnoticed, allowing potential threats
    to exploit vulnerabilities within the Kubernetes cluster. Misconfigurations can lead to delayed detection of anomalous
    activities such as unauthorized access attempts or resource misuse, increasing the risk of data breaches and service disruptions.
  description: This rule checks that Prometheus, used for monitoring Kubernetes, has alerts configured according to security
    best practices. A well-configured alerting system can promptly notify administrators of suspicious activities or resource
    anomalies. Proper alert configurations should cover key metrics like API server access, node resource usage, and pod status
    changes. This helps in maintaining operational visibility and ensures compliance with security standards by providing
    timely incident response capabilities.
  references:
  - https://kubernetes.io/docs/concepts/cluster-administration/monitoring/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
  - https://prometheus.io/docs/alerting/latest/overview/
  compliance:
  - nist_800_171_r2_multi_cloud_3_14_3_3.14.3_Monitor_system_security_alerts_and_0016
  - nist_800_171_r2_multi_cloud_3_1_4_3.1.4_Separate_the_duties_of_individuals_to_0027
- rule_id: k8s.monitoring.server.audit_policy_comprehensive_enforced
  service: monitoring
  resource: server
  requirement: Audit Policy Comprehensive Enforced
  scope: monitoring.server.audit_policy_comprehensive_enforced
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enforce Comprehensive Audit Policy on Kubernetes Monitoring Servers
  rationale: Without a comprehensive audit policy, malicious activities such as unauthorized access, privilege escalation,
    and data breaches may go undetected. Attackers could exploit this lack of visibility to compromise the cluster, exfiltrate
    sensitive data, or disrupt services. Implementing a thorough audit policy helps in identifying suspicious activities and
    ensuring accountability, thereby strengthening the overall security posture.
  description: This control checks that a comprehensive audit policy is enforced on Kubernetes monitoring servers. It validates
    the presence of audit configurations that capture significant events like authentication attempts, configuration changes,
    and access to sensitive resources. A well-configured audit policy enables detection of unauthorized activities and facilitates
    forensic investigations. A good configuration includes logging of successful and failed requests, access to secrets, and
    modifications to roles and bindings. This enhances security by providing visibility and traceability, which are critical
    for compliance with security standards such as CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/audit-policy/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance: []
- rule_id: k8s.monitoring.server.automated_compliance_dashboard_enabled
  service: monitoring
  resource: server
  requirement: Automated Compliance Dashboard Enabled
  scope: monitoring.server.automated_compliance_dashboard_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enable Automated Compliance Dashboard for Monitoring Servers
  rationale: Without an automated compliance dashboard, vulnerabilities in the Kubernetes environment can go unnoticed, increasing
    the risk of unauthorized access or data breaches. Attackers could exploit unmonitored security gaps, such as misconfigured
    permissions or outdated components, leading to potential regulatory non-compliance and system compromise.
  description: This rule checks if the automated compliance dashboard is enabled on Kubernetes monitoring servers. The dashboard
    should be configured to continuously monitor and report on the system's compliance with security policies and standards
    like the CIS Kubernetes Benchmark. A properly configured dashboard helps identify security risks in real-time, enforce
    compliance, and ensure that any deviations are quickly addressed, thereby mitigating potential threats and enhancing the
    overall security posture.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-runasnonroot/
  - https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
  - https://kubernetes.io/docs/concepts/overview/components/
  compliance: []
- rule_id: k8s.monitoring.server.behavioral_analysis_enabled
  service: monitoring
  resource: server
  requirement: Behavioral Analysis Enabled
  scope: monitoring.server.behavioral_analysis_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Behavioral Analysis on Monitoring Servers
  rationale: Without behavioral analysis, anomalies and potential security incidents may go undetected, allowing malicious
    activities to persist within the cluster. Attackers could exploit this to escalate privileges, access sensitive information,
    or disrupt services.
  description: This rule checks if behavioral analysis features are enabled on Kubernetes monitoring servers. A properly configured
    behavioral analysis system can detect deviations from normal server activity, such as unexpected resource usage or unauthorized
    access attempts. This proactive approach helps in identifying potential security threats early, mitigating risks like
    data breaches and service disruptions. An optimal configuration aligns with best practices, ensuring that alerts are generated
    for suspicious activities, thereby enhancing overall security posture.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/
  - https://kubernetes.io/docs/tasks/monitoring/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance: []
- rule_id: k8s.monitoring.server.compliance_dashboard_automated
  service: monitoring
  resource: server
  requirement: Compliance Dashboard Automated
  scope: monitoring.server.compliance_dashboard_automated
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Automate Compliance Dashboard for Kubernetes Monitoring
  rationale: Without an automated compliance dashboard, security teams may overlook critical vulnerabilities and non-compliance
    issues in real-time. This could lead to potential breaches, as attackers can exploit unpatched vulnerabilities before
    they are detected and addressed manually. Automating this process ensures continuous monitoring and quick response to
    security threats.
  description: This rule verifies that the Kubernetes monitoring server's compliance dashboard is set up for automated monitoring
    and reporting. It checks that the dashboard continuously scans for compliance with security benchmarks, like the CIS Kubernetes
    Benchmark, and alerts on any deviations from specified configurations. A well-configured automated dashboard helps in
    early detection of vulnerabilities, minimizing the risk of unauthorized access and ensuring alignment with industry standards.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/secure-pod/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance: []
- rule_id: k8s.monitoring.server.correlation_enabled
  service: monitoring
  resource: server
  requirement: Correlation Enabled
  scope: monitoring.server.correlation_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Log Correlation in Kubernetes Monitoring Servers
  rationale: Without log correlation, detecting and investigating security incidents becomes challenging, as logs from different
    sources may not be linked, potentially allowing attackers to hide their tracks across multiple services. Enabling correlation
    ensures that all related logs can be analyzed together, improving detection of lateral movement and other malicious activities.
  description: This control verifies that log correlation is enabled on Kubernetes monitoring servers. Proper configuration
    ensures that logs from various components and services are correlated, facilitating comprehensive security monitoring
    and incident response. A well-configured correlation system aggregates logs from different sources, applies timestamps,
    and connects related events, providing a coherent view of security-related activities. This helps in identifying security
    threats more efficiently, reducing the risk of data breaches and unauthorized access.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/cluster-administration/monitoring/
  compliance: []
- rule_id: k8s.monitoring.server.security_alerts_configured_enforced
  service: monitoring
  resource: server
  requirement: Security Alerts Configured Enforced
  scope: monitoring.server.security_alerts_configured_enforced
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Ensure Security Alerts are Configured in Kubernetes Monitoring
  rationale: Without properly configured security alerts on the Kubernetes monitoring server, critical security incidents
    may go unnoticed, leading to potential data breaches or system compromises. Attack vectors such as unauthorized access
    or privilege escalation could exploit these gaps, causing severe damage to the infrastructure.
  description: This control checks that security alerts are actively configured and enforced on the Kubernetes monitoring
    server. It ensures that alerts are set up to detect unauthorized access attempts, configuration changes, and potential
    security breaches. A well-configured alerting system provides timely notifications, enabling swift incident response and
    mitigating security risks. Properly enforced alerts align with security best practices and help maintain compliance with
    standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/
  - https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/
  compliance: []
- rule_id: k8s.namespace.dedicated.for_clearinghouse
  service: namespace
  resource: dedicated
  requirement: For Clearinghouse
  scope: namespace.dedicated.for_clearinghouse
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Isolate Clearinghouse Functions in Dedicated Namespace
  rationale: Using a dedicated namespace for clearinghouse functions isolates sensitive operations, reducing the risk of namespace-level
    attacks such as privilege escalation or unauthorized resource access. In a multi-tenant cluster, this separation ensures
    that misconfigurations or vulnerabilities in one namespace do not compromise others, thus maintaining data integrity and
    compliance with regulations like HIPAA.
  description: This rule checks that clearinghouse operations are contained within a dedicated Kubernetes namespace. A dedicated
    namespace provides a logical boundary that enforces security policies such as network policies, resource quotas, and RBAC
    tailored specifically for sensitive clearinghouse activities. Proper namespace isolation prevents accidental exposure
    of sensitive data and ensures that access controls are appropriately enforced. This configuration aligns with best practices
    for securing critical services by limiting the blast radius of potential security incidents.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  - https://kubernetes.io/docs/concepts/policy/rbac/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - hipaa_multi_cloud_164_308_a_4_ii_a_0009
- rule_id: k8s.namespace.default.no_user_resources
  service: namespace
  resource: default
  requirement: No User Resources
  scope: namespace.default.no_user_resources
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Default Namespace is Free of User-Defined Resources
  rationale: The default namespace in Kubernetes is often a target for attacks due to its common usage and accessibility.
    Allowing user-defined resources in the default namespace can lead to accidental exposure of sensitive resources, privilege
    escalation, and potential namespace pollution. Misconfigurations here can be exploited by attackers to gain unauthorized
    access to resources, deploy malicious workloads, or disrupt services.
  description: This control checks that the default namespace does not contain any user-created resources such as pods, services,
    or deployments. A clean default namespace prevents misconfigurations that could lead to security vulnerabilities. By ensuring
    no user resources are deployed here, it minimizes the risk of unauthorized access and helps maintain a clear separation
    of workloads, which is critical for auditability and compliance with security standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/
  compliance:
  - cis_kubernetes_kubernetes_5.6.4_0134
  - cis_kubernetes_kubernetes_5.7.4_0140
- rule_id: k8s.namespace.existence.and_management_check
  service: namespace
  resource: existence
  requirement: And Management Check
  scope: namespace.existence.and_management_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Namespace Existence and Access Management
  rationale: Namespaces are fundamental security boundaries within Kubernetes. Misconfigured namespaces can lead to unauthorized
    access and resource conflicts, enabling potential privilege escalation or denial-of-service attacks. Proper management
    is critical to maintaining secure and isolated environments for workloads.
  description: This control verifies that all necessary namespaces exist in the Kubernetes cluster and that their access is
    properly managed through Role-Based Access Control (RBAC). It ensures namespaces are not missing, preventing potential
    disruptions in service operations. Additionally, it checks for configured access controls, ensuring that only authorized
    users and services can interact with resources within each namespace. This reduces the risk of unauthorized access and
    data breaches, thereby maintaining isolation and compliance with security best practices.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - cis_kubernetes_kubernetes_5.7.1_0137
- rule_id: k8s.namespace.isolation.check
  service: namespace
  resource: isolation
  requirement: Check
  scope: namespace.isolation.check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Namespace Network Policies are Configured
  rationale: Without proper network policy configurations, namespaces can communicate freely with each other, potentially
    allowing attackers to move laterally within the cluster. This can lead to data breaches and unauthorized access to sensitive
    resources.
  description: This check ensures that Kubernetes namespaces have network policies configured to restrict traffic between
    pods and external endpoints. Properly configured network policies act as a firewall for pod communication, allowing only
    explicitly permitted traffic. This reduces the attack surface by preventing unauthorized access and data leakage, and
    it aligns with best practices for secure multi-tenancy in Kubernetes environments.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/
  compliance:
  - cis_kubernetes_kubernetes_5.6.1_0131
- rule_id: k8s.namespace.isolation.configured
  service: namespace
  resource: isolation
  requirement: Configured
  scope: namespace.isolation.configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Namespace Isolation for Multi-Tenancy Security
  rationale: Without proper namespace isolation, there is a risk of privilege escalation and unauthorized access between tenants.
    Attackers could exploit misconfigured namespaces to access sensitive resources across the cluster, leading to data breaches
    and denial-of-service attacks.
  description: This rule verifies that namespaces are configured to ensure isolation between workloads, particularly in a
    multi-tenant environment. It checks for appropriate use of NetworkPolicies, ResourceQuotas, and Role-Based Access Control
    (RBAC) to prevent resource contention and unauthorized access. Proper namespace isolation mitigates risks by restricting
    communication and access to resources, ensuring that tenants operate within their defined boundaries.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  - https://kubernetes.io/docs/concepts/policy/network-policies/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance:
  - cis_kubernetes_kubernetes_1.6.3_0172
- rule_id: k8s.namespace.namespace.dedicated_for_clearinghouse_configured
  service: namespace
  resource: namespace
  requirement: Dedicated For Clearinghouse Configured
  scope: namespace.namespace.dedicated_for_clearinghouse_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Dedicated Namespace for Clearinghouse Operations
  rationale: Misconfiguration of namespaces can lead to unauthorized access and privilege escalation. By dedicating a specific
    namespace for clearinghouse operations, you isolate sensitive operations from other workloads, reducing the risk of lateral
    movement by attackers and minimizing the attack surface.
  description: This control checks that a dedicated namespace is configured specifically for clearinghouse operations. A properly
    configured namespace should have specific network policies, resource quotas, and access controls that restrict operations
    to only the required resources and users. This ensures that sensitive operations related to clearinghouse activities are
    isolated from other application components, mitigating the risk of unauthorized access, data leakage, and compliance violations.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance: []
- rule_id: k8s.namespace.namespace.resources_configured
  service: namespace
  resource: namespace
  requirement: Resources Configured
  scope: namespace.namespace.resources_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Resource Quotas and Limits in Namespaces
  rationale: Not configuring resource quotas and limits in a Kubernetes namespace can lead to resource exhaustion, impacting
    the availability of the cluster. Malicious or misconfigured workloads can consume excessive CPU or memory, degrading the
    performance of other applications and potentially causing denial of service (DoS) conditions.
  description: This rule checks that resource quotas and limits are defined for each Kubernetes namespace. Properly setting
    resource quotas and limits ensures that no single namespace can monopolize cluster resources, which is crucial for maintaining
    multi-tenant isolation and preventing resource contention. This configuration helps mitigate risks related to resource
    hogging and ensures stable and predictable cluster performance.
  references:
  - https://kubernetes.io/docs/concepts/policy/resource-quotas/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  compliance: []
- rule_id: k8s.network.anomaly.detection_enabled
  service: network
  resource: anomaly
  requirement: Detection Enabled
  scope: network.anomaly.detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Network Anomaly Detection for Threat Mitigation
  rationale: Network anomaly detection is crucial for identifying and mitigating potential security threats such as unauthorized
    access, lateral movement, and data exfiltration. Without it, malicious activities may go unnoticed, allowing attackers
    to exploit vulnerabilities within the Kubernetes environment. This can lead to data breaches and compromised workloads.
  description: This rule checks if network anomaly detection is enabled across your Kubernetes clusters. Properly configured
    anomaly detection tools can identify deviations from normal network patterns, flagging potential security incidents. This
    involves monitoring network traffic and employing machine learning techniques to detect anomalies. A good configuration
    includes setting up alerts and integrating with logging systems to ensure timely response to threats, thereby enhancing
    the overall security posture of your Kubernetes deployments.
  references:
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/network/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - soc2_multi_cloud_cc_7_2_0016
- rule_id: k8s.network.change.audit_enabled
  service: network
  resource: change
  requirement: Audit Enabled
  scope: network.change.audit_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enable Auditing for Network Configuration Changes
  rationale: Without audit logging for network configuration changes, unauthorized modifications may go unnoticed, leading
    to potential exposure of sensitive data or denial-of-service attacks. Attackers could exploit unmonitored changes to insert
    malicious configurations or disrupt service availability.
  description: This control checks that auditing is enabled for all network-related changes in the Kubernetes cluster. A good
    configuration ensures that every change to network policies, services, and ingress resources is logged. This logging is
    crucial for detecting unauthorized access attempts, tracing suspicious activity, and meeting compliance requirements.
    Proper audit logging enables quick identification and response to security incidents, helping to maintain the integrity
    and availability of the cluster.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - soc2_multi_cloud_cc_3_4_0006
- rule_id: k8s.network.change.monitoring_enabled
  service: network
  resource: change
  requirement: Monitoring Enabled
  scope: network.change.monitoring_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Monitoring for Kubernetes Network Changes
  rationale: Without monitoring network changes, organizations are vulnerable to undetected configuration drift, unauthorized
    modifications, and potential breaches. Attackers can exploit these changes to manipulate network policies, facilitating
    lateral movement or data exfiltration within the cluster.
  description: This rule checks that all network changes within the Kubernetes cluster are monitored using established logging
    and alerting systems. Properly configured monitoring should capture events such as the creation, modification, or deletion
    of network policies. This ensures that any unauthorized or suspicious changes are promptly detected and remediated, thus
    preventing potential security incidents and ensuring compliance with security standards.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-7_0039
  - fedramp_moderate_multi_cloud_SI-4_0367
  - fedramp_moderate_multi_cloud_SI-4_1_0368
  - fedramp_moderate_multi_cloud_SI-4_2_0369
  - fedramp_moderate_multi_cloud_SI-4_4_0370
  - fedramp_moderate_multi_cloud_SI-4_5_0371
  - fedramp_moderate_multi_cloud_SI-4_10_0372
  - fedramp_moderate_multi_cloud_SI-4_11_0373
  - fedramp_moderate_multi_cloud_SI-4_12_0374
  - fedramp_moderate_multi_cloud_SI-4_14_0375
  - fedramp_moderate_multi_cloud_SI-4_16_0376
  - fedramp_moderate_multi_cloud_SI-4_18_0377
  - fedramp_moderate_multi_cloud_SI-4_19_0378
  - fedramp_moderate_multi_cloud_SI-4_20_0379
  - fedramp_moderate_multi_cloud_SI-4_22_0380
  - fedramp_moderate_multi_cloud_SI-4_23_0381
  - hipaa_multi_cloud_164_308_a_6_i_0015
  - nist_800_53_rev5_multi_cloud_CA-7-g_0343
  - nist_800_53_rev5_multi_cloud_SC-43-b_1322
  - nist_800_53_rev5_multi_cloud_SI-4-c_1393
  - soc2_multi_cloud_cc_7_1_0015
  - soc2_multi_cloud_cc_7_3_0017
- rule_id: k8s.network.changes.alerting_configured
  service: network
  resource: changes
  requirement: Alerting Configured
  scope: network.changes.alerting_configured
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Configure Alerts for Kubernetes Network Policy Changes
  rationale: Network policy changes in Kubernetes, if unmonitored, can lead to unauthorized access and potential data breaches.
    Attackers might exploit misconfigured or untracked changes to gain lateral movement within the cluster, bypassing intended
    network restrictions. By configuring alerts, immediate detection and response to unexpected changes is possible, mitigating
    potential security incidents.
  description: This rule ensures that any changes to Kubernetes network policies trigger alerts to a monitoring system. It
    checks for the presence of alerting mechanisms that notify administrators of modifications to network configurations.
    A good configuration involves integrating network policy changes with centralized logging and alerting solutions like
    Prometheus or third-party monitoring tools. This approach enhances security by providing real-time visibility and response
    capabilities to unauthorized or accidental network policy modifications.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - nist_800_53_rev5_multi_cloud_PM-31-e_0915
- rule_id: k8s.network.changes.audit_enabled
  service: network
  resource: changes
  requirement: Audit Enabled
  scope: network.changes.audit_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enable Audit Logging for Network Policy Changes
  rationale: Without audit logging for network policy changes, malicious or unauthorized modifications can go undetected,
    potentially leading to security breaches. Attackers could exploit this to alter network configurations, allowing unauthorized
    access or data exfiltration.
  description: This control checks that audit logging is enabled for changes to Kubernetes network policies. A proper configuration
    involves setting up audit policies to log all modifications to resource objects related to network policies. This helps
    in tracing unauthorized changes, ensuring accountability, and maintaining compliance with security standards. Enabling
    audit logging for network changes ensures that any attempts to modify network configurations are logged and can be reviewed
    for suspicious activity.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/
  compliance:
  - nist_800_53_rev5_multi_cloud_PM-31-b_0912
- rule_id: k8s.network.changes.logged
  service: network
  resource: changes
  requirement: Logged
  scope: network.changes.logged
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Audit Logging for Kubernetes Network Configuration Changes
  rationale: Without audit logging of network configuration changes, unauthorized or erroneous alterations can go undetected,
    potentially exposing the cluster to security vulnerabilities such as unauthorized access, data breaches, or network disruptions.
    Attackers could exploit these changes to redirect traffic, bypass security controls, or launch man-in-the-middle attacks.
  description: This control checks for the presence and correct configuration of audit logging for all network changes within
    the Kubernetes cluster. It ensures that any changes to network policies, services, ingress, and egress rules are logged
    in detail. Properly configured audit logs should capture who made the change, what was changed, when the change occurred,
    and from where it was initiated. This logging is crucial for incident response, forensic analysis, and ensuring compliance
    with security standards, as it provides visibility into network configuration changes and helps in identifying unauthorized
    or potentially harmful modifications.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  compliance: []
- rule_id: k8s.network.changes.monitored
  service: network
  resource: changes
  requirement: Monitored
  scope: network.changes.monitored
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Monitor and Audit Kubernetes Network Configuration Changes
  rationale: Unmonitored network configuration changes can lead to unauthorized access and potential data breaches. Attackers
    may exploit unsecured network policies to gain lateral movement within the cluster, intercept sensitive data, or disrupt
    service availability. Ensuring these changes are tracked helps in early detection of suspicious activities and mitigates
    potential attack vectors.
  description: This control verifies that all network configuration changes, including those to Network Policies and Services,
    are monitored and logged. Proper monitoring involves setting up audit logs and alerts for changes to critical network
    components. A good configuration ensures that audit logs are stored securely and reviewed regularly. This aids in maintaining
    a secure cluster by ensuring any unauthorized or unexpected changes can be detected and remediated promptly, thereby enhancing
    compliance with security standards.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance:
  - fedramp_moderate_multi_cloud_CA-7_0093
  - fedramp_moderate_multi_cloud_CA-7_1_0094
  - fedramp_moderate_multi_cloud_CA-7_4_0095
  - nist_800_53_rev5_multi_cloud_CA-7-b_0338
  - nist_800_53_rev5_multi_cloud_CA-7-e_0341
  - nist_800_53_rev5_multi_cloud_PM-31-c_0913
- rule_id: k8s.network.default.deny_egress
  service: network
  resource: default
  requirement: Deny Egress
  scope: network.default.deny_egress
  domain: network_security
  subcategory: network_policy
  severity: medium
  title: Enforce Default Deny Egress Network Policy
  rationale: Without a default deny egress policy, workloads can inadvertently or maliciously communicate with external networks,
    exposing sensitive data and expanding the attack surface. This misconfiguration can lead to data exfiltration, unauthorized
    access, and increased vulnerability to attacks such as lateral movement or command and control communications.
  description: This rule verifies the existence of a default deny egress network policy in Kubernetes namespaces. A compliant
    configuration ensures that no egress traffic is allowed unless explicitly permitted. This restricts workloads from initiating
    outbound connections by default, thereby minimizing risks associated with unintended data exposure and malicious network
    activities. By implementing this control, organizations can maintain strict network segmentation and adhere to the least
    privilege principle in network communications.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-egress-traffic
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#restricting-traffic
  - https://kubernetes.io/docs/concepts/security/overview/#network-policies
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-7_0047
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-5_0131
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-7_0132
  - fedramp_moderate_multi_cloud_AC-17_0034
  - fedramp_moderate_multi_cloud_AC-17_1_0035
  - fedramp_moderate_multi_cloud_AC-17_2_0036
  - fedramp_moderate_multi_cloud_AC-17_3_0037
  - fedramp_moderate_multi_cloud_AC-17_4_0038
  - fedramp_moderate_multi_cloud_CM-7_0119
  - fedramp_moderate_multi_cloud_CM-7_1_0120
  - fedramp_moderate_multi_cloud_CM-7_2_0121
  - fedramp_moderate_multi_cloud_CM-7_5_0122
  - fedramp_moderate_multi_cloud_SC-7_0332
  - fedramp_moderate_multi_cloud_SC-7_10_0338
  - fedramp_moderate_multi_cloud_SC-7_12_0339
  - fedramp_moderate_multi_cloud_SC-7_18_0340
  - fedramp_moderate_multi_cloud_SC-7_20_0341
  - fedramp_moderate_multi_cloud_SC-7_21_0342
  - fedramp_moderate_multi_cloud_SC-7_3_0333
  - fedramp_moderate_multi_cloud_SC-7_4_0334
  - fedramp_moderate_multi_cloud_SC-7_5_0335
  - fedramp_moderate_multi_cloud_SC-7_7_0336
  - fedramp_moderate_multi_cloud_SC-7_8_0337
  - hipaa_multi_cloud_164_312_a_1_0022
  - hipaa_multi_cloud_164_312_e_1_0030
  - iso27001_2022_multi_cloud_A.12.6_0003
  - iso27001_2022_multi_cloud_A.13.1_0004
  - iso27001_2022_multi_cloud_A.8.10_0065
  - iso27001_2022_multi_cloud_A.8.11_0066
  - iso27001_2022_multi_cloud_A.8.12_0067
  - iso27001_2022_multi_cloud_A.8.13_0068
  - iso27001_2022_multi_cloud_A.8.14_0069
  - iso27001_2022_multi_cloud_A.8.15_0070
  - iso27001_2022_multi_cloud_A.8.17_0072
  - iso27001_2022_multi_cloud_A.8.18_0073
  - iso27001_2022_multi_cloud_A.8.19_0074
  - iso27001_2022_multi_cloud_A.8.1_0064
  - iso27001_2022_multi_cloud_A.8.20_0076
  - iso27001_2022_multi_cloud_A.8.21_0077
  - iso27001_2022_multi_cloud_A.8.2_0075
  - nist_800_171_r2_multi_cloud_3_13_11_3.13.11_Employ_FIPS-validated_cryptograph_0005
  - nist_800_171_r2_multi_cloud_3_13_16_3.13.16_Protect_the_confidentiality_of_CU_0007
  - nist_800_171_r2_multi_cloud_3_13_1_3.13.1_Monitor_control_and_protect_commu_0004
  - nist_800_171_r2_multi_cloud_3_13_2_3.13.2_Employ_architectural_designs_softw_0008
  - nist_800_171_r2_multi_cloud_3_13_3_3.13.3_Separate_user_functionality_from_sy_0009
  - nist_800_171_r2_multi_cloud_3_13_4_3.13.4_Prevent_unauthorized_and_unintended_0010
  - nist_800_171_r2_multi_cloud_3_13_5_3.13.5_Implement_subnetworks_for_publicly_0011
  - nist_800_171_r2_multi_cloud_3_13_6_3.13.6_Deny_network_communications_traffic_0012
  - nist_800_171_r2_multi_cloud_3_13_8_3.13.8_Implement_cryptographic_mechanisms_0013
  - nist_800_171_r2_multi_cloud_3_1_14_3.1.14_Route_remote_access_via_managed_acc_0023
  - nist_800_171_r2_multi_cloud_3_1_3_3.1.3_Control_the_flow_of_CUI_in_accordance_0026
  - nist_800_171_r2_multi_cloud_3_3_1_3.3.1_Create_and_retain_system_audit_logs_a_0031
  - nist_800_171_r2_multi_cloud_3_4_7_3.4.7_Restrict_disable_or_prevent_the_use_0040
  - nist_800_53_rev5_multi_cloud_CM-7-b_0412
  - nist_800_53_rev5_multi_cloud_SC-5-b_1192
  - nist_800_53_rev5_multi_cloud_SC-7-a_1224
  - nist_800_53_rev5_multi_cloud_SC-7-c_1226
  - pci_dss_v4_multi_cloud_1.2.1_0003
  - pci_dss_v4_multi_cloud_1.2.3_0004
  - pci_dss_v4_multi_cloud_1.2.5_0005
  - pci_dss_v4_multi_cloud_1.2.6_0006
  - pci_dss_v4_multi_cloud_1.2.7_0007
  - pci_dss_v4_multi_cloud_1.2.8_0008
  - pci_dss_v4_multi_cloud_1.3.1_0009
  - pci_dss_v4_multi_cloud_1.3.3_0011
  - pci_dss_v4_multi_cloud_11.3.1_0166
  - pci_dss_v4_multi_cloud_12.1.2_0178
  - pci_dss_v4_multi_cloud_12.1.3_0179
  - pci_dss_v4_multi_cloud_12.1.4_0180
  - pci_dss_v4_multi_cloud_12.10.3_0200
  - pci_dss_v4_multi_cloud_12.10.4.1_0202
  - pci_dss_v4_multi_cloud_12.10.4_0201
  - pci_dss_v4_multi_cloud_12.10.5_0203
  - pci_dss_v4_multi_cloud_12.10.6_0204
  - pci_dss_v4_multi_cloud_12.10.7_0205
  - pci_dss_v4_multi_cloud_12.3.1_0181
  - pci_dss_v4_multi_cloud_12.3.3_0182
  - pci_dss_v4_multi_cloud_12.3.4_0183
  - pci_dss_v4_multi_cloud_12.5.1_0186
  - pci_dss_v4_multi_cloud_12.5.3_0187
  - pci_dss_v4_multi_cloud_12.6.1_0188
  - pci_dss_v4_multi_cloud_12.6.2_0189
  - pci_dss_v4_multi_cloud_12.6.3.1_0191
  - pci_dss_v4_multi_cloud_12.6.3_0190
  - pci_dss_v4_multi_cloud_12.7.1_0192
  - pci_dss_v4_multi_cloud_12.8.1_0193
  - pci_dss_v4_multi_cloud_12.8.2_0194
  - pci_dss_v4_multi_cloud_12.8.3_0195
  - pci_dss_v4_multi_cloud_12.8.4_0196
  - pci_dss_v4_multi_cloud_12.8.5_0197
  - pci_dss_v4_multi_cloud_2.2.2_0021
  - pci_dss_v4_multi_cloud_2.2.6_0025
  - pci_dss_v4_multi_cloud_2.2.7_0026
  - pci_dss_v4_multi_cloud_7.2.1_0091
  - pci_dss_v4_multi_cloud_7.2.2_0092
  - pci_dss_v4_multi_cloud_7.2.3_0093
  - pci_dss_v4_multi_cloud_7.2.5.1_0095
  - pci_dss_v4_multi_cloud_7.2.5_0094
  - pci_dss_v4_multi_cloud_9.2.2_0123
  - pci_dss_v4_multi_cloud_9.2.4_0125
  - pci_dss_v4_multi_cloud_9.3.2_0128
  - pci_dss_v4_multi_cloud_9.4.1.1_0131
  - pci_dss_v4_multi_cloud_9.4.2_0133
  - rbi_bank_multi_cloud_4.3_0018
  - rbi_nbfc_multi_cloud_2.8_0012
  - rbi_nbfc_multi_cloud_5.4_0028
  - soc2_multi_cloud_cc_6_7_0013
- rule_id: k8s.network.encryption.in_transit_enabled
  service: network
  resource: encryption
  requirement: In Transit Enabled
  scope: network.encryption.in_transit_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Network Traffic is Encrypted In Transit
  rationale: Without encryption in transit, data transmitted between Kubernetes components can be intercepted by attackers,
    leading to potential data breaches and unauthorized access. Attackers can exploit unencrypted traffic to perform man-in-the-middle
    attacks, compromising sensitive information and jeopardizing the integrity of the cluster.
  description: This check verifies that all network communications within the Kubernetes cluster are encrypted using TLS.
    A proper configuration involves ensuring that all components, including the API server, kubelets, and etcd, are set to
    use TLS for all network traffic. This mitigates risks associated with data interception and unauthorized access, ensuring
    that sensitive information remains secure and compliant with industry standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-8_0133
- rule_id: k8s.network.external.access_restricted
  service: network
  resource: external
  requirement: Access Restricted
  scope: network.external.access_restricted
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict External Network Access via Network Policies
  rationale: Improperly configured external network access can expose sensitive Kubernetes workloads to unauthorized entities,
    leading to data breaches or denial-of-service attacks. Restricting access minimizes the attack surface by ensuring only
    legitimate, necessary connections are permitted.
  description: This rule checks that network policies are in place to restrict external access to Kubernetes services. A well-configured
    network policy specifies which ingress and egress traffic is allowed, thus preventing unauthorized access to resources
    within the cluster. This control helps enforce the principle of least privilege and ensures that only expected and secure
    connections are established, aligning with compliance standards and reducing the risk of external attacks.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  compliance:
  - rbi_bank_multi_cloud_4.2_0017
- rule_id: k8s.network.flow.logging_enabled
  service: network
  resource: flow
  requirement: Logging Enabled
  scope: network.flow.logging_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable Network Flow Logging for Traffic Visibility
  rationale: Without network flow logging, malicious activities such as data exfiltration or lateral movement within a cluster
    may go undetected. Attackers might exploit unmonitored ingress and egress network paths to perform unauthorized actions,
    making it crucial to monitor and log network flows for timely detection and response.
  description: This rule checks whether network flow logging is enabled in Kubernetes clusters, ensuring that traffic between
    pods and external networks is logged. A properly configured logging setup captures detailed records of network activity,
    aiding in the detection of anomalies and the investigation of security incidents. Enabling network flow logs helps identify
    unauthorized access attempts, track data movement, and maintain compliance with security regulations.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance:
  - iso27001_2022_multi_cloud_A.8.16_0071
  - pci_dss_v4_multi_cloud_8.4.1_0113
  - pci_dss_v4_multi_cloud_8.5.1_0116
  - pci_dss_v4_multi_cloud_8.6.1_0117
  - pci_dss_v4_multi_cloud_9.1.1_0119
  - pci_dss_v4_multi_cloud_10.1.1_0141
- rule_id: k8s.network.flow.logs_enabled
  service: network
  resource: flow
  requirement: Logs Enabled
  scope: network.flow.logs_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Enable and Configure Network Flow Logs for Security Monitoring
  rationale: Without proper logging of network flows, detecting and investigating unauthorized access or data exfiltration
    becomes challenging. Network flow logs provide critical insights into the communication patterns and potential anomalous
    behavior within the Kubernetes cluster, helping to identify security incidents and mitigate risks such as DDoS attacks
    or lateral movement by attackers.
  description: This control checks if network flow logging is enabled and properly configured in the Kubernetes environment.
    A correct configuration involves enabling logs for all network flows and ensuring that they are retained for an appropriate
    duration to support forensic investigations. This setup aids in monitoring network traffic, detecting unusual patterns,
    and ensuring compliance with industry regulations and standards. Properly configured network flow logs can help detect
    unauthorized access attempts, data breaches, and other security incidents, allowing timely response and mitigation.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/
  compliance:
  - pci_dss_v4_multi_cloud_4.1.1_0058
  - rbi_bank_multi_cloud_15.2_0009
  - rbi_bank_multi_cloud_5.2_0020
  - rbi_nbfc_multi_cloud_3.5_0021
- rule_id: k8s.network.ingress.ingress_controller_secure_enabled
  service: network
  resource: ingress
  requirement: Ingress Controller Secure Enabled
  scope: network.ingress.ingress_controller_secure_enabled
  domain: network_security
  subcategory: network_policy
  severity: medium
  title: Enable TLS for Ingress Controllers
  rationale: Without enabling TLS for ingress controllers, data transmitted between clients and services can be intercepted
    and manipulated by attackers, leading to potential data breaches. Attack vectors include man-in-the-middle attacks where
    traffic is intercepted and modified, and eavesdropping on sensitive information.
  description: This rule checks that all ingress controllers have TLS enabled to encrypt traffic between clients and services.
    A properly configured ingress controller with TLS helps ensure data confidentiality and integrity by encrypting data in
    transit. This configuration mitigates risks of unauthorized access and data leakage, aligning with industry best practices
    and standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  compliance: []
- rule_id: k8s.network.ingress.tls_termination_enabled_enforced
  service: network
  resource: ingress
  requirement: Tls Termination Enabled Enforced
  scope: network.ingress.tls_termination_enabled_enforced
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce TLS Termination on Ingress Resources
  rationale: Without TLS termination, data transmitted over the network is vulnerable to interception and man-in-the-middle
    attacks. Attackers can exploit unencrypted ingress traffic to gain unauthorized access to sensitive information, compromising
    confidentiality and integrity.
  description: This rule verifies that TLS termination is properly enforced on all Kubernetes ingress resources. It checks
    for the presence of TLS configurations such as valid certificates and appropriate HTTPS endpoints. Proper enforcement
    ensures that all ingress traffic is encrypted, protecting data in transit and aligning with security best practices and
    compliance standards. This reduces the risk of data breaches and unauthorized access.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/tasks/traffic-management/ingress/secure-ingress/
  - https://kubernetes.io/docs/concepts/security/overview/#data-encryption
  compliance: []
- rule_id: k8s.network.internal.traffic_restricted
  service: network
  resource: internal
  requirement: Traffic Restricted
  scope: network.internal.traffic_restricted
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Enforce Network Policies for Internal Traffic Restriction
  rationale: Without properly configured network policies, internal Kubernetes traffic is susceptible to lateral movement
    attacks. Unauthorized access could lead to data breaches and compromise of cluster integrity. Network policies serve as
    a critical control to prevent attackers from exploiting misconfigured services or vulnerabilities within the internal
    network.
  description: This rule verifies that Kubernetes internal network traffic is restricted using Network Policies. It checks
    for policies that explicitly allow only necessary traffic between pods, namespaces, and external services, minimizing
    the risk of unauthorized access. A well-configured network policy should define ingress and egress rules, ensuring that
    only the required traffic is permitted. This reduces the attack surface and helps in maintaining compliance with security
    benchmarks such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-9_0040
- rule_id: k8s.network.inventory.documented
  service: network
  resource: inventory
  requirement: Documented
  scope: network.inventory.documented
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Maintain Comprehensive Network Inventory Documentation
  rationale: Without thorough documentation of the Kubernetes network inventory, administrators may overlook misconfigurations
    or unauthorized modifications, leading to potential security breaches. Attackers could exploit undocumented network paths
    to intercept or reroute traffic, access sensitive data, or escalate privileges within the cluster.
  description: This control checks that all components in the Kubernetes network inventory, such as services, pods, and network
    policies, are comprehensively documented. Proper documentation should include configurations, intended functions, and
    access controls. This ensures that the network architecture is transparent and auditable, reducing the risk of misconfigurations
    and unauthorized network changes. It supports security audits, facilitates incident response, and enhances compliance
    with security benchmarks such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/
  compliance:
  - rbi_bank_multi_cloud_4.2_0017
- rule_id: k8s.network.isolate.healthcare_clearinghouse
  service: network
  resource: isolate
  requirement: Healthcare Clearinghouse
  scope: network.isolate.healthcare_clearinghouse
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Implement Network Policies to Isolate Healthcare Data
  rationale: Without proper network isolation, sensitive healthcare data could be exposed to unauthorized access or lateral
    movement within the cluster, making it vulnerable to data breaches. Attack vectors such as compromised pods or misconfigured
    services can exploit this to gain unauthorized access to data or escalate privileges.
  description: This rule checks if Kubernetes network policies are implemented to isolate workloads handling healthcare data,
    ensuring that only authorized services can communicate with these workloads. A correctly configured network policy restricts
    traffic to and from healthcare-related pods, reducing the attack surface and preventing unauthorized access. Proper implementation
    aids in compliance with regulations like HIPAA, safeguarding patient data and maintaining data integrity.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-network-access
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  compliance:
  - hipaa_multi_cloud_164_308_a_4_ii_a_0009
- rule_id: k8s.network.log.enabled
  service: network
  resource: log
  requirement: Enabled
  scope: network.log.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable and Configure Network Logging
  rationale: Without network logging, malicious activities such as unauthorized access or data exfiltration can go undetected.
    Network logs provide critical insights into traffic patterns and anomalies, enabling quick response to security incidents
    and minimizing potential damage.
  description: This rule checks if network logging is enabled and properly configured in Kubernetes clusters. Proper network
    logging involves capturing detailed logs of network traffic, which includes source and destination IPs, ports, and protocols.
    This helps in detecting suspicious activities, maintaining audit trails, and supporting forensic investigations. Ensuring
    that network logs are enabled and configured according to best practices enhances visibility into network operations and
    aids in compliance with security standards.
  references:
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance: []
- rule_id: k8s.network.monitoring.alerts_configured
  service: network
  resource: monitoring
  requirement: Alerts Configured
  scope: network.monitoring.alerts_configured
  domain: logging_and_monitoring
  subcategory: monitoring
  severity: medium
  title: Configure Network Monitoring Alerts for Anomalous Traffic
  rationale: Failure to properly configure network monitoring alerts can lead to undetected malicious activities such as data
    exfiltration, denial of service attacks, or lateral movement within the cluster. Alerts provide real-time notifications
    of suspicious activities, enabling rapid incident response and minimizing potential damage.
  description: This rule checks that network monitoring tools are configured to generate alerts for anomalous traffic patterns
    and potential security incidents in the Kubernetes cluster. A well-configured alert system should cover unusual ingress/egress
    patterns, unauthorized access attempts, or abnormal resource consumption, helping to detect and mitigate threats quickly.
    It ensures compliance with security best practices and regulatory standards by providing visibility into the network layer,
    thus protecting sensitive workloads and data.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/network/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/
  compliance:
  - nist_800_53_rev5_multi_cloud_CA-7-f_0342
  - soc2_multi_cloud_cc_7_4_0018
- rule_id: k8s.network.monitoring.configured
  service: network
  resource: monitoring
  requirement: Configured
  scope: network.monitoring.configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Network Monitoring Tools Are Properly Configured
  rationale: Improper configuration of network monitoring tools can lead to undetected network anomalies, exposing the cluster
    to risks such as data breaches or lateral movement by attackers. Without effective monitoring, it becomes challenging
    to detect and respond to unauthorized access or unusual network activities, increasing the potential for prolonged security
    incidents.
  description: This control verifies that network monitoring solutions are correctly configured within a Kubernetes cluster.
    A well-configured network monitoring setup includes specifying appropriate alert thresholds, monitoring critical network
    paths, and integrating with logging and alerting systems. Ensuring these configurations helps in quickly identifying and
    mitigating potential security threats, maintaining the integrity and availability of the cluster.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  compliance:
  - nist_800_53_rev5_multi_cloud_CA-7-d_0340
- rule_id: k8s.network.monitoring.logged
  service: network
  resource: monitoring
  requirement: Logged
  scope: network.monitoring.logged
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: medium
  title: Ensure Network Monitoring Logs All Traffic
  rationale: Without comprehensive logging of network traffic, malicious activities such as unauthorized access, data exfiltration,
    or lateral movements within the cluster may go undetected. Attackers could exploit these gaps to persist in the environment
    or escalate privileges without leaving a trace.
  description: This control verifies that network monitoring in Kubernetes is configured to log all network traffic, including
    ingress and egress. A well-configured logging system should capture details such as source and destination IPs, ports,
    and protocol used. This ensures that any anomalous or unauthorized network activities can be detected and investigated
    promptly. Proper logging configurations aid in incident response and forensic investigations, and help ensure compliance
    with security standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_53_rev5_multi_cloud_CA-7-c_0339
- rule_id: k8s.network.monitoring.unauthorized_access
  service: network
  resource: monitoring
  requirement: Unauthorized Access
  scope: network.monitoring.unauthorized_access
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Network Policy to Prevent Unauthorized Access
  rationale: Without proper network policies, malicious actors can exploit unrestricted network access to perform lateral
    movement within the cluster, intercept sensitive data, or launch denial-of-service attacks. This misconfiguration can
    lead to data breaches and compromise the availability and integrity of critical applications.
  description: This control checks for the presence and enforcement of network policies that restrict access to Kubernetes
    network monitoring components. A well-defined network policy should specify which pods can communicate with network monitoring
    services, reducing the attack surface and mitigating risks of unauthorized access. It ensures that only legitimate and
    authenticated traffic is allowed, aligning with security best practices and regulatory compliance.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-4-b_1392
- rule_id: k8s.network.networkpolicy.anomaly_detection_enabled
  service: network
  resource: networkpolicy
  requirement: Anomaly Detection Enabled
  scope: network.networkpolicy.anomaly_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Anomaly Detection in Network Policies
  rationale: Without anomaly detection in network policies, unusual or malicious network activities may go unnoticed, potentially
    leading to data breaches or unauthorized access. Attackers could exploit misconfigurations or vulnerabilities to bypass
    security controls, making it crucial to detect and respond to such anomalies promptly.
  description: This rule checks if anomaly detection mechanisms are enabled in Kubernetes network policies, ensuring that
    any irregular network traffic patterns are identified and flagged. A well-configured anomaly detection system helps in
    early identification of potential threats, allowing for timely response and mitigation. Good configuration involves setting
    up monitoring tools that integrate with network policies to track deviations from typical traffic patterns, providing
    insights into potential security incidents.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance: []
- rule_id: k8s.network.networkpolicy.dns_filtering_enabled
  service: network
  resource: networkpolicy
  requirement: Dns Filtering Enabled
  scope: network.networkpolicy.dns_filtering_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable DNS Filtering in Network Policies
  rationale: Without DNS filtering, malicious actors can exploit DNS requests to exfiltrate data, conduct command and control
    activities, or perform DNS tunneling attacks. This misconfiguration can lead to unauthorized data access, resource misuse,
    and compromised cluster integrity.
  description: This control checks if Kubernetes NetworkPolicies have DNS filtering enabled, ensuring that only legitimate
    DNS traffic is allowed. A properly configured DNS filter in NetworkPolicies restricts DNS queries to known and trusted
    domains, significantly reducing the attack surface by preventing unauthorized DNS traffic from compromising the cluster.
    This security measure is essential for mitigating risks such as data exfiltration and unauthorized access through DNS-based
    techniques.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#dns-policy
  - https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/
  - https://kubernetes.io/docs/concepts/security/network-policies/
  compliance: []
- rule_id: k8s.network.networkpolicy.egress_traffic_controlled_enabled
  service: network
  resource: networkpolicy
  requirement: Egress Traffic Controlled Enabled
  scope: network.networkpolicy.egress_traffic_controlled_enabled
  domain: network_security
  subcategory: network_policy
  severity: medium
  title: Enforce Egress Traffic Restrictions with NetworkPolicies
  rationale: Without controlling egress traffic, malicious insiders or compromised pods can exfiltrate sensitive data or communicate
    with unauthorized external systems. This can lead to data breaches and non-compliance with data protection standards.
    Attackers can exploit open egress to bypass firewall restrictions, making it crucial to enforce strict egress rules.
  description: This rule checks for the presence of egress rules in Kubernetes NetworkPolicies to ensure that all outbound
    traffic is explicitly allowed or denied. A well-configured NetworkPolicy with egress controls restricts pods from communicating
    with external networks unless specifically permitted, thereby minimizing the attack surface and preventing data leakage.
    Proper egress restrictions are essential for adhering to security best practices and ensuring network segmentation.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#egress-policies
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-traffic-with-network-policies
  - https://kubernetes.io/docs/reference/access-authn-authz/network-policies/
  compliance: []
- rule_id: k8s.network.networkpolicy.encryption_in_transit_enabled
  service: network
  resource: networkpolicy
  requirement: Encryption In Transit Enabled
  scope: network.networkpolicy.encryption_in_transit_enabled
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure Network Policies Enforce TLS for Ingress and Egress Traffic
  rationale: Without encryption in transit, data transmitted between services can be intercepted by attackers through man-in-the-middle
    attacks, leading to data breaches and exposure of sensitive information. Ensuring network policies enforce TLS mitigates
    these risks by encrypting data, making it unreadable to unauthorized parties.
  description: This rule checks that all Kubernetes network policies enforce Transport Layer Security (TLS) for both ingress
    and egress traffic within the cluster. Properly configured network policies should specify the use of TLS to secure communications
    between pods and external services. This helps protect against data interception and ensures compliance with security
    best practices by maintaining confidentiality and integrity of the data transmitted across the network.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#what-you-can-do-with-network-policies
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#protecting-cluster-components-from-network-interception-and-tampering
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  compliance: []
- rule_id: k8s.network.networkpolicy.internal_traffic_restricted
  service: network
  resource: networkpolicy
  requirement: Internal Traffic Restricted
  scope: network.networkpolicy.internal_traffic_restricted
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Enforce NetworkPolicies to Restrict Internal Traffic
  rationale: Without proper NetworkPolicies, all internal traffic is allowed by default, potentially exposing services to
    lateral movement attacks. Misconfiguration can enable unauthorized access, data exfiltration, or service disruption as
    attackers exploit open communication paths within the cluster.
  description: This control checks if Kubernetes NetworkPolicies are configured to restrict internal traffic between pods
    and services. A well-defined NetworkPolicy should explicitly specify allowed traffic routes and block all others by default.
    This ensures that pods can only communicate with intended services, reducing the risk of lateral movement by unauthorized
    entities within the cluster. Proper configuration helps to enforce the principle of least privilege and is crucial for
    isolating workloads, thereby enhancing the overall security posture of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/overview/#network-policies
  compliance: []
- rule_id: k8s.network.networkpolicy.isolate_healthcare_clearinghouse_configured
  service: network
  resource: networkpolicy
  requirement: Isolate Healthcare Clearinghouse Configured
  scope: network.networkpolicy.isolate_healthcare_clearinghouse_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce NetworkPolicy for Healthcare Clearinghouse Isolation
  rationale: Without a dedicated NetworkPolicy, sensitive data within the healthcare clearinghouse could be exposed to unauthorized
    internal or external entities. This misconfiguration increases the risk of data breaches, unauthorized access, and potential
    compliance violations with healthcare data protection regulations.
  description: This rule checks that a NetworkPolicy is implemented to isolate the healthcare clearinghouse namespace, ensuring
    that only approved communication is allowed. A properly configured NetworkPolicy restricts both ingress and egress traffic,
    which minimizes the attack surface by preventing unauthorized access to sensitive healthcare data and services. This control
    helps maintain compliance with industry standards by enforcing strict communication boundaries, thereby protecting patient
    information from unauthorized exposure.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/overview/#security-goals
  compliance: []
- rule_id: k8s.network.networkpolicy.namespace_isolation_enforced
  service: network
  resource: networkpolicy
  requirement: Namespace Isolation Enforced
  scope: network.networkpolicy.namespace_isolation_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Namespace Isolation with Network Policies
  rationale: Without enforced namespace isolation, attackers can exploit network vulnerabilities to access resources across
    different namespaces, leading to potential data breaches and unauthorized access. Namespace isolation is critical in mitigating
    lateral movement within the cluster, reducing the attack surface, and ensuring that network traffic is explicitly allowed
    between namespaces only when necessary.
  description: This rule checks that Network Policies are configured to enforce isolation between namespaces. Proper configuration
    involves creating default deny-all ingress and egress policies for each namespace, ensuring that only explicitly defined
    traffic is permitted. By enforcing namespace isolation, you limit communications to only what is necessary, reducing the
    risk of unauthorized access and data leaks, and aligning with best practices for secure cluster configurations.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes/
  compliance: []
- rule_id: k8s.network.networkpolicy.network_policies_enforced
  service: network
  resource: networkpolicy
  requirement: Network Policies Enforced
  scope: network.networkpolicy.network_policies_enforced
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Enforce Network Policies for Pod Communication Control
  rationale: Without network policies, all pods can communicate with each other by default, increasing the risk of lateral
    movement by attackers. Misconfigured or absent network policies can lead to unauthorized data access and potential data
    exfiltration if a pod is compromised.
  description: This rule checks that Kubernetes clusters have network policies configured and enforced to control pod-to-pod
    communication. A proper configuration involves creating network policies that define allowed ingress and egress traffic
    for each pod, ensuring that only legitimate and necessary communications are permitted. This minimizes the attack surface,
    prevents unauthorized access, and aligns with security best practices by segmenting network traffic within the cluster.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/overview/#network-policies
  compliance: []
- rule_id: k8s.network.networkpolicy.network_segmentation_enforced
  service: network
  resource: networkpolicy
  requirement: Network Segmentation Enforced
  scope: network.networkpolicy.network_segmentation_enforced
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Enforce Network Segmentation with Network Policies
  rationale: Without enforced network segmentation, Kubernetes clusters are vulnerable to lateral movement by attackers, unauthorized
    data access, and potential data exfiltration. Network Policies act as a firewall for controlling traffic flow at the IP
    address or port level, crucial for isolating workloads and adhering to the principle of least privilege.
  description: This control checks for the presence and proper configuration of Network Policies within a Kubernetes cluster
    to enforce network segmentation. A valid configuration will define ingress and egress rules that restrict the communication
    between pods to only what is necessary. This reduces the attack surface by preventing unauthorized access and minimizing
    the potential impact of compromised pods. An adequately enforced Network Policy helps in achieving compliance with industry
    standards like the CIS Kubernetes Benchmark, which mandates restricting communication paths to mitigate risks.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.network.networkpolicy.no_public_ip_configured
  service: network
  resource: networkpolicy
  requirement: No Public Ip Configured
  scope: network.networkpolicy.no_public_ip_configured
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Restrict Public IP Exposure in NetworkPolicies
  rationale: Exposing network policies with public IPs can lead to unauthorized access and potential data breaches. Attackers
    may exploit public IP exposure to bypass internal security controls and execute attacks such as DDoS or unauthorized data
    extraction.
  description: This rule checks that Kubernetes NetworkPolicies do not configure public IPs to limit exposure to external
    networks. Proper configuration ensures that only intended internal traffic is allowed, reducing the risk of unauthorized
    access and aligning with security best practices. A secure configuration involves using private IPs and defining access
    control rules that limit traffic to specific internal sources and destinations.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/services-networking/service/
  compliance: []
- rule_id: k8s.network.networkpolicy.rate_limiting_enabled
  service: network
  resource: networkpolicy
  requirement: Rate Limiting Enabled
  scope: network.networkpolicy.rate_limiting_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Rate Limiting in Network Policies
  rationale: Without rate limiting, Kubernetes network policies may allow excessive requests, leading to denial-of-service
    attacks or resource exhaustion. Attack vectors include malicious actors flooding services with requests to degrade performance
    or cause downtime.
  description: This rule checks whether rate limiting is configured in Kubernetes network policies. A properly configured
    rate limit ensures that excessive or malicious traffic is controlled, protecting services from being overwhelmed. Effective
    rate limiting is essential for maintaining service availability and performance, preventing denial-of-service attacks,
    and ensuring compliance with industry standards.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/
  - https://kubernetes.io/docs/tasks/administer-cluster/enforce-netpol/
  compliance:
  - fedramp_moderate_multi_cloud_SC-5_0331
  - nist_800_53_rev5_multi_cloud_SC-5-a_1191
- rule_id: k8s.network.networkpolicy.restrict_egress_ports_configured
  service: network
  resource: networkpolicy
  requirement: Restrict Egress Ports Configured
  scope: network.networkpolicy.restrict_egress_ports_configured
  domain: network_security
  subcategory: network_policy
  severity: medium
  title: Enforce Egress Port Restrictions in Network Policies
  rationale: Improper egress port configurations in Kubernetes network policies can expose the cluster to data exfiltration
    and unauthorized communication with external networks. Attackers may exploit open egress ports to establish outbound connections,
    enabling data leaks or command-and-control activities. Restricting egress ports helps minimize these risks by controlling
    which ports are allowed for outbound traffic, thereby reducing the attack surface.
  description: This rule verifies that Kubernetes network policies explicitly define restricted egress ports for each namespace.
    A correctly configured egress policy specifies allowed ports and protocols for outbound traffic, preventing unrestricted
    communication that could lead to data leakage or external attacks. Properly configured egress restrictions ensure that
    only necessary traffic is allowed, aligning with security best practices and reducing potential attack vectors. This validation
    helps maintain compliance with security frameworks and mitigates the risk of unauthorized data flow.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#networkpolicy-resource
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-network-access
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies
  compliance: []
- rule_id: k8s.network.networkpolicy.restricted_to_security_services
  service: network
  resource: networkpolicy
  requirement: Restricted To Security Services
  scope: network.networkpolicy.restricted_to_security_services
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Network Policies to Security Services Only
  rationale: Restricting network policies to security services prevents unauthorized access and lateral movement within the
    cluster. Misconfiguration can expose sensitive information and services to potential attackers, increasing the risk of
    data breaches and unauthorized data manipulation.
  description: This rule checks that network policies are configured to specifically allow only necessary traffic to security
    services within the Kubernetes cluster. By limiting network communication pathways, it mitigates risks such as unauthorized
    access and data exfiltration. A properly configured network policy ensures that only defined ingress and egress traffic
    is permitted, thereby enforcing a zero-trust network model and aligning with best practices and compliance standards.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - nist_800_171_r2_multi_cloud_3_14_2_3.14.2_Provide_protection_from_malicious_c_0015
- rule_id: k8s.network.networkpolicy.review_scheduled
  service: network
  resource: networkpolicy
  requirement: Review Scheduled
  scope: network.networkpolicy.review_scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Regular NetworkPolicy Reviews for Compliance
  rationale: Neglecting regular reviews of Kubernetes NetworkPolicies can lead to outdated or overly permissive configurations,
    exposing the cluster to unauthorized access and lateral movement by attackers. Regular reviews help ensure policies are
    up-to-date with the current security requirements and threat landscape.
  description: This control checks that a schedule for reviewing Kubernetes NetworkPolicies is in place and adhered to. A
    proper schedule ensures that NetworkPolicies are evaluated regularly to maintain optimal security configurations. By enforcing
    this practice, organizations can prevent stale policies that might allow unauthorized network traffic, reducing the risk
    of data breaches and maintaining compliance with security standards.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#what-you-can-do-with-network-policies
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/#network-policies
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-2-b_0367
- rule_id: k8s.network.networkpolicy.risk_assessment_configured
  service: network
  resource: networkpolicy
  requirement: Risk Assessment Configured
  scope: network.networkpolicy.risk_assessment_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce NetworkPolicy for Pod Communication Control
  rationale: Without properly configured NetworkPolicies, Kubernetes clusters are vulnerable to unauthorized inter-pod communication,
    increasing the risk of lateral movement by attackers who exploit compromised pods to access sensitive data or escalate
    privileges. NetworkPolicies help mitigate these risks by explicitly defining allowed traffic paths, thereby reducing the
    attack surface.
  description: This rule checks that NetworkPolicies are implemented for all namespaces to ensure that pod-to-pod communication
    is restricted according to security best practices. A well-configured NetworkPolicy allows only necessary traffic, blocking
    unauthorized access and reducing the likelihood of a successful attack. Effective NetworkPolicy configurations are crucial
    for compliance with security frameworks like CIS Kubernetes Benchmark and for maintaining a secure and controlled network
    environment.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/overview/components/#kube-proxy
  compliance:
  - hipaa_multi_cloud_164_308_a_1_ii_a_0001
- rule_id: k8s.network.networkpolicy.service_to_service_encryption_enforced
  service: network
  resource: networkpolicy
  requirement: Service To Service Encryption Enforced
  scope: network.networkpolicy.service_to_service_encryption_enforced
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Enforce mTLS for Pod-to-Pod Communications
  rationale: Without service-to-service encryption, sensitive data can be intercepted during transit between pods, exposing
    it to unauthorized access and potential man-in-the-middle attacks. This misconfiguration compromises data confidentiality
    and integrity, increasing the risk of data breaches and non-compliance with regulations.
  description: This check ensures that mutual TLS (mTLS) is enforced for all pod-to-pod communications within the Kubernetes
    cluster. The validation process involves verifying that network policies are configured to require encrypted connections,
    leveraging tools like Istio or Linkerd for mTLS. Properly configured mTLS protects data in transit from eavesdropping
    and tampering, thereby reducing vulnerabilities to network-level attacks.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-layer-security-tls-for-all-communication
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/security/implementing-network-policies/
  compliance: []
- rule_id: k8s.network.networkpolicy.tls_encryption_enabled
  service: network
  resource: networkpolicy
  requirement: Tls Encryption Enabled
  scope: network.networkpolicy.tls_encryption_enabled
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure TLS Encryption for Ingress Traffic in Network Policies
  rationale: Without TLS encryption, sensitive data transmitted over the network can be intercepted by attackers through man-in-the-middle
    (MITM) attacks, leading to data breaches and non-compliance with regulatory standards. TLS ensures data integrity and
    confidentiality, significantly reducing the risk of unauthorized data access.
  description: This control checks that all ingress traffic defined by Kubernetes NetworkPolicies is encrypted using TLS.
    A properly configured NetworkPolicy should specify that connections to services utilize TLS, ensuring that all data in
    transit is securely encrypted. This safeguard helps protect sensitive information from being exposed during transmission.
    By enforcing TLS, organizations can prevent unauthorized data interception and meet compliance requirements such as those
    outlined in the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - fedramp_moderate_multi_cloud_CA-9_0099
- rule_id: k8s.network.networkpolicy.unused_policies_removed_configured
  service: network
  resource: networkpolicy
  requirement: Unused Policies Removed Configured
  scope: network.networkpolicy.unused_policies_removed_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Remove Unused NetworkPolicies to Minimize Attack Surface
  rationale: Unused NetworkPolicies can create confusion and increase the attack surface by allowing potential misconfigurations
    that attackers might exploit. They can also lead to an accumulation of stale configurations that make it harder to manage
    and audit network security effectively. If misconfigured, these policies might inadvertently allow unauthorized traffic,
    undermining the intended security posture.
  description: This rule checks for the presence of unused NetworkPolicies in the Kubernetes cluster. A good configuration
    ensures that only active and necessary NetworkPolicies are deployed, reducing the risk of misconfigurations that could
    expose the cluster to unauthorized access. By removing unused policies, clusters maintain a cleaner and more secure environment,
    which simplifies auditing and ensures that all policies are relevant and correctly applied to the intended workloads.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-best-practices/
  compliance: []
- rule_id: k8s.network.no.public_access
  service: network
  resource: 'no'
  requirement: Public Access
  scope: network.no.public_access
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict Public Network Access to Kubernetes Services
  rationale: Allowing public access to Kubernetes services increases the risk of unauthorized access and potential data breaches.
    Attackers can exploit exposed services to penetrate the network, leading to data exfiltration or service disruption. Properly
    restricting public access minimizes the attack surface and helps maintain a secure environment.
  description: This rule checks that Kubernetes services are appropriately configured to restrict public access. Services
    should be exposed only to necessary internal networks or specific external IPs using network policies or ingress controllers.
    This helps prevent unauthorized access and ensures compliance with security standards by limiting exposure to only trusted
    entities.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/
  - https://kubernetes.io/docs/concepts/services-networking/ingress/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance:
  - iso27001_2022_multi_cloud_A.13.1_0004
  - rbi_bank_multi_cloud_8.2_0025
- rule_id: k8s.network.no.public_ip
  service: network
  resource: 'no'
  requirement: Public Ip
  scope: network.no.public_ip
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Restrict Public IP Exposure for Kubernetes Services
  rationale: Exposing Kubernetes services with public IPs can lead to unauthorized access and potential data breaches. Attackers
    can exploit exposed services to gain entry into the cluster, execute DDoS attacks, or intercept sensitive data. By restricting
    public IP exposure, you minimize the attack surface and protect the cluster from external threats.
  description: This control checks that Kubernetes services do not have public IPs assigned unless absolutely necessary. It
    ensures that services intended for internal traffic are not inadvertently exposed to the internet. A secure configuration
    involves using internal IPs and leveraging Kubernetes Network Policies to regulate traffic. This reduces the risk of unauthorized
    access and aligns with industry best practices for securing workloads.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-25_1277
- rule_id: k8s.network.pod.inventory_configured
  service: network
  resource: pod
  requirement: Inventory Configured
  scope: network.pod.inventory_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pod Network Policy is Configured
  rationale: Without a properly configured Network Policy, pods may be exposed to unnecessary network traffic, increasing
    the risk of unauthorized access, data exfiltration, or denial of service attacks. Misconfigured Network Policies can lead
    to unauthorized lateral movement within the cluster.
  description: This rule checks whether a Network Policy is applied to each pod to control the inbound and outbound traffic.
    A well-configured Network Policy allows you to specify how pods communicate with each other and other network endpoints,
    reducing the attack surface by limiting network exposure. Ensuring that every pod has a Network Policy helps achieve a
    defense-in-depth strategy by enforcing least privilege principles within network communications.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.network.policies.documented
  service: network
  resource: policies
  requirement: Documented
  scope: network.policies.documented
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Comprehensive Documentation for Kubernetes Network Policies
  rationale: Undocumented or poorly documented network policies can lead to misconfigurations that expose services to unintended
    network traffic. This increases the risk of unauthorized access, data breaches, and lateral movement within the cluster
    by attackers exploiting network vulnerabilities.
  description: This control checks that all Kubernetes network policies are thoroughly documented to reflect their intended
    purpose and configuration. Proper documentation includes details of allowed and denied traffic, the namespaces and pods
    affected, and the rationale behind specific policy decisions. This ensures that network policies are correctly implemented
    and maintained, reducing the risk of security breaches and aiding in compliance with standards like the CIS Kubernetes
    Benchmark. Having well-documented network policies also facilitates audits and simplifies troubleshooting by providing
    clear guidance on network security intentions.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/#network-policy
  compliance:
  - iso27001_2022_multi_cloud_A.5.1_0005
  - iso27001_2022_multi_cloud_A.5.10_0006
  - iso27001_2022_multi_cloud_A.5.11_0007
  - iso27001_2022_multi_cloud_A.5.12_0008
  - iso27001_2022_multi_cloud_A.5.13_0009
  - iso27001_2022_multi_cloud_A.5.14_0010
  - iso27001_2022_multi_cloud_A.5.16_0012
  - iso27001_2022_multi_cloud_A.5.17_0013
- rule_id: k8s.network.policy.default_deny_ingress_check
  service: network
  resource: policy
  requirement: Default Deny Ingress Check
  scope: network.policy.default_deny_ingress_check
  domain: network_security
  subcategory: network_policy
  severity: medium
  title: Enforce Default Deny Ingress Policy for Network Security
  rationale: Without a default deny ingress policy, Kubernetes clusters are susceptible to unauthorized access and potential
    lateral movement by attackers. This misconfiguration can lead to exposure of sensitive services and data, making the cluster
    a target for network-based attacks such as data exfiltration or application layer attacks.
  description: This rule checks that every namespace in the Kubernetes cluster has a network policy that defaults to denying
    all ingress traffic unless explicitly allowed. A properly configured default deny ingress policy ensures that only traffic
    from trusted sources and services is permitted, minimizing the attack surface. Good configuration involves applying a
    network policy with 'ingress' rules that specify 'deny all' as the default action and then selectively allowing traffic
    to specific pods or services as required.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-17_0013
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-2_0042
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-7_0047
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-5_0131
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-7_0132
  - fedramp_moderate_multi_cloud_AC-17_0034
  - fedramp_moderate_multi_cloud_AC-17_1_0035
  - fedramp_moderate_multi_cloud_AC-17_2_0036
  - fedramp_moderate_multi_cloud_AC-17_3_0037
  - fedramp_moderate_multi_cloud_AC-17_4_0038
  - fedramp_moderate_multi_cloud_CM-2_0101
  - fedramp_moderate_multi_cloud_CM-2_2_0102
  - fedramp_moderate_multi_cloud_CM-2_3_0103
  - fedramp_moderate_multi_cloud_CM-2_7_0104
  - fedramp_moderate_multi_cloud_CM-7_0119
  - fedramp_moderate_multi_cloud_CM-7_1_0120
  - fedramp_moderate_multi_cloud_CM-7_2_0121
  - fedramp_moderate_multi_cloud_CM-7_5_0122
  - fedramp_moderate_multi_cloud_SC-5_0331
  - fedramp_moderate_multi_cloud_SC-7_0332
  - fedramp_moderate_multi_cloud_SC-7_3_0333
  - fedramp_moderate_multi_cloud_SC-7_4_0334
  - fedramp_moderate_multi_cloud_SC-7_5_0335
  - fedramp_moderate_multi_cloud_SC-7_7_0336
  - fedramp_moderate_multi_cloud_SC-7_8_0337
  - fedramp_moderate_multi_cloud_SC-7_10_0338
  - fedramp_moderate_multi_cloud_SC-7_12_0339
  - fedramp_moderate_multi_cloud_SC-7_18_0340
  - fedramp_moderate_multi_cloud_SC-7_20_0341
  - fedramp_moderate_multi_cloud_SC-7_21_0342
  - gdpr_multi_cloud_Article_25_Data_protection_by_design_and_by_defaul_0001
  - gdpr_multi_cloud_Article_32_Security_of_processing_0003
  - hipaa_multi_cloud_164_308_a_1_ii_b_0002
  - hipaa_multi_cloud_164_308_a_5_ii_b_0012
  - hipaa_multi_cloud_164_312_a_1_0022
  - hipaa_multi_cloud_164_312_e_1_0030
  - hipaa_multi_cloud_164_312_e_2_i_0031
  - iso27001_2022_multi_cloud_A.12.6_0003
  - iso27001_2022_multi_cloud_A.13.1_0004
  - iso27001_2022_multi_cloud_A.5.19_0015
  - iso27001_2022_multi_cloud_A.8.1_0064
  - iso27001_2022_multi_cloud_A.8.10_0065
  - iso27001_2022_multi_cloud_A.8.11_0066
  - iso27001_2022_multi_cloud_A.8.12_0067
  - iso27001_2022_multi_cloud_A.8.13_0068
  - iso27001_2022_multi_cloud_A.8.14_0069
  - iso27001_2022_multi_cloud_A.8.15_0070
  - iso27001_2022_multi_cloud_A.8.17_0072
  - iso27001_2022_multi_cloud_A.8.18_0073
  - iso27001_2022_multi_cloud_A.8.19_0074
  - iso27001_2022_multi_cloud_A.8.2_0075
  - iso27001_2022_multi_cloud_A.8.20_0076
  - iso27001_2022_multi_cloud_A.8.21_0077
  - iso27001_2022_multi_cloud_A.8.27_0083
  - iso27001_2022_multi_cloud_A.8.3_0085
  - iso27001_2022_multi_cloud_A.8.30_0086
  - iso27001_2022_multi_cloud_A.8.31_0087
  - iso27001_2022_multi_cloud_A.8.32_0088
  - iso27001_2022_multi_cloud_A.8.33_0089
  - iso27001_2022_multi_cloud_A.8.34_0090
  - iso27001_2022_multi_cloud_A.8.9_0096
  - nist_800_171_r2_multi_cloud_3_12_4_3.12.4_Develop_document_and_periodically_0003
  - nist_800_171_r2_multi_cloud_3_13_1_3.13.1_Monitor_control_and_protect_commu_0004
  - nist_800_171_r2_multi_cloud_3_13_11_3.13.11_Employ_FIPS-validated_cryptograph_0005
  - nist_800_171_r2_multi_cloud_3_13_16_3.13.16_Protect_the_confidentiality_of_CU_0007
  - nist_800_171_r2_multi_cloud_3_13_2_3.13.2_Employ_architectural_designs_softw_0008
  - nist_800_171_r2_multi_cloud_3_13_3_3.13.3_Separate_user_functionality_from_sy_0009
  - nist_800_171_r2_multi_cloud_3_13_4_3.13.4_Prevent_unauthorized_and_unintended_0010
  - nist_800_171_r2_multi_cloud_3_13_5_3.13.5_Implement_subnetworks_for_publicly_0011
  - nist_800_171_r2_multi_cloud_3_13_6_3.13.6_Deny_network_communications_traffic_0012
  - nist_800_171_r2_multi_cloud_3_13_8_3.13.8_Implement_cryptographic_mechanisms_0013
  - nist_800_171_r2_multi_cloud_3_1_14_3.1.14_Route_remote_access_via_managed_acc_0023
  - nist_800_171_r2_multi_cloud_3_1_2_3.1.2_Limit_system_access_to_the_types_of_t_0024
  - nist_800_171_r2_multi_cloud_3_1_20_3.1.20_Verify_and_control_limit_connection_0025
  - nist_800_171_r2_multi_cloud_3_1_3_3.1.3_Control_the_flow_of_CUI_in_accordance_0026
  - nist_800_171_r2_multi_cloud_3_3_1_3.3.1_Create_and_retain_system_audit_logs_a_0031
  - nist_800_171_r2_multi_cloud_3_4_2_3.4.2_Establish_and_enforce_security_config_0038
  - nist_800_171_r2_multi_cloud_3_4_6_3.4.6_Employ_the_principle_of_least_functio_0039
  - nist_800_171_r2_multi_cloud_3_4_7_3.4.7_Restrict_disable_or_prevent_the_use_0040
  - nist_800_171_r2_multi_cloud_3_6_1_3.6.1_Establish_an_operational_incident-han_0049
  - nist_800_53_rev5_multi_cloud_AC-17-b_0152
  - nist_800_53_rev5_multi_cloud_CM-2-a_0366
  - nist_800_53_rev5_multi_cloud_CM-7-b_0412
  - nist_800_53_rev5_multi_cloud_SC-5-b_1192
  - nist_800_53_rev5_multi_cloud_SC-7-a_1224
  - nist_800_53_rev5_multi_cloud_SC-7-b_1225
  - nist_800_53_rev5_multi_cloud_SC-7-c_1226
  - pci_dss_v4_multi_cloud_1.1.1_0001
  - pci_dss_v4_multi_cloud_1.1.2_0002
  - pci_dss_v4_multi_cloud_1.2.1_0003
  - pci_dss_v4_multi_cloud_1.2.3_0004
  - pci_dss_v4_multi_cloud_1.2.5_0005
  - pci_dss_v4_multi_cloud_1.2.6_0006
  - pci_dss_v4_multi_cloud_1.2.7_0007
  - pci_dss_v4_multi_cloud_1.2.8_0008
  - pci_dss_v4_multi_cloud_1.3.1_0009
  - pci_dss_v4_multi_cloud_1.3.2_0010
  - pci_dss_v4_multi_cloud_1.3.3_0011
  - pci_dss_v4_multi_cloud_2.2.2_0021
  - pci_dss_v4_multi_cloud_2.2.3_0022
  - pci_dss_v4_multi_cloud_2.2.6_0025
  - pci_dss_v4_multi_cloud_2.2.7_0026
  - pci_dss_v4_multi_cloud_7.2.1_0091
  - pci_dss_v4_multi_cloud_7.2.2_0092
  - pci_dss_v4_multi_cloud_7.2.3_0093
  - pci_dss_v4_multi_cloud_7.2.5_0094
  - pci_dss_v4_multi_cloud_7.2.5.1_0095
  - pci_dss_v4_multi_cloud_8.3.9_0111
  - pci_dss_v4_multi_cloud_8.3.11_0112
  - pci_dss_v4_multi_cloud_9.2.2_0123
  - pci_dss_v4_multi_cloud_9.2.4_0125
  - pci_dss_v4_multi_cloud_9.3.2_0128
  - pci_dss_v4_multi_cloud_9.4.1.1_0131
  - pci_dss_v4_multi_cloud_9.4.2_0133
  - pci_dss_v4_multi_cloud_11.1.1_0162
  - pci_dss_v4_multi_cloud_11.1.2_0163
  - pci_dss_v4_multi_cloud_11.2.1_0164
  - pci_dss_v4_multi_cloud_11.2.2_0165
  - pci_dss_v4_multi_cloud_11.3.1_0166
  - pci_dss_v4_multi_cloud_11.3.2_0168
  - pci_dss_v4_multi_cloud_11.3.2.1_0169
  - pci_dss_v4_multi_cloud_11.4.3_0171
  - pci_dss_v4_multi_cloud_12.1.2_0178
  - pci_dss_v4_multi_cloud_12.1.3_0179
  - pci_dss_v4_multi_cloud_12.1.4_0180
  - pci_dss_v4_multi_cloud_12.3.1_0181
  - pci_dss_v4_multi_cloud_12.3.3_0182
  - pci_dss_v4_multi_cloud_12.3.4_0183
  - pci_dss_v4_multi_cloud_12.5.1_0186
  - pci_dss_v4_multi_cloud_12.5.3_0187
  - pci_dss_v4_multi_cloud_12.6.1_0188
  - pci_dss_v4_multi_cloud_12.6.2_0189
  - pci_dss_v4_multi_cloud_12.6.3_0190
  - pci_dss_v4_multi_cloud_12.6.3.1_0191
  - pci_dss_v4_multi_cloud_12.7.1_0192
  - pci_dss_v4_multi_cloud_12.8.1_0193
  - pci_dss_v4_multi_cloud_12.8.2_0194
  - pci_dss_v4_multi_cloud_12.8.3_0195
  - pci_dss_v4_multi_cloud_12.8.4_0196
  - pci_dss_v4_multi_cloud_12.8.5_0197
  - pci_dss_v4_multi_cloud_12.10.3_0200
  - pci_dss_v4_multi_cloud_12.10.4_0201
  - pci_dss_v4_multi_cloud_12.10.4.1_0202
  - pci_dss_v4_multi_cloud_12.10.5_0203
  - pci_dss_v4_multi_cloud_12.10.6_0204
  - pci_dss_v4_multi_cloud_12.10.7_0205
  - rbi_bank_multi_cloud_14.1_0007
  - rbi_bank_multi_cloud_4.3_0018
  - rbi_nbfc_multi_cloud_2.8_0012
  - rbi_nbfc_multi_cloud_5.4_0028
  - soc2_multi_cloud_cc_3_1_0003
  - soc2_multi_cloud_cc_3_2_0004
  - soc2_multi_cloud_cc_5_2_0008
  - soc2_multi_cloud_cc_6_1_0009
  - soc2_multi_cloud_cc_6_6_0012
  - soc2_multi_cloud_cc_6_7_0013
  - soc2_multi_cloud_cc_6_8_0014
  - cis_kubernetes_kubernetes_1.6.4_0173
- rule_id: k8s.network.policy.namespace_compliance_check
  service: network
  resource: policy
  requirement: Namespace Compliance Check
  scope: network.policy.namespace_compliance_check
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enforce Namespace-Specific Network Policies
  rationale: Without namespace-specific network policies, there is a risk of unintended cross-namespace communication, which
    can lead to data leakage and unauthorized access. Attack vectors include lateral movement within clusters and exploitation
    of misconfigured network policies that allow access to sensitive resources across namespaces.
  description: This rule checks for the presence and configuration of network policies that enforce traffic restrictions at
    the namespace level. It ensures that each namespace has a defined network policy to control inbound and outbound traffic.
    A well-configured policy limits the network communication to only required pods and services, reducing the attack surface
    by preventing unauthorized inter-namespace traffic, and ensuring compliance with security best practices such as those
    outlined in the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
  compliance:
  - cis_kubernetes_kubernetes_5.3.2_0127
- rule_id: k8s.network.rate.limiting_enabled
  service: network
  resource: rate
  requirement: Limiting Enabled
  scope: network.rate.limiting_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Network Rate Limiting for Traffic Control
  rationale: Without network rate limiting, Kubernetes clusters are susceptible to Denial of Service (DoS) attacks, where
    an attacker may overwhelm the network by sending excessive requests. This can lead to service disruption, degraded performance,
    and increased costs, as resources are consumed beyond normal operating conditions. Proper rate limiting helps mitigate
    these risks by controlling the volume of traffic and ensuring fair resource usage.
  description: This control checks whether network rate limiting is configured for Kubernetes services and ingress resources.
    Proper configuration involves setting limits on the number of requests a client can make within a certain timeframe. Effective
    rate limiting helps protect the cluster from DoS attacks by ensuring that no single client can monopolize network resources.
    It ensures a balanced load across services, enhances availability, and contributes to a resilient infrastructure. The
    validation process involves reviewing Kubernetes configurations for Ingress resources and ensuring that rate limit policies
    are correctly applied according to best practices.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#annotations
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/tasks/administer-cluster/limitrange/
  compliance: []
- rule_id: k8s.network.restrict.admin_access
  service: network
  resource: restrict
  requirement: Admin Access
  scope: network.restrict.admin_access
  domain: network_security
  subcategory: network_policy
  severity: high
  title: Enforce Network Policies to Restrict Admin Protocols
  rationale: Failure to restrict administrative protocols such as SSH and RDP can lead to unauthorized access, exposing the
    cluster to potential breaches. Attackers could exploit these protocols to gain control over the cluster, exfiltrate data,
    or disrupt services. Implementing network policies helps mitigate the risk of lateral movement and privilege escalation
    within the cluster.
  description: This control checks whether network policies are in place to restrict access to administrative protocols like
    SSH and RDP. A proper configuration should define network policies that explicitly allow only specific pods or namespaces
    to communicate over these protocols, based on the principle of least privilege. By ensuring these configurations, it minimizes
    the attack surface, prevents unauthorized access, and aligns with compliance requirements such as those specified in the
    CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - nist_800_171_r2_multi_cloud_3_11_2_3.11.2_Scan_for_vulnerabilities_in_organiz_0001
  - nist_800_171_r2_multi_cloud_3_1_12_3.1.12_Monitor_and_control_remote_access_s_0021
  - rbi_bank_multi_cloud_4.3_0018
  - rbi_nbfc_multi_cloud_2.8_0012
  - rbi_nbfc_multi_cloud_5.4_0028
- rule_id: k8s.network.restrict.egress_to_internet
  service: network
  resource: restrict
  requirement: Egress To Internet
  scope: network.restrict.egress_to_internet
  domain: network_security
  subcategory: network_policy
  severity: medium
  title: Restrict Egress Traffic to Prevent Uncontrolled Internet Access
  rationale: Allowing unrestricted egress from Kubernetes pods can expose the cluster to various attack vectors, such as data
    exfiltration, external command-and-control servers, and unauthorized access to external services. By restricting egress
    traffic, you can minimize the risk of compromised pods communicating with malicious external entities, thereby maintaining
    the integrity and confidentiality of your applications and data.
  description: This control checks that Kubernetes Network Policies are in place to restrict egress traffic from pods to the
    internet. Proper configuration ensures that pods can only communicate with explicitly permitted external endpoints, reducing
    the attack surface. A secure configuration typically involves defining specific egress rules that limit traffic to necessary
    external IPs or ranges and ports. This helps achieve compliance with security standards and ensures that all outbound
    communications are intentional and monitored.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-allow-all-egress-traffic
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#limiting-pod-communication
  - https://kubernetes.io/docs/concepts/network-policy/
  compliance:
  - pci_dss_v4_multi_cloud_1.3.2_0010
  - pci_dss_v4_multi_cloud_11.3.2_0168
  - pci_dss_v4_multi_cloud_11.3.2.1_0169
- rule_id: k8s.network.restrict.external_access
  service: network
  resource: restrict
  requirement: External Access
  scope: network.restrict.external_access
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict External Network Access Using Network Policies
  rationale: Misconfiguring external network access can expose the cluster to unauthorized access and data breaches. Attackers
    could exploit open network paths to access sensitive resources, inject malicious traffic, or exfiltrate data. By restricting
    external network access, we mitigate risks such as man-in-the-middle attacks and unauthorized data exposure.
  description: This control checks for the presence and correct configuration of NetworkPolicies that limit external access
    to Kubernetes services. A properly configured NetworkPolicy should explicitly define allowed ingress and egress traffic,
    ensuring that only necessary communication is permitted to and from external networks. These configurations help in adhering
    to the principle of least privilege, reducing the attack surface, and aligning with best practices for secure network
    design.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-network-access
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  compliance:
  - iso27001_2022_multi_cloud_A.8.2_0075
  - iso27001_2022_multi_cloud_A.8.20_0076
  - pci_dss_v4_multi_cloud_1.3.3_0011
  - pci_dss_v4_multi_cloud_9.4.2_0133
- rule_id: k8s.network.restrict.ingress_ports
  service: network
  resource: restrict
  requirement: Ingress Ports
  scope: network.restrict.ingress_ports
  domain: network_security
  subcategory: network_policy
  severity: medium
  title: Restrict Ingress Ports with Network Policies
  rationale: Failing to properly restrict ingress ports can expose applications to unauthorized access and potential attacks
    such as port scanning, denial of service, or exploitation of vulnerabilities in services exposed to the internet. By controlling
    ingress traffic, you reduce the attack surface and limit potential entry points for malicious actors.
  description: This control checks that Kubernetes Network Policies are configured to restrict ingress ports to only those
    necessary for application functionality, minimizing exposure. A good configuration involves specifying the allowed ingress
    ports in Network Policies for each application, ensuring that only intended traffic can reach the services. This helps
    to enforce the principle of least privilege, reduce the risk of accidental exposure, and align with security best practices
    and compliance requirements.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#networkpolicy-resource
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-network-access
  - https://kubernetes.io/docs/concepts/services-networking/service/#external-traffic-policy
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-7-b_0412
  - pci_dss_v4_multi_cloud_1.2.3_0004
  - pci_dss_v4_multi_cloud_12.3.1_0181
  - pci_dss_v4_multi_cloud_12.3.3_0182
  - pci_dss_v4_multi_cloud_12.3.4_0183
  - pci_dss_v4_multi_cloud_9.2.2_0123
- rule_id: k8s.network.restrict.internet_access
  service: network
  resource: restrict
  requirement: Internet Access
  scope: network.restrict.internet_access
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce Network Policies to Restrict Pod Internet Access
  rationale: Allowing unrestricted internet access from pods increases the risk of data exfiltration, unauthorized communication
    with external endpoints, and exposure to internet-borne threats. Attackers who compromise a pod can exploit unrestricted
    internet access to pivot into other parts of the network or launch further attacks. By enforcing network policies, you
    can limit communication to only required endpoints, reducing the attack surface and preventing lateral movement.
  description: This control checks for the presence and enforcement of network policies that restrict internet access for
    pods. A good configuration involves defining ingress and egress rules that specify allowed traffic sources and destinations,
    effectively blocking unnecessary internet communication. Implementing such network policies helps protect sensitive data,
    ensures regulatory compliance, and aligns with security best practices by minimizing potential attack vectors.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-pod-to-pod-communication
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - iso27001_2022_multi_cloud_A.8.21_0077
- rule_id: k8s.network.restrict.public_ip_access
  service: network
  resource: restrict
  requirement: Public Ip Access
  scope: network.restrict.public_ip_access
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict Public IP Exposure for Kubernetes Services
  rationale: Exposing Kubernetes services to the public internet without restriction increases the risk of unauthorized access
    and potential exploitation. Attack vectors include brute force attacks, unauthorized data access, and service disruption
    by malicious actors exploiting open ports and services.
  description: This control checks that Kubernetes services do not have unrestricted public IP access unless explicitly required
    and secured. It ensures that LoadBalancer and NodePort services are configured with appropriate firewall rules or IP whitelisting
    to minimize exposure. Proper configuration helps prevent unauthorized access by ensuring that only trusted sources can
    interact with sensitive services, thus maintaining compliance with security standards and reducing the attack surface.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-17_0013
  - nist_800_53_rev5_multi_cloud_SC-7-b_1225
- rule_id: k8s.network.review.scheduled
  service: network
  resource: review
  requirement: Scheduled
  scope: network.review.scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Schedule Regular Network Policy Audits
  rationale: Without regular audits of network policies, there is a risk of outdated or misconfigured rules that could allow
    unauthorized access or data exfiltration. This can lead to potential security breaches such as lateral movement by attackers
    within the cluster or exposure of sensitive data.
  description: This control checks that a schedule is in place for regular audits of Kubernetes network policies. A properly
    configured schedule ensures that network policies are reviewed and updated in accordance with the latest security best
    practices. This process helps identify and rectify any misconfigurations that could lead to unauthorized access or data
    exposure. It also supports compliance with security standards like the CIS Kubernetes Benchmark by ensuring network policies
    are consistently aligned with current security requirements.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  compliance: []
- rule_id: k8s.network.risk.assessment_configured
  service: network
  resource: risk
  requirement: Assessment Configured
  scope: network.risk.assessment_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Network Policy Configuration
  rationale: Without properly configured network policies, Kubernetes clusters are vulnerable to lateral movement attacks,
    where attackers gain unauthorized access to sensitive workloads by exploiting overly permissive network configurations.
    Proper network policy configuration limits communication between pods, reducing the attack surface and mitigating risks
    such as data exfiltration and privilege escalation.
  description: This rule checks for the presence and correct configuration of Network Policies within a Kubernetes cluster.
    A well-configured Network Policy ensures that only allowed traffic can flow between pods, namespaces, or external endpoints.
    It verifies that policies are defined for each namespace, preventing unrestricted pod-to-pod communication. This helps
    in enforcing the principle of least privilege, thus enhancing overall cluster security by minimizing potential attack
    vectors.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance: []
- rule_id: k8s.network.secure.transport_policy
  service: network
  resource: secure
  requirement: Transport Policy
  scope: network.secure.transport_policy
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Network Policies for Secure Pod Communication
  rationale: Without network policies, Kubernetes pods can communicate with each other without restriction, which can lead
    to unauthorized access and data exposure. This lack of control allows potential attackers to exploit vulnerabilities or
    misconfigurations to move laterally within the cluster, compromising sensitive data or services.
  description: This control checks that Network Policies are applied to limit pod-to-pod communication based on defined rules,
    such as allowing traffic only from trusted sources or specific namespaces. By enforcing Network Policies, you can ensure
    that only authorized traffic is allowed within the cluster, reducing the risk of unauthorized access and potential data
    breaches. Properly configured Network Policies act as a form of micro-segmentation, providing an additional layer of security
    that aligns with security best practices and compliance requirements.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - hipaa_multi_cloud_164_312_c_1_0027
- rule_id: k8s.network.service.authorization_policies_enforced
  service: network
  resource: service
  requirement: Authorization Policies Enforced
  scope: network.service.authorization_policies_enforced
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Network Service Authorization Policies
  rationale: Without enforced authorization policies, Kubernetes network services are vulnerable to unauthorized access and
    potential lateral movement within the cluster. This can lead to data breaches and exploitation through privilege escalation
    or unauthorized resource manipulation.
  description: This rule checks that Kubernetes network services have proper authorization policies configured using Role-Based
    Access Control (RBAC). The configuration should ensure that only authenticated and authorized entities can access network
    services, thus reducing the risk of unauthorized access and meeting compliance requirements. A secure setup should include
    clearly defined roles and permissions, restricting access based on the principle of least privilege. This enhances the
    security posture by preventing unauthorized lateral movement and access to sensitive data.
  references:
  - https://kubernetes.io/docs/concepts/security/controlling-access/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance: []
- rule_id: k8s.network.service.dns_policies_enforced_enabled
  service: network
  resource: service
  requirement: Dns Policies Enforced Enabled
  scope: network.service.dns_policies_enforced_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce DNS Policies on Kubernetes Services
  rationale: Without DNS policies enforced, Kubernetes services may become susceptible to DNS spoofing and cache poisoning
    attacks. This misconfiguration can allow attackers to redirect traffic to malicious servers, leading to data theft or
    further infiltration into the cluster. Enforcing DNS policies helps ensure that DNS requests are securely handled according
    to predefined rules, mitigating these risks.
  description: This rule checks that DNS policies are enforced on Kubernetes services. A correct configuration ensures that
    DNS traffic is handled according to network policies, preventing unauthorized access and tampering. The configuration
    should specify 'dnsPolicy' as 'Default', 'ClusterFirst', or 'None', based on the security requirements of the application.
    Enforcing DNS policies enhances security by controlling how DNS queries are resolved, thus reducing the attack surface
    for DNS-based attacks.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
  - https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance: []
- rule_id: k8s.network.service.mtls_enforced
  service: network
  resource: service
  requirement: Mtls Enforced
  scope: network.service.mtls_enforced
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce mTLS for Service Communication
  rationale: Without mTLS, communication between services can be intercepted or tampered with by attackers. This increases
    the risk of man-in-the-middle (MITM) attacks where sensitive data can be read or altered. Enforcing mTLS ensures that
    data in transit is encrypted and transmitted only between authenticated services, mitigating the risk of eavesdropping
    and unauthorized access.
  description: This control checks that mutual TLS (mTLS) is enabled for all service-to-service communication within the Kubernetes
    cluster. It verifies that services are configured to require certificate-based authentication and encryption for all network
    traffic, ensuring that only trusted entities can establish connections. A proper mTLS configuration includes setting up
    a trusted Certificate Authority (CA) and issuing certificates to all services. This enhances security by preventing unauthorized
    entities from communicating with services, thereby protecting sensitive data and maintaining integrity.
  references:
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/services-networking/service/
  - https://kubernetes.io/docs/tasks/security/authentication/certificates/
  compliance: []
- rule_id: k8s.network.ssh.restricted
  service: network
  resource: ssh
  requirement: Restricted
  scope: network.ssh.restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Direct SSH Access to Kubernetes Nodes
  rationale: Direct SSH access to Kubernetes nodes can expose critical infrastructure to unauthorized access and potential
    exploitation. Attackers gaining SSH access can execute arbitrary commands, escalate privileges, and compromise the entire
    cluster. Limiting SSH access reduces the attack surface and helps prevent unauthorized operations that could lead to data
    breaches or system disruptions.
  description: This control checks that SSH access to Kubernetes nodes is restricted according to best security practices.
    It ensures that SSH is disabled or only accessible through a bastion host with strict logging and monitoring. Proper configuration
    involves using tools like SSH key-based authentication, IP whitelisting, and enabling two-factor authentication if SSH
    is necessary. This helps prevent unauthorized access and maintains compliance with security benchmarks, such as the CIS
    Kubernetes Benchmark, by enforcing stringent access control measures.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/#ssh-access-to-nodes
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - rbi_bank_multi_cloud_9.1_0026
- rule_id: k8s.network.supplier.service_monitoring_enabled
  service: network
  resource: supplier
  requirement: Service Monitoring Enabled
  scope: network.supplier.service_monitoring_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable and Configure Service Monitoring for Network Suppliers
  rationale: Without proper service monitoring, malicious activities such as unauthorized access and data exfiltration in
    the network supplier services might go undetected, leading to potential breaches and compliance violations. Attackers
    could exploit misconfigurations or vulnerabilities in the network services to pivot to other parts of the cluster.
  description: This control checks if service monitoring is enabled and correctly configured for network supplier services
    in Kubernetes. Properly configured service monitoring helps in early detection of anomalies, potential intrusions, or
    unauthorized activities by providing visibility into service health and performance. By ensuring that network suppliers
    have monitoring enabled, organizations can respond to threats more effectively, maintain compliance with security standards,
    and uphold the integrity of their Kubernetes environment.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/cluster-administration/monitoring/
  compliance:
  - iso27001_2022_multi_cloud_A.5.22_0019
- rule_id: k8s.network.suspicious.activity_monitoring
  service: network
  resource: suspicious
  requirement: Activity Monitoring
  scope: network.suspicious.activity_monitoring
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Monitor Network Traffic for Suspicious Activity
  rationale: Without effective network activity monitoring, malicious actors can exploit unmonitored traffic to perform lateral
    movement, data exfiltration, or denial-of-service attacks. Proper monitoring helps identify anomalies and potential breaches
    early, reducing the risk of significant damage.
  description: This rule checks that network monitoring is configured to log and analyze traffic patterns for unusual activity
    in Kubernetes clusters. Effective monitoring involves setting up tools and policies to capture and review network data
    continuously. A well-configured monitoring system can alert administrators to unauthorized access attempts, unusual connection
    patterns, or data transfer activities, providing an opportunity to respond promptly to potential threats.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/network/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IR-4_0072
- rule_id: k8s.network.threat.detection_traffic_monitoring
  service: network
  resource: threat
  requirement: Detection Traffic Monitoring
  scope: network.threat.detection_traffic_monitoring
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Enable and Configure Network Traffic Monitoring in Kubernetes
  rationale: Without effective network traffic monitoring, Kubernetes environments are vulnerable to undetected anomalous
    activities, such as lateral movement attacks, data exfiltration, and DDoS. Attackers could exploit misconfigured network
    policies to compromise workloads and access sensitive data, posing significant risks to confidentiality, integrity, and
    availability.
  description: This control checks if Kubernetes clusters have network traffic monitoring enabled, ensuring that all ingress
    and egress traffic is logged and analyzed for anomalies. Proper configurations involve using tools like network policy
    controllers or service meshes to enforce and monitor network communications. This setup helps in identifying unusual traffic
    patterns, detecting potential breaches early, and maintaining compliance with security standards. A robust network traffic
    monitoring configuration includes defining comprehensive network policies, using logging tools to capture traffic details,
    and integrating with SIEM systems for ongoing threat analysis.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/monitoring-overview/
  compliance:
  - iso27001_2022_multi_cloud_A.5.7_0039
- rule_id: k8s.network.tls.encryption_enabled
  service: network
  resource: tls
  requirement: Encryption Enabled
  scope: network.tls.encryption_enabled
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure TLS Encryption for Ingress Traffic
  rationale: Without TLS encryption, sensitive data transmitted between clients and Kubernetes services can be intercepted
    by attackers, leading to data breaches. Man-in-the-middle attacks are a significant risk when communication is not encrypted,
    allowing attackers to eavesdrop and potentially modify data in transit.
  description: This rule checks that TLS encryption is enabled for all ingress traffic to Kubernetes services. It verifies
    that Ingress controllers are configured to use HTTPS with valid TLS certificates, ensuring that data is encrypted during
    transmission. Proper TLS configuration helps protect against eavesdropping, data tampering, and other network-based attacks.
    Ensuring TLS is enabled aligns with industry standards such as the CIS Kubernetes Benchmark, which emphasizes the importance
    of encrypting sensitive data in transit.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/#protecting-cluster-components
  compliance: []
- rule_id: k8s.network.tls.enforced
  service: network
  resource: tls
  requirement: Enforced
  scope: network.tls.enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce TLS Encryption for Kubernetes Network Traffic
  rationale: Failing to enforce TLS for Kubernetes network traffic exposes data in transit to interception and tampering.
    Attackers could exploit unencrypted traffic to perform man-in-the-middle attacks, gaining unauthorized access to sensitive
    data or credentials.
  description: This rule checks that TLS encryption is enforced for all network communications within the Kubernetes cluster.
    Proper enforcement of TLS ensures that data exchanged between Kubernetes components and with external services is encrypted,
    safeguarding against eavesdropping and data manipulation. A correctly configured TLS setup includes valid certificates
    and protocols that meet security standards, significantly reducing the risk of unauthorized data access.
  references:
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/certificates/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_CA-9-b_0350
  - nist_800_53_rev5_multi_cloud_PM-17-b_0876
- rule_id: k8s.network.unused.policies_removed
  service: network
  resource: unused
  requirement: Policies Removed
  scope: network.unused.policies_removed
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Remove Unused Network Policies for Enhanced Security
  rationale: Unused network policies in Kubernetes can lead to unnecessary complexity and potential security vulnerabilities.
    Attackers may exploit these unused policies to conduct lateral movement or reconnaissance within the cluster, increasing
    the risk of unauthorized access and data breaches. Removing unused policies helps maintain the principle of least privilege
    and reduces the attack surface.
  description: This check ensures that any unused network policies in the Kubernetes environment are identified and removed.
    A good configuration involves actively managing and auditing network policies to ensure they are necessary, up-to-date,
    and aligned with current security requirements. By removing unused policies, you minimize the potential for misconfigurations
    and security loopholes that attackers could exploit, thereby improving the overall security posture of the Kubernetes
    cluster.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - rbi_bank_multi_cloud_4.2_0017
- rule_id: k8s.node.auto.repair_enabled
  service: node
  resource: auto
  requirement: Repair Enabled
  scope: node.auto.repair_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Node Auto-Repair for Self-Healing
  rationale: Without node auto-repair, nodes may remain in a failed state, leading to service disruptions and potential exploitation.
    Attackers can exploit unpatched or malfunctioning nodes to escalate privileges or disrupt services, compromising the cluster's
    integrity and availability.
  description: This control checks that the Kubernetes cluster has node auto-repair enabled. Node auto-repair automatically
    identifies and repairs nodes that fail health checks, ensuring high availability and reducing the risk of prolonged security
    vulnerabilities. A properly configured auto-repair mechanism helps maintain operational resilience and addresses potential
    security gaps by automatically bringing nodes back to a healthy state.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#monitoring-and-auto-repair
  - https://kubernetes.io/docs/concepts/architecture/nodes/#self-healing-nodes
  - https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/
  compliance:
  - hipaa_multi_cloud_164_308_a_7_ii_c_0020
- rule_id: k8s.node.automatic.updates_enabled
  service: node
  resource: automatic
  requirement: Updates Enabled
  scope: node.automatic.updates_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enable Automatic Updates for Kubernetes Nodes
  rationale: Failure to enable automatic updates on Kubernetes nodes can leave nodes vulnerable to known security vulnerabilities
    and exploits. Attackers may exploit outdated components to gain unauthorized access or disrupt services. Ensuring nodes
    are automatically updated reduces the attack surface by patching known vulnerabilities promptly.
  description: This control checks whether Kubernetes nodes are configured to receive automatic updates. Good configuration
    involves enabling automatic updates to ensure nodes are up-to-date with the latest security patches and features. This
    helps protect the cluster from known vulnerabilities and exploits, thereby maintaining system integrity and compliance
    with security standards.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
  - https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-2-d_1345
- rule_id: k8s.node.automatic.upgrades_enabled
  service: node
  resource: automatic
  requirement: Upgrades Enabled
  scope: node.automatic.upgrades_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Node Automatic Upgrades are Enabled
  rationale: Without automatic upgrades enabled on Kubernetes nodes, they may remain vulnerable to known security exploits
    due to outdated software. Attackers can leverage these vulnerabilities to gain unauthorized access or disrupt services.
    Enabling automatic upgrades ensures nodes are patched with the latest security fixes, reducing the attack surface and
    improving the cluster's resilience against exploitation.
  description: This control checks that automatic upgrades are enabled for Kubernetes nodes, ensuring they automatically receive
    the latest security patches and updates. A well-configured upgrade process minimizes the risk of running vulnerable software
    versions, aligning with security best practices and compliance standards. Proper configuration involves enabling features
    like live kernel patching and using managed node groups that support automatic updates, which helps maintain node security
    posture and operational stability.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
  - https://kubernetes.io/docs/tasks/administer-cluster/rolling-update-cluster/
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-2-c_1344
- rule_id: k8s.node.configuration.compliance_check
  service: node
  resource: configuration
  requirement: Compliance Check
  scope: node.configuration.compliance_check
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enforce Node Configuration Compliance with CIS Benchmarks
  rationale: Misconfigured node settings can expose Kubernetes clusters to various security threats, including unauthorized
    access and privilege escalation. Ensuring node configurations comply with the CIS Kubernetes Benchmark reduces the attack
    surface and mitigates the risk of exploitation through well-known vulnerabilities.
  description: This rule checks that each Kubernetes node's configuration aligns with the CIS Kubernetes Benchmark standards.
    It validates settings such as API server authentication, authorization protocols, and secure default configurations. Proper
    compliance ensures that nodes are not weak links within the cluster, thereby reducing vulnerabilities that could be exploited
    by attackers. Adhering to these benchmarks helps maintain a strong security posture and meet regulatory compliance requirements.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/
  - https://kubernetes.io/docs/setup/best-practices/node-conformance/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-c_0399
- rule_id: k8s.node.configuration.management_enabled
  service: node
  resource: configuration
  requirement: Management Enabled
  scope: node.configuration.management_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable and Enforce Node Configuration Management
  rationale: Without proper management of node configurations, Kubernetes clusters are vulnerable to unauthorized changes,
    potentially leading to security breaches. Attack vectors include unauthorized nodes joining the cluster and adversaries
    altering node settings to disrupt services or gain further access.
  description: This control checks if node configuration management tools, such as kubelet configurations, are enabled and
    enforced. Proper configuration management ensures that nodes adhere to security standards, preventing unauthorized access
    and changes. A well-managed node configuration helps maintain system integrity, enforces compliance with security policies,
    and reduces the risk of misconfigurations that can be exploited by attackers.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
  - https://kubernetes.io/docs/concepts/architecture/nodes/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-b_0398
- rule_id: k8s.node.deletion.protection_enabled
  service: node
  resource: deletion
  requirement: Protection Enabled
  scope: node.deletion.protection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Node Deletion Protection to Prevent Accidental Loss
  rationale: Without node deletion protection, a Kubernetes node can be inadvertently or maliciously deleted, leading to service
    disruptions, loss of data, and reduced cluster capacity. Attack vectors include unauthorized access to the Kubernetes
    API or insufficiently protected credentials, allowing attackers to issue node deletion commands.
  description: This control checks that node deletion protection is enabled, preventing nodes from being accidentally or maliciously
    removed from the cluster. A properly configured deletion protection setting ensures that nodes cannot be deleted without
    explicit administrator approval. This minimizes the risk of accidental downtime and unauthorized node removal, thereby
    enhancing the resilience and security of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/concepts/architecture/nodes/#node-deletion
  - https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - nist_800_53_rev5_multi_cloud_CP-2-a_0462
- rule_id: k8s.node.labeling.for_inventory_tracking
  service: node
  resource: labeling
  requirement: For Inventory Tracking
  scope: node.labeling.for_inventory_tracking
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Node Labeling for Enhanced Inventory and Access Control
  rationale: Improper node labeling can lead to mismanagement of resources and potential unauthorized access, as labels are
    often used to apply policies and configurations. Without consistent labeling, there is a risk of deploying workloads on
    inappropriate nodes, which could be exploited by attackers to gain access to sensitive workloads through misconfigured
    permissions or lack of isolation.
  description: This rule validates that each Kubernetes node is labeled according to predefined inventory and access control
    standards. Proper node labeling helps ensure that resources are allocated correctly, policies are enforced accurately,
    and workloads are isolated as intended. For example, labels can dictate node selection constraints for pods, ensuring
    workloads run only on compliant nodes. This reduces the risk of configuration drift and unauthorized access, maintaining
    a secure and compliant cluster environment.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#node-management
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-8_0048
- rule_id: k8s.node.labeling.standard
  service: node
  resource: labeling
  requirement: Standard
  scope: node.labeling.standard
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Consistent Node Labeling Standards
  rationale: Inconsistent or incorrect node labeling can lead to misrouting of workloads and exposure of sensitive data to
    unauthorized users. Attackers can exploit misconfigured labels to escalate privileges, access unauthorized resources,
    or disrupt applications by misdirecting traffic. Standardized node labeling ensures predictable application behavior and
    aids in effective workload isolation, thereby strengthening the cluster's security posture.
  description: This control checks that all Kubernetes nodes have labels applied consistently according to predefined standards.
    Properly configured node labels ensure that workloads are scheduled correctly and that security policies are enforced
    based on node characteristics. Good configuration involves using a standardized naming convention for labels, avoiding
    sensitive information in labels, and ensuring that labels are used effectively for access control and workload isolation.
    Consistent node labeling helps in maintaining a clear infrastructure layout, preventing unauthorized access, and ensuring
    compliance with security policies and standards.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  - https://kubernetes.io/docs/setup/best-practices/node-conformance/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-8-a_0424
  - nist_800_53_rev5_multi_cloud_CM-8-b_0425
- rule_id: k8s.node.labels.inventory_maintained
  service: node
  resource: labels
  requirement: Inventory Maintained
  scope: node.labels.inventory_maintained
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Node Labels are Consistently Managed and Monitored
  rationale: Improperly managed node labels can lead to unauthorized access and misrouting of workloads, which can compromise
    the security and integrity of the Kubernetes cluster. Attackers may exploit misconfigured labels to execute privilege
    escalation attacks by manipulating node selection or resource allocation policies.
  description: This rule checks that node labels are consistently applied and managed according to best practices. A well-maintained
    inventory of node labels helps ensure that workloads are scheduled on appropriate nodes, enhancing security by preventing
    unauthorized access and ensuring compliance with security policies. Proper configuration includes verifying that labels
    are aligned with security policies and monitoring changes to prevent unauthorized modifications.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-metadata/
  compliance:
  - nist_800_171_r2_multi_cloud_3_4_1_3.4.1_Establish_and_maintain_baseline_confi_0037
- rule_id: k8s.node.memory.protection_mechanisms_enabled
  service: node
  resource: memory
  requirement: Protection Mechanisms Enabled
  scope: node.memory.protection_mechanisms_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Node Memory Protection Mechanisms
  rationale: Without memory protection mechanisms, Kubernetes nodes are vulnerable to various security threats, such as buffer
    overflow attacks, which can lead to unauthorized access and control over node processes. These vulnerabilities can be
    exploited to elevate privileges, execute arbitrary code, or disrupt services, compromising the cluster's integrity and
    availability.
  description: This control checks that Kubernetes nodes are configured with memory protection mechanisms like DEP (Data Execution
    Prevention) and ASLR (Address Space Layout Randomization) to mitigate risks of memory-based attacks. Properly configured
    memory protection ensures that executable code cannot run in non-executable memory regions, reducing the attack surface
    and preventing common exploits. This configuration aligns with security best practices and compliance standards, such
    as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/architecture/nodes/
  - https://kubernetes.io/docs/setup/best-practices/cluster-large/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-16_1433
- rule_id: k8s.node.node.auto_upgrade_enabled
  service: node
  resource: node
  requirement: Auto Upgrade Enabled
  scope: node.node.auto_upgrade_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Automatic Node Upgrades for Security Patching
  rationale: Failure to enable automatic node upgrades can leave nodes vulnerable to security exploits due to outdated software.
    Attackers often target known vulnerabilities in Kubernetes nodes to gain unauthorized access or disrupt services. Ensuring
    automatic updates reduce the window of exposure to such risks.
  description: This control verifies that automatic updates are enabled on Kubernetes nodes, ensuring they receive the latest
    security patches and bug fixes promptly. Proper configuration involves setting the node pool to auto-upgrade mode in managed
    Kubernetes services like GKE or AKS. This proactive measure helps prevent security breaches by minimizing the duration
    that vulnerabilities remain exploitable, thus strengthening the overall security posture and compliance with security
    standards.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
  compliance:
  - rbi_bank_multi_cloud_21.2_0013
- rule_id: k8s.node.node.automatic_updates_enabled
  service: node
  resource: node
  requirement: Automatic Updates Enabled
  scope: node.node.automatic_updates_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enable Automatic Operating System Updates on Nodes
  rationale: Failing to enable automatic updates on Kubernetes nodes can leave the system vulnerable to known security vulnerabilities
    and exploits. Attackers may exploit these unpatched vulnerabilities to gain unauthorized access, execute arbitrary code,
    or cause denial-of-service conditions. Ensuring timely updates closes these security gaps and enhances the overall resilience
    of the Kubernetes cluster.
  description: This check verifies that the operating system on each Kubernetes node is configured to automatically apply
    updates. A properly configured system should regularly check for and install security patches without manual intervention.
    This practice is crucial for mitigating risks associated with unpatched vulnerabilities, maintaining system integrity,
    and ensuring compliance with security benchmarks such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/setup/best-practices/node-conformance/
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.node.node.config_enforced
  service: node
  resource: node
  requirement: Config Enforced
  scope: node.node.config_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Node Configuration Compliance with Security Best Practices
  rationale: Improper node configuration can lead to vulnerabilities that expose the cluster to unauthorized access, privilege
    escalation, and data breaches. Attackers can exploit misconfigurations to gain control over nodes, leading to potential
    denial of service, data tampering, or lateral movement within the cluster.
  description: This control checks that each Kubernetes node adheres to security best practices for configuration management.
    It ensures that the node's kubelet is securely configured, including settings related to authentication, authorization,
    and TLS encryption. Proper configuration mitigates risks such as unauthorized access and privilege escalation by enforcing
    strict access controls and secure communication channels. Adhering to these standards supports compliance with the CIS
    Kubernetes Benchmark and other industry best practices.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/architecture/nodes/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance: []
- rule_id: k8s.node.node.configuration_management_enabled
  service: node
  resource: node
  requirement: Configuration Management Enabled
  scope: node.node.configuration_management_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Configuration Management on Kubernetes Nodes
  rationale: Without configuration management, Kubernetes nodes may not consistently apply security patches or configurations,
    increasing the risk of vulnerabilities being exploited. Attackers could leverage misconfigured nodes to gain unauthorized
    access, escalate privileges, or disrupt services.
  description: This control checks if configuration management tools (e.g., Puppet, Chef, Ansible) are enabled on Kubernetes
    nodes to ensure consistent application of security settings and patches. Proper configuration management ensures nodes
    adhere to security best practices, reducing the attack surface by preventing drift and unauthorized changes. It enhances
    security by automating updates and maintaining compliance with standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/
  - https://kubernetes.io/docs/concepts/architecture/nodes/
  compliance: []
- rule_id: k8s.node.node.deletion_protection_enabled
  service: node
  resource: node
  requirement: Deletion Protection Enabled
  scope: node.node.deletion_protection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Deletion Protection for Kubernetes Nodes
  rationale: Without deletion protection, Kubernetes nodes may be accidentally or maliciously deleted, leading to potential
    data loss, service disruptions, and increased attack surfaces. Attackers or erroneous scripts could exploit this vulnerability
    to remove nodes, resulting in the loss of critical workloads and compromising the availability and integrity of the cluster.
  description: This rule checks that deletion protection is enabled on Kubernetes nodes, preventing unauthorized or accidental
    deletion. Proper configuration involves setting policies that require additional authorization steps for node deletion,
    ensuring nodes are safeguarded against unintended removal. This protection is critical for maintaining the stability and
    security of the Kubernetes environment, as nodes are fundamental components that host pods and manage workloads. Enabling
    deletion protection helps in mitigating risks associated with unintentional downtime and security breaches.
  references:
  - https://kubernetes.io/docs/concepts/architecture/nodes/#node-deletion
  - https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podnodeconstraint
  compliance: []
- rule_id: k8s.node.node.edr_integration_enabled
  service: node
  resource: node
  requirement: Edr Integration Enabled
  scope: node.node.edr_integration_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Endpoint Detection and Response (EDR) Integration on Nodes
  rationale: Without EDR integration, Kubernetes nodes are vulnerable to undetected threats, including malware and unauthorized
    access. EDR provides real-time monitoring and response capabilities that are essential for identifying and mitigating
    security incidents. Misconfiguration can lead to delayed detection of breaches, increasing the risk of data exfiltration
    and service disruption.
  description: This rule checks that Endpoint Detection and Response (EDR) solutions are properly configured and enabled on
    Kubernetes nodes. EDR integration involves deploying agents that monitor system activities and alert administrators to
    suspicious behavior. A correctly configured EDR setup ensures comprehensive monitoring, logs collection, and threat detection,
    enhancing the security posture by providing detailed visibility into node activities and enabling rapid incident response.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/
  - https://kubernetes.io/docs/concepts/architecture/nodes/
  compliance: []
- rule_id: k8s.node.node.inventory_labels_maintained
  service: node
  resource: node
  requirement: Inventory Labels Maintained
  scope: node.node.inventory_labels_maintained
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Node Inventory Labels are Consistently Maintained
  rationale: Maintaining accurate and up-to-date inventory labels on nodes is crucial for effective resource management and
    security monitoring. Without proper labels, nodes may be misclassified, leading to potential misconfigurations and unauthorized
    access. Attackers could exploit unlabelled or incorrectly labelled nodes to move laterally within the cluster or hide
    anomalous activities.
  description: This rule checks that inventory labels on Kubernetes nodes are consistently maintained and adhere to predefined
    standards. Properly maintained labels facilitate efficient resource allocation, monitoring, and access control policies.
    A well-labeled node inventory ensures that security policies are applied correctly and that nodes are easily identifiable
    for auditing and incident response purposes. Ensuring consistency in node labeling reduces the risk of privilege escalation
    and helps in maintaining compliance with security benchmarks like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  compliance: []
- rule_id: k8s.node.node.labeling_standard_configured
  service: node
  resource: node
  requirement: Labeling Standard Configured
  scope: node.node.labeling_standard_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Node Labeling Standards for Security Compliance
  rationale: Misconfigured or inconsistent node labels can lead to security risks such as unauthorized access or resource
    misallocation. Attackers may exploit incorrectly labeled nodes to bypass security policies or gain unintended access to
    sensitive workloads. Proper labeling helps ensure that nodes are correctly identified and managed according to security
    policies, reducing the risk of privilege escalation and lateral movement within the cluster.
  description: This control checks that all Kubernetes nodes adhere to a predefined labeling standard. A standardized labeling
    scheme is essential for implementing effective node-level security policies and resource management. This validation involves
    ensuring that each node has the required labels and that these labels are used consistently across the cluster. Proper
    node labeling facilitates accurate scheduling decisions, access control, and monitoring, ultimately enhancing the security
    and compliance posture of the Kubernetes infrastructure.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  - https://kubernetes.io/docs/concepts/security/overview/#node-security
  compliance: []
- rule_id: k8s.node.node.memory_protection_mechanisms_enabled
  service: node
  resource: node
  requirement: Memory Protection Mechanisms Enabled
  scope: node.node.memory_protection_mechanisms_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Kernel Memory Protection on Kubernetes Nodes
  rationale: If memory protection mechanisms on Kubernetes nodes are not enabled, nodes become susceptible to attacks such
    as buffer overflows, memory corruption, and unauthorized access to sensitive data. These vulnerabilities can be exploited
    by attackers to execute arbitrary code, escalate privileges, or compromise node integrity, ultimately leading to cluster-wide
    security breaches.
  description: This rule checks whether kernel-level memory protection features, such as Address Space Layout Randomization
    (ASLR) and Data Execution Prevention (DEP), are enabled on Kubernetes nodes. Enabling these features helps prevent memory-based
    attacks by randomizing memory address space and ensuring code execution restrictions. Proper configuration involves ensuring
    that these mechanisms are active and correctly enforced across all nodes, thereby reducing the attack surface and enhancing
    the overall security posture of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/overview/components/#node
  - https://kubernetes.io/docs/setup/best-practices/
  compliance: []
- rule_id: k8s.node.node.node_hardening_applied_enforced
  service: node
  resource: node
  requirement: Node Hardening Applied Enforced
  scope: node.node.node_hardening_applied_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Node Hardening for Secure Configuration Compliance
  rationale: Misconfigured or weakly configured nodes can expose Kubernetes clusters to various security threats, such as
    unauthorized access and privilege escalation. Nodes without enforced hardening are susceptible to attacks that exploit
    vulnerabilities in the underlying operating system or container runtime, potentially leading to cluster compromise.
  description: This rule checks that node hardening practices are enforced on Kubernetes nodes. Specifically, it ensures that
    security configurations such as kernel parameters, network settings, and file permissions are aligned with hardening guidelines
    like the CIS Kubernetes Benchmark. Proper node hardening minimizes the attack surface by ensuring that only necessary
    services are running, sensitive data is protected, and access controls are tightly managed. This reduces the risk of unauthorized
    access and potential exploits targeting the node's operating system.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/
  - https://kubernetes.io/docs/tasks/administer-cluster/nodes/
  compliance: []
- rule_id: k8s.node.node.os_patch_enforced
  service: node
  resource: node
  requirement: Os Patch Enforced
  scope: node.node.os_patch_enforced
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure Nodes are Regularly Patched with OS Security Updates
  rationale: Unpatched operating systems on Kubernetes nodes can be exploited by attackers to gain unauthorized access, escalate
    privileges, and execute arbitrary code. This can lead to data breaches and service disruptions. Regular patching reduces
    exposure to known vulnerabilities and strengthens the security posture of the Kubernetes cluster.
  description: This control verifies that Kubernetes nodes are configured to automatically apply operating system security
    patches. It checks for the presence of patch management tools and ensures they are properly configured to enforce regular
    updates. A well-configured patch management system minimizes the attack surface by addressing known vulnerabilities promptly,
    thereby protecting nodes from exploitation and maintaining compliance with security standards such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/setup/best-practices/node-conformance/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/overview/components/#node
  compliance: []
- rule_id: k8s.node.node.resource_utilization_audited
  service: node
  resource: node
  requirement: Resource Utilization Audited
  scope: node.node.resource_utilization_audited
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Audit Node Resource Utilization for Security Compliance
  rationale: Failure to audit resource utilization on Kubernetes nodes can lead to undetected performance bottlenecks, potential
    denial of service attacks, and inefficient resource allocation, which may be exploited by attackers to degrade service
    availability or escalate privileges. Regular auditing helps in identifying abnormal patterns and securing the cluster
    against such vulnerabilities.
  description: This rule verifies that resource utilization metrics for Kubernetes nodes are being properly audited. It checks
    for the presence of monitoring solutions that track CPU, memory, and disk usage, ensuring they are configured to log data
    at appropriate intervals. Proper auditing of these metrics enables the detection of anomalies that could signify security
    incidents or misconfigurations, helping to maintain a robust security posture and optimize resource management. A well-configured
    audit process aids in compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
  - https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/
  - https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/
  compliance: []
- rule_id: k8s.node.node.runtime_security_enabled
  service: node
  resource: node
  requirement: Runtime Security Enabled
  scope: node.node.runtime_security_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Node Runtime Security Features are Enabled
  rationale: Without runtime security features enabled on Kubernetes nodes, the cluster is vulnerable to unauthorized access
    and malicious activities, such as privilege escalation or container escape. Attackers could exploit vulnerabilities in
    the container runtime to gain access to host resources, leading to potential data breaches or service disruptions.
  description: This rule checks that runtime security features, such as AppArmor, SELinux, or seccomp, are enabled and properly
    configured on Kubernetes nodes. Proper configuration of these security mechanisms helps isolate containers from the host,
    restricts their capabilities, and reduces the potential attack surface. By enforcing these settings, the cluster is better
    protected against unauthorized access and exploits, thereby enhancing the overall security posture.
  references:
  - https://kubernetes.io/docs/tutorials/security/seccomp/
  - https://kubernetes.io/docs/tutorials/security/apparmor/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.node.node.tpm_verification_enabled
  service: node
  resource: node
  requirement: Tpm Verification Enabled
  scope: node.node.tpm_verification_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable TPM Verification on Kubernetes Nodes
  rationale: Without TPM verification enabled, a Kubernetes node is vulnerable to unauthorized access and data tampering.
    TPM (Trusted Platform Module) provides hardware-based security functions, ensuring the integrity of the node's boot process
    and safeguarding cryptographic keys. A compromised node could allow attackers to deploy malicious workloads, exfiltrate
    sensitive data, or disrupt cluster operations.
  description: This control checks that TPM verification is enabled on each Kubernetes node. Proper configuration involves
    ensuring the TPM module is active and correctly integrated with the node's boot process. TPM provides a hardware root
    of trust, which helps protect against rootkit and bootkit attacks. Enabling TPM verification ensures that nodes only run
    trusted code, reducing the risk of unauthorized access and enhancing the overall security posture of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/setup/best-practices/cluster-large/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/architecture/nodes/
  compliance: []
- rule_id: k8s.node.os.patch_compliance
  service: node
  resource: os
  requirement: Patch Compliance
  scope: node.os.patch_compliance
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure Nodes Run Up-to-Date OS Patches
  rationale: Unpatched operating systems on Kubernetes nodes can expose the cluster to vulnerabilities that may be exploited
    by attackers. This includes vulnerabilities that allow privilege escalation, remote code execution, or denial of service
    attacks. Keeping the OS up-to-date mitigates these risks by applying security patches that fix known vulnerabilities.
  description: This control checks if the operating systems on Kubernetes nodes are running the latest security patches. It
    involves verifying the patch level of each node against a known baseline of security patches. A well-configured patch
    compliance setup ensures that nodes are not susceptible to exploits that target known vulnerabilities. By maintaining
    up-to-date patches, clusters adhere to security best practices, reducing attack surfaces and complying with industry standards
    such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - pci_dss_v4_multi_cloud_6.2.2_0076
  - pci_dss_v4_multi_cloud_6.2.3.1_0078
  - pci_dss_v4_multi_cloud_6.2.4_0079
  - rbi_bank_multi_cloud_21.2_0013
  - rbi_nbfc_multi_cloud_2.6_0010
  - rbi_nbfc_multi_cloud_5.2_0026
- rule_id: k8s.node.pod.node_auto_upgrade_enabled
  service: node
  resource: pod
  requirement: Node Auto Upgrade Enabled
  scope: node.pod.node_auto_upgrade_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Automatic Node Upgrades for Security Patching
  rationale: Failing to enable automatic node upgrades can leave nodes running outdated software with known vulnerabilities.
    This exposes the cluster to potential exploits, such as privilege escalation attacks, or container escape vulnerabilities,
    by attackers leveraging unpatched security flaws.
  description: This control ensures that the Kubernetes nodes have automatic upgrades enabled, which is critical for applying
    security patches and updates in a timely manner. Proper configuration involves setting up the cluster to automatically
    update node components, such as the kubelet and container runtime, ensuring that security vulnerabilities are quickly
    addressed. This reduces the attack surface by preventing attackers from exploiting known vulnerabilities that have been
    patched in newer versions.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
  - https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/
  compliance: []
- rule_id: k8s.node.resource.monitoring_enabled
  service: node
  resource: resource
  requirement: Monitoring Enabled
  scope: node.resource.monitoring_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable and Configure Node Resource Monitoring
  rationale: Without proper monitoring of Kubernetes node resources, anomalous activities or resource exhaustion may go undetected,
    leading to potential breaches or denial of service. Attackers could exploit unmonitored nodes to perform lateral movement
    or escalate privileges within the cluster. Monitoring helps in early detection of such threats.
  description: This control checks that Kubernetes node resources have monitoring enabled according to best security practices.
    It ensures that resource usage metrics such as CPU, memory, and disk are collected and analyzed for unusual patterns.
    A well-configured monitoring system can alert administrators to potential security incidents or resource misconfigurations,
    thereby reducing the attack surface and complying with industry standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/
  compliance:
  - rbi_nbfc_multi_cloud_3.2_0018
  - rbi_nbfc_multi_cloud_6.4_0032
- rule_id: k8s.node.resource.utilization_monitored
  service: node
  resource: resource
  requirement: Utilization Monitored
  scope: node.resource.utilization_monitored
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Monitor Node Resource Utilization Metrics
  rationale: Failure to monitor node resource utilization can lead to unanticipated resource exhaustion, making nodes unavailable
    for critical workloads. This can be exploited in a Denial of Service (DoS) attack where an attacker consumes resources
    to the point where legitimate workloads cannot be scheduled or executed, leading to system downtime.
  description: This control checks that node-level resource utilization metrics such as CPU, memory, and storage are actively
    monitored. A well-configured monitoring system should alert administrators to spikes or unusual patterns in resource consumption,
    which could indicate a security incident or misconfiguration. Proper monitoring helps in early detection of anomalies,
    ensuring that resources are managed efficiently and securely, and aids in maintaining compliance with standards like the
    CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/cluster-administration/monitoring/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/
  compliance:
  - soc2_multi_cloud_a1_1_0024
- rule_id: k8s.persistentvolume.backup.policy_defined
  service: persistentvolume
  resource: backup
  requirement: Policy Defined
  scope: persistentvolume.backup.policy_defined
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Backup Policies for Persistent Volumes
  rationale: Without a defined backup policy for persistent volumes, critical data may be at risk of loss due to accidental
    deletion or malicious attacks such as ransomware. Misconfigured or absent policies can lead to non-compliance with data
    protection regulations, potentially resulting in data breaches and financial penalties.
  description: This rule verifies that all persistent volumes in the Kubernetes cluster have an associated backup policy.
    A well-defined backup policy specifies the frequency of backups, retention duration, and access controls to ensure that
    data can be restored quickly and securely in case of data loss incidents. Proper backup policies help in maintaining data
    integrity, minimizing downtime, and ensuring compliance with industry standards and regulations. This rule enhances security
    by preventing data loss and ensuring business continuity.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/
  - https://kubernetes.io/docs/tasks/administer-cluster/data-management/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-6_0056
  - nist_800_53_rev5_multi_cloud_CP-9-b_0510
- rule_id: k8s.persistentvolume.persistentvolume.csi_encryption_verified_enforced
  service: persistentvolume
  resource: persistentvolume
  requirement: Csi Encryption Verified Enforced
  scope: persistentvolume.persistentvolume.csi_encryption_verified_enforced
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Enforce CSI Volume Encryption for PersistentVolumes
  rationale: If CSI encryption is not enforced on PersistentVolumes, sensitive data stored on these volumes could be accessed
    by unauthorized users or malicious actors in the event of a data breach. This increases the risk of data leakage, non-compliance
    with data protection regulations, and potential exploitation by ransomware attacks.
  description: This rule checks that PersistentVolumes using CSI drivers have encryption enabled and enforced. A properly
    configured CSI encryption setup ensures that data at rest is protected by cryptographic methods, reducing the likelihood
    of unauthorized data access. This involves verifying the encryption settings in the StorageClass and ensuring that encryption
    keys are managed securely. Adhering to these practices helps maintain data confidentiality and integrity, aligning with
    industry security standards.
  references:
  - https://kubernetes.io/docs/concepts/storage/volumes/#csi
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#storageclasses
  - https://kubernetes.io/docs/concepts/security/overview/#data-encryption-at-rest
  compliance: []
- rule_id: k8s.persistentvolume.persistentvolume.host_path_restricted_enforced
  service: persistentvolume
  resource: persistentvolume
  requirement: Host Path Restricted Enforced
  scope: persistentvolume.persistentvolume.host_path_restricted_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict HostPath Usage in PersistentVolumes
  rationale: Using HostPath in PersistentVolumes can expose the host file system to containers, increasing the risk of unauthorized
    access, data leakage, and potential privilege escalation. Attackers could exploit poorly configured HostPaths to access
    sensitive files or gain higher privileges on the host system.
  description: This control checks that PersistentVolumes do not use HostPath without explicit, justified need. A secure configuration
    involves using higher-level storage abstractions like PersistentVolumeClaims with dynamic provisioning, or ensuring HostPath
    volumes are scoped to specific, non-sensitive directories with appropriate permissions. This reduces the attack surface
    by preventing containers from accessing the host's file system in an unrestricted manner.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes
  - https://kubernetes.io/docs/concepts/storage/volumes/#hostpath
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance: []
- rule_id: k8s.persistentvolume.persistentvolume.pv_access_modes_restricted_enabled
  service: persistentvolume
  resource: persistentvolume
  requirement: Pv Access Modes Restricted Enabled
  scope: persistentvolume.persistentvolume.pv_access_modes_restricted_enabled
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict PersistentVolume Access Modes
  rationale: If PersistentVolume (PV) access modes are overly permissive, it can lead to unauthorized access or malicious
    data manipulation. This misconfiguration could allow containers to mount volumes with write permissions when only read
    access is necessary, increasing the risk of data breach and corruption. Attackers could exploit this to exfiltrate sensitive
    data or inject malicious content.
  description: This check ensures that PersistentVolumes are configured with the most restrictive access modes necessary for
    their intended use. A well-configured PV should limit access to only those modes required by the application, such as
    'ReadOnlyMany' instead of 'ReadWriteMany', to minimize the potential attack surface. By validating and enforcing restricted
    access modes, this control helps prevent unauthorized data access and modification, thus enhancing the security posture
    of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
  - https://kubernetes.io/docs/concepts/security/overview/#data-security
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.persistentvolume.persistentvolume.rbac_enforced_enabled
  service: persistentvolume
  resource: persistentvolume
  requirement: Rbac Enforced Enabled
  scope: persistentvolume.persistentvolume.rbac_enforced_enabled
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Enforce RBAC for PersistentVolumes
  rationale: If RBAC is not enforced on PersistentVolumes, unauthorized users could gain access to sensitive data stored in
    the cluster, leading to data breaches and potential data loss. Without proper RBAC, attackers could escalate privileges
    or perform unauthorized operations on PersistentVolumes, exploiting vulnerabilities through misconfigured access controls.
  description: This control checks that Role-Based Access Control (RBAC) is properly enforced on PersistentVolumes. A secure
    configuration ensures that only authorized users and service accounts have the ability to create, modify, or delete PersistentVolumes.
    This helps prevent unauthorized access and modifications, reducing the risk of data exposure and unauthorized data manipulation.
    Proper RBAC configuration involves defining roles with specific permissions and binding those roles to users or groups
    that truly require access, in line with the principle of least privilege.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/
  compliance: []
- rule_id: k8s.persistentvolume.persistentvolume.storage_class_encryption_enabled_enforced
  service: persistentvolume
  resource: persistentvolume
  requirement: Storage Class Encryption Enabled Enforced
  scope: persistentvolume.persistentvolume.storage_class_encryption_enabled_enforced
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Enforce Encryption for PersistentVolume Storage Classes
  rationale: Without encryption enabled on PersistentVolume storage classes, sensitive data stored on disk can be susceptible
    to unauthorized access, data breaches, and potential data corruption. This misconfiguration exposes the data at rest to
    potential attackers who might compromise the underlying storage infrastructure or gain unauthorized access to the physical
    hardware. Encrypting data at rest ensures that even if unauthorized access occurs, the data remains protected through
    cryptographic measures.
  description: This control checks if the encryption feature is enabled for all PersistentVolume storage classes within the
    Kubernetes environment. A properly configured PersistentVolume storage class with encryption ensures that all data written
    to the storage is encrypted at rest, adhering to security best practices and compliance requirements. A correct configuration
    involves setting up storage classes that support encryption and ensuring that these are used for all PersistentVolumes.
    This mitigates the risk of unauthorized access to sensitive data, thereby enhancing the overall security posture of the
    Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes
  - https://kubernetes.io/docs/concepts/storage/storage-classes/#encryption-keys
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  compliance: []
- rule_id: k8s.pod.backup.annotation_present
  service: pod
  resource: backup
  requirement: Annotation Present
  scope: pod.backup.annotation_present
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Backup Annotation is Present on Pods
  rationale: The absence of backup annotations on Kubernetes pods can lead to inconsistencies in backup operations and the
    potential loss of critical data. Annotations provide essential metadata that can be used by backup tools to identify and
    manage backup operations effectively. Without these annotations, there is a risk of improper backup scheduling, resulting
    in data loss, compliance breaches, and potential exposure to security vulnerabilities.
  description: This control checks for the presence of specific backup-related annotations on Kubernetes pods. Properly configured
    annotations ensure that backup tools can accurately identify which pods require backup, how frequently they should be
    backed up, and any specific requirements for the backup process. By validating these annotations, organizations can ensure
    that their data protection strategies are implemented consistently across their Kubernetes infrastructure, reducing the
    risk of data loss and enhancing compliance with industry standards.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  - https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/
  - https://kubernetes.io/docs/concepts/architecture/nodes/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-6_0056
  - pci_dss_v4_multi_cloud_3.1.1_0032
  - pci_dss_v4_multi_cloud_3.3.1.1_0034
- rule_id: k8s.pod.backup.annotations_configured
  service: pod
  resource: backup
  requirement: Annotations Configured
  scope: pod.backup.annotations_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pod Backups Have Security Annotations
  rationale: Properly configured annotations on pod backups are crucial for enforcing security policies and ensuring compliance
    with organizational and industry standards. Without these annotations, there is a risk of misconfigurations leading to
    unauthorized access, data breaches, or failure to meet regulatory requirements such as NIST SP 800-53. Attackers could
    exploit unannotated or misconfigured backups to gain insights into sensitive application data or operations.
  description: This control checks that all Kubernetes pod backups include necessary security annotations as per defined policies.
    Annotations are metadata that help manage and enforce security measures, such as specifying backup retention policies
    or encryption requirements. Proper configuration ensures that backups are consistently handled according to security best
    practices, reducing the risk of exposure or unauthorized access. It also aids in maintaining compliance with frameworks
    like the CIS Kubernetes Benchmark by ensuring that security configurations are consistently applied.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  - https://kubernetes.io/docs/concepts/configuration/overview/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_CP-9-b_0510
- rule_id: k8s.pod.configuration.compliance_check
  service: pod
  resource: configuration
  requirement: Compliance Check
  scope: pod.configuration.compliance_check
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure Pod Security Standards are Enforced
  rationale: Without enforcing Pod Security Standards, Kubernetes pods may be susceptible to vulnerabilities such as privilege
    escalation, unauthorized network access, and resource hijacking. Attackers can exploit misconfigurations to execute arbitrary
    code, access sensitive data, or disrupt services.
  description: This control checks that Kubernetes pods adhere to defined Pod Security Standards, ensuring configurations
    align with best practices for security. Validations focus on preventing privilege escalation, enforcing strict access
    controls, and ensuring secure runtime configurations. Good configuration includes disabling host networking, ensuring
    containers run as non-root users, and enforcing read-only root file systems. Implementing these standards mitigates risks
    of container escapes, unauthorized access, and data breaches.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/setup/best-practices/enforcing-pod-security-standards/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-c_0399
- rule_id: k8s.pod.container.config_enforced
  service: pod
  resource: container
  requirement: Config Enforced
  scope: pod.container.config_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pod Configuration is Enforced with PodSecurityPolicy
  rationale: Improperly configured pod settings can lead to vulnerabilities such as privilege escalation, unauthorized network
    access, and data breaches. Attackers may exploit misconfigurations to execute arbitrary code, gain elevated privileges,
    or access sensitive data. Enforcing configuration policies ensures that all pods adhere to security best practices, reducing
    the risk of such attacks.
  description: This rule checks that PodSecurityPolicies (PSP) or Pod Security Admission (PSA) are enforced on all pods. It
    ensures that configurations such as privilege escalation, host network usage, and container capabilities are restricted
    according to predefined security policies. Proper configuration helps in adhering to security benchmarks, preventing unauthorized
    access, and mitigating potential security threats by enforcing a consistent security posture across the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.pod.container.horizontal_autoscaler_enabled
  service: pod
  resource: container
  requirement: Horizontal Autoscaler Enabled
  scope: pod.container.horizontal_autoscaler_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Horizontal Pod Autoscaler is Configured
  rationale: Without a properly configured Horizontal Pod Autoscaler (HPA), applications may experience downtime or degraded
    performance due to the inability to automatically scale in response to demand. This can lead to resource exhaustion, potential
    denial of service, and increased vulnerability to attacks that exploit overloaded systems. Proper autoscaling helps maintain
    availability, performance, and resilience against such risks.
  description: This control checks that the Horizontal Pod Autoscaler (HPA) is enabled and correctly configured for Kubernetes
    pods. An effective HPA configuration ensures that the application dynamically adjusts the number of pod replicas in response
    to workload changes, based on metrics like CPU utilization or custom metrics. This proactive scaling helps in preventing
    resource exhaustion, which could otherwise lead to service outages or increased exposure to attacks. A good configuration
    includes setting appropriate thresholds for scaling events and ensuring that the HPA is integrated with the monitoring
    and alerting systems for timely responses.
  references:
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  - https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#autoscaling
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  compliance:
  - rbi_nbfc_multi_cloud_3.2_0018
  - rbi_nbfc_multi_cloud_6.4_0032
- rule_id: k8s.pod.container.host_network_disabled
  service: pod
  resource: container
  requirement: Host Network Disabled
  scope: pod.container.host_network_disabled
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Disable Host Network Access in Pods
  rationale: Allowing pods to use the host's network namespace can expose the host to attacks such as network eavesdropping,
    IP spoofing, and unauthorized access to other services running on the host. This configuration can be exploited by a compromised
    pod to perform lateral movement and gain access to sensitive host-level resources.
  description: This rule checks that the 'hostNetwork' field is set to 'false' for all pod specifications. By disabling host
    network access, pods cannot share the host's network stack, reducing the risk of network-based attacks and ensuring isolation
    between the host and pod network traffic. A secure configuration prevents pods from interfering with host-level network
    settings and accessing the host's network interfaces directly.
  references:
  - https://kubernetes.io/docs/concepts/configuration/pod-overview/#pod-networking
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  compliance:
  - nist_800_171_r2_multi_cloud_3_4_7_3.4.7_Restrict_disable_or_prevent_the_use_0040
- rule_id: k8s.pod.container.image_signing_verified_enforced
  service: pod
  resource: container
  requirement: Image Signing Verified Enforced
  scope: pod.container.image_signing_verified_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Image Signature Verification for Pod Containers
  rationale: Failure to verify container image signatures can lead to the deployment of untrusted or malicious images, increasing
    the risk of unauthorized code execution and potential breaches. Attackers could exploit unsigned images to introduce vulnerabilities,
    exploit known security flaws, or execute arbitrary code within the cluster.
  description: This rule checks if container images used in Kubernetes Pods have their signatures verified before deployment.
    Proper image signature verification ensures that only images from trusted sources are executed, reducing the risk of running
    compromised or unauthorized software. A good configuration involves integrating admission controllers or custom policies
    that enforce image signature checks, thus enhancing the overall security posture by ensuring image integrity and authenticity.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook
  - https://kubernetes.io/docs/concepts/containers/images/#image-verification
  - https://kubernetes.io/blog/2019/04/15/production-grade-container-image-security/
  compliance: []
- rule_id: k8s.pod.container.sbom_management_enabled
  service: pod
  resource: container
  requirement: Sbom Management Enabled
  scope: pod.container.sbom_management_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable SBOM Management in Pod Containers
  rationale: Software Bill of Materials (SBOM) management is crucial for identifying and tracking the components within a
    container image. Without SBOM management, there is an increased risk of using outdated or vulnerable software components,
    which can be exploited by attackers. Misconfiguration can lead to unauthorized access and potential data breaches by allowing
    vulnerable libraries to remain unnoticed.
  description: This rule checks whether SBOM management is enabled for each container within a Kubernetes pod. An effective
    SBOM management ensures that all software components and their dependencies are documented, enabling security teams to
    identify and remediate vulnerabilities promptly. Proper configuration includes the automatic generation and validation
    of SBOMs, which helps maintain integrity, supports compliance with industry standards, and mitigates risks associated
    with unpatched software.
  references:
  - https://kubernetes.io/blog/2021/11/18/sbom-and-security/
  - https://kubernetes.io/docs/concepts/containers/images/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  compliance: []
- rule_id: k8s.pod.container.seccomp_profile_docker_default
  service: pod
  resource: container
  requirement: Seccomp Profile Docker Default
  scope: pod.container.seccomp_profile_docker_default
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Default Seccomp Profile for Containers
  rationale: If a container runs without a seccomp profile or with an overly permissive one, it increases the risk of system
    calls being exploited by attackers to escalate privileges or perform malicious actions within the host. This could lead
    to container escape, data leakage, or unauthorized access to host resources.
  description: This control checks whether each container within a pod is configured to use the 'docker/default' seccomp profile.
    The default seccomp profile provides a balanced set of restrictions that limit the container's ability to make potentially
    dangerous system calls, thereby reducing the attack surface. Properly setting the seccomp profile helps in adhering to
    security best practices by limiting system call access and preventing exploitation of known kernel vulnerabilities.
  references:
  - https://kubernetes.io/docs/tutorials/security/seccomp/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
  - https://kubernetes.io/docs/tasks/configure-pod-container/seccomp/
  compliance:
  - cis_kubernetes_kubernetes_5.6.2_0132
  - cis_kubernetes_kubernetes_5.7.2_0138
- rule_id: k8s.pod.container.security_context_memory_protection_enabled
  service: pod
  resource: container
  requirement: Security Context Memory Protection Enabled
  scope: pod.container.security_context_memory_protection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Seccomp Profile for Pod Containers
  rationale: If Seccomp is not enabled, containers may run with fewer restrictions on system calls, potentially allowing exploitation
    through system-level vulnerabilities. Attackers could leverage this to escalate privileges, execute arbitrary code, or
    perform denial-of-service attacks.
  description: This rule checks that each container within a Kubernetes pod has a Seccomp profile set in its security context.
    A Seccomp profile restricts the set of system calls that can be made by a container, reducing the potential attack surface.
    A well-configured Seccomp profile helps mitigate risks associated with kernel-level exploits by limiting the capabilities
    of a container to interact with the host kernel, thereby enhancing the security posture of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/tutorials/clusters/seccomp/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-16_1433
- rule_id: k8s.pod.container.signed_images_required_enforced
  service: pod
  resource: container
  requirement: Signed Images Required Enforced
  scope: pod.container.signed_images_required_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Image Signature Verification for Pod Containers
  rationale: If image signatures are not enforced, there is a risk that containers might run untrusted or malicious images.
    Attackers could inject malicious code into these images, leading to potential data breaches, privilege escalation, or
    service disruptions within the Kubernetes cluster.
  description: This rule checks whether Kubernetes pod containers are configured to only run images that have been signed
    by a trusted source. It ensures that the ImagePolicyWebhook admission controller is in place to validate image signatures
    before they are deployed. Enforcing signed images helps prevent unauthorized or tampered images from being executed, thus
    maintaining the integrity and trustworthiness of the containerized applications.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/#image-verification
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enforce-pod-security-standards
  compliance: []
- rule_id: k8s.pod.container.vulnerability_scanning_enabled_enforced
  service: pod
  resource: container
  requirement: Vulnerability Scanning Enabled Enforced
  scope: pod.container.vulnerability_scanning_enabled_enforced
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enforce Automated Vulnerability Scanning on Pod Containers
  rationale: Without automated vulnerability scanning, container images may contain unpatched vulnerabilities that can be
    exploited by attackers to gain unauthorized access or disrupt services. Vulnerability scanning helps identify known vulnerabilities
    in container images before deployment, reducing the risk of exploitation and ensuring a secure runtime environment.
  description: This control verifies that all Kubernetes pod containers have automated vulnerability scanning enabled and
    enforced. A good configuration requires integration with a vulnerability scanning tool that checks container images for
    known vulnerabilities before they are deployed. By ensuring this, the system can prevent the deployment of containers
    with critical vulnerabilities, thus maintaining the integrity and security of the Kubernetes cluster. Additionally, it
    supports compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/#image-vulnerability-scanning
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance: []
- rule_id: k8s.pod.deployment.anomaly_detection_enabled
  service: pod
  resource: deployment
  requirement: Anomaly Detection Enabled
  scope: pod.deployment.anomaly_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Anomaly Detection in Pod Deployments
  rationale: Without anomaly detection, abnormal behavior and potential security incidents in pod deployments may go unnoticed,
    leading to data breaches, unauthorized access, or service disruption. Attack vectors include privilege escalation and
    exploitation of unpatched vulnerabilities.
  description: Checks if anomaly detection mechanisms are enabled for pod deployments to identify deviations from normal behavior.
    This includes monitoring CPU/memory usage patterns and network traffic. Proper configuration involves integrating with
    tools that support anomaly detection, such as Kubernetes-native solutions or external monitoring systems. This helps in
    early detection of potential threats, enabling prompt response to mitigate risks.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/
  compliance: []
- rule_id: k8s.pod.disaster.recovery_plan_documented
  service: pod
  resource: disaster
  requirement: Recovery Plan Documented
  scope: pod.disaster.recovery_plan_documented
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: low
  title: Document Disaster Recovery Plan for Kubernetes Pods
  rationale: Without a documented disaster recovery plan, Kubernetes environments are vulnerable to prolonged outages and
    data loss in the event of a failure. Misconfiguration can lead to inadequate backup procedures, leaving the system exposed
    to data corruption or loss due to unexpected failures or attacks, such as ransomware. A well-documented plan ensures quick
    recovery, minimizing downtime and maintaining business continuity.
  description: This control verifies that a comprehensive disaster recovery plan is documented for Kubernetes pods. A good
    configuration includes details on backup schedules, restoration procedures, and roles and responsibilities during a disaster.
    This documentation helps in ensuring that in case of a failure, the recovery process is efficient and effective, thereby
    reducing the time to restore operations and protecting critical data against loss. It also assists in meeting regulatory
    compliance requirements by demonstrating preparedness for incidents.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/disaster-recovery/
  - https://kubernetes.io/docs/concepts/cluster-administration/backup-restore/
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-2_0053
  - hipaa_multi_cloud_164_308_a_7_i_0017
  - hipaa_multi_cloud_164_308_a_7_ii_a_0018
- rule_id: k8s.pod.disruption.budget_configured
  service: pod
  resource: disruption
  requirement: Budget Configured
  scope: pod.disruption.budget_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure PodDisruptionBudget is Configured
  rationale: Without a properly configured PodDisruptionBudget, critical applications may experience unexpected downtime during
    node maintenance or scaling events, increasing the risk of service unavailability and potential breaches. Attackers could
    exploit service disruptions to launch additional attacks during recovery phases.
  description: This rule checks that a PodDisruptionBudget (PDB) is defined for critical application pods to control voluntary
    disruptions such as node upgrades or scale-downs. A well-configured PDB ensures that a minimum number of pods remains
    available, maintaining service continuity and reducing the attack surface during disruption periods. A valid configuration
    specifies either 'minAvailable' or 'maxUnavailable' to control the number of pods that can be disrupted at a time, following
    best practices for high availability.
  references:
  - https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  - https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  - https://kubernetes.io/docs/concepts/architecture/nodes/#node-maintenance
  compliance:
  - nist_800_53_rev5_multi_cloud_CP-2-a_0462
  - nist_800_53_rev5_multi_cloud_SC-6_1193
- rule_id: k8s.pod.horizontal.autoscaler_enabled
  service: pod
  resource: horizontal
  requirement: Autoscaler Enabled
  scope: pod.horizontal.autoscaler_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Horizontal Pod Autoscaler for Resource Management
  rationale: Without enabling the Horizontal Pod Autoscaler (HPA), Kubernetes pods may become overutilized, leading to performance
    degradation and potential denial-of-service conditions. Unmanaged resource consumption can exhaust cluster resources,
    making it easier for attackers to launch resource-based attacks or disrupt service availability.
  description: This rule checks if the Horizontal Pod Autoscaler is enabled and configured correctly for Kubernetes pods.
    A properly configured HPA dynamically adjusts the number of pod replicas based on observed CPU utilization or other select
    metrics, ensuring efficient resource utilization. This proactive resource management helps mitigate risks associated with
    resource exhaustion and denial-of-service attacks by automatically scaling pods in response to demand spikes, thus maintaining
    service reliability and security.
  references:
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis
  compliance: []
- rule_id: k8s.pod.host.network_disabled
  service: pod
  resource: host
  requirement: Network Disabled
  scope: pod.host.network_disabled
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Disable Host Network for Pods to Limit Exposure
  rationale: Allowing pods to use the host's network namespace can lead to significant security risks, such as unauthorized
    access to the host network, potential privilege escalation, and lateral movement within the cluster. Attackers may exploit
    this to intercept sensitive data or disrupt network services. Disabling host networking minimizes these risks by isolating
    pod networks, thus reducing the attack surface.
  description: This control checks whether the 'hostNetwork' field is set to false in pod specifications. A secure configuration
    ensures that pods do not share the host's network namespace. This isolation prevents pods from accessing the host's network
    interfaces and services directly, thereby limiting the impact of compromised pods and enhancing network security within
    the cluster. Proper configuration also aligns with security best practices and compliance standards such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/cluster-administration/networking/#default-pod-to-pod-communication
  compliance: []
- rule_id: k8s.pod.labeling.for_inventory_tracking
  service: pod
  resource: labeling
  requirement: For Inventory Tracking
  scope: pod.labeling.for_inventory_tracking
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Consistent Pod Labeling for Inventory and Compliance
  rationale: Without consistent labeling of pods, it becomes challenging to track resources for inventory purposes, which
    may lead to unmanaged or orphaned resources. This can result in increased attack surface due to unmonitored deployments,
    and difficulty in meeting compliance requirements. Labels are crucial for organizing, tracking, and managing resources
    effectively, thus enhancing security by facilitating better access control and auditability.
  description: This rule checks for the presence of predefined labels on all pods within the Kubernetes cluster. Proper labeling
    should include identifiers for the application, environment, and owner. Consistent labels enable effective resource tracking
    and management, assist in enforcing security policies, and help in auditing for compliance purposes. A well-implemented
    labeling strategy reduces the risk of unauthorized access and ensures that all deployed resources are accounted for and
    manageable under security policies.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/configuration/overview/
  - https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-8_0048
- rule_id: k8s.pod.labeling.standard
  service: pod
  resource: labeling
  requirement: Standard
  scope: pod.labeling.standard
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Consistent Pod Labeling for Security and Compliance
  rationale: Inconsistent or missing pod labels can lead to challenges in tracking, managing, and securing Kubernetes workloads.
    Attackers might exploit these weaknesses to bypass security policies, access unauthorized resources, or inject malicious
    workloads. Consistent labeling helps ensure that security controls and policies are accurately applied and enforced, reducing
    the risk of misconfigurations and unauthorized access.
  description: This control checks that all Kubernetes pods are labeled according to a predefined, consistent standard. Labels
    must be applied to all pods to facilitate effective resource management, access control, and auditing. Proper labeling
    supports network policies, role-based access control (RBAC), and security monitoring by ensuring that all resources are
    accounted for and can be associated with security policies. By enforcing a consistent labeling standard, organizations
    can improve their security posture, comply with industry regulations, and prevent unauthorized access and potential breaches.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-8-a_0424
  - nist_800_53_rev5_multi_cloud_CM-8-b_0425
- rule_id: k8s.pod.labels.inventory_maintained
  service: pod
  resource: labels
  requirement: Inventory Maintained
  scope: pod.labels.inventory_maintained
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Consistent Pod Labeling for Inventory Management
  rationale: Inconsistent or missing pod labels can lead to poor inventory management, making it difficult to track and monitor
    resources effectively. This can expose the cluster to risks such as unauthorized access, misconfiguration, and inefficient
    resource utilization. Attackers can exploit untracked pods to persist malicious workloads or evade detection.
  description: This control checks that all pods have consistent and predefined labels that adhere to organizational inventory
    management policies. A good configuration involves having a standard set of labels for each pod, which may include application
    name, environment, version, and owner. This practice helps in operational visibility, resource allocation, and applying
    security policies more efficiently. By ensuring proper labeling, organizations can prevent unauthorized access, quickly
    identify unapproved or rogue workloads, and maintain compliance with security standards.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - nist_800_171_r2_multi_cloud_3_4_1_3.4.1_Establish_and_maintain_baseline_confi_0037
- rule_id: k8s.pod.node.host_pid_disabled_enforced
  service: pod
  resource: node
  requirement: Host Pid Disabled Enforced
  scope: pod.node.host_pid_disabled_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Host PID Namespace Sharing in Pods
  rationale: Allowing pods to share the host's process ID namespace can lead to security vulnerabilities by enabling potential
    attackers to view and signal processes on the host, which could lead to privilege escalation and unauthorized access to
    host resources. Disabling host PID namespace sharing minimizes the attack surface and protects against these risks.
  description: This rule checks that the 'hostPID' field is set to 'false' in pod specifications, ensuring that pods do not
    share the host's PID namespace. A correct configuration prevents pods from accessing the host's process table, thereby
    reducing the risk of privilege escalation and unauthorized process manipulation. This practice aligns with security best
    practices outlined in the CIS Kubernetes Benchmark and enhances the overall security posture by isolating pod-level processes
    from the host.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance: []
- rule_id: k8s.pod.persistentvolume.backup_configured
  service: pod
  resource: persistentvolume
  requirement: Backup Configured
  scope: pod.persistentvolume.backup_configured
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: medium
  title: Ensure PersistentVolume Backups Are Configured and Verified
  rationale: Without proper backup configuration, data stored in PersistentVolumes can be lost due to accidental deletion,
    hardware failure, or malicious attacks such as ransomware. This can result in significant business disruptions and data
    breaches. Ensuring backups are configured and regularly verified mitigates the risk of data loss and contributes to business
    continuity.
  description: This control checks that PersistentVolumes used by Kubernetes Pods have an associated backup policy that is
    both configured and operational. A valid configuration typically involves automated, regular backups, stored securely
    offsite, and includes periodic restoration tests to verify data integrity and availability. Implementing this control
    helps ensure data resilience against failures and attacks, supports disaster recovery processes, and aligns with compliance
    requirements such as PCI DSS.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/
  - https://kubernetes.io/docs/tasks/administer-cluster/backup-etcd/
  compliance:
  - pci_dss_v4_multi_cloud_3.1.2_0033
  - pci_dss_v4_multi_cloud_3.3.1.2_0035
  - pci_dss_v4_multi_cloud_11.3.1.2_0167
- rule_id: k8s.pod.pod.annotation_standard_configured
  service: pod
  resource: pod
  requirement: Annotation Standard Configured
  scope: pod.pod.annotation_standard_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pod Annotations for Security Policies Are Configured
  rationale: Annotations in Kubernetes pods can be used to enforce security policies or provide metadata that influences security
    controls. If annotations are not properly configured, it may lead to misinterpretation of security policies, leaving pods
    vulnerable to unauthorized access or privilege escalation. For example, misconfigured annotations could fail to trigger
    necessary security policies, exposing the organization to insider threats or external attacks.
  description: This control checks that Kubernetes pods have the appropriate annotations configured in accordance with security
    best practices. It verifies that annotations intended for security policy enforcement, such as those used by admission
    controllers or third-party security tools, are present and correct. Proper configuration helps to enforce security policies
    consistently across the cluster, reducing the risk of misconfiguration and ensuring adherence to compliance requirements,
    such as those outlined in the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  - https://kubernetes.io/docs/concepts/policy/pod-security-admission/
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  compliance: []
- rule_id: k8s.pod.pod.annotations_for_inventory_metadata_configured
  service: pod
  resource: pod
  requirement: Annotations For Inventory Metadata Configured
  scope: pod.pod.annotations_for_inventory_metadata_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pods Have Inventory Metadata Annotations
  rationale: Without proper inventory metadata annotations, it becomes challenging to track and audit the configuration and
    state of pods, complicating incident response and vulnerability management. This can lead to unmonitored changes, unauthorized
    access, and difficulty in maintaining compliance with security policies.
  description: This rule checks that each pod is annotated with inventory metadata, which includes details such as the application's
    owner, purpose, and environment. By maintaining these annotations, organizations can improve visibility and traceability
    of deployments, ensure accountability, and facilitate automated compliance checks. A well-annotated pod setup aids in
    incident investigation and helps prevent configuration drift by making sure all changes are logged and auditable.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  - https://kubernetes.io/docs/concepts/configuration/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  compliance: []
- rule_id: k8s.pod.pod.annotations_maintained
  service: pod
  resource: pod
  requirement: Annotations Maintained
  scope: pod.pod.annotations_maintained
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Critical Annotations Are Applied to Pods
  rationale: Annotations in Kubernetes pods provide metadata that can be used for security policies, monitoring, and automation.
    If critical annotations are missing or misconfigured, it may lead to security gaps such as unauthorized access, lack of
    audit trails, or misconfigured network policies. Attack vectors include privilege escalation within the cluster or data
    exfiltration due to inadequate logging and monitoring.
  description: This rule checks whether required critical annotations are applied and maintained on Kubernetes pods. Proper
    annotations can trigger security policies through admission controllers, facilitate auditing, and ensure compliance with
    security standards. A secure configuration involves ensuring that annotations related to security policies, such as `seccomp`,
    `appArmor`, and network policies, are present and correctly configured. This reduces the risk of unauthorized access and
    enhances compliance with security benchmarks.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/policy/pod-security-admission/
  compliance: []
- rule_id: k8s.pod.pod.disruption_budget_configured
  service: pod
  resource: pod
  requirement: Disruption Budget Configured
  scope: pod.pod.disruption_budget_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pod Disruption Budgets are Configured
  rationale: Without a Pod Disruption Budget (PDB), critical applications may experience unexpected downtime during voluntary
    disruptions, such as node maintenance or cluster scaling. This can lead to service unavailability and impact the resilience
    of applications, potentially opening up avenues for denial-of-service attacks if critical services are unavailable.
  description: This control checks that all Kubernetes pods have an associated Pod Disruption Budget configured. A PDB sets
    a limit on the number of disruptions that can occur at any one time, ensuring that a minimum number of pods remain available.
    Proper configuration of disruption budgets helps maintain application availability and resilience against planned disruptions,
    reducing the risk of accidental denial of service during maintenance operations.
  references:
  - https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  - https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  - https://kubernetes.io/docs/concepts/scheduling-eviction/eviction-policy/
  compliance: []
- rule_id: k8s.pod.pod.ebpf_monitoring_enabled
  service: pod
  resource: pod
  requirement: Ebpf Monitoring Enabled
  scope: pod.pod.ebpf_monitoring_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable eBPF Monitoring in Pods for Threat Detection
  rationale: Without eBPF monitoring, malicious activities such as unauthorized network connections or process executions
    can go undetected in Kubernetes pods. This exposes the cluster to potential lateral movement by attackers and makes it
    challenging to trace the source of security incidents.
  description: This control checks if eBPF monitoring is enabled on Kubernetes pods, which facilitates real-time kernel-level
    monitoring and tracing. Proper eBPF configuration allows for the detection of anomalous system calls and network traffic,
    enhancing the visibility of pod activities and enabling quicker incident response. A correctly configured setup includes
    eBPF programs that are actively collecting and reporting data to a monitoring system, thus ensuring compliance with best
    security practices and detection capabilities.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  - https://kubernetes.io/docs/tasks/debug/debug-application/
  compliance: []
- rule_id: k8s.pod.pod.falco_enabled_enforced
  service: pod
  resource: pod
  requirement: Falco Enabled Enforced
  scope: pod.pod.falco_enabled_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Falco Security Monitoring on Kubernetes Pods
  rationale: Without enforcing Falco for runtime security monitoring, pods are vulnerable to unnoticed malicious activity
    such as unauthorized access, privilege escalation, and data exfiltration. Attackers can exploit vulnerabilities in running
    containers, leading to potential breaches and disruption of services.
  description: This rule checks that Falco, an open-source runtime security tool, is enabled and correctly configured on Kubernetes
    pods. Proper configuration involves ensuring Falco is actively monitoring for suspicious behavior using predefined security
    rules. This helps identify unauthorized actions, such as running non-whitelisted processes or modifying system binaries,
    thereby enhancing the security posture by providing real-time alerts and mitigating risks before they escalate.
  references:
  - https://kubernetes.io/docs/concepts/architecture/security-overview/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance: []
- rule_id: k8s.pod.pod.inventory_labels_maintained
  service: pod
  resource: pod
  requirement: Inventory Labels Maintained
  scope: pod.pod.inventory_labels_maintained
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pods Have Accurate Inventory Labels
  rationale: Inventory labels on pods are crucial for ensuring efficient resource management, auditing, and security monitoring
    within Kubernetes clusters. Misconfigured or missing labels can lead to difficulty in tracking resource usage, hinder
    incident response efforts, and cause potential compliance issues. Attackers may exploit unlabeled or improperly labeled
    pods to obfuscate malicious activities and evade detection.
  description: This control checks that all pods have accurate and consistent inventory labels applied. These labels should
    include metadata such as application name, environment, and version. Properly configured labels help in grouping and identifying
    pods for monitoring, logging, and applying security policies. They enable administrators to enforce security controls
    through tools like network policies and facilitate compliance with regulatory standards by providing a clear audit trail
    of resource usage.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/
  - https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
  compliance: []
- rule_id: k8s.pod.pod.labeling_standard_configured
  service: pod
  resource: pod
  requirement: Labeling Standard Configured
  scope: pod.pod.labeling_standard_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Pod Labeling Standards for Security Compliance
  rationale: Without standardized labeling, it becomes challenging to manage and secure workloads across a Kubernetes cluster.
    Proper labeling helps in organizing resources, enforcing security policies, and controlling access using tools like RBAC.
    Misconfigured or absent labels can lead to unauthorized access, data breaches, and difficulties in auditing and monitoring,
    which can be exploited by attackers to move laterally within the cluster.
  description: This control verifies that all Kubernetes pods adhere to a predefined labeling scheme. The labeling standard
    should include labels for environment, application type, and compliance status, among others. Proper labeling enhances
    security by facilitating effective resource management, enabling precise network policies, and simplifying monitoring
    and logging. It ensures that security policies can be applied consistently across the cluster, reducing the risk of unauthorized
    access and ensuring compliance with security best practices such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/administer-cluster/psp/
  compliance: []
- rule_id: k8s.pod.pod.patch_management_enabled
  service: pod
  resource: pod
  requirement: Patch Management Enabled
  scope: pod.pod.patch_management_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure Regular Patch Updates for Kubernetes Pods
  rationale: Without regular patch updates, Kubernetes pods can be vulnerable to known exploits and security vulnerabilities.
    Attackers can exploit these vulnerabilities to gain unauthorized access, execute arbitrary code, or escalate privileges.
    Ensuring patch management guards against such threats by applying security updates promptly.
  description: This control verifies that Kubernetes pods are configured to receive regular patch updates. A good configuration
    involves using image tags that correspond to specific, updated versions rather than 'latest', and implementing automated
    processes for testing and deploying new patches. By maintaining up-to-date images, vulnerabilities are minimized, reducing
    the risk of exploitation and maintaining compliance with security standards.
  references:
  - https://kubernetes.io/docs/concepts/configuration/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/
  - https://kubernetes.io/docs/concepts/containers/images/
  compliance: []
- rule_id: k8s.pod.pod.psa_enforced
  service: pod
  resource: pod
  requirement: Psa Enforced
  scope: pod.pod.psa_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Pod Security Admission Policies
  rationale: Failure to enforce Pod Security Admission (PSA) policies can lead to elevated security risks, such as privilege
    escalation, data breaches, and unauthorized access to sensitive resources. Without PSA, there is an increased risk of
    deploying pods with insecure configurations that could be exploited by attackers to compromise the cluster.
  description: This control checks if Pod Security Admission policies are enforced in the cluster. PSA provides a mechanism
    to define and apply security standards at the namespace level, ensuring that pods adhere to defined security profiles
    such as 'privileged', 'baseline', or 'restricted'. By enforcing these policies, you can prevent the deployment of pods
    with configurations that pose security risks, such as running as root or accessing sensitive host resources. Proper enforcement
    of PSA enhances overall cluster security by reducing the attack surface and aligning with industry security standards.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-admission/
  - https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecurity
  compliance: []
- rule_id: k8s.pod.pod.resource_limits_configured
  service: pod
  resource: pod
  requirement: Resource Limits Configured
  scope: pod.pod.resource_limits_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Resource Limits on Pods
  rationale: Without resource limits, a single pod can consume excessive CPU or memory resources, potentially leading to node
    instability, application downtime, and denial of service (DoS) conditions. Attackers may exploit misconfigured resource
    limits to disrupt services or degrade system performance, impacting availability.
  description: This rule ensures that every Kubernetes pod is configured with explicit CPU and memory resource limits. It
    verifies that both 'requests' and 'limits' are set for resources to prevent any pod from monopolizing node resources.
    Proper configuration provides isolation between workloads, preventing resource contention and ensuring fair resource distribution
    across all running applications, thus maintaining system stability and protecting against denial-of-service attacks.
  references:
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/
  - https://kubernetes.io/docs/concepts/policy/limit-range/
  compliance:
  - rbi_nbfc_multi_cloud_3.2_0018
  - rbi_nbfc_multi_cloud_6.4_0032
- rule_id: k8s.pod.pod.security_standard_review_scheduled
  service: pod
  resource: pod
  requirement: Security Standard Review Scheduled
  scope: pod.pod.security_standard_review_scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Schedule Regular Security Reviews for Pods
  rationale: Regular security reviews of pod configurations help identify vulnerabilities and misconfigurations that may lead
    to unauthorized access or privilege escalation. Without scheduled reviews, pods might run with outdated or insecure settings,
    making them susceptible to attacks, such as exploiting known vulnerabilities in container images or misconfigured network
    policies.
  description: This rule checks if there is a regular schedule for security reviews of Kubernetes pod configurations. A well-defined
    review schedule ensures that pods are evaluated against current security standards and best practices. This includes verifying
    compliance with Pod Security Standards, ensuring network policies are restrictive, and that RBAC policies are correctly
    applied. Regular reviews help maintain a secure environment by proactively identifying and mitigating potential security
    risks, thus ensuring compliance with industry standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/cluster-administration/network-policies/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-2-b_0367
- rule_id: k8s.pod.pod.termination_grace_period_configured
  service: pod
  resource: pod
  requirement: Termination Grace Period Configured
  scope: pod.pod.termination_grace_period_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Pod Termination Grace Period Configuration
  rationale: A properly configured termination grace period ensures that a pod has enough time to gracefully terminate its
    processes, which helps preserve data integrity and allows for proper resource cleanup. Without this, abrupt termination
    may lead to data corruption, potential security vulnerabilities, or inconsistent states which could be exploited by attackers.
  description: This control checks whether each Kubernetes pod has a termination grace period configured. A well-defined termination
    grace period allows applications to handle termination signals appropriately, ensuring that all necessary shutdown routines
    are executed. This reduces the risk of leaving sensitive data in an inconsistent state, prevents resource leaks, and mitigates
    potential attack vectors where abruptly terminated processes might be exploited. The recommended practice is to set a
    termination grace period that aligns with the application's shutdown process requirements, typically 30 seconds or more,
    depending on the application.
  references:
  - https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  - https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  compliance: []
- rule_id: k8s.pod.resource.limits_set
  service: pod
  resource: resource
  requirement: Limits Set
  scope: pod.resource.limits_set
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Pod Resource Limit Configuration
  rationale: Setting resource limits on pods is crucial to prevent resource exhaustion attacks, which can lead to Denial of
    Service (DoS) by monopolizing cluster resources. Without limits, a rogue or compromised pod could consume excessive CPU
    or memory, affecting the availability and performance of other applications. This configuration also aids in maintaining
    predictable performance and enforcing fair resource sharing across applications.
  description: This rule checks that each Kubernetes pod has defined resource limits for CPU and memory. Configuring these
    limits ensures that no single pod can overuse cluster resources, which is essential for maintaining system stability and
    security. Proper configuration involves setting 'limits.cpu' and 'limits.memory' in the pod specification. This practice
    mitigates the risk of resource starvation for other pods and contributes to compliance with security benchmarks such as
    the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/
  - https://kubernetes.io/docs/concepts/policy/limit-range/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-5_0131
- rule_id: k8s.pod.resource.requests_and_limits_set
  service: pod
  resource: resource
  requirement: Requests And Limits Set
  scope: pod.resource.requests_and_limits_set
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Set Pod Resource Requests and Limits to Prevent Resource Exhaustion
  rationale: Failing to set resource requests and limits on pods can lead to resource exhaustion, impacting cluster availability
    and performance. Attackers can exploit this by deploying pods that consume excessive CPU or memory, causing denial of
    service to other applications.
  description: This control checks that all Kubernetes pods have resource requests and limits defined for CPU and memory.
    Properly setting these values ensures that each pod receives the resources it needs without exceeding what the cluster
    can provide. This not only helps in maintaining application performance but also prevents a single pod from monopolizing
    cluster resources, which can lead to denial of service attacks. By adhering to this configuration, you align with security
    best practices and benchmarks like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/
  compliance:
  - soc2_multi_cloud_a1_1_0024
- rule_id: k8s.pod.seccomp.profile_runtime_default_check
  service: pod
  resource: seccomp
  requirement: Profile Runtime Default Check
  scope: pod.seccomp.profile_runtime_default_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Seccomp Default Profile for Pods
  rationale: Without a default seccomp profile, pods may run with unconfined syscalls, increasing the risk of privilege escalation
    and unauthorized access to host resources. Attackers can exploit this to perform system calls that could compromise the
    host system.
  description: This control checks that all Kubernetes pods enforce the 'RuntimeDefault' seccomp profile. The 'RuntimeDefault'
    profile restricts the set of system calls available to containers, reducing the attack surface and mitigating risks of
    syscall-based attacks. Proper configuration helps prevent privilege escalation and ensures compliance with security benchmarks,
    such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tutorials/security/seccomp/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
  - https://kubernetes.io/docs/tasks/configure-pod-container/seccomp/
  compliance:
  - cis_kubernetes_kubernetes_1.6.6_0175
- rule_id: k8s.pod.security.admission_hostpid_restrictions
  service: pod
  resource: security
  requirement: Admission Hostpid Restrictions
  scope: pod.security.admission_hostpid_restrictions
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Restrict HostPID Namespace Access in Pods
  rationale: Allowing pods to share the host's PID namespace poses significant security risks by enabling potential process
    snooping and privilege escalation attacks. Malicious actors can exploit this access to interfere with or control host
    processes, leading to unauthorized access or denial of service. Ensuring that pods do not have unnecessary hostPID access
    is critical for maintaining isolation between host and container processes, thereby minimizing attack vectors.
  description: This control checks that the HostPID field is set to false in pod specifications, thereby preventing pods from
    accessing the host's PID namespace. Proper configuration ensures that containerized processes are isolated from host processes,
    reducing the risk of process-level attacks. This improves the security posture by ensuring compliance with established
    security benchmarks like the CIS Kubernetes Benchmark, enhancing process isolation, and preventing potential privilege
    escalation.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  compliance:
  - cis_kubernetes_kubernetes_5.2.13_0125
  - cis_kubernetes_kubernetes_5.2.3_0115
- rule_id: k8s.pod.security.allow_privilege_escalation_restricted
  service: pod
  resource: security
  requirement: Allow Privilege Escalation Restricted
  scope: pod.security.allow_privilege_escalation_restricted
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict Privilege Escalation in Pods
  rationale: Allowing privilege escalation in containers can enable potential attackers to gain elevated privileges within
    a pod, leading to unauthorized access or control over the container and potentially the host system. This could be exploited
    through various attack vectors, such as exploiting vulnerable applications or misconfigurations, to perform privilege
    escalation attacks.
  description: This rule checks that the `allowPrivilegeEscalation` setting is set to `false` in the pod's security context.
    A properly configured setting prevents processes within a container from gaining additional privileges, thus mitigating
    risks associated with privilege escalation attacks. This enhances the security posture by ensuring least privilege principles
    are adhered to and aligns with the CIS Kubernetes Benchmark and other security standards.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#securitycontext-v1-core
  compliance:
  - cis_kubernetes_kubernetes_5.2.6_0118
- rule_id: k8s.pod.security.capabilities_enforcement
  service: pod
  resource: security
  requirement: Capabilities Enforcement
  scope: pod.security.capabilities_enforcement
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Excessive Linux Capabilities in Pods
  rationale: Allowing excessive Linux capabilities in containers can lead to privilege escalation attacks. An attacker exploiting
    a misconfigured pod could gain elevated permissions within a container, potentially compromising the host node. Reducing
    capabilities to only those necessary minimizes the attack surface and prevents unauthorized actions within the cluster.
  description: This control checks that pods are configured with a minimal set of Linux capabilities, adhering to the principle
    of least privilege. It validates that the 'capabilities' field in the security context of a pod's containers does not
    include unnecessary capabilities beyond what is required for the application to function. Properly configured capabilities
    prevent privilege escalation and reduce the risk of container breakout, thereby enhancing the overall security of the
    Kubernetes environment.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#capabilities
  compliance:
  - cis_kubernetes_kubernetes_5.2.10_0122
- rule_id: k8s.pod.security.host_network_policy_check
  service: pod
  resource: security
  requirement: Host Network Policy Check
  scope: pod.security.host_network_policy_check
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Restrict Host Network Access for Pods
  rationale: If a pod is allowed to use the host network, it can potentially listen on network interfaces and gain access
    to network traffic that it shouldn't. This can be exploited to intercept traffic, perform man-in-the-middle attacks, or
    scan the network for vulnerabilities, posing significant security risks.
  description: This rule checks that pods are not configured to use the host network unless absolutely necessary. A pod using
    the host network shares the network namespace of the host, which can lead to exposure of sensitive internal network traffic
    and increase the attack surface. Properly configured, pods should use their own network namespace to limit the potential
    impact of a compromised pod. This control ensures that any exception to this is explicitly justified and documented, enhancing
    the overall security posture of the cluster by reducing unnecessary exposure.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - cis_kubernetes_kubernetes_5.2.5_0117
- rule_id: k8s.pod.security.patches_applied
  service: pod
  resource: security
  requirement: Patches Applied
  scope: pod.security.patches_applied
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure Pods are Running with Latest Security Patches
  rationale: Running pods without the latest security patches exposes your applications to known vulnerabilities that can
    be exploited by attackers. This may lead to unauthorized access, data breaches, or disruptions in service. By ensuring
    that your pods are up-to-date with the latest security patches, you reduce the attack surface and mitigate risks associated
    with unpatched vulnerabilities.
  description: This control checks that all container images running in pods are up-to-date with the latest security patches.
    It involves verifying that the images used in deployments are regularly scanned for vulnerabilities and updated accordingly.
    A good configuration ensures that images are derived from trusted sources and are maintained with a patch management process.
    This practice helps in maintaining system integrity and compliance with security standards such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/
  - https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - nist_800_53_rev5_multi_cloud_SI-2-d_1345
  - pci_dss_v4_multi_cloud_6.2.3.1_0078
  - pci_dss_v4_multi_cloud_6.2.4_0079
- rule_id: k8s.pod.security.policy_configuration_check
  service: pod
  resource: security
  requirement: Policy Configuration Check
  scope: pod.security.policy_configuration_check
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Pod Security Admission Policies
  rationale: Without properly configured pod security policies, Kubernetes clusters are vulnerable to privilege escalation,
    data breaches, and unauthorized access. Attackers could exploit misconfigured pods to execute arbitrary code, escalate
    privileges, or access sensitive data within the cluster. Proper policy enforcement helps mitigate these risks by ensuring
    that only compliant pods are deployed.
  description: This rule checks that Kubernetes Pod Security Admission (PSA) policies are correctly configured to enforce
    security best practices. It validates the presence of policies that restrict the use of privileged containers, host namespaces,
    and potentially dangerous capabilities. A well-configured PSA ensures that all pods meet the organization's security requirements
    before they are allowed to run, significantly reducing the attack surface and preventing the deployment of insecure workloads.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-admission/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecurity
  compliance:
  - cis_kubernetes_kubernetes_1.6.2_0171
- rule_id: k8s.pod.security.privileged_containers_check
  service: pod
  resource: security
  requirement: Privileged Containers Check
  scope: pod.security.privileged_containers_check
  domain: identity_and_access_management
  subcategory: access_control
  severity: critical
  title: Prohibit Privileged Containers in Pods
  rationale: Allowing privileged containers in Kubernetes can lead to severe security vulnerabilities. Such containers can
    access the host's resources, potentially leading to privilege escalation attacks where malicious actors gain unauthorized
    access to the host system. This can compromise the entire cluster by enabling the attacker to bypass namespace isolation
    and access sensitive data or inject malicious code.
  description: 'This control checks that no pods in the Kubernetes cluster are configured with privileged containers enabled.
    Privileged containers have elevated permissions that allow them to perform operations typically restricted by the container
    runtime. By ensuring that the ''privileged: true'' setting is not used in pod specifications, this control helps maintain
    an isolated and secure environment, reducing the risk of container breakout and unauthorized access to the host system.
    Proper configuration involves setting ''privileged: false'' in the SecurityContext of all pods.'
  references:
  - https://kubernetes.io/docs/concepts/workloads/pods/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - cis_kubernetes_kubernetes_5.2.2_0114
- rule_id: k8s.pod.security.security_context_verification
  service: pod
  resource: security
  requirement: Security Context Verification
  scope: pod.security.security_context_verification
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Pod Security Context Configuration
  rationale: Misconfigured security contexts can lead to privilege escalation, unauthorized access, and potential compromise
    of the cluster. Attackers may exploit improperly set permissions to perform actions such as accessing the host filesystem
    or running with elevated privileges. Ensuring security contexts are correctly configured mitigates these risks by enforcing
    least privilege and isolating processes within the pod.
  description: This control checks that each pod has a security context defined, enforcing settings such as non-root user
    execution, read-only root filesystem, and specific capabilities. A well-configured security context prevents pods from
    running with excessive privileges, limits the attack surface, and adheres to the principle of least privilege. This validation
    helps in maintaining robust security by ensuring pods operate under controlled conditions, reducing the likelihood of
    successful exploitation and aiding compliance with security benchmarks.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy-deprecated
  compliance:
  - cis_kubernetes_kubernetes_5.6.3_0133
  - cis_kubernetes_kubernetes_5.7.3_0139
  - cis_kubernetes_kubernetes_1.6.7_0176
- rule_id: k8s.pod.security.standard_restricted
  service: pod
  resource: security
  requirement: Standard Restricted
  scope: pod.security.standard_restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Restricted Pod Security Standards
  rationale: Misconfiguring Pod Security Standards can expose the cluster to privilege escalation and unauthorized access.
    Attackers may exploit loose security configurations to gain elevated privileges, access sensitive data, or disrupt services.
    Ensuring a 'restricted' profile minimizes the attack surface by enforcing strict security controls such as disabling host
    namespaces and running containers as non-root.
  description: This rule verifies that all pods in the cluster adhere to the 'restricted' security profile, which enforces
    stringent security settings. It checks configurations like disallowing privileged containers, ensuring read-only root
    filesystems, and preventing usage of host namespaces. These controls help mitigate risks such as container escape, unauthorized
    data access, and privilege escalation, thereby enhancing compliance with security benchmarks and standards.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecurity
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-7_0047
  - fedramp_moderate_multi_cloud_CM-7_0119
  - fedramp_moderate_multi_cloud_CM-7_1_0120
  - fedramp_moderate_multi_cloud_CM-7_2_0121
  - fedramp_moderate_multi_cloud_CM-7_5_0122
  - hipaa_multi_cloud_164_308_a_4_ii_a_0009
  - hipaa_multi_cloud_164_308_a_5_ii_b_0012
  - nist_800_171_r2_multi_cloud_3_4_2_3.4.2_Establish_and_enforce_security_config_0038
  - nist_800_171_r2_multi_cloud_3_4_6_3.4.6_Employ_the_principle_of_least_functio_0039
  - pci_dss_v4_multi_cloud_6.2.3_0077
  - rbi_nbfc_multi_cloud_2.7_0011
  - rbi_nbfc_multi_cloud_5.3_0027
  - soc2_multi_cloud_cc_a_1_1_0021
- rule_id: k8s.pod.security.standard_review_scheduled
  service: pod
  resource: security
  requirement: Standard Review Scheduled
  scope: pod.security.standard_review_scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Schedule Regular Pod Security Reviews
  rationale: Without regular security reviews, pod configurations may become outdated, leading to increased risk of vulnerabilities
    and exposure to exploitation such as privilege escalation or unauthorized access. Attackers could exploit misconfigurations
    to gain control over cluster resources or access sensitive data.
  description: This control checks if a regular review process is in place for Kubernetes pod security configurations. A properly
    scheduled review ensures that pod security settings adhere to current best practices and standards, such as the CIS Kubernetes
    Benchmark. It helps identify and rectify misconfigurations that could lead to security breaches, thus enhancing the overall
    security posture of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  compliance: []
- rule_id: k8s.pod.security.standard_vulnerability_checks
  service: pod
  resource: security
  requirement: Standard Vulnerability Checks
  scope: pod.security.standard_vulnerability_checks
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Implement Comprehensive Pod Vulnerability Scanning
  rationale: Neglecting to perform comprehensive vulnerability scans on Kubernetes pods can lead to the exploitation of known
    vulnerabilities, enabling attackers to compromise containerized applications. This can result in unauthorized access,
    data breaches, and disruption of services by exploiting outdated or misconfigured software components within the pods.
  description: This control ensures that Kubernetes pods undergo comprehensive vulnerability scanning to identify and mitigate
    known security vulnerabilities. The scanning process should include assessing images for known vulnerabilities before
    deployment and continuously monitoring running pods for newly discovered vulnerabilities. A proper configuration involves
    integrating vulnerability scanning tools into the CI/CD pipeline and enabling runtime scanning to detect vulnerabilities
    in active pods. By implementing this control, organizations can proactively address security weaknesses, reduce the attack
    surface, and maintain compliance with industry security standards.
  references:
  - https://kubernetes.io/docs/concepts/containers/images/#image-scanning
  - https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_RA-5_0115
- rule_id: k8s.pod.securitycontext.capabilities_restricted_enforced
  service: pod
  resource: securitycontext
  requirement: Capabilities Restricted Enforced
  scope: pod.securitycontext.capabilities_restricted_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Minimal Capabilities in Pod SecurityContext
  rationale: Allowing excessive capabilities in a Pod's security context can lead to privilege escalation, where an attacker
    might exploit unnecessary permissions to gain control over the node or cluster. Restricting capabilities reduces the attack
    surface and mitigates risks such as unauthorized access to system calls and sensitive kernel functionality.
  description: This rule checks that the security context of a Kubernetes Pod specifies only the minimal set of capabilities
    necessary for the application to function. By default, Linux capabilities extend privileges to programs, which can be
    exploited if not properly restricted. A secure configuration involves removing all capabilities not explicitly required
    by the application. This helps prevent privilege escalation and limits potential damage from compromised containers.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#capabilities
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-capabilities
  compliance: []
- rule_id: k8s.pod.securitycontext.dangerous_capabilities_dropped_enforced
  service: pod
  resource: securitycontext
  requirement: Dangerous Capabilities Dropped Enforced
  scope: pod.securitycontext.dangerous_capabilities_dropped_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Pod Security by Dropping Dangerous Capabilities
  rationale: Allowing pods to retain dangerous capabilities increases the risk of privilege escalation and unauthorized access.
    Attackers can exploit these capabilities to gain control over the host system or other pods, potentially leading to data
    breaches or service disruption.
  description: This rule checks that the security context of Kubernetes pods is configured to drop dangerous Linux capabilities
    by default. Capabilities such as CAP_SYS_ADMIN, CAP_NET_ADMIN, and others are known to provide significant control over
    the host system if not properly restricted. Ensuring these are dropped helps mitigate risks associated with privilege
    escalation and enforces a principle of least privilege, enhancing the overall security posture of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#capabilities
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  compliance: []
- rule_id: k8s.pod.securitycontext.defined
  service: pod
  resource: securitycontext
  requirement: Defined
  scope: pod.securitycontext.defined
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pods Define SecurityContext for Enhanced Isolation
  rationale: A misconfigured or undefined SecurityContext in Kubernetes pods can lead to vulnerabilities such as privilege
    escalation, unauthorized access, and data breaches. Attackers may exploit these vulnerabilities to gain elevated privileges
    or execute malicious code within the cluster. Properly defining a SecurityContext helps mitigate risks by enforcing security
    policies around user privileges and resource access.
  description: This control checks that each Kubernetes pod has a SecurityContext defined. A well-configured SecurityContext
    ensures that key security settings such as runAsUser, runAsGroup, and capabilities are explicitly specified, reducing
    the risk of privilege escalation and unauthorized access. By enforcing these configurations, the cluster's security posture
    is strengthened, and compliance with standards like the CIS Kubernetes Benchmark is maintained. Specifically, the SecurityContext
    should disable privileged mode, set read-only root filesystem, and drop unnecessary Linux capabilities.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-b_0398
- rule_id: k8s.pod.securitycontext.memory_protection_enabled
  service: pod
  resource: securitycontext
  requirement: Memory Protection Enabled
  scope: pod.securitycontext.memory_protection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Memory Protection in Pod Security Context
  rationale: Enabling memory protection in a Kubernetes pod's security context is crucial to prevent unauthorized memory access
    and mitigate potential exploitation of vulnerabilities. Without proper memory protection, attackers could manipulate application
    memory, leading to data breaches or compromised applications. This control is particularly vital in environments where
    sensitive data is processed, ensuring the application adheres to security best practices and regulatory requirements.
  description: This rule checks if the security context of a Kubernetes pod is configured to enable memory protection mechanisms,
    such as read-only root filesystems or limiting access to shared memory segments. A secure configuration helps prevent
    unauthorized access to memory, reduces the attack surface, and aligns with security standards like the CIS Kubernetes
    Benchmark. Proper implementation restricts the actions that can be performed by processes within the container, enhancing
    the overall security of the workload.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance: []
- rule_id: k8s.pod.securitycontext.net_raw_capability_dropped_enforced
  service: pod
  resource: securitycontext
  requirement: Net Raw Capability Dropped Enforced
  scope: pod.securitycontext.net_raw_capability_dropped_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure NET_RAW Capability is Dropped in Pod SecurityContext
  rationale: Allowing the NET_RAW capability in pods can expose the cluster to potential security risks such as packet spoofing,
    man-in-the-middle attacks, or unauthorized network traffic capture. Dropping this capability minimizes the attack surface
    by restricting pods from crafting arbitrary network packets.
  description: This control checks that the NET_RAW capability is not present in the securityContext of Kubernetes pods. A
    correctly configured pod will have this capability explicitly dropped, which helps to prevent potential misuse of raw
    sockets. This configuration aligns with security best practices by reducing the privileges of the container process, thereby
    adhering to the principle of least privilege.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#capabilities
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  compliance: []
- rule_id: k8s.pod.securitycontext.non_root
  service: pod
  resource: securitycontext
  requirement: Non Root
  scope: pod.securitycontext.non_root
  domain: infrastructure_security
  subcategory: configuration_management
  severity: critical
  title: Enforce Non-Root User in Pod Security Context
  rationale: Running containers as root increases the risk of privilege escalation and lateral movement within the cluster.
    Attackers exploiting vulnerabilities in applications or container runtimes can gain root access, which could lead to full
    control over the node and potentially the entire Kubernetes cluster.
  description: This rule checks whether the 'runAsNonRoot' field is set to true in the pod's security context. The 'runAsNonRoot'
    setting ensures that containers are not started with root privileges, reducing the attack surface by limiting the permissions
    available to applications running inside the container. A well-configured pod security context can prevent privilege escalation
    attacks and conforms to best practices as outlined in the Kubernetes Pod Security Standards.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  compliance:
  - iso27001_2022_multi_cloud_A.12.6_0003
  - iso27001_2022_multi_cloud_A.8.7_0094
  - pci_dss_v4_multi_cloud_2.2.3_0022
  - soc2_multi_cloud_cc_6_8_0014
- rule_id: k8s.pod.securitycontext.privilege_escalation_disabled_enforced
  service: pod
  resource: securitycontext
  requirement: Privilege Escalation Disabled Enforced
  scope: pod.securitycontext.privilege_escalation_disabled_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce No Privilege Escalation in Pod SecurityContext
  rationale: Disabling privilege escalation in Kubernetes pods is critical to prevent attackers from gaining higher permissions
    from a compromised container. Without this control, an attacker could exploit vulnerabilities to escalate their privileges,
    potentially allowing them to execute arbitrary commands as the root user, access sensitive data, or disrupt operations
    within the cluster.
  description: This rule ensures that the 'allowPrivilegeEscalation' setting in a pod's SecurityContext is set to 'false'.
    This configuration is checked against all pods in the cluster to ensure they do not allow privilege escalation. A properly
    configured SecurityContext prevents containers from acquiring additional privileges, thereby reducing the attack surface
    and mitigating the risk of privilege escalation attacks within the cluster. Ensuring 'allowPrivilegeEscalation' is set
    to 'false' aligns with Kubernetes security best practices and contributes to a more secure cluster environment.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#admission-control-plugins
  compliance: []
- rule_id: k8s.pod.securitycontext.privileged_containers_prohibited_enforced
  service: pod
  resource: securitycontext
  requirement: Privileged Containers Prohibited Enforced
  scope: pod.securitycontext.privileged_containers_prohibited_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: critical
  title: Enforce Non-Privileged Pod Security Context
  rationale: Privileged containers pose significant security risks as they grant elevated access to the host system, potentially
    allowing attackers to bypass container isolation and compromise the underlying infrastructure. If misconfigured, attackers
    could exploit vulnerabilities to gain root access, execute arbitrary code, or conduct privilege escalation attacks, compromising
    the entire cluster.
  description: 'This control checks that all pods are configured with a security context that prohibits privileged containers.
    A good configuration involves setting `privileged: false` within the pod''s security context. This helps ensure that containers
    run with the least privilege necessary, reducing the attack surface and mitigating the risks of unauthorized host access
    or privilege escalation attacks. Proper enforcement of this control aligns with best practices and industry standards,
    enhancing the overall security posture of the Kubernetes environment.'
  references:
  - https://kubernetes.io/docs/concepts/workloads/pods/security-context/#set-the-security-context-for-a-pod
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance: []
- rule_id: k8s.pod.securitycontext.readonly_rootfs
  service: pod
  resource: securitycontext
  requirement: Readonly Rootfs
  scope: pod.securitycontext.readonly_rootfs
  domain: infrastructure_security
  subcategory: configuration_management
  severity: critical
  title: Enforce Read-Only Root Filesystem for Pods
  rationale: Configuring a read-only root filesystem is crucial to prevent unauthorized modifications to the file system of
    a running pod. If misconfigured, attackers who gain access to a container might exploit write permissions to the root
    file system to install malware, tamper with application binaries, or escalate privileges, leading to potential data breaches
    and system compromise.
  description: This rule checks that the 'readOnlyRootFilesystem' attribute in the pod's security context is set to 'true'.
    A properly configured read-only root filesystem ensures that the container's root file system is immutable, protecting
    against unauthorized changes and enhancing the pod's overall security by limiting what an attacker can do. It aligns with
    security best practices and industry standards to reduce attack surfaces and improve container integrity.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#securitycontext-v1-core
  compliance:
  - nist_800_171_r2_multi_cloud_3_14_2_3.14.2_Provide_protection_from_malicious_c_0015
- rule_id: k8s.pod.securitycontext.seccomp_profile_enforced_enabled
  service: pod
  resource: securitycontext
  requirement: Seccomp Profile Enforced Enabled
  scope: pod.securitycontext.seccomp_profile_enforced_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Seccomp Profile in Pod SecurityContext
  rationale: Without enforcing a Seccomp profile, pods may execute system calls that could be exploited by attackers to gain
    unauthorized access or escalate privileges. Seccomp acts as an additional layer of defense by restricting the system calls
    available to a container, reducing the attack surface and preventing the exploitation of kernel vulnerabilities.
  description: This control checks if a Seccomp profile is explicitly set to either 'runtime/default' or a custom profile
    in the pod's SecurityContext. A properly configured Seccomp profile limits the set of system calls that a container can
    make, thereby minimizing the potential for malicious activity. Enabling seccomp profiles helps adhere to security best
    practices and complies with the CIS Kubernetes Benchmark, reducing the risk of container escape and other forms of attacks.
  references:
  - https://kubernetes.io/docs/tutorials/security/seccomp/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#seccomp
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#baseline
  compliance: []
- rule_id: k8s.pod.securitycontext.verification
  service: pod
  resource: securitycontext
  requirement: Verification
  scope: pod.securitycontext.verification
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pod SecurityContext Limits Privileges
  rationale: Misconfigured Pod SecurityContexts can lead to elevated privileges, allowing attackers to exploit vulnerabilities,
    escalate privileges, or perform unauthorized actions. For example, allowing a pod to run as the root user can enable attackers
    to compromise the host node.
  description: This control checks that the SecurityContext of each Kubernetes pod is configured to restrict privileges. Specifically,
    it ensures settings such as 'runAsNonRoot', 'readOnlyRootFilesystem', and 'allowPrivilegeEscalation' are properly set
    to enforce least privilege principles. A properly configured SecurityContext helps mitigate risks of privilege escalation,
    reduces the attack surface, and ensures pods operate with only the permissions they need, thus enhancing the security
    posture of the cluster.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance: []
- rule_id: k8s.pod.service.account_automount_configured
  service: pod
  resource: service
  requirement: Account Automount Configured
  scope: pod.service.account_automount_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Default Service Account Token Automount in Pods
  rationale: If the default service account token is automatically mounted in pods, it can be exploited by an attacker who
    gains access to the pod. This could lead to privilege escalation, allowing the attacker to interact with the Kubernetes
    API server and potentially perform unauthorized actions within the cluster.
  description: 'This check ensures that Kubernetes pods are configured to not automatically mount the default service account
    token unless explicitly required. By setting `automountServiceAccountToken: false` in the pod specification, you prevent
    unnecessary exposure of credentials that could be used by an attacker to escalate privileges. This configuration reduces
    the attack surface by limiting access to the Kubernetes API and aligns with security best practices and the CIS Kubernetes
    Benchmark.'
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-service-account
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  compliance: []
- rule_id: k8s.pod.serviceaccount.automount_check
  service: pod
  resource: serviceaccount
  requirement: Automount Check
  scope: pod.serviceaccount.automount_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Automatic ServiceAccount Token Mounting in Pods
  rationale: Automatically mounting the default service account token in pods can lead to unauthorized access and privilege
    escalation if a pod is compromised. Attackers could use the token to interact with the Kubernetes API server and gain
    control over additional resources within the cluster. Disabling automatic token mounting minimizes this risk by ensuring
    that only pods which specifically require access to the API server are granted service account tokens.
  description: 'This check validates that pods do not automatically mount the default service account token by setting `automountServiceAccountToken:
    false` in their configuration. A compliant configuration restricts the exposure of service account tokens to only those
    pods that explicitly require them, thus reducing the attack surface and enhancing overall security posture. Proper configuration
    ensures compliance with security standards such as the CIS Kubernetes Benchmark, which advocates for the principle of
    least privilege.'
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#serviceaccount
  compliance:
  - cis_kubernetes_kubernetes_5.1.6_0105
- rule_id: k8s.pod.termination.grace_period_set
  service: pod
  resource: termination
  requirement: Grace Period Set
  scope: pod.termination.grace_period_set
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Pod Termination Grace Period is Configured
  rationale: A misconfigured pod termination grace period can lead to abrupt shutdowns, risking data loss and service disruptions.
    Attackers could exploit this to cause denial of service by overwhelming the system with ungraceful pod terminations.
  description: This control checks that all Kubernetes pods have a termination grace period explicitly set. A well-configured
    grace period allows pods to shut down gracefully, ensuring that any in-flight transactions are completed and resources
    are properly released, reducing the risk of data inconsistency and service unavailability. A grace period aligned with
    application needs also helps maintain compliance with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination
  - https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/
  - https://kubernetes.io/docs/concepts/configuration/overview/#container-properties
  compliance:
  - soc2_multi_cloud_cc_c_1_2_0023
- rule_id: k8s.pod_security.root.containers_restricted
  service: pod_security
  resource: root
  requirement: Containers Restricted
  scope: pod_security.root.containers_restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Privileged Container Permissions in Pods
  rationale: Allowing containers to run with privileged permissions can lead to severe security risks. Attackers with access
    to a privileged container can potentially gain control over the host system, escalate privileges, or exploit host resources,
    leading to unauthorized access and data breaches.
  description: This rule checks if pods are configured to prevent running containers with privileged permissions. A well-configured
    Kubernetes environment should ensure that the 'privileged' flag in container security contexts is set to 'false', thereby
    restricting containers from accessing host-level resources and kernel capabilities. This enhances security by minimizing
    the attack surface and adhering to the principle of least privilege.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - cis_kubernetes_kubernetes_5.2.7_0119
- rule_id: k8s.policy.capabilities.restriction_check
  service: policy
  resource: capabilities
  requirement: Restriction Check
  scope: policy.capabilities.restriction_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Pod Capabilities to Minimal Set
  rationale: Allowing excessive capabilities in containers can introduce security vulnerabilities, such as privilege escalation.
    Attackers may exploit granted capabilities to perform actions like modifying system files or network settings, which could
    compromise the entire cluster's security.
  description: This check ensures that the capabilities assigned to containers in your Kubernetes cluster are restricted to
    the minimal set necessary for application functionality. It validates that PodSecurityPolicies or Pod Security Standards
    are configured to drop all unnecessary Linux capabilities. Adhering to this practice reduces the attack surface by limiting
    what a compromised container can do, aligning with security best practices and compliance requirements like the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#capabilities
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container
  compliance:
  - cis_kubernetes_kubernetes_5.2.9_0121
- rule_id: k8s.policy.control.mechanisms_check
  service: policy
  resource: control
  requirement: Mechanisms Check
  scope: policy.control.mechanisms_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Network Policies are Enforced
  rationale: Without proper network policies, applications running in a Kubernetes cluster may be exposed to unauthorized
    access and lateral movement by malicious actors. This can lead to data breaches, service disruptions, and non-compliance
    with security standards. Network policies allow for fine-grained control over traffic flow between pods, reducing the
    attack surface by only allowing necessary communications.
  description: This control checks that network policies are defined and enforced within the Kubernetes cluster. Network policies
    are essential for segmenting network traffic and controlling ingress and egress at the pod level. A well-configured network
    policy ensures that only authorized traffic is permitted, thereby preventing unauthorized access and potential attacks.
    It validates that policies are in place and properly restrict traffic in line with security best practices and compliance
    requirements.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy
  compliance:
  - cis_kubernetes_kubernetes_5.2.1_0113
- rule_id: k8s.policy.namespace.resource_quotas_enforced
  service: policy
  resource: namespace
  requirement: Resource Quotas Enforced
  scope: policy.namespace.resource_quotas_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Resource Quotas on Namespaces
  rationale: Without enforced resource quotas, a single namespace can consume excessive resources, potentially leading to
    resource starvation for other workloads. This lack of control can be exploited by attackers to perform Denial of Service
    (DoS) attacks by saturating system resources, disrupting the availability of services, and impacting overall cluster stability.
  description: This rule checks whether resource quotas are applied to all namespaces within the Kubernetes cluster. A properly
    configured resource quota ensures that each namespace is limited in terms of CPU, memory, and other resources, preventing
    any single tenant from monopolizing cluster resources. This not only helps in maintaining fair resource allocation but
    also protects against accidental or malicious resource overconsumption, thereby maintaining cluster stability and availability.
  references:
  - https://kubernetes.io/docs/concepts/policy/resource-quotas/
  - https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  compliance: []
- rule_id: k8s.policy.net.raw_capability_restricted
  service: policy
  resource: net
  requirement: Raw Capability Restricted
  scope: policy.net.raw_capability_restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict NET_RAW Capability in Kubernetes Pods
  rationale: Allowing the NET_RAW capability in Kubernetes pods can expose the cluster to several security risks, such as
    packet sniffing and man-in-the-middle attacks. Attackers could exploit this capability to perform network reconnaissance
    or intercept network traffic, compromising the confidentiality and integrity of data. By restricting NET_RAW, you mitigate
    these risks and enhance the overall security posture of your cluster.
  description: This check ensures that the NET_RAW capability is not granted to containers running in Kubernetes pods unless
    explicitly required. A secure configuration involves using PodSecurityPolicies or SecurityContext to deny the NET_RAW
    capability by default. This reduces the attack surface by preventing unauthorized packet generation or manipulation, thus
    safeguarding against network-based attacks. Properly configured, this control helps maintain compliance with security
    benchmarks and best practices.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - cis_kubernetes_kubernetes_5.2.8_0120
- rule_id: k8s.policy.podsecuritypolicy.automated_compliance_reporting_enabled
  service: policy
  resource: podsecuritypolicy
  requirement: Automated Compliance Reporting Enabled
  scope: policy.podsecuritypolicy.automated_compliance_reporting_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Enable Automated Compliance Reporting for PodSecurityPolicy
  rationale: Without automated compliance reporting, deviations from security policies can go unnoticed, increasing the risk
    of vulnerabilities being exploited. Attackers may exploit misconfigurations to gain unauthorized access or escalate privileges
    within the Kubernetes cluster.
  description: This control checks if PodSecurityPolicy is configured with automated compliance reporting to ensure continuous
    monitoring of policy adherence. By validating this configuration, it helps identify and rectify security policy violations
    in real-time, thus reducing the attack surface. A well-configured compliance reporting system aids in maintaining alignment
    with security benchmarks like the CIS Kubernetes Benchmark and mitigates risks associated with unauthorized access and
    privilege escalation.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance: []
- rule_id: k8s.policy.podsecuritypolicy.cis_benchmark_compliance_enabled
  service: policy
  resource: podsecuritypolicy
  requirement: Cis Benchmark Compliance Enabled
  scope: policy.podsecuritypolicy.cis_benchmark_compliance_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure PodSecurityPolicies Adhere to CIS Benchmarks
  rationale: If PodSecurityPolicies (PSPs) are not configured according to CIS Benchmarks, it may lead to vulnerabilities
    such as privilege escalation, host filesystem access, or running of untrusted workloads. Attackers could exploit these
    misconfigurations to gain unauthorized access or execute arbitrary code within the cluster.
  description: This control checks that PodSecurityPolicies are configured to meet CIS Kubernetes Benchmark standards. It
    ensures that policies enforce strict security measures such as disabling privileged containers, restricting host network
    and host PID access, and setting appropriate user and group IDs. Proper configuration mitigates security risks by constraining
    what pods can do and what resources they can access, thus adhering to industry best practices and compliance requirements.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance: []
- rule_id: k8s.policy.podsecuritypolicy.compliance_reporting_automated
  service: policy
  resource: podsecuritypolicy
  requirement: Compliance Reporting Automated
  scope: policy.podsecuritypolicy.compliance_reporting_automated
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Automate Compliance Reporting for PodSecurityPolicies
  rationale: Without automated compliance reporting for PodSecurityPolicies, organizations face increased risks of undetected
    policy violations, which may lead to unauthorized access or privilege escalation attacks. Attackers can exploit improperly
    configured pods to gain access to sensitive resources or elevate their privileges within the cluster, undermining the
    security posture.
  description: This control checks for the presence of automated compliance reporting mechanisms for PodSecurityPolicies within
    the Kubernetes cluster. It ensures that any deviations from the defined security policies are logged and reported promptly.
    A well-configured automated reporting system alerts administrators to potential security violations, enabling rapid response
    and remediation. Compliance with this control helps maintain adherence to industry standards such as the CIS Kubernetes
    Benchmark, reducing the risk of security breaches.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  - https://kubernetes.io/docs/concepts/overview/components/#api-server
  compliance: []
- rule_id: k8s.policy.podsecuritypolicy.gitops_integration_enabled
  service: policy
  resource: podsecuritypolicy
  requirement: Gitops Integration Enabled
  scope: policy.podsecuritypolicy.gitops_integration_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure GitOps Integration for PodSecurityPolicy
  rationale: Without GitOps integration, configuration drift can occur, leading to potential security vulnerabilities such
    as unauthorized access or privilege escalation. Attackers could exploit these vulnerabilities to gain control over Kubernetes
    resources. GitOps provides a declarative approach to managing Kubernetes configurations, ensuring changes are tracked
    and auditable, reducing the risk of security misconfigurations.
  description: This control checks if GitOps integration is enabled for PodSecurityPolicies (PSPs) to ensure that any changes
    to security policies are version-controlled and auditable. A proper GitOps setup involves storing PSPs in a Git repository,
    where they are automatically applied and monitored for unauthorized changes. It ensures that security policies are consistently
    enforced across environments, thus enhancing security by preventing configuration drift and unauthorized modifications.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/configuration/overview/
  - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/
  compliance: []
- rule_id: k8s.policy.podsecuritypolicy.identity_verification_enabled
  service: policy
  resource: podsecuritypolicy
  requirement: Identity Verification Enabled
  scope: policy.podsecuritypolicy.identity_verification_enabled
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enable PodSecurityPolicy for Identity Verification
  rationale: Without identity verification, unauthorized entities can exploit access controls, potentially leading to privilege
    escalation attacks or unauthorized access to sensitive workloads. Proper identity verification mitigates risks associated
    with impersonation and unauthorized access, ensuring that only authenticated and authorized identities can interact with
    the Kubernetes API server.
  description: This rule checks that PodSecurityPolicies are configured to enforce identity verification, ensuring that only
    authenticated identities can create, modify, or delete pods. Proper configuration involves setting up policies that verify
    the identity of service accounts and users interacting with the cluster. This helps prevent unauthorized access and ensures
    compliance with security best practices by controlling which entities can execute specific actions within the cluster.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/policy/pod-security-standards/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance: []
- rule_id: k8s.policy.podsecuritypolicy.risk_assessment_integrated_enabled
  service: policy
  resource: podsecuritypolicy
  requirement: Risk Assessment Integrated Enabled
  scope: policy.podsecuritypolicy.risk_assessment_integrated_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable PodSecurityPolicy for Risk Assessment
  rationale: Without the integration of risk assessment in PodSecurityPolicy, clusters are vulnerable to misconfigurations
    that could lead to privilege escalation, data breaches, and unauthorized access. Attackers could exploit lax security
    policies to escalate privileges or compromise sensitive data.
  description: This control checks whether PodSecurityPolicy incorporates risk assessment mechanisms to evaluate and enforce
    security constraints on pods. A properly configured PodSecurityPolicy ensures that pods do not run with excessive privileges
    or access unnecessary resources, thus minimizing potential attack vectors. It validates that policies are set to restrict
    capabilities such as running as root, accessing host namespaces, or using host networking. This reduces the attack surface
    and aligns with security best practices and standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance: []
- rule_id: k8s.rbac.access.review_process
  service: rbac
  resource: access
  requirement: Review Process
  scope: rbac.access.review_process
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regular Review of RBAC Policies to Prevent Over-Privilege
  rationale: Without regular reviews of RBAC policies, there is a risk of over-privileged access which can lead to unauthorized
    actions within the cluster. Attackers could exploit excessive permissions to escalate privileges, access sensitive data,
    or disrupt services. Regular reviews help identify and rectify unnecessary or excessive permissions, mitigating potential
    security breaches.
  description: This control checks that a process is in place for the periodic review of Kubernetes RBAC roles and bindings.
    A well-structured review process involves evaluating current roles and permissions, ensuring they align with the principle
    of least privilege, and adjusting them as necessary. Effective reviews can help prevent privilege escalation attacks and
    ensure compliance with security best practices by identifying over-privileged accounts and reducing their access rights.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - soc2_multi_cloud_cc_6_2_0010
- rule_id: k8s.rbac.activity.monitoring_configured
  service: rbac
  resource: activity
  requirement: Monitoring Configured
  scope: rbac.activity.monitoring_configured
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Ensure RBAC Activity Monitoring is Enabled
  rationale: Without proper monitoring of RBAC activities, unauthorized access attempts or privilege escalations may go undetected.
    Attackers could exploit misconfigurations to gain elevated permissions, compromising cluster integrity and confidentiality.
    Effective monitoring provides visibility into access patterns and potential security incidents, allowing timely response
    to threats.
  description: This control checks that monitoring for RBAC (Role-Based Access Control) activities is configured according
    to best practices. It ensures that all access and permission changes within the Kubernetes cluster are logged and monitored.
    This includes tracking user and service account actions to detect anomalies such as unauthorized access attempts or privilege
    escalations. Properly configured monitoring helps in maintaining compliance with security standards and provides an audit
    trail for incident investigation.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-43-b_1322
- rule_id: k8s.rbac.change.control_role_defined
  service: rbac
  resource: change
  requirement: Control Role Defined
  scope: rbac.change.control_role_defined
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Ensure RBAC Roles are Defined for Change Management
  rationale: Improperly configured or undefined RBAC roles can lead to unauthorized access, allowing malicious actors to modify
    critical resources within the Kubernetes cluster. This could result in privilege escalation or unauthorized data access,
    compromising the integrity and confidentiality of the system.
  description: This control checks that all Kubernetes Role-Based Access Control (RBAC) roles involved in change management
    are explicitly defined and configured according to security best practices. A well-structured RBAC policy ensures that
    only authorized users or service accounts can perform actions that modify critical resources, reducing the risk of unauthorized
    changes. Properly managing RBAC roles is essential to enforce the principle of least privilege and maintain a secure Kubernetes
    environment.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#role-based-access-control-rbac
  - https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-3_0043
- rule_id: k8s.rbac.change.management_policy_configured
  service: rbac
  resource: change
  requirement: Management Policy Configured
  scope: rbac.change.management_policy_configured
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce RBAC Change Management Policies
  rationale: Without proper management policies for RBAC changes, unauthorized users could escalate privileges or gain access
    to sensitive resources, leading to potential data breaches or system compromises. Attackers could exploit misconfigured
    RBAC to perform lateral movements within the cluster or execute unauthorized commands.
  description: This control checks that RBAC change management policies are in place and properly configured to ensure that
    any modifications to RBAC permissions are logged, reviewed, and authorized. A well-configured management policy should
    enforce strict access controls, require multi-factor authentication for sensitive changes, and integrate with audit logging
    systems. This approach minimizes the risk of privilege escalation and unauthorized access, thereby maintaining the integrity
    and security of Kubernetes clusters.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/concepts/cluster-administration/audit/
  compliance:
  - soc2_multi_cloud_cc_8_1_0020
- rule_id: k8s.rbac.change.management_roles_defined
  service: rbac
  resource: change
  requirement: Management Roles Defined
  scope: rbac.change.management_roles_defined
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Define and Enforce Management Roles in Kubernetes RBAC
  rationale: Failing to define and enforce management roles in Kubernetes RBAC can lead to unauthorized access and privilege
    escalation. Attackers could exploit misconfigured roles to gain administrative control, access sensitive data, or disrupt
    services. Proper role definitions mitigate these risks by ensuring that only authorized users have the necessary permissions
    to perform management tasks.
  description: This control verifies that management roles in Kubernetes RBAC are explicitly defined and properly configured
    according to best practices. It checks that roles have the minimum necessary permissions and that they are assigned to
    appropriate users or service accounts. A well-configured management role setup helps prevent unauthorized access, limits
    the potential impact of compromised accounts, and ensures compliance with security standards. Proper configuration involves
    using the principle of least privilege, regularly reviewing role bindings, and employing namespace-specific roles where
    applicable.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#roles-and-clusterroles
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - pci_dss_v4_multi_cloud_2.2.1_0020
  - rbi_bank_multi_cloud_13.1_0006
- rule_id: k8s.rbac.cluster.admin_role_access_review
  service: rbac
  resource: cluster
  requirement: Admin Role Access Review
  scope: rbac.cluster.admin_role_access_review
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Review and Limit Cluster Admin Role Bindings
  rationale: Misconfigured admin role bindings can lead to unauthorized personnel gaining full control over the cluster, potentially
    leading to privilege escalation, data breaches, or service disruptions. Attackers could exploit excessive permissions
    to alter configurations, deploy malicious workloads, or exfiltrate sensitive data.
  description: This control examines the role bindings within the RBAC configuration to ensure that the 'cluster-admin' role
    is only assigned to trusted and necessary users or service accounts. Proper validation involves checking that role bindings
    granting 'cluster-admin' privileges are limited and justified. By restricting this powerful role's access, the risk of
    privilege escalation and unauthorized actions within the cluster is minimized.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#clusterrolebinding
  - https://kubernetes.io/docs/concepts/security/overview/#rbac-authorization
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-access-to-cluster-admin-role
  compliance:
  - cis_kubernetes_kubernetes_1.6.1_0170
- rule_id: k8s.rbac.cluster.admin_role_usage_check
  service: rbac
  resource: cluster
  requirement: Admin Role Usage Check
  scope: rbac.cluster.admin_role_usage_check
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Restrict Cluster Admin Role Usage
  rationale: If the Cluster Admin role is misconfigured or overused, it can lead to unauthorized access and privilege escalation,
    allowing attackers to compromise the entire cluster. Limiting its usage reduces the attack surface and mitigates risks
    associated with excessive permissions.
  description: This control checks for the excessive assignment of the Cluster Admin role across the cluster. A proper configuration
    should ensure that only a minimal number of trusted entities have this role, and their access is regularly audited. This
    helps enforce the principle of least privilege, curbing potential lateral movement by attackers who gain access to compromised
    credentials.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings
  - https://kubernetes.io/docs/concepts/security/overview/#privilege-escalation
  - https://kubernetes.io/docs/tasks/administer-cluster/restrict-access-to-resources/
  compliance:
  - cis_kubernetes_kubernetes_5.1.1_0100
- rule_id: k8s.rbac.clusterrolebinding.admin_bindings_audited_enforced
  service: rbac
  resource: clusterrolebinding
  requirement: Admin Bindings Audited Enforced
  scope: rbac.clusterrolebinding.admin_bindings_audited_enforced
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Audit Admin ClusterRoleBindings for Unauthorized Access
  rationale: If admin-level ClusterRoleBindings are not properly audited, it increases the risk of privilege escalation and
    unauthorized access to critical resources within the Kubernetes cluster. Attackers could exploit these misconfigurations
    to gain elevated permissions, leading to potential data breaches or disruption of services.
  description: This control checks that all ClusterRoleBindings with admin privileges are logged and monitored for unauthorized
    changes. A properly configured audit log captures all access and modification events related to admin ClusterRoleBindings,
    allowing security teams to detect suspicious activities and respond promptly. Ensuring that audit logging is enforced
    helps maintain accountability and traceability of administrative actions, thus enhancing the overall security posture
    of the cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/security/overview/#security-audit
  compliance: []
- rule_id: k8s.rbac.configuration.change_audit_enabled
  service: rbac
  resource: configuration
  requirement: Change Audit Enabled
  scope: rbac.configuration.change_audit_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enable and Audit RBAC Configuration Changes
  rationale: Without auditing enabled for RBAC configuration changes, unauthorized modifications can go undetected, potentially
    leading to privilege escalation and unauthorized access to sensitive resources. Attackers could exploit this blind spot
    to modify permissions or roles, compromising the cluster's security posture.
  description: This rule verifies that auditing is enabled for changes to RBAC configurations in Kubernetes. A properly configured
    audit trail is essential for tracking changes to roles, role bindings, and other RBAC resources. This ensures that any
    unauthorized or suspicious modifications are logged and can be reviewed, helping to maintain compliance with security
    standards and detect potential security breaches.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/config-api/apiserver-audit.v1/
  compliance:
  - fedramp_moderate_multi_cloud_CM-3_0105
  - fedramp_moderate_multi_cloud_CM-3_1_0106
  - fedramp_moderate_multi_cloud_CM-3_2_0107
  - fedramp_moderate_multi_cloud_CM-3_4_0108
  - fedramp_moderate_multi_cloud_CM-3_6_0109
- rule_id: k8s.rbac.configuration.change_monitoring
  service: rbac
  resource: configuration
  requirement: Change Monitoring
  scope: rbac.configuration.change_monitoring
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Monitor RBAC Configuration Changes for Unauthorized Access
  rationale: If RBAC configuration changes are not monitored, attackers could alter permissions to gain unauthorized access
    to Kubernetes resources. This lack of oversight can lead to privilege escalation and potential data breaches, as adversaries
    could modify roles and bindings without detection.
  description: This control checks that any changes to RBAC configurations, such as roles, role bindings, cluster roles, and
    cluster role bindings, are actively monitored and logged. A well-configured monitoring system alerts administrators to
    unauthorized or suspicious changes, helping to quickly identify and mitigate potential security threats. Proper monitoring
    ensures that only authorized changes are made, maintaining the integrity of access controls and compliance with security
    policies.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/administer-cluster/audit/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-6-d_0400
- rule_id: k8s.rbac.csr.approval_access_control_check
  service: rbac
  resource: csr
  requirement: Approval Access Control Check
  scope: rbac.csr.approval_access_control_check
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce CSR Approval Access Control via RBAC
  rationale: Improper configuration of RBAC for CSR approval can lead to unauthorized certificate signing, allowing attackers
    to establish trust within the cluster. This could enable man-in-the-middle attacks, unauthorized access to cluster resources,
    and potential escalation of privileges.
  description: This control checks that RBAC policies are correctly configured to restrict CSR approval to authorized personnel
    or systems only. A secure configuration ensures that certificate requests are reviewed and approved based on an established
    access policy, minimizing the risk of unauthorized certificate issuance. Properly configured RBAC for CSR approval aids
    in maintaining a secure and compliant Kubernetes environment by enforcing controlled access to certificate signing processes.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_5.1.11_0110
- rule_id: k8s.rbac.database.access_restricted
  service: rbac
  resource: database
  requirement: Access Restricted
  scope: rbac.database.access_restricted
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce RBAC Policies for Database Access
  rationale: Without strict RBAC policies, unauthorized users or services could gain access to sensitive database resources,
    potentially leading to data breaches or unauthorized data manipulation. Attack vectors include privilege escalation and
    lateral movement within the cluster, which can compromise sensitive information and disrupt services.
  description: This rule checks that Kubernetes Role-Based Access Control (RBAC) is configured to strictly limit access to
    database resources only to authorized roles and service accounts. A well-configured RBAC policy ensures that only users
    with a legitimate need can access or modify database resources, effectively reducing the attack surface and preventing
    unauthorized access in compliance with security best practices and regulations.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#rbac
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - rbi_bank_multi_cloud_8.2_0025
- rule_id: k8s.rbac.default.service_account_audit
  service: rbac
  resource: default
  requirement: Service Account Audit
  scope: rbac.default.service_account_audit
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Audit Default Service Account Usage
  rationale: If the default service account is not audited, it could lead to unauthorized access and privilege escalation
    within the cluster. Attackers can exploit the default service account to perform lateral movements or escalate privileges,
    thereby compromising cluster security.
  description: This control checks that audit logging is enabled for the use of the default service account in Kubernetes.
    A well-configured audit log tracks access to resources and helps detect unauthorized access attempts. Ensuring that the
    default service account is audited helps maintain visibility over actions performed within the cluster, aiding in the
    early detection of potential security breaches.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  compliance:
  - cis_kubernetes_kubernetes_5.1.5_0104
- rule_id: k8s.rbac.impersonate.bind_escalate_permissions_check
  service: rbac
  resource: impersonate
  requirement: Bind Escalate Permissions Check
  scope: rbac.impersonate.bind_escalate_permissions_check
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Prevent Unauthorized Role Escalation via Impersonation in RBAC
  rationale: If impersonation permissions are not properly restricted, a malicious user could escalate their privileges by
    impersonating a higher-privileged user or service account. This could lead to unauthorized access to sensitive resources,
    data breaches, or system disruptions. By ensuring that impersonation bindings are secure, we mitigate the risk of privilege
    escalation attacks.
  description: This control checks that Kubernetes Role-Based Access Control (RBAC) configurations do not allow users to bind
    or escalate their permissions through impersonation without proper authorization. A secure setup should ensure that only
    trusted entities have the ability to impersonate other users or service accounts, thereby preventing unauthorized privilege
    escalation. This is achieved by auditing RBAC rules to confirm that 'impersonate' verbs are not granted to users who do
    not need them, and ensuring that all such permissions are carefully reviewed and justified.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-binding-and-cluster-role-binding
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#referring-to-subjects
  compliance:
  - cis_kubernetes_kubernetes_5.1.8_0107
- rule_id: k8s.rbac.inactive.service_account_disabled
  service: rbac
  resource: inactive
  requirement: Service Account Disabled
  scope: rbac.inactive.service_account_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Unused Service Accounts
  rationale: Inactive service accounts in Kubernetes pose a security risk as they can be exploited by attackers to gain unauthorized
    access to resources. These accounts may retain permissions that are no longer necessary, providing potential attack vectors
    for privilege escalation and lateral movement within the cluster.
  description: This rule checks for service accounts that have not been used for a specified period and ensures they are disabled.
    A well-configured system will identify inactive service accounts and disable or delete them to reduce the attack surface.
    By doing so, it minimizes the risk of unauthorized access and aligns with security best practices, thereby enhancing overall
    cluster security.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance:
  - nist_800_171_r2_multi_cloud_3_5_6_3.5.6_Disable_identifiers_after_a_defined_p_0046
- rule_id: k8s.rbac.inactive.user_cleanup
  service: rbac
  resource: inactive
  requirement: User Cleanup
  scope: rbac.inactive.user_cleanup
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regularly Remove Inactive Kubernetes RBAC Users
  rationale: Inactive RBAC users can become a security liability if they are not removed, as their credentials may be exploited
    by malicious actors. Attackers could potentially gain unauthorized access and perform actions within the cluster if old
    accounts are compromised, leading to data breaches or disruption of services.
  description: This control checks for the presence of inactive users in the Kubernetes RBAC system and ensures they are removed
    according to best practices. An inactive user is defined as an account that has not been used for a specified period.
    Regular cleanup of such users reduces the attack surface, minimizes the risk of unauthorized access, and helps maintain
    compliance with security standards. A good configuration involves setting policies for automatic deactivation and removal
    of unused accounts after a certain period of inactivity.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/overview/#rbac-authorization
  compliance:
  - pci_dss_v4_multi_cloud_8.2.6_0104
- rule_id: k8s.rbac.incident.response_team_access_configured
  service: rbac
  resource: incident
  requirement: Response Team Access Configured
  scope: rbac.incident.response_team_access_configured
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Ensure RBAC Permissions for Incident Response Team
  rationale: Without proper RBAC permissions, the incident response team may be hindered in accessing necessary resources
    during a security incident, leading to delayed remediation and potential escalation of threats. Misconfigurations could
    allow unauthorized users access or deny authorized users, increasing the risk of data breaches or extended downtime.
  description: This control verifies that the Role-Based Access Control (RBAC) settings in Kubernetes grant the incident response
    team appropriate access to resources necessary for incident handling. A good configuration involves defining roles and
    bindings that specifically allow the incident response team to view and manage incidents without granting excessive permissions.
    This ensures that the team can respond promptly while maintaining the principle of least privilege, reducing the attack
    surface and minimizing the risk of privilege escalation.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/
  compliance:
  - soc2_multi_cloud_cc_7_5_0019
- rule_id: k8s.rbac.least.privilege_enforcement
  service: rbac
  resource: least
  requirement: Privilege Enforcement
  scope: rbac.least.privilege_enforcement
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce RBAC Least Privilege Principle
  rationale: Misconfigured RBAC roles can lead to excessive privileges, allowing attackers to escalate privileges, access
    sensitive data, or alter system configurations. This control minimizes the attack surface by ensuring users and services
    only have the permissions necessary for their tasks, reducing the risk of unauthorized access and potential lateral movement
    within the cluster.
  description: This rule verifies that Kubernetes Role-Based Access Control (RBAC) configurations adhere to the principle
    of least privilege. It checks that roles and role bindings are not overly permissive and are restricted to necessary actions
    only. A good configuration involves defining roles with the minimum permissions required and regularly reviewing them
    to prevent privilege creep. Ensuring this reduces the risk of privilege escalation, data breaches, and compliance violations.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#rbac-authorization
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-rbac/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-5_0005
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-6_0006
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-2_0042
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-6_0046
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-7_0047
  - canada_pbmm_moderate_multi_cloud_CCCS_IA-4_0064
  - fedramp_moderate_multi_cloud_AC-5_0017
  - fedramp_moderate_multi_cloud_AC-6_0018
  - fedramp_moderate_multi_cloud_AC-6_1_0019
  - fedramp_moderate_multi_cloud_AC-6_2_0020
  - fedramp_moderate_multi_cloud_AC-6_3_0021
  - fedramp_moderate_multi_cloud_AC-6_5_0022
  - fedramp_moderate_multi_cloud_AC-6_7_0023
  - fedramp_moderate_multi_cloud_AC-6_8_0024
  - fedramp_moderate_multi_cloud_AC-6_9_0025
  - fedramp_moderate_multi_cloud_AC-6_10_0026
  - fedramp_moderate_multi_cloud_CM-2_0101
  - fedramp_moderate_multi_cloud_CM-2_2_0102
  - fedramp_moderate_multi_cloud_CM-2_3_0103
  - fedramp_moderate_multi_cloud_CM-2_7_0104
  - fedramp_moderate_multi_cloud_CM-6_0116
  - fedramp_moderate_multi_cloud_CM-6_1_0117
  - fedramp_moderate_multi_cloud_CM-6_2_0118
  - fedramp_moderate_multi_cloud_CM-7_0119
  - fedramp_moderate_multi_cloud_CM-7_1_0120
  - fedramp_moderate_multi_cloud_CM-7_2_0121
  - fedramp_moderate_multi_cloud_CM-7_5_0122
  - fedramp_moderate_multi_cloud_CM-9_0128
  - fedramp_moderate_multi_cloud_IA-4_0178
  - fedramp_moderate_multi_cloud_IA-4_4_0179
  - gdpr_multi_cloud_Article_25_Data_protection_by_design_and_by_defaul_0001
  - hipaa_multi_cloud_164_308_a_1_ii_b_0002
  - hipaa_multi_cloud_164_308_a_3_i_0004
  - hipaa_multi_cloud_164_308_a_3_ii_a_0005
  - hipaa_multi_cloud_164_308_a_3_ii_b_0006
  - hipaa_multi_cloud_164_308_a_3_ii_c_0007
  - hipaa_multi_cloud_164_308_a_4_i_0008
  - hipaa_multi_cloud_164_308_a_4_ii_b_0010
  - hipaa_multi_cloud_164_308_a_4_ii_c_0011
  - hipaa_multi_cloud_164_308_a_8_0021
  - hipaa_multi_cloud_164_312_a_1_0022
  - iso27001_2022_multi_cloud_A.5.15_0011
  - iso27001_2022_multi_cloud_A.5.18_0014
  - iso27001_2022_multi_cloud_A.5.19_0015
  - iso27001_2022_multi_cloud_A.5.2_0016
  - iso27001_2022_multi_cloud_A.5.20_0017
  - iso27001_2022_multi_cloud_A.5.21_0018
  - iso27001_2022_multi_cloud_A.5.23_0020
  - iso27001_2022_multi_cloud_A.5.24_0021
  - iso27001_2022_multi_cloud_A.5.25_0022
  - iso27001_2022_multi_cloud_A.5.26_0023
  - iso27001_2022_multi_cloud_A.5.27_0024
  - iso27001_2022_multi_cloud_A.5.28_0025
  - iso27001_2022_multi_cloud_A.5.29_0026
  - iso27001_2022_multi_cloud_A.8.22_0078
  - iso27001_2022_multi_cloud_A.8.23_0079
  - iso27001_2022_multi_cloud_A.8.25_0081
  - iso27001_2022_multi_cloud_A.8.26_0082
  - iso27001_2022_multi_cloud_A.8.27_0083
  - iso27001_2022_multi_cloud_A.8.29_0084
  - iso27001_2022_multi_cloud_A.8.3_0085
  - iso27001_2022_multi_cloud_A.8.30_0086
  - iso27001_2022_multi_cloud_A.8.31_0087
  - iso27001_2022_multi_cloud_A.8.32_0088
  - iso27001_2022_multi_cloud_A.8.33_0089
  - iso27001_2022_multi_cloud_A.8.34_0090
  - iso27001_2022_multi_cloud_A.8.9_0096
  - iso27001_2022_multi_cloud_A.9.2_0097
  - iso27001_2022_multi_cloud_A.9.3_0098
  - iso27001_2022_multi_cloud_A.9.4_0099
  - nist_800_171_r2_multi_cloud_3_12_4_3.12.4_Develop_document_and_periodically_0003
  - nist_800_171_r2_multi_cloud_3_13_1_3.13.1_Monitor_control_and_protect_commu_0004
  - nist_800_171_r2_multi_cloud_3_13_11_3.13.11_Employ_FIPS-validated_cryptograph_0005
  - nist_800_171_r2_multi_cloud_3_13_16_3.13.16_Protect_the_confidentiality_of_CU_0007
  - nist_800_171_r2_multi_cloud_3_13_2_3.13.2_Employ_architectural_designs_softw_0008
  - nist_800_171_r2_multi_cloud_3_13_3_3.13.3_Separate_user_functionality_from_sy_0009
  - nist_800_171_r2_multi_cloud_3_13_4_3.13.4_Prevent_unauthorized_and_unintended_0010
  - nist_800_171_r2_multi_cloud_3_13_5_3.13.5_Implement_subnetworks_for_publicly_0011
  - nist_800_171_r2_multi_cloud_3_13_6_3.13.6_Deny_network_communications_traffic_0012
  - nist_800_171_r2_multi_cloud_3_13_8_3.13.8_Implement_cryptographic_mechanisms_0013
  - nist_800_171_r2_multi_cloud_3_14_1_3.14.1_Identify_report_and_correct_syste_0014
  - nist_800_171_r2_multi_cloud_3_14_6_3.14.6_Monitor_organizational_systems_inc_0018
  - nist_800_171_r2_multi_cloud_3_14_7_3.14.7_Identify_unauthorized_use_of_organi_0019
  - nist_800_171_r2_multi_cloud_3_1_2_3.1.2_Limit_system_access_to_the_types_of_t_0024
  - nist_800_171_r2_multi_cloud_3_1_20_3.1.20_Verify_and_control_limit_connection_0025
  - nist_800_171_r2_multi_cloud_3_1_3_3.1.3_Control_the_flow_of_CUI_in_accordance_0026
  - nist_800_171_r2_multi_cloud_3_1_5_3.1.5_Employ_the_principle_of_least_privile_0028
  - nist_800_171_r2_multi_cloud_3_1_6_3.1.6_Use_non-privileged_accounts_or_roles_0029
  - nist_800_171_r2_multi_cloud_3_1_7_3.1.7_Prevent_non-privileged_users_from_exe_0030
  - nist_800_171_r2_multi_cloud_3_3_1_3.3.1_Create_and_retain_system_audit_logs_a_0031
  - nist_800_171_r2_multi_cloud_3_4_2_3.4.2_Establish_and_enforce_security_config_0038
  - nist_800_171_r2_multi_cloud_3_4_6_3.4.6_Employ_the_principle_of_least_functio_0039
  - nist_800_53_rev5_multi_cloud_AC-5-b_0090
  - nist_800_53_rev5_multi_cloud_CA-2-d_0303
  - nist_800_53_rev5_multi_cloud_CM-2-a_0366
  - nist_800_53_rev5_multi_cloud_CM-9-b_0428
  - pci_dss_v4_multi_cloud_2.1.1_0018
  - pci_dss_v4_multi_cloud_2.2.3_0022
  - pci_dss_v4_multi_cloud_7.1.1_0089
  - pci_dss_v4_multi_cloud_7.1.2_0090
  - pci_dss_v4_multi_cloud_8.1.1_0099
  - pci_dss_v4_multi_cloud_8.2.7_0105
  - rbi_bank_multi_cloud_3.2_0015
  - rbi_nbfc_multi_cloud_2.1_0005
  - rbi_nbfc_multi_cloud_2.5_0009
  - rbi_nbfc_multi_cloud_2.10_0014
  - rbi_nbfc_multi_cloud_5.1_0025
  - soc2_multi_cloud_cc_1_3_0001
  - soc2_multi_cloud_cc_2_1_0002
  - soc2_multi_cloud_cc_3_1_0003
  - soc2_multi_cloud_cc_3_2_0004
  - soc2_multi_cloud_cc_3_3_0005
  - soc2_multi_cloud_cc_5_2_0008
  - soc2_multi_cloud_cc_6_3_0011
- rule_id: k8s.rbac.least.privilege_for_clearinghouse
  service: rbac
  resource: least
  requirement: Privilege For Clearinghouse
  scope: rbac.least.privilege_for_clearinghouse
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce RBAC Least Privilege for Data Clearinghouse
  rationale: Misconfigured RBAC permissions can lead to excessive access rights, increasing the risk of unauthorized data
    access and potential data breaches. Attackers can exploit overly permissive roles to escalate privileges or exfiltrate
    sensitive information from the clearinghouse, jeopardizing compliance with regulations like HIPAA.
  description: This control verifies that Role-Based Access Control (RBAC) is configured to grant the minimum required privileges
    necessary for data clearinghouse operations. It ensures that roles and bindings are strictly defined to prevent over-permissioning.
    By limiting access, it minimizes the attack surface and helps maintain compliance with security standards, thereby protecting
    sensitive data from unauthorized access.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#role-based-access-control-rbac
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - hipaa_multi_cloud_164_308_a_4_ii_a_0009
- rule_id: k8s.rbac.log.access_restricted
  service: rbac
  resource: log
  requirement: Access Restricted
  scope: rbac.log.access_restricted
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict Access to Kubernetes Audit Logs via RBAC
  rationale: If unauthorized users gain access to Kubernetes audit logs, they can gain insights into the cluster's operations
    and exploit this information to target potential vulnerabilities. Misconfiguration of RBAC policies can lead to privilege
    escalation or unauthorized data exposure, making it crucial to restrict log access to only those who require it for their
    roles.
  description: This rule checks that RBAC policies are configured to restrict access to Kubernetes audit logs. A secure configuration
    ensures that only authorized users and service accounts can view or modify audit logs. This practice helps prevent unauthorized
    access and tampering, maintaining the integrity and confidentiality of sensitive audit information. Proper RBAC configuration
    mitigates risks such as privilege escalation and unauthorized access, aligning with security benchmarks and compliance
    requirements. Specific configurations should deny 'get', 'watch', and 'list' permissions on audit logs to users who do
    not need them.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - nist_800_171_r2_multi_cloud_3_3_8_3.3.8_Protect_audit_information_and_audit_l_0036
- rule_id: k8s.rbac.minimal.privileges_assigned
  service: rbac
  resource: minimal
  requirement: Privileges Assigned
  scope: rbac.minimal.privileges_assigned
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce Least Privilege in Kubernetes RBAC Policies
  rationale: Misconfigured RBAC policies can grant excessive permissions, leading to unauthorized access and potential exploitation.
    Attackers may exploit overly permissive roles to escalate privileges, execute arbitrary commands, or access sensitive
    data, compromising cluster security and compliance with standards like the CIS Kubernetes Benchmark.
  description: This rule checks that RBAC policies adhere to the principle of least privilege by ensuring that roles and bindings
    grant only the necessary permissions required for users and service accounts to perform their functions. A good configuration
    involves defining roles with minimal scope and associating them with specific users or service accounts. This reduces
    the attack surface, limits potential damage from compromised credentials, and helps maintain compliance with security
    frameworks.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - fedramp_moderate_multi_cloud_AC-6_0018
  - fedramp_moderate_multi_cloud_AC-6_1_0019
  - fedramp_moderate_multi_cloud_AC-6_2_0020
  - fedramp_moderate_multi_cloud_AC-6_3_0021
  - fedramp_moderate_multi_cloud_AC-6_5_0022
  - fedramp_moderate_multi_cloud_AC-6_7_0023
  - fedramp_moderate_multi_cloud_AC-6_8_0024
  - fedramp_moderate_multi_cloud_AC-6_9_0025
  - fedramp_moderate_multi_cloud_AC-6_10_0026
- rule_id: k8s.rbac.no.anonymous_access
  service: rbac
  resource: 'no'
  requirement: Anonymous Access
  scope: rbac.no.anonymous_access
  domain: identity_and_access_management
  subcategory: access_control
  severity: critical
  title: Disable Anonymous Access in Kubernetes RBAC
  rationale: Allowing anonymous access in Kubernetes can lead to unauthorized users gaining access to cluster resources. This
    can be exploited by attackers to escalate privileges, exfiltrate data, or disrupt services. Disabling anonymous access
    is crucial to prevent unauthorized access and potential security breaches.
  description: This control checks if Kubernetes Role-Based Access Control (RBAC) is configured to block anonymous access.
    A secure configuration should ensure that no permissions are granted to the 'system:anonymous' user. By disabling anonymous
    access, you reduce the attack surface and enforce authentication, ensuring that only authenticated users can interact
    with the cluster. This helps prevent unauthorized access and aligns with security best practices.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#anonymous-requests
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - nist_800_53_rev5_multi_cloud_IA-4-b_0543
- rule_id: k8s.rbac.no.cluster_admin_binding
  service: rbac
  resource: 'no'
  requirement: Cluster Admin Binding
  scope: rbac.no.cluster_admin_binding
  domain: infrastructure_security
  subcategory: configuration_management
  severity: critical
  title: Prohibit ClusterRoleBinding to Cluster Admin Role
  rationale: Binding users or service accounts to the cluster-admin role grants unrestricted access to all resources within
    the cluster, posing a significant security risk. Attackers gaining access to such credentials can perform unauthorized
    actions, leading to potential data breaches, resource manipulation, or infrastructure compromise.
  description: This rule checks for any ClusterRoleBindings that assign the cluster-admin role, which provides full control
    over all resources in the cluster. A secure configuration entails minimizing such bindings to the absolute minimum number
    of users or service accounts necessary for administrative operations. This reduces the attack surface by ensuring that
    only trusted and essential personnel have elevated privileges, thereby enhancing overall security posture.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding
  - https://kubernetes.io/docs/concepts/security/overview/#role-based-access-control-rbac
  - https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/#imperative-management-of-rbac
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-5_0005
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-6_0006
  - fedramp_moderate_multi_cloud_AC-5_0017
  - fedramp_moderate_multi_cloud_AC-6_0018
  - fedramp_moderate_multi_cloud_AC-6_1_0019
  - fedramp_moderate_multi_cloud_AC-6_2_0020
  - fedramp_moderate_multi_cloud_AC-6_3_0021
  - fedramp_moderate_multi_cloud_AC-6_5_0022
  - fedramp_moderate_multi_cloud_AC-6_7_0023
  - fedramp_moderate_multi_cloud_AC-6_8_0024
  - fedramp_moderate_multi_cloud_AC-6_9_0025
  - fedramp_moderate_multi_cloud_AC-6_10_0026
  - fedramp_moderate_multi_cloud_IA-4_0178
  - fedramp_moderate_multi_cloud_IA-4_4_0179
  - hipaa_multi_cloud_164_308_a_3_i_0004
  - hipaa_multi_cloud_164_308_a_3_ii_a_0005
  - hipaa_multi_cloud_164_308_a_3_ii_b_0006
  - hipaa_multi_cloud_164_308_a_3_ii_c_0007
  - hipaa_multi_cloud_164_308_a_4_i_0008
  - hipaa_multi_cloud_164_308_a_4_ii_b_0010
  - hipaa_multi_cloud_164_308_a_4_ii_c_0011
  - hipaa_multi_cloud_164_312_a_1_0022
  - iso27001_2022_multi_cloud_A.12.6_0003
  - iso27001_2022_multi_cloud_A.5.15_0011
  - iso27001_2022_multi_cloud_A.5.18_0014
  - iso27001_2022_multi_cloud_A.5.2_0016
  - iso27001_2022_multi_cloud_A.5.20_0017
  - iso27001_2022_multi_cloud_A.5.21_0018
  - iso27001_2022_multi_cloud_A.5.23_0020
  - iso27001_2022_multi_cloud_A.5.24_0021
  - iso27001_2022_multi_cloud_A.5.25_0022
  - iso27001_2022_multi_cloud_A.5.26_0023
  - iso27001_2022_multi_cloud_A.5.27_0024
  - iso27001_2022_multi_cloud_A.5.28_0025
  - iso27001_2022_multi_cloud_A.5.29_0026
  - iso27001_2022_multi_cloud_A.8.22_0078
  - iso27001_2022_multi_cloud_A.8.23_0079
  - iso27001_2022_multi_cloud_A.8.25_0081
  - iso27001_2022_multi_cloud_A.8.26_0082
  - iso27001_2022_multi_cloud_A.8.29_0084
  - iso27001_2022_multi_cloud_A.9.2_0097
  - iso27001_2022_multi_cloud_A.9.4_0099
  - nist_800_171_r2_multi_cloud_3_12_4_3.12.4_Develop_document_and_periodically_0003
  - nist_800_171_r2_multi_cloud_3_14_1_3.14.1_Identify_report_and_correct_syste_0014
  - nist_800_171_r2_multi_cloud_3_14_6_3.14.6_Monitor_organizational_systems_inc_0018
  - nist_800_171_r2_multi_cloud_3_14_7_3.14.7_Identify_unauthorized_use_of_organi_0019
  - nist_800_171_r2_multi_cloud_3_1_2_3.1.2_Limit_system_access_to_the_types_of_t_0024
  - nist_800_171_r2_multi_cloud_3_1_20_3.1.20_Verify_and_control_limit_connection_0025
  - nist_800_171_r2_multi_cloud_3_1_5_3.1.5_Employ_the_principle_of_least_privile_0028
  - nist_800_171_r2_multi_cloud_3_1_6_3.1.6_Use_non-privileged_accounts_or_roles_0029
  - nist_800_171_r2_multi_cloud_3_1_7_3.1.7_Prevent_non-privileged_users_from_exe_0030
  - nist_800_53_rev5_multi_cloud_AC-5-b_0090
  - nist_800_53_rev5_multi_cloud_SC-25_1277
  - pci_dss_v4_multi_cloud_2.1.1_0018
  - pci_dss_v4_multi_cloud_7.1.1_0089
  - pci_dss_v4_multi_cloud_7.1.2_0090
  - rbi_bank_multi_cloud_3.2_0015
  - rbi_nbfc_multi_cloud_2.5_0009
  - rbi_nbfc_multi_cloud_5.1_0025
  - soc2_multi_cloud_cc_1_3_0001
  - soc2_multi_cloud_cc_3_3_0005
  - soc2_multi_cloud_cc_6_1_0009
  - soc2_multi_cloud_cc_6_3_0011
- rule_id: k8s.rbac.no.root_access_key
  service: rbac
  resource: 'no'
  requirement: Root Access Key
  scope: rbac.no.root_access_key
  domain: identity_and_access_management
  subcategory: access_control
  severity: critical
  title: Prohibit Cluster-Wide Privileges for Service Accounts
  rationale: Allowing service accounts to have cluster-wide privileges, especially with root access, poses a significant security
    risk. Misconfigured RBAC roles can lead to privilege escalation, allowing attackers to perform unauthorized actions across
    the entire cluster. This can lead to data breaches, service disruptions, and compliance violations, as attackers could
    gain control over critical resources or data.
  description: This control checks that no Kubernetes service accounts are granted cluster-wide privileges, specifically avoiding
    the use of any role or role binding that provides root-level access. A secure configuration ensures that service accounts
    are only granted the minimum permissions necessary for their function, following the principle of least privilege. By
    restricting excessive permissions, this control helps mitigate potential attack vectors such as privilege escalation and
    unauthorized resource access, thereby protecting the cluster's integrity and confidentiality.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding
  - https://kubernetes.io/docs/concepts/security/overview/#service-accounts
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  compliance:
  - rbi_bank_multi_cloud_9.1_0026
- rule_id: k8s.rbac.no.shared_service_accounts
  service: rbac
  resource: 'no'
  requirement: Shared Service Accounts
  scope: rbac.no.shared_service_accounts
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Unique Service Accounts Per Application
  rationale: Sharing service accounts across multiple applications can lead to elevated privileges and unauthorized access.
    If a single application is compromised, an attacker can potentially gain access to all resources available to the shared
    service account. This increases the attack surface and can lead to data breaches, lateral movement, and privilege escalation
    within the cluster.
  description: This rule checks that each application within the Kubernetes cluster uses a unique service account, rather
    than sharing one across multiple applications. A proper configuration involves defining distinct service accounts for
    each application and binding them with the least privilege necessary through RBAC policies. This reduces the risk of privilege
    escalation and limits the potential impact of a compromised application by isolating its access rights, thereby adhering
    to the principle of least privilege.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IA-4_0064
  - hipaa_multi_cloud_164_312_a_2_i_0023
  - hipaa_multi_cloud_164_312_a_2_ii_0024
  - hipaa_multi_cloud_164_312_a_2_iv_0025
  - rbi_bank_multi_cloud_9.3_0027
- rule_id: k8s.rbac.no.single_user_full_control
  service: rbac
  resource: 'no'
  requirement: Single User Full Control
  scope: rbac.no.single_user_full_control
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Prevent Full Cluster Admin Privileges for Single User
  rationale: If a single user is granted full cluster admin privileges, it poses a significant security risk as this user
    could potentially make unauthorized changes, access sensitive data, or disrupt services. This is especially dangerous
    if their credentials are compromised, allowing attackers to gain complete control over the cluster.
  description: This rule checks that no individual user account is assigned the cluster-admin role or equivalent permissions
    across the Kubernetes cluster. Best practices dictate using role-based access control (RBAC) to limit permissions according
    to the principle of least privilege, thereby reducing the attack surface and mitigating risks associated with credential
    compromise. A secure configuration ensures that users have only the permissions necessary to perform their tasks, limiting
    potential damage if an account is compromised.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - iso27001_2022_multi_cloud_A.5.3_0027
  - iso27001_2022_multi_cloud_A.5.30_0028
  - iso27001_2022_multi_cloud_A.5.31_0029
  - iso27001_2022_multi_cloud_A.5.32_0030
  - iso27001_2022_multi_cloud_A.5.33_0031
  - iso27001_2022_multi_cloud_A.5.34_0032
  - iso27001_2022_multi_cloud_A.5.35_0033
  - iso27001_2022_multi_cloud_A.5.36_0034
  - iso27001_2022_multi_cloud_A.5.37_0035
- rule_id: k8s.rbac.nodes.proxy_access_check
  service: rbac
  resource: nodes
  requirement: Proxy Access Check
  scope: rbac.nodes.proxy_access_check
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict Unauthorized Node Proxy Access via RBAC
  rationale: Misconfigured proxy access on nodes can lead to unauthorized users gaining access to sensitive cluster information
    and operations, such as viewing pod logs or executing commands. This can be exploited by attackers to pivot within the
    cluster or exfiltrate data, leading to potential breaches and compliance violations.
  description: This rule verifies that Role-Based Access Control (RBAC) policies are correctly configured to restrict proxy
    access to Kubernetes nodes. It ensures that only authorized users and service accounts can use the node proxy feature,
    which is crucial for accessing the Kubernetes API through nodes. Proper configuration includes assigning roles with specific
    permissions to trusted entities, thereby minimizing the attack surface and maintaining cluster integrity.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#security-checklist
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-node-access
  compliance:
  - cis_kubernetes_kubernetes_5.1.10_0109
- rule_id: k8s.rbac.oidc.authentication_configured
  service: rbac
  resource: oidc
  requirement: Authentication Configured
  scope: rbac.oidc.authentication_configured
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Configure OIDC Authentication for Kubernetes RBAC
  rationale: Without proper OIDC authentication, Kubernetes clusters are vulnerable to unauthorized access. Attackers may
    exploit misconfigured authentication to gain access to sensitive workloads and data, leading to potential data breaches
    and system compromise.
  description: This rule checks that OpenID Connect (OIDC) authentication is properly configured for Kubernetes Role-Based
    Access Control (RBAC). A valid configuration involves setting up the necessary OIDC issuer, client ID, and required claims
    to ensure that only authenticated users can interact with the cluster. Proper OIDC configuration mitigates the risk of
    unauthorized access and ensures compliance with security standards by enforcing identity verification through a trusted
    identity provider.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/#authentication
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IA-2_0062
  - fedramp_moderate_multi_cloud_IA-2_0170
  - fedramp_moderate_multi_cloud_IA-2_1_0171
  - fedramp_moderate_multi_cloud_IA-2_2_0172
  - fedramp_moderate_multi_cloud_IA-2_5_0173
  - fedramp_moderate_multi_cloud_IA-2_6_0174
  - fedramp_moderate_multi_cloud_IA-2_8_0175
  - fedramp_moderate_multi_cloud_IA-2_12_0176
  - fedramp_moderate_multi_cloud_IA-4_0178
  - fedramp_moderate_multi_cloud_IA-4_4_0179
  - fedramp_moderate_multi_cloud_IA-5_0180
  - fedramp_moderate_multi_cloud_IA-5_1_0181
  - fedramp_moderate_multi_cloud_IA-5_2_0182
  - fedramp_moderate_multi_cloud_IA-5_6_0183
  - fedramp_moderate_multi_cloud_IA-5_7_0184
  - fedramp_moderate_multi_cloud_IA-5_8_0185
  - fedramp_moderate_multi_cloud_IA-5_13_0186
  - hipaa_multi_cloud_164_312_d_0029
  - iso27001_2022_multi_cloud_A.5.15_0011
  - iso27001_2022_multi_cloud_A.5.2_0016
  - iso27001_2022_multi_cloud_A.5.20_0017
  - iso27001_2022_multi_cloud_A.5.21_0018
  - iso27001_2022_multi_cloud_A.5.23_0020
  - iso27001_2022_multi_cloud_A.5.24_0021
  - iso27001_2022_multi_cloud_A.5.25_0022
  - iso27001_2022_multi_cloud_A.5.26_0023
  - iso27001_2022_multi_cloud_A.5.27_0024
  - iso27001_2022_multi_cloud_A.5.28_0025
  - iso27001_2022_multi_cloud_A.5.29_0026
  - iso27001_2022_multi_cloud_A.8.5_0092
  - iso27001_2022_multi_cloud_A.9.2_0097
  - nist_800_171_r2_multi_cloud_3_3_5_3.3.5_Correlate_audit_record_review_analys_0035
  - nist_800_171_r2_multi_cloud_3_5_2_3.5.2_Authenticate_or_verify_the_identiti_0043
  - nist_800_171_r2_multi_cloud_3_5_3_3.5.3_Use_multifactor_authentication_for_lo_0044
  - nist_800_53_rev5_multi_cloud_MA-4-c_0684
  - rbi_nbfc_multi_cloud_2.5_0009
  - rbi_nbfc_multi_cloud_5.1_0025
- rule_id: k8s.rbac.persistentvolume.creation_access_check
  service: rbac
  resource: persistentvolume
  requirement: Creation Access Check
  scope: rbac.persistentvolume.creation_access_check
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict PersistentVolume Creation with RBAC
  rationale: If RBAC permissions for PersistentVolume creation are not properly restricted, unauthorized users may create
    volumes that could lead to data exfiltration or unauthorized data access. Attackers could exploit this misconfiguration
    to create PersistentVolumes with sensitive data deliberately exposed or to consume excessive storage resources, impacting
    the cluster's availability and performance.
  description: This rule checks that RBAC policies are configured to restrict who can create PersistentVolumes. A secure configuration
    ensures only authorized users or service accounts with legitimate need can define PersistentVolumes, reducing the risk
    of unauthorized data exposure or resource consumption. A good configuration involves assigning create permissions to a
    minimal set of roles, typically limited to cluster administrators or specific service accounts required by trusted applications,
    ensuring compliance with least privilege principles.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - cis_kubernetes_kubernetes_5.1.9_0108
- rule_id: k8s.rbac.pod.creation_access_check
  service: rbac
  resource: pod
  requirement: Creation Access Check
  scope: rbac.pod.creation_access_check
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce Pod Creation Permissions via RBAC
  rationale: Without proper RBAC controls, unauthorized users can create pods, potentially leading to privilege escalation,
    data exfiltration, or resource exhaustion attacks. By restricting pod creation permissions, you minimize the attack surface
    and ensure that only trusted and authenticated entities can deploy workloads.
  description: This control verifies that Role-Based Access Control (RBAC) policies are configured to restrict pod creation
    to authorized users only. It checks whether roles and role bindings are properly set up to delegate pod creation permissions
    explicitly. A secure configuration ensures that only users or service accounts with a legitimate need can create pods,
    helping to prevent unauthorized access, privilege escalation, and unintentional resource consumption.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - cis_kubernetes_kubernetes_5.1.4_0103
- rule_id: k8s.rbac.policies.documented
  service: rbac
  resource: policies
  requirement: Documented
  scope: rbac.policies.documented
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure RBAC Policies Are Fully Documented and Reviewed
  rationale: Undocumented or poorly documented RBAC policies can lead to misconfigurations, where excessive permissions may
    be granted. This increases the risk of privilege escalation and unauthorized access to cluster resources, which are common
    attack vectors for malicious actors seeking to exploit vulnerabilities in the cluster.
  description: This rule checks that all RBAC (Role-Based Access Control) policies within the Kubernetes cluster are documented
    with clear descriptions of their purpose and scope. Proper documentation includes details about the roles, bindings, and
    permissions granted, ensuring they align with the principle of least privilege. This practice helps in regular audits,
    simplifying the identification of over-permissive roles and reducing the risk of unauthorized access.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - iso27001_2022_multi_cloud_A.5.1_0005
  - iso27001_2022_multi_cloud_A.5.10_0006
  - iso27001_2022_multi_cloud_A.5.11_0007
  - iso27001_2022_multi_cloud_A.5.12_0008
  - iso27001_2022_multi_cloud_A.5.13_0009
  - iso27001_2022_multi_cloud_A.5.14_0010
  - iso27001_2022_multi_cloud_A.5.16_0012
  - iso27001_2022_multi_cloud_A.5.17_0013
- rule_id: k8s.rbac.role.break_glass_controls_enabled
  service: rbac
  resource: role
  requirement: Break Glass Controls Enabled
  scope: rbac.role.break_glass_controls_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Emergency Access Controls for RBAC Roles
  rationale: Without proper emergency access controls, attackers or malicious insiders could exploit elevated privileges during
    a crisis, leading to unauthorized access to sensitive resources. This control mitigates the risk of privilege escalation
    attacks by ensuring that any emergency access is logged, justified, and time-bound.
  description: This rule verifies that Kubernetes RBAC roles have emergency access controls, or 'break glass' procedures,
    configured. Such controls require that any use of elevated permissions during an emergency is temporary, logged, and approved.
    Proper setup involves defining specific conditions under which these roles can be used, ensuring that access is revoked
    immediately after the emergency, and maintaining an audit trail. This approach reduces the security risk of persistent
    elevated privileges and aids in compliance with best practices for access management.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.rbac.role.change_audit_enabled
  service: rbac
  resource: role
  requirement: Change Audit Enabled
  scope: rbac.role.change_audit_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enable RBAC Role Change Auditing
  rationale: Without auditing RBAC role changes, unauthorized modifications might go undetected, potentially leading to privilege
    escalation or data breaches. Attackers could exploit unmonitored changes to gain elevated access or disrupt cluster operations.
  description: This rule checks that auditing is enabled for changes made to Kubernetes RBAC roles. Proper auditing ensures
    that any modification to roles is logged, allowing administrators to track and respond to unauthorized attempts to alter
    permissions. A good configuration involves setting up an audit policy that captures 'create', 'update', and 'delete' events
    for RBAC resources. This helps in maintaining a secure and compliant environment by providing visibility into access control
    changes.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/policy/audit-policy/
  compliance:
  - soc2_multi_cloud_cc_3_4_0006
- rule_id: k8s.rbac.role.least_privilege_enforced
  service: rbac
  resource: role
  requirement: Least Privilege Enforced
  scope: rbac.role.least_privilege_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce RBAC Roles with Least Privilege
  rationale: Misconfigured RBAC roles with excessive permissions increase the risk of privilege escalation and unauthorized
    access to Kubernetes resources. Attackers exploiting these roles can perform malicious actions, such as data exfiltration
    or service disruption, compromising the entire cluster's security.
  description: This control checks that RBAC roles are configured to grant the minimal set of permissions necessary for their
    intended function. A good configuration ensures that roles do not have unnecessary access to resources, adhering strictly
    to the principle of least privilege. This reduces the attack surface and mitigates risks associated with overly permissive
    roles, thus enhancing the cluster's security posture.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.rbac.role.review_scheduled
  service: rbac
  resource: role
  requirement: Review Scheduled
  scope: rbac.role.review_scheduled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regularly Audit Kubernetes RBAC Role Configurations
  rationale: Failure to regularly review and audit Kubernetes RBAC roles can lead to privilege escalation, unauthorized access,
    and potential data breaches. Attackers can exploit overly permissive roles or stale configurations to gain access to sensitive
    resources.
  description: This control ensures that all Kubernetes RBAC roles are subject to regular audits to verify adherence to the
    principle of least privilege. It checks for the existence of a scheduled review process for role configuration changes,
    ensuring that roles are not overly permissive and are periodically validated against current security policies. This practice
    helps to identify and mitigate potential security risks by ensuring that roles align with organizational security standards
    and regulatory requirements.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-2-b_0367
- rule_id: k8s.rbac.role.separation_of_duties_enforced
  service: rbac
  resource: role
  requirement: Separation Of Duties Enforced
  scope: rbac.role.separation_of_duties_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Role-Based Access Control (RBAC) for Separation of Duties
  rationale: Without proper separation of duties, a single user might have excessive permissions, increasing the risk of insider
    threats or accidental misuse of privileges. This can lead to unauthorized access to sensitive resources, data breaches,
    or service disruptions.
  description: This control checks that Kubernetes RBAC roles are configured to enforce separation of duties by ensuring that
    permissions are appropriately distributed among different roles. It verifies that no single role has excessive access
    that violates the principle of least privilege. Proper configuration involves assigning specific permissions to users
    based on their job functions, thereby minimizing the risk of unauthorized access and ensuring compliance with security
    best practices and regulatory standards.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#role-based-access-control-rbac
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-5_0005
  - fedramp_moderate_multi_cloud_AC-5_0017
  - iso27001_2022_multi_cloud_A.5.18_0014
  - nist_800_171_r2_multi_cloud_3_14_1_3.14.1_Identify_report_and_correct_syste_0014
  - nist_800_171_r2_multi_cloud_3_14_6_3.14.6_Monitor_organizational_systems_inc_0018
  - nist_800_171_r2_multi_cloud_3_14_7_3.14.7_Identify_unauthorized_use_of_organi_0019
  - nist_800_53_rev5_multi_cloud_AC-5-b_0090
  - soc2_multi_cloud_cc_6_3_0011
- rule_id: k8s.rbac.role.wildcard_permissions_prohibited_enforced
  service: rbac
  resource: role
  requirement: Wildcard Permissions Prohibited Enforced
  scope: rbac.role.wildcard_permissions_prohibited_enforced
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Disallow Wildcard Permissions in Kubernetes Roles
  rationale: Wildcard permissions in RBAC roles can lead to excessive privileges, enabling users or services to perform unauthorized
    actions across the cluster. This misconfiguration poses a risk of privilege escalation, data breaches, and unauthorized
    resource manipulation. Attackers can exploit these broad permissions to gain control over critical cluster resources.
  description: This rule checks that Kubernetes RBAC roles do not include wildcard permissions, which grant unrestricted access
    to resources. A secure configuration requires specifying exact permissions needed for each role, limiting the risk of
    privilege escalation and unauthorized access. By enforcing this control, you ensure that roles adhere to the principle
    of least privilege, enhancing overall cluster security and reducing potential attack surfaces.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#referring-to-resources
  - https://kubernetes.io/docs/concepts/security/overview/#role-based-access-control
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.rbac.rolebinding.admission_controller_enforced
  service: rbac
  resource: rolebinding
  requirement: Admission Controller Enforced
  scope: rbac.rolebinding.admission_controller_enforced
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce RoleBinding with Validating Admission Controllers
  rationale: If RoleBindings are not enforced through validating admission controllers, attackers could exploit misconfigurations
    to escalate privileges by binding excessive permissions to service accounts. This could allow unauthorized access to sensitive
    resources or the ability to execute privileged operations within the cluster.
  description: This rule checks that Kubernetes RoleBindings are subject to validating admission controllers to ensure that
    permissions are only granted following security best practices. A properly configured admission controller evaluates RoleBinding
    objects before they are persisted in the cluster, preventing the application of policies that could lead to privilege
    escalation or unauthorized access. This helps maintain the principle of least privilege and mitigates risks associated
    with over-privileged roles.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance: []
- rule_id: k8s.rbac.rolebinding.audit_enabled
  service: rbac
  resource: rolebinding
  requirement: Audit Enabled
  scope: rbac.rolebinding.audit_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enable Audit Logging for RBAC RoleBindings
  rationale: Without audit logging, unauthorized changes to RBAC RoleBindings may go undetected, potentially allowing privilege
    escalation or unauthorized access to sensitive resources. Attackers could exploit these misconfigurations to escalate
    their permissions or access restricted data, leading to data breaches or disruption of services.
  description: This rule checks that audit logging is enabled for RBAC RoleBindings in Kubernetes. Audit logging captures
    detailed records of access and modification events related to RoleBindings, providing visibility into who accessed or
    changed permissions. Proper audit configuration ensures that any unauthorized or suspicious activity is logged for further
    analysis, aiding in incident response and forensic investigations. A good configuration logs all events related to RoleBindings,
    including creation, updates, and deletions, ensuring compliance with security standards and regulations.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - soc2_multi_cloud_cc_4_2_0007
- rule_id: k8s.rbac.rolebinding.least_privilege
  service: rbac
  resource: rolebinding
  requirement: Least Privilege
  scope: rbac.rolebinding.least_privilege
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce Least Privilege in Kubernetes RoleBindings
  rationale: Without enforcing least privilege in RoleBindings, users or service accounts can gain excessive permissions,
    increasing the risk of privilege escalation attacks. Misconfigured RoleBindings can allow unauthorized access to sensitive
    resources and operations, leading to potential data breaches or service disruptions.
  description: This rule checks that Kubernetes RoleBindings assign the minimal set of permissions necessary for users or
    service accounts to perform their tasks. A good configuration involves mapping users to Roles that have only the necessary
    permissions for their specific duties. Enforcing least privilege helps prevent unauthorized access and minimizes the attack
    surface by ensuring that users or services cannot perform actions outside their intended scope.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-6_0006
  - fedramp_moderate_multi_cloud_AC-2_0002
  - fedramp_moderate_multi_cloud_AC-2_1_0003
  - fedramp_moderate_multi_cloud_AC-2_2_0004
  - fedramp_moderate_multi_cloud_AC-2_3_0005
  - fedramp_moderate_multi_cloud_AC-2_4_0006
  - fedramp_moderate_multi_cloud_AC-2_5_0007
  - fedramp_moderate_multi_cloud_AC-2_7_0008
  - fedramp_moderate_multi_cloud_AC-2_9_0009
  - fedramp_moderate_multi_cloud_AC-2_11_0010
  - fedramp_moderate_multi_cloud_AC-2_12_0011
  - fedramp_moderate_multi_cloud_AC-2_13_0012
  - fedramp_moderate_multi_cloud_AC-20_0046
  - fedramp_moderate_multi_cloud_AC-20_1_0047
  - fedramp_moderate_multi_cloud_AC-20_2_0048
  - fedramp_moderate_multi_cloud_AC-21_0049
  - fedramp_moderate_multi_cloud_AC-22_0050
  - iso27001_2022_multi_cloud_A.9.3_0098
- rule_id: k8s.rbac.rolebinding.rbac_enabled_enforced
  service: rbac
  resource: rolebinding
  requirement: Rbac Enabled Enforced
  scope: rbac.rolebinding.rbac_enabled_enforced
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Enforce RoleBinding with RBAC for Access Control
  rationale: Without proper enforcement of RoleBindings in RBAC, users or service accounts may gain unintended permissions,
    leading to potential privilege escalation or unauthorized access to cluster resources. This misconfiguration can expose
    sensitive data and increase the risk of insider threats or compromised accounts accessing critical resources.
  description: This rule checks that RoleBindings in the Kubernetes cluster are properly configured to enforce RBAC policies.
    It ensures that each RoleBinding is linked to appropriate Roles or ClusterRoles and that permissions granted are minimal
    and necessary for the function. Proper enforcement prevents unauthorized access by ensuring only authenticated and authorized
    users or service accounts can perform actions defined by their roles, thereby reducing the attack surface and mitigating
    the risk of privilege escalation.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#role-based-access-control-rbac
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.rbac.rolebinding.review
  service: rbac
  resource: rolebinding
  requirement: Review
  scope: rbac.rolebinding.review
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regular Review of RoleBindings for Least Privilege
  rationale: Misconfigured RoleBindings can lead to excessive permissions, allowing unauthorized users or service accounts
    to escalate privileges or access sensitive resources. This increases the risk of data breaches and lateral movement within
    the cluster by malicious actors.
  description: This rule checks that RoleBindings in your Kubernetes cluster are regularly reviewed to ensure they adhere
    to the principle of least privilege. A good configuration involves ensuring each RoleBinding grants only the necessary
    permissions needed for specific tasks and does not include wildcard or overly broad permissions. Regular reviews help
    prevent privilege escalation and unauthorized access, reinforcing your cluster's security posture.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/
  compliance:
  - hipaa_multi_cloud_164_308_a_3_i_0004
  - hipaa_multi_cloud_164_308_a_3_ii_a_0005
  - hipaa_multi_cloud_164_308_a_3_ii_b_0006
  - hipaa_multi_cloud_164_308_a_3_ii_c_0007
  - soc2_multi_cloud_cc_1_3_0001
- rule_id: k8s.rbac.rolebinding.scope_limited
  service: rbac
  resource: rolebinding
  requirement: Scope Limited
  scope: rbac.rolebinding.scope_limited
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Limit RoleBinding Scope to Minimize Access
  rationale: If RoleBindings are not properly scoped, they can grant excessive permissions to users or service accounts, increasing
    the risk of privilege escalation. This can lead to unauthorized access to resources within the cluster, compromise of
    sensitive data, or disruption of services. By limiting the scope of RoleBindings, you reduce the attack surface and help
    prevent these security threats.
  description: This control checks that RoleBindings in Kubernetes are limited in scope to only the necessary namespaces or
    resources. A well-configured RoleBinding restricts permissions to the minimal required level for users or service accounts,
    adhering to the principle of least privilege. This practice helps in preventing privilege escalation attacks and ensures
    compliance with industry best practices and regulatory standards.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding
  - https://kubernetes.io/docs/concepts/security/overview/#rbac-authorization
  - https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
  compliance:
  - fedramp_moderate_multi_cloud_AC-6_0018
  - fedramp_moderate_multi_cloud_AC-6_1_0019
  - fedramp_moderate_multi_cloud_AC-6_2_0020
  - fedramp_moderate_multi_cloud_AC-6_3_0021
  - fedramp_moderate_multi_cloud_AC-6_5_0022
  - fedramp_moderate_multi_cloud_AC-6_7_0023
  - fedramp_moderate_multi_cloud_AC-6_8_0024
  - fedramp_moderate_multi_cloud_AC-6_9_0025
  - fedramp_moderate_multi_cloud_AC-6_10_0026
  - pci_dss_v4_multi_cloud_7.1.2_0090
- rule_id: k8s.rbac.rolebinding.separation
  service: rbac
  resource: rolebinding
  requirement: Separation
  scope: rbac.rolebinding.separation
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce RoleBinding Separation for Least Privilege
  rationale: Improper separation of RoleBindings can lead to privilege escalation attacks, where users gain unauthorized access
    to resources by exploiting overly permissive roles. Ensuring strict RoleBinding separation minimizes the attack surface
    and helps prevent unauthorized access to sensitive resources, maintaining a secure environment.
  description: This rule checks that RoleBindings in Kubernetes are configured to enforce the principle of least privilege
    by ensuring that each RoleBinding is associated with a specific role and user/group, without overlap that could lead to
    privilege escalation. Proper configuration involves assigning only the necessary permissions required for a user or service
    account to perform its intended function, thereby reducing the risk of unauthorized access and enhancing security compliance
    with industry standards.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - iso27001_2022_multi_cloud_A.5.3_0027
  - iso27001_2022_multi_cloud_A.5.30_0028
  - iso27001_2022_multi_cloud_A.5.31_0029
  - iso27001_2022_multi_cloud_A.5.32_0030
  - iso27001_2022_multi_cloud_A.5.33_0031
  - iso27001_2022_multi_cloud_A.5.34_0032
  - iso27001_2022_multi_cloud_A.5.35_0033
  - iso27001_2022_multi_cloud_A.5.36_0034
  - iso27001_2022_multi_cloud_A.5.37_0035
- rule_id: k8s.rbac.rolebinding.to_specific_namespaces
  service: rbac
  resource: rolebinding
  requirement: To Specific Namespaces
  scope: rbac.rolebinding.to_specific_namespaces
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict RoleBinding Scope to Specific Namespaces
  rationale: If RoleBindings are not restricted to specific namespaces, users can gain unauthorized access to resources across
    multiple namespaces, leading to potential data breaches. Attackers could exploit overly permissive RoleBindings to escalate
    privileges or exfiltrate sensitive information from the cluster.
  description: This control checks that RoleBindings are configured to apply only to specific namespaces rather than cluster-wide.
    A good configuration ensures that the scope of permissions granted by RoleBindings is limited, reducing the risk of privilege
    escalation and unauthorized access. RoleBindings should be scoped to the least privileged namespace required for their
    purpose, aligning with the principle of least privilege and aiding compliance with security standards such as the CIS
    Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding
  - https://kubernetes.io/docs/concepts/overview/components/#namespace
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#namespace-scoped
  compliance:
  - iso27001_2022_multi_cloud_A.8.22_0078
  - iso27001_2022_multi_cloud_A.8.23_0079
  - iso27001_2022_multi_cloud_A.8.25_0081
  - iso27001_2022_multi_cloud_A.8.26_0082
  - iso27001_2022_multi_cloud_A.8.29_0084
- rule_id: k8s.rbac.roles.no_wildcard_in_rules
  service: rbac
  resource: roles
  requirement: No Wildcard In Rules
  scope: rbac.roles.no_wildcard_in_rules
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Specific Resource Permissions in RBAC Roles
  rationale: Using wildcards in RBAC role permissions can lead to excessive privilege allocation, increasing the risk of privilege
    escalation attacks. Attackers could exploit these broad permissions to gain unauthorized access to resources, potentially
    compromising the entire cluster. By enforcing specific resource permissions, the principle of least privilege is upheld,
    minimizing the attack surface and reducing the likelihood of unauthorized actions.
  description: This control checks that no Kubernetes RBAC roles use wildcards ('*') in their rules, ensuring that permissions
    are explicitly defined for specific resources and actions. A properly configured RBAC role should list each resource and
    verb separately, avoiding broad permissions that could be abused. By adhering to this practice, organizations can limit
    permissions to only what is necessary for operations, thus enhancing security and maintaining compliance with industry
    standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#referring-to-resources
  - https://kubernetes.io/docs/concepts/security/overview/#security-best-practices
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-rbac-rules
  compliance:
  - cis_kubernetes_kubernetes_5.1.3_0102
- rule_id: k8s.rbac.secret.auto_rotation_configured
  service: rbac
  resource: secret
  requirement: Auto Rotation Configured
  scope: rbac.secret.auto_rotation_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Automatic Secret Rotation for Enhanced Security
  rationale: Without automatic rotation, secrets may become susceptible to exposure and unauthorized access if they are not
    periodically updated. An attacker who gains access to static secrets could maintain persistent access to sensitive information.
    Auto-rotation mitigates this risk by regularly updating secrets, thereby reducing the attack window and improving resilience
    against credential-based attacks.
  description: This check ensures that Kubernetes secrets are configured for automatic rotation, which involves setting up
    mechanisms or tools that periodically update the secrets used by Kubernetes services. A proper configuration would typically
    involve integrating with tools like External Secrets Operator or Vault, which handle secret management and rotation. This
    proactive measure not only complies with best practices like those outlined in the CIS Kubernetes Benchmark but also enhances
    security by reducing the lifespan of any potentially compromised credentials.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/concepts/security/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  compliance: []
- rule_id: k8s.rbac.secrets.access_review
  service: rbac
  resource: secrets
  requirement: Access Review
  scope: rbac.secrets.access_review
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Ensure RBAC Policies Limit Secret Access
  rationale: Improperly configured RBAC policies can lead to unauthorized access to secrets, which may contain sensitive information
    such as credentials, API keys, and tokens. If an attacker gains access to these secrets, they could exploit them to escalate
    privileges, access restricted resources, or compromise the entire cluster. This control is crucial for minimizing the
    attack surface and mitigating insider threats.
  description: This control checks that RBAC roles and role bindings are configured to restrict access to Kubernetes secrets
    only to those service accounts, users, or groups that absolutely need them. It ensures that only authorized entities have
    the ability to read or modify secrets, aligning with the principle of least privilege. A well-configured RBAC policy reduces
    the risk of data breaches by preventing unauthorized entities from accessing confidential information stored in secrets.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#rbac
  compliance:
  - cis_kubernetes_kubernetes_5.1.2_0101
- rule_id: k8s.rbac.segregation.of_duties_enforced
  service: rbac
  resource: segregation
  requirement: Of Duties Enforced
  scope: rbac.segregation.of_duties_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce RBAC Segregation of Duties
  rationale: Improper configuration of RBAC roles can lead to privilege escalation and unauthorized access. By not enforcing
    segregation of duties, a single user may gain excessive privileges, increasing the risk of insider threats and accidental
    misuse of resources. Attackers could exploit these misconfigurations to escalate privileges and compromise the Kubernetes
    cluster.
  description: This rule checks that RBAC roles and bindings are configured to ensure segregation of duties, where different
    users and service accounts have distinct sets of permissions necessary for their roles. A good configuration involves
    creating roles with the least privilege necessary for their tasks, and binding these roles to users or service accounts
    accordingly. This helps prevent privilege escalation, reduces the risk of accidental or malicious actions, and ensures
    that access to resources is appropriately restricted, thereby enhancing the security posture of the Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - iso27001_2022_multi_cloud_A.5.3_0027
  - iso27001_2022_multi_cloud_A.5.30_0028
  - iso27001_2022_multi_cloud_A.5.31_0029
  - iso27001_2022_multi_cloud_A.5.32_0030
  - iso27001_2022_multi_cloud_A.5.33_0031
  - iso27001_2022_multi_cloud_A.5.34_0032
  - iso27001_2022_multi_cloud_A.5.35_0033
  - iso27001_2022_multi_cloud_A.5.36_0034
  - iso27001_2022_multi_cloud_A.5.37_0035
- rule_id: k8s.rbac.separation.of_duties_enforced
  service: rbac
  resource: separation
  requirement: Of Duties Enforced
  scope: rbac.separation.of_duties_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce RBAC Separation of Duties
  rationale: Misconfiguring RBAC roles can lead to privilege escalation where users gain unauthorized access to sensitive
    resources or perform unauthorized operations. This increases the risk of data breaches, service disruption, and potential
    compliance violations.
  description: This rule checks that Kubernetes Role-Based Access Control (RBAC) is configured to enforce separation of duties,
    ensuring that roles are assigned to users based on the principle of least privilege. Proper configuration involves creating
    distinct roles with specific permissions that align with job functions and ensuring that no single user has excessive
    permissions. This limits the potential impact of compromised credentials and reduces the risk of accidental or malicious
    changes to the cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#rbac-authorization
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-rbac/
  compliance: []
- rule_id: k8s.rbac.serviceaccount.automount_disabled_enabled
  service: rbac
  resource: serviceaccount
  requirement: Automount Disabled Enabled
  scope: rbac.serviceaccount.automount_disabled_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable Automounting of ServiceAccount Tokens
  rationale: Automounting ServiceAccount tokens by default can lead to unintended access where pods automatically receive
    a token that grants them permissions they should not have. If an attacker exploits a vulnerability within the pod, they
    could use this token to access the Kubernetes API server and potentially escalate privileges or access sensitive data.
  description: This rule checks that automountServiceAccountToken is set to false in the ServiceAccount configuration. This
    prevents the automatic inclusion of a ServiceAccount token in the pod's environment, reducing the risk of token misuse.
    A properly configured setting ensures that only pods explicitly requiring API access receive a token, minimizing attack
    vectors and adhering to the principle of least privilege.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#service-account-automounting
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  compliance: []
- rule_id: k8s.rbac.serviceaccount.break_glass_access_controlled_enforced
  service: rbac
  resource: serviceaccount
  requirement: Break Glass Access Controlled Enforced
  scope: rbac.serviceaccount.break_glass_access_controlled_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict Break Glass ServiceAccount Access with RBAC
  rationale: Improperly configured break glass access can lead to unauthorized elevation of privileges and potential compromise
    of the Kubernetes cluster. Attackers exploiting this can gain administrative access, bypassing normal security controls
    and causing disruption or data exfiltration.
  description: This rule ensures that any 'break glass' service accounts used for emergency access are tightly controlled
    and auditable. It checks that such service accounts have constrained permissions, are used only when absolutely necessary,
    and access is logged. A good configuration restricts these accounts to minimal privileges and employs role-based access
    control (RBAC) policies for fine-grained access management. This minimizes the risk of privilege escalation and helps
    maintain a secure environment.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#service-accounts
  - https://kubernetes.io/docs/concepts/configuration/secret/
  compliance: []
- rule_id: k8s.rbac.serviceaccount.cert_auto_rotation_enabled
  service: rbac
  resource: serviceaccount
  requirement: Cert Auto Rotation Enabled
  scope: rbac.serviceaccount.cert_auto_rotation_enabled
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enable Automatic Rotation of Service Account Tokens
  rationale: Failure to enable automatic rotation of service account tokens can lead to prolonged exposure of credentials,
    increasing the risk of unauthorized access if a token is compromised. Without rotation, tokens remain valid indefinitely,
    providing persistent access to attackers who may exploit them to escalate privileges or access sensitive resources.
  description: This control checks that service accounts are configured to automatically rotate their tokens, ensuring that
    credentials are regularly refreshed. A good configuration involves setting the 'BoundServiceAccountTokenVolume' feature
    gate and ensuring that service account tokens have a defined, limited lifespan. This practice reduces the duration of
    potential misuse in case of token leakage and aligns with the principle of least privilege by minimizing the attack surface.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/overview/#service-account-tokens
  compliance: []
- rule_id: k8s.rbac.serviceaccount.certificate_auth_configured_enforced
  service: rbac
  resource: serviceaccount
  requirement: Certificate Auth Configured Enforced
  scope: rbac.serviceaccount.certificate_auth_configured_enforced
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Certificate-Based Authentication for Service Accounts
  rationale: Without certificate-based authentication, service accounts may be vulnerable to unauthorized access through insecure
    methods. This can lead to privilege escalation attacks or unauthorized actions within the cluster, potentially compromising
    sensitive workloads and data.
  description: This control checks that service accounts in Kubernetes are configured to use certificate-based authentication,
    ensuring that every service account has a certificate issued by the cluster's certificate authority. Proper certificate
    management helps authenticate service accounts securely, reducing the risk of unauthorized access. A well-configured certificate
    authentication process is essential for verifying the identity of service accounts and preventing security breaches.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance: []
- rule_id: k8s.rbac.serviceaccount.emergency_access_audit_enforced
  service: rbac
  resource: serviceaccount
  requirement: Emergency Access Audit Enforced
  scope: rbac.serviceaccount.emergency_access_audit_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Audit Emergency Access for Service Accounts
  rationale: Inadequate auditing of emergency access granted to service accounts can lead to undetected malicious activities
    or misuse of privileges. This can result in unauthorized access to sensitive resources, data breaches, and compromise
    of the Kubernetes cluster. Ensuring audits are in place helps detect and respond to such activities promptly, mitigating
    potential damage.
  description: This control checks that all Kubernetes service accounts are configured to log and audit emergency access events.
    Proper configuration includes enabling audit logs for any privilege escalation or role changes associated with service
    accounts. This helps administrators monitor and track any unusual or unauthorized access patterns, ensuring that any elevation
    of permissions is justified and documented, thereby maintaining the integrity and security of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance: []
- rule_id: k8s.rbac.serviceaccount.inactive_cleanup
  service: rbac
  resource: serviceaccount
  requirement: Inactive Cleanup
  scope: rbac.serviceaccount.inactive_cleanup
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Cleanup Unused Service Accounts to Prevent Unauthorized Access
  rationale: Inactive service accounts in Kubernetes can be exploited by attackers to gain unauthorized access to cluster
    resources. These accounts may have permissions that can be misused if not properly managed. Ensuring inactive accounts
    are cleaned up reduces the attack surface and mitigates potential privilege escalation and lateral movement within the
    cluster.
  description: This rule checks for service accounts in the Kubernetes cluster that have not been used for a specified period
    and flags them for cleanup. A good configuration involves regularly auditing service accounts and either disabling or
    deleting those that are no longer necessary. This practice helps in maintaining a minimal set of permissions aligned with
    the principle of least privilege and limits exposure to potential security threats.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/overview/#service-accounts
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-2_0002
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-20_0016
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-21_0017
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-22_0018
- rule_id: k8s.rbac.serviceaccount.least_privilege
  service: rbac
  resource: serviceaccount
  requirement: Least Privilege
  scope: rbac.serviceaccount.least_privilege
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Ensure ServiceAccounts Use Minimal RBAC Permissions
  rationale: Misconfigured RBAC permissions for ServiceAccounts can lead to excessive privileges, increasing the risk of privilege
    escalation and unauthorized access. Attackers exploiting this can execute commands or access sensitive data within the
    cluster, posing significant security threats.
  description: This control checks that each ServiceAccount in the Kubernetes cluster is assigned only the permissions necessary
    to perform its intended functions, following the principle of least privilege. A properly configured ServiceAccount should
    have RBAC roles and role bindings that limit its access to only the resources it needs to interact with. This reduces
    potential attack surfaces and helps in maintaining a secure environment by preventing privilege escalation and data breaches.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance:
  - nist_800_171_r2_multi_cloud_3_14_1_3.14.1_Identify_report_and_correct_syste_0014
  - nist_800_171_r2_multi_cloud_3_14_6_3.14.6_Monitor_organizational_systems_inc_0018
  - nist_800_171_r2_multi_cloud_3_14_7_3.14.7_Identify_unauthorized_use_of_organi_0019
  - nist_800_171_r2_multi_cloud_3_1_5_3.1.5_Employ_the_principle_of_least_privile_0028
- rule_id: k8s.rbac.serviceaccount.management_review
  service: rbac
  resource: serviceaccount
  requirement: Management Review
  scope: rbac.serviceaccount.management_review
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Regular Review of ServiceAccount RBAC Permissions
  rationale: If ServiceAccount RBAC permissions are not regularly reviewed, there is a risk of privilege escalation and unauthorized
    access to cluster resources. Attackers could exploit over-permissive roles or roles that are no longer necessary, potentially
    compromising the security of the cluster and the applications running within it.
  description: This control checks that ServiceAccount RBAC permissions are periodically reviewed to ensure they adhere to
    the principle of least privilege. A good configuration involves regularly auditing and updating role bindings associated
    with ServiceAccounts to remove unnecessary permissions. This reduces the attack surface by limiting the access each ServiceAccount
    has within the cluster, preventing potential privilege escalation attacks and minimizing the impact of compromised credentials.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/overview/#service-accounts
  compliance:
  - pci_dss_v4_multi_cloud_8.2.7_0105
  - soc2_multi_cloud_cc_1_3_0001
  - soc2_multi_cloud_cc_6_2_0010
- rule_id: k8s.rbac.serviceaccount.mfa_enforced
  service: rbac
  resource: serviceaccount
  requirement: Mfa Enforced
  scope: rbac.serviceaccount.mfa_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce MFA for Kubernetes Service Accounts
  rationale: Without multi-factor authentication (MFA), service accounts are vulnerable to unauthorized access if credentials
    are leaked or compromised. This can lead to privilege escalation, data breaches, and unauthorized actions within the cluster.
    Attackers can exploit stolen credentials to gain access to critical resources, potentially causing severe disruption or
    data loss.
  description: This control checks whether multi-factor authentication is enforced for Kubernetes service accounts. It ensures
    that any access to service accounts requires an additional verification factor beyond the standard credentials. A good
    configuration involves integrating an external identity provider that supports MFA, thereby adding a layer of security
    to prevent unauthorized access. This measure mitigates risks associated with credential theft and helps maintain the integrity
    of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance: []
- rule_id: k8s.rbac.serviceaccount.privileges_restricted
  service: rbac
  resource: serviceaccount
  requirement: Privileges Restricted
  scope: rbac.serviceaccount.privileges_restricted
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict ServiceAccount Privileges Using RBAC
  rationale: Misconfigured ServiceAccount privileges can lead to privilege escalation and unauthorized access within your
    Kubernetes cluster. Attackers could exploit excessive permissions to gain control over cluster resources, potentially
    leading to data breaches, denial of service, or other malicious activities.
  description: This rule checks that all ServiceAccounts in the cluster have minimal necessary privileges by evaluating the
    RoleBindings and ClusterRoleBindings associated with them. A well-configured RBAC policy ensures that ServiceAccounts
    have the least privilege necessary, reducing the attack surface and mitigating the risk of privilege escalation. Properly
    restricting ServiceAccount permissions helps maintain compliance with security standards and keeps the cluster secure
    from unauthorized actions.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy-deprecated
  compliance:
  - iso27001_2022_multi_cloud_A.8.22_0078
  - iso27001_2022_multi_cloud_A.8.23_0079
  - iso27001_2022_multi_cloud_A.8.25_0081
  - iso27001_2022_multi_cloud_A.8.26_0082
  - iso27001_2022_multi_cloud_A.8.29_0084
- rule_id: k8s.rbac.serviceaccount.restricted_roles
  service: rbac
  resource: serviceaccount
  requirement: Restricted Roles
  scope: rbac.serviceaccount.restricted_roles
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Restrict ServiceAccount Bindings to Privileged Roles
  rationale: Misconfigured ServiceAccount bindings to privileged roles can lead to privilege escalation, enabling unauthorized
    access to sensitive resources or operations. Attackers can exploit these excessive permissions to execute malicious actions
    within the cluster, leading to potential data breaches and service disruptions.
  description: This rule checks that ServiceAccounts are not bound to overly privileged roles, such as those granting cluster-admin
    or equivalent permissions. A secure configuration restricts ServiceAccounts to the minimum necessary permissions, adhering
    to the principle of least privilege. By ensuring appropriate role bindings, the risk of accidental or malicious privilege
    escalation within the Kubernetes cluster is significantly reduced.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  compliance:
  - nist_800_171_r2_multi_cloud_3_1_6_3.1.6_Use_non-privileged_accounts_or_roles_0029
- rule_id: k8s.rbac.serviceaccount.review_enabled
  service: rbac
  resource: serviceaccount
  requirement: Review Enabled
  scope: rbac.serviceaccount.review_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure ServiceAccount Reviews for RBAC Compliance
  rationale: Without regular reviews of ServiceAccount permissions, there is a risk of privilege escalation and unauthorized
    access to Kubernetes resources. Attackers may exploit overly permissive roles associated with ServiceAccounts to perform
    lateral movements and access sensitive data or disrupt services.
  description: This control checks that all ServiceAccounts and their associated roles are regularly reviewed and aligned
    with the principle of least privilege. The review ensures that ServiceAccounts do not possess excessive permissions and
    are only granted the necessary access required for their function. Properly configured reviews help mitigate the risk
    of privilege escalation and unauthorized access by ensuring that permissions are continuously aligned with current operational
    needs and security policies.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - nist_800_53_rev5_multi_cloud_AC-2-j_0026
- rule_id: k8s.rbac.serviceaccount.scope_limited
  service: rbac
  resource: serviceaccount
  requirement: Scope Limited
  scope: rbac.serviceaccount.scope_limited
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Limit Service Account Permissions via RBAC
  rationale: If service accounts are granted excessive permissions, it increases the risk of privilege escalation and unauthorized
    actions within the cluster. Attackers could exploit overly permissive roles to access sensitive resources or disrupt workloads.
    Minimizing service account privileges restricts potential attack vectors and aligns with the principle of least privilege.
  description: This rule checks that Kubernetes RBAC configurations for service accounts are scoped to the minimum necessary
    permissions. It ensures that each service account is only granted access to the resources it needs to function, as defined
    by specific roles and role bindings. By adhering to this configuration, the security posture of the cluster is strengthened,
    as it mitigates the risk of privilege escalation and reduces the potential impact of compromised credentials.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/overview/#service-accounts
  compliance:
  - pci_dss_v4_multi_cloud_7.1.2_0090
- rule_id: k8s.rbac.serviceaccount.serviceaccount_token_automount_disabled
  service: rbac
  resource: serviceaccount
  requirement: Serviceaccount Token Automount Disabled
  scope: rbac.serviceaccount.serviceaccount_token_automount_disabled
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Disable Automounting of Service Account Tokens by Default
  rationale: If service account tokens are automatically mounted into pods, it increases the risk of token exposure and misuse.
    An attacker who gains access to a pod could potentially use the token to gain unauthorized access to cluster resources,
    escalating privileges or compromising other parts of the system. Disabling the automounting of service account tokens
    by default reduces the attack surface by ensuring that only pods that explicitly require access to the Kubernetes API
    have it.
  description: This control checks that the `automountServiceAccountToken` is set to `false` on service accounts. By default,
    Kubernetes automatically mounts service account tokens into every pod, which can lead to unnecessary exposure of credentials.
    A good configuration explicitly disables this behavior unless the pod requires access to the API server. This helps mitigate
    potential security breaches by ensuring that only designated pods have access to sensitive tokens, thereby adhering to
    the principle of least privilege.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#tokens
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IA-2_0062
  - canada_pbmm_moderate_multi_cloud_CCCS_IA-4_0064
  - fedramp_moderate_multi_cloud_AC-2_0002
  - fedramp_moderate_multi_cloud_AC-2_1_0003
  - fedramp_moderate_multi_cloud_AC-2_2_0004
  - fedramp_moderate_multi_cloud_AC-2_3_0005
  - fedramp_moderate_multi_cloud_AC-2_4_0006
  - fedramp_moderate_multi_cloud_AC-2_5_0007
  - fedramp_moderate_multi_cloud_AC-2_7_0008
  - fedramp_moderate_multi_cloud_AC-2_9_0009
  - fedramp_moderate_multi_cloud_AC-2_11_0010
  - fedramp_moderate_multi_cloud_AC-2_12_0011
  - fedramp_moderate_multi_cloud_AC-2_13_0012
  - fedramp_moderate_multi_cloud_AC-20_0046
  - fedramp_moderate_multi_cloud_AC-20_1_0047
  - fedramp_moderate_multi_cloud_AC-20_2_0048
  - fedramp_moderate_multi_cloud_AC-21_0049
  - fedramp_moderate_multi_cloud_AC-22_0050
  - fedramp_moderate_multi_cloud_IA-2_0170
  - fedramp_moderate_multi_cloud_IA-2_1_0171
  - fedramp_moderate_multi_cloud_IA-2_2_0172
  - fedramp_moderate_multi_cloud_IA-2_5_0173
  - fedramp_moderate_multi_cloud_IA-2_6_0174
  - fedramp_moderate_multi_cloud_IA-2_8_0175
  - fedramp_moderate_multi_cloud_IA-2_12_0176
  - fedramp_moderate_multi_cloud_IA-4_0178
  - fedramp_moderate_multi_cloud_IA-4_4_0179
  - fedramp_moderate_multi_cloud_IA-5_0180
  - fedramp_moderate_multi_cloud_IA-5_1_0181
  - fedramp_moderate_multi_cloud_IA-5_2_0182
  - fedramp_moderate_multi_cloud_IA-5_6_0183
  - fedramp_moderate_multi_cloud_IA-5_7_0184
  - fedramp_moderate_multi_cloud_IA-5_8_0185
  - fedramp_moderate_multi_cloud_IA-5_13_0186
  - hipaa_multi_cloud_164_308_a_3_i_0004
  - hipaa_multi_cloud_164_308_a_3_ii_a_0005
  - hipaa_multi_cloud_164_308_a_3_ii_b_0006
  - hipaa_multi_cloud_164_308_a_3_ii_c_0007
  - hipaa_multi_cloud_164_308_a_4_i_0008
  - hipaa_multi_cloud_164_308_a_4_ii_b_0010
  - hipaa_multi_cloud_164_308_a_4_ii_c_0011
  - hipaa_multi_cloud_164_312_a_1_0022
  - iso27001_2022_multi_cloud_A.5.15_0011
  - iso27001_2022_multi_cloud_A.5.18_0014
  - iso27001_2022_multi_cloud_A.5.2_0016
  - iso27001_2022_multi_cloud_A.5.20_0017
  - iso27001_2022_multi_cloud_A.5.21_0018
  - iso27001_2022_multi_cloud_A.5.23_0020
  - iso27001_2022_multi_cloud_A.5.24_0021
  - iso27001_2022_multi_cloud_A.5.25_0022
  - iso27001_2022_multi_cloud_A.5.26_0023
  - iso27001_2022_multi_cloud_A.5.27_0024
  - iso27001_2022_multi_cloud_A.5.28_0025
  - iso27001_2022_multi_cloud_A.5.29_0026
  - iso27001_2022_multi_cloud_A.8.5_0092
  - iso27001_2022_multi_cloud_A.9.2_0097
  - iso27001_2022_multi_cloud_A.9.3_0098
  - iso27001_2022_multi_cloud_A.9.4_0099
  - nist_800_171_r2_multi_cloud_3_12_4_3.12.4_Develop_document_and_periodically_0003
  - nist_800_171_r2_multi_cloud_3_1_2_3.1.2_Limit_system_access_to_the_types_of_t_0024
  - nist_800_171_r2_multi_cloud_3_1_20_3.1.20_Verify_and_control_limit_connection_0025
  - nist_800_171_r2_multi_cloud_3_3_5_3.3.5_Correlate_audit_record_review_analys_0035
  - nist_800_171_r2_multi_cloud_3_5_2_3.5.2_Authenticate_or_verify_the_identiti_0043
  - nist_800_171_r2_multi_cloud_3_5_3_3.5.3_Use_multifactor_authentication_for_lo_0044
  - pci_dss_v4_multi_cloud_2.1.1_0018
  - pci_dss_v4_multi_cloud_7.1.1_0089
  - pci_dss_v4_multi_cloud_8.1.1_0099
  - rbi_bank_multi_cloud_3.2_0015
  - rbi_nbfc_multi_cloud_2.5_0009
  - rbi_nbfc_multi_cloud_5.1_0025
- rule_id: k8s.rbac.serviceaccount.token_auth_secure_enforced
  service: rbac
  resource: serviceaccount
  requirement: Token Auth Secure Enforced
  scope: rbac.serviceaccount.token_auth_secure_enforced
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Secure Token Authentication for Service Accounts
  rationale: Misconfigured token authentication for service accounts can lead to unauthorized access to the Kubernetes API,
    allowing attackers to perform malicious actions such as privilege escalation or data exfiltration. Secure token management
    mitigates risks of token leakage and abuse.
  description: This rule checks that service account tokens are configured with secure authentication mechanisms, such as
    short-lived tokens, to prevent unauthorized API access. It ensures tokens are not overly permissive and are rotated regularly.
    Proper token configuration helps in minimizing the attack surface by adhering to the principle of least privilege and
    reducing the chance of token misuse.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance: []
- rule_id: k8s.rbac.serviceaccount.token_automount_disabled
  service: rbac
  resource: serviceaccount
  requirement: Token Automount Disabled
  scope: rbac.serviceaccount.token_automount_disabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Disable ServiceAccount Token Automount by Default
  rationale: If ServiceAccount tokens are automatically mounted, they can be inadvertently exposed, leading to potential unauthorized
    access to the Kubernetes API. This exposure can be exploited by attackers to escalate privileges or gain unauthorized
    access to sensitive resources within the cluster.
  description: This rule checks that the 'automountServiceAccountToken' field is explicitly set to 'false' in the ServiceAccount
    configuration. By disabling automatic token mounting, you limit the exposure of ServiceAccount credentials, thereby reducing
    the risk of unauthorized API access. This setting ensures that ServiceAccount tokens are only mounted when absolutely
    necessary, aligning with the principle of least privilege and minimizing the attack surface.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection
  - https://kubernetes.io/docs/concepts/configuration/secret/#service-account-token-secrets
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/
  compliance: []
- rule_id: k8s.rbac.serviceaccount.token_creation_access_check
  service: rbac
  resource: serviceaccount
  requirement: Token Creation Access Check
  scope: rbac.serviceaccount.token_creation_access_check
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Restrict ServiceAccount Token Creation Privileges
  rationale: If a ServiceAccount has excessive permissions to create tokens, it increases the risk of privilege escalation
    and unauthorized access to the Kubernetes API. Attackers could exploit these privileges to impersonate other users or
    services, potentially leading to data breaches or unauthorized modifications of cluster resources.
  description: This control checks whether ServiceAccounts have limited permissions for creating tokens. A secure configuration
    ensures that only the necessary ServiceAccounts have the capability to generate tokens, thus minimizing the exposure to
    potential compromise. By restricting token creation privileges, you mitigate the risk of privilege escalation and safeguard
    against unauthorized access to sensitive cluster operations. A good configuration involves defining precise RBAC roles
    and binding them appropriately to ServiceAccounts, ensuring that only trusted processes can create tokens when necessary.
  references:
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - cis_kubernetes_kubernetes_5.1.13_0112
- rule_id: k8s.rbac.serviceaccount.token_expiration_configured
  service: rbac
  resource: serviceaccount
  requirement: Token Expiration Configured
  scope: rbac.serviceaccount.token_expiration_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Configure ServiceAccount Token Expiration
  rationale: Without token expiration, service account tokens remain valid indefinitely, increasing the risk of token misuse
    if compromised. Attackers who obtain long-lived tokens can maintain persistent access to the Kubernetes cluster, potentially
    escalating privileges or exfiltrating sensitive data.
  description: This rule checks that ServiceAccounts in Kubernetes have token expiration configured to limit the lifetime
    of tokens issued to them. A well-configured token expiration policy ensures that tokens are automatically invalidated
    after a specified period, reducing the window of opportunity for attackers to exploit compromised tokens. This enhances
    cluster security by enforcing token renewal and minimizing the potential impact of a token leak.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance:
  - nist_800_171_r2_multi_cloud_3_5_6_3.5.6_Disable_identifiers_after_a_defined_p_0046
- rule_id: k8s.rbac.serviceaccount.unique_identifiers
  service: rbac
  resource: serviceaccount
  requirement: Unique Identifiers
  scope: rbac.serviceaccount.unique_identifiers
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Unique ServiceAccount Identifiers for RBAC
  rationale: Without unique identifiers for ServiceAccounts, it's challenging to correctly implement role-based access controls
    (RBAC), which can lead to privilege escalation and unauthorized access. Attackers could exploit non-unique identifiers
    to impersonate service accounts, gain unauthorized access to Kubernetes resources, and potentially compromise the cluster.
  description: This control checks that each ServiceAccount within the Kubernetes cluster has a unique identifier. A unique
    ServiceAccount identifier ensures that roles and permissions can be accurately assigned and audited, reducing the risk
    of privilege misuse. By confirming these identifiers are unique, you prevent security breaches that stem from misconfigured
    RBAC policies and enhance traceability and accountability within the cluster.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - nist_800_53_rev5_multi_cloud_IA-4-b_0543
- rule_id: k8s.rbac.serviceaccount.unique_serviceaccount_enforced
  service: rbac
  resource: serviceaccount
  requirement: Unique Serviceaccount Enforced
  scope: rbac.serviceaccount.unique_serviceaccount_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Ensure Each Pod Uses a Unique ServiceAccount
  rationale: Using a unique service account for each pod mitigates the risk of privilege escalation and minimizes the attack
    surface. If a service account is shared among multiple pods, a compromise of one pod can lead to unauthorized access across
    others, potentially allowing attackers to escalate privileges and access sensitive data or resources within the cluster.
  description: This control checks that each Kubernetes pod is associated with a unique service account, rather than using
    the default service account or sharing one among multiple pods. A unique service account should have the minimum necessary
    permissions for its associated pod, adhering to the principle of least privilege. Implementing this control helps contain
    potential security breaches, ensuring that a compromised pod cannot affect others by leveraging shared credentials or
    excessive permissions.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#use-case-minimum-privilege
  compliance: []
- rule_id: k8s.rbac.serviceaccount.usage_audited
  service: rbac
  resource: serviceaccount
  requirement: Usage Audited
  scope: rbac.serviceaccount.usage_audited
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Audit ServiceAccount Usage for Security Compliance
  rationale: Failure to audit ServiceAccount usage can lead to undetected unauthorized access and privilege escalation, allowing
    attackers to exploit elevated permissions and compromise the cluster's security. This is critical for identifying misuse
    and ensuring only intended actions are performed by ServiceAccounts.
  description: This control checks whether the usage of ServiceAccounts is being audited effectively. It verifies that audit
    logs capture all ServiceAccount activities, including creation, modification, and usage of associated credentials. A properly
    configured audit system helps identify security anomalies and unauthorized access attempts, thereby mitigating potential
    security risks and ensuring compliance with regulations like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  compliance:
  - rbi_bank_multi_cloud_9.3_0027
- rule_id: k8s.rbac.serviceaccount.usage_review
  service: rbac
  resource: serviceaccount
  requirement: Usage Review
  scope: rbac.serviceaccount.usage_review
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure ServiceAccount Usage Is Regularly Reviewed
  rationale: Without regular reviews of ServiceAccount usage, there is a risk of privilege escalation or unauthorized access.
    Attackers may exploit unused or overly permissive ServiceAccounts to gain access to sensitive resources or escalate their
    privileges within the cluster. Regular usage reviews help identify and mitigate these risks, ensuring that ServiceAccounts
    are only used as intended.
  description: This control checks that ServiceAccounts in your Kubernetes cluster have their usage regularly reviewed and
    audited. A good configuration involves maintaining an inventory of ServiceAccounts, monitoring their usage patterns, and
    ensuring their permissions are minimal and necessary for their function. This practice helps to mitigate risks of privilege
    escalation and unauthorized access by ensuring that ServiceAccounts do not retain unnecessary permissions or are not used
    for unintended purposes.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - fedramp_moderate_multi_cloud_AC-20_0046
  - fedramp_moderate_multi_cloud_AC-20_1_0047
  - fedramp_moderate_multi_cloud_AC-20_2_0048
  - fedramp_moderate_multi_cloud_AC-21_0049
  - fedramp_moderate_multi_cloud_AC-22_0050
  - fedramp_moderate_multi_cloud_AC-2_0002
  - fedramp_moderate_multi_cloud_AC-2_11_0010
  - fedramp_moderate_multi_cloud_AC-2_12_0011
  - fedramp_moderate_multi_cloud_AC-2_13_0012
  - fedramp_moderate_multi_cloud_AC-2_1_0003
  - fedramp_moderate_multi_cloud_AC-2_2_0004
  - fedramp_moderate_multi_cloud_AC-2_3_0005
  - fedramp_moderate_multi_cloud_AC-2_4_0006
  - fedramp_moderate_multi_cloud_AC-2_5_0007
  - fedramp_moderate_multi_cloud_AC-2_7_0008
  - fedramp_moderate_multi_cloud_AC-2_9_0009
  - nist_800_53_rev5_multi_cloud_AC-2-g_0023
- rule_id: k8s.rbac.system.masters_group_usage_check
  service: rbac
  resource: system
  requirement: Masters Group Usage Check
  scope: rbac.system.masters_group_usage_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Access to 'system:masters' RBAC Group
  rationale: If improperly configured, the 'system:masters' group can grant administrative privileges to unauthorized users,
    leading to potential full control over the Kubernetes cluster. This could be exploited to escalate privileges, modify
    cluster state, or access sensitive data.
  description: This control checks that only authorized users and service accounts are members of the 'system:masters' group,
    which inherently has cluster-admin privileges. A secure configuration ensures that access is limited to trusted administrators,
    minimizing the risk of privilege escalation and unauthorized cluster modifications. This is aligned with the principle
    of least privilege and helps comply with security standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-binding-examples
  - https://kubernetes.io/docs/setup/best-practices/certificates/#configure-certificates
  compliance:
  - cis_kubernetes_kubernetes_5.1.7_0106
- rule_id: k8s.rbac.unique.service_account_per_user
  service: rbac
  resource: unique
  requirement: Service Account Per User
  scope: rbac.unique.service_account_per_user
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Assign Unique Service Accounts for Each User
  rationale: Sharing service accounts among multiple users can lead to untracked and unauthorized access, increasing the risk
    of privilege escalation if a user's credentials are compromised. By assigning unique service accounts, you ensure accountability
    and traceability, as each user's actions can be individually monitored and audited. This configuration minimizes the impact
    of a potential breach and aligns with the principle of least privilege.
  description: This rule checks that each user operating within the Kubernetes cluster has been assigned a unique service
    account. A properly configured environment where each user has their own service account allows for granular permission
    settings tailored to the specific needs and roles of the user. It prevents users from gaining elevated privileges through
    shared accounts and simplifies auditing by associating actions directly with the responsible user. This setup aligns with
    security best practices and assists in compliance with regulatory standards by ensuring clear access boundaries and responsibilities.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/configuration/overview/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_IA-4_0064
  - hipaa_multi_cloud_164_312_a_2_i_0023
  - hipaa_multi_cloud_164_312_a_2_ii_0024
  - hipaa_multi_cloud_164_312_a_2_iv_0025
- rule_id: k8s.rbac.unused.role_cleanup
  service: rbac
  resource: unused
  requirement: Role Cleanup
  scope: rbac.unused.role_cleanup
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Remove Unused RBAC Roles
  rationale: Unused RBAC roles in Kubernetes can lead to security risks such as privilege escalation and unauthorized access.
    These roles may inadvertently provide permissions that can be exploited if compromised or mistakenly assigned. Removing
    unused roles reduces the attack surface and helps prevent threat actors from leveraging stale permissions.
  description: This rule checks for and identifies RBAC roles in the Kubernetes environment that are not currently in use.
    A role is considered unused if it is not bound to any subjects via RoleBindings or ClusterRoleBindings. By ensuring regular
    cleanup of these unused roles, organizations can minimize the risk of privilege misuse and maintain a tighter access control
    policy, aligning with security best practices and compliance requirements.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding
  - https://kubernetes.io/docs/tasks/administer-cluster/review-access/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-2_0002
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-20_0016
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-21_0017
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-22_0018
- rule_id: k8s.rbac.unused.service_account_cleanup
  service: rbac
  resource: unused
  requirement: Service Account Cleanup
  scope: rbac.unused.service_account_cleanup
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Cleanup of Unused Service Accounts
  rationale: Unused service accounts pose a risk of being exploited for unauthorized access if not properly managed. Attackers
    can leverage orphaned accounts to gain entry into the Kubernetes cluster and escalate privileges, especially if these
    accounts have excessive permissions. Proactively managing and removing unused service accounts reduces the attack surface
    and mitigates the risk of unauthorized access.
  description: This control checks for the presence of unused service accounts in the Kubernetes cluster and ensures they
    are removed or disabled. A 'good' configuration involves regularly auditing service accounts to verify their necessity
    and revoking access for those that are not in active use. By maintaining only necessary service accounts, you minimize
    the potential for privilege escalation, reduce the risk of credential compromise, and ensure compliance with security
    standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - fedramp_moderate_multi_cloud_AC-20_0046
  - fedramp_moderate_multi_cloud_AC-20_1_0047
  - fedramp_moderate_multi_cloud_AC-20_2_0048
  - fedramp_moderate_multi_cloud_AC-21_0049
  - fedramp_moderate_multi_cloud_AC-22_0050
  - fedramp_moderate_multi_cloud_AC-2_0002
  - fedramp_moderate_multi_cloud_AC-2_11_0010
  - fedramp_moderate_multi_cloud_AC-2_12_0011
  - fedramp_moderate_multi_cloud_AC-2_13_0012
  - fedramp_moderate_multi_cloud_AC-2_1_0003
  - fedramp_moderate_multi_cloud_AC-2_2_0004
  - fedramp_moderate_multi_cloud_AC-2_3_0005
  - fedramp_moderate_multi_cloud_AC-2_4_0006
  - fedramp_moderate_multi_cloud_AC-2_5_0007
  - fedramp_moderate_multi_cloud_AC-2_7_0008
  - fedramp_moderate_multi_cloud_AC-2_9_0009
  - nist_800_53_rev5_multi_cloud_AC-2-j_0026
- rule_id: k8s.rbac.user.access_review_policy_implemented
  service: rbac
  resource: user
  requirement: Access Review Policy Implemented
  scope: rbac.user.access_review_policy_implemented
  domain: identity_and_access_management
  subcategory: access_control
  severity: medium
  title: Implement RBAC User Access Review Policies
  rationale: Without periodic access reviews, users may retain permissions they no longer need, leading to potential security
    risks such as privilege escalation and unauthorized data access. Attackers could exploit stale or excessive permissions
    to gain access to sensitive resources.
  description: This control checks whether Kubernetes RBAC users have access review policies in place, ensuring that user
    permissions are regularly audited and updated as needed. A proper configuration involves defining policies that mandate
    periodic reviews of user permissions, removing unnecessary privileges, and adhering to the principle of least privilege.
    This reduces the attack surface by ensuring only necessary permissions are granted, thus mitigating risks associated with
    privilege misuse.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#rbac
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - nist_800_171_r2_multi_cloud_3_5_6_3.5.6_Disable_identifiers_after_a_defined_p_0046
- rule_id: k8s.rbac.user.access_review_scheduled
  service: rbac
  resource: user
  requirement: Access Review Scheduled
  scope: rbac.user.access_review_scheduled
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Schedule Regular RBAC User Access Reviews
  rationale: Failing to regularly review RBAC user access can lead to privilege creep, where users accumulate permissions
    they no longer need, increasing the risk of unauthorized access and data breaches. Attackers could exploit outdated permissions
    to gain elevated access and compromise the cluster's security.
  description: This control ensures that Kubernetes RBAC user access reviews are scheduled and conducted regularly. It validates
    that the access review process is configured to examine user permissions, ensuring they align with the principle of least
    privilege. Proper configuration involves setting up automated reminders or tasks to review and adjust user roles and permissions
    as necessary. This reduces the risk of unauthorized access and potential privilege escalation, thus enhancing the overall
    security posture of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/concepts/security/rbac-good-practices/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-20_0016
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-21_0017
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-22_0018
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-2_0002
  - pci_dss_v4_multi_cloud_8.2.6_0104
- rule_id: k8s.rbac.user.action_audit_enabled
  service: rbac
  resource: user
  requirement: Action Audit Enabled
  scope: rbac.user.action_audit_enabled
  domain: logging_and_monitoring
  subcategory: audit_logging
  severity: high
  title: Enable Audit Logging for RBAC User Actions
  rationale: Without enabling audit logging for RBAC user actions, malicious activities such as privilege escalation and unauthorized
    access may go undetected. This could lead to data breaches or unauthorized resource manipulation, compromising the integrity
    and availability of the Kubernetes cluster.
  description: This check ensures that audit logging is enabled for actions performed by RBAC users. It verifies that the
    audit policy configuration captures all critical operations performed by users with RBAC roles. Properly configured audit
    logging allows for the detection and investigation of suspicious activities, thereby enhancing security monitoring and
    ensuring compliance with industry standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/#logging-and-monitoring
  compliance:
  - nist_800_171_r2_multi_cloud_3_3_2_3.3.2_Ensure_that_the_actions_of_individual_0032
- rule_id: k8s.rbac.user.identity_audit_enabled
  service: rbac
  resource: user
  requirement: Identity Audit Enabled
  scope: rbac.user.identity_audit_enabled
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enable and Monitor RBAC User Identity Auditing
  rationale: Without proper identity auditing in RBAC, unauthorized access and malicious activities may go undetected. Attackers
    could exploit weak audit trails to cover their tracks after gaining unauthorized access to the Kubernetes cluster, potentially
    exfiltrating data or modifying configurations without detection.
  description: This control ensures that RBAC user identity auditing is enabled, capturing detailed logs of user activities
    within the Kubernetes cluster. Proper configuration involves setting up audit policies that log who accessed what resources
    and when. This audit trail is crucial for forensic analysis and compliance reporting, providing visibility into user actions
    and helping to identify suspicious behavior or unauthorized access attempts.
  references:
  - https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/cluster-administration/logging/
  compliance:
  - hipaa_multi_cloud_164_312_a_2_i_0023
  - hipaa_multi_cloud_164_312_a_2_ii_0024
  - hipaa_multi_cloud_164_312_a_2_iv_0025
- rule_id: k8s.rbac.user.registration_authorization_review
  service: rbac
  resource: user
  requirement: Registration Authorization Review
  scope: rbac.user.registration_authorization_review
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce RBAC Authorization for User Registration
  rationale: Without proper RBAC authorization for user registration, there is a risk of unauthorized users gaining access
    to sensitive resources within the cluster. This could lead to privilege escalation, data breaches, or malicious activities
    by impersonating legitimate users.
  description: This control ensures that RBAC policies are correctly applied to manage user registration and authorization.
    It checks for appropriate role bindings that limit user capabilities to only authorized operations. Proper configuration
    involves defining roles and permissions that align with the principle of least privilege, ensuring that users can only
    access resources necessary for their function, thus minimizing potential attack vectors.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/reference/access-authn-authz/authorization/
  compliance:
  - soc2_multi_cloud_cc_6_2_0010
- rule_id: k8s.rbac.user.unique_identifiers
  service: rbac
  resource: user
  requirement: Unique Identifiers
  scope: rbac.user.unique_identifiers
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Unique User Identifiers in RBAC
  rationale: Using non-unique identifiers for Kubernetes RBAC users can lead to privilege escalation and unauthorized access.
    Attackers may exploit these identifiers to impersonate users or gain elevated permissions, thereby compromising the cluster's
    integrity and confidentiality.
  description: This rule checks that each Kubernetes RBAC user is assigned a unique identifier to prevent identity overlap
    and maintain clear audit trails. A good configuration ensures that every user has distinct credentials, reducing the risk
    of identity spoofing and unauthorized access. This practice is essential for tracking user actions accurately and aligns
    with security best practices, helping to enforce principle of least privilege and support compliance with industry standards.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/configuration/overview/
  compliance:
  - nist_800_53_rev5_multi_cloud_IA-4-b_0543
- rule_id: k8s.rbac.webhook.configuration_access_review
  service: rbac
  resource: webhook
  requirement: Configuration Access Review
  scope: rbac.webhook.configuration_access_review
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Enforce RBAC Webhook Access Review for Configuration Changes
  rationale: If the RBAC webhook configuration is not properly reviewed, it may lead to unauthorized access or privilege escalation
    within the Kubernetes cluster. Attackers could exploit misconfigured RBAC settings to gain elevated privileges, execute
    arbitrary code, or access sensitive data, compromising the entire cluster's security.
  description: This rule validates that the Kubernetes RBAC webhook is configured to perform access reviews of any configuration
    changes. A proper review involves checking that only authorized entities have permissions for key operations like creating,
    modifying, or deleting critical resources. Correct configuration helps in ensuring that access controls are strictly enforced,
    which minimizes the risk of unauthorized access and supports compliance with security standards such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - cis_kubernetes_kubernetes_5.1.12_0111
- rule_id: k8s.resource.annotation.standard
  service: resource
  resource: annotation
  requirement: Standard
  scope: resource.annotation.standard
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Standardized Resource Annotations
  rationale: Inconsistent or absent resource annotations can lead to mismanaged resources, inefficient operations, and increased
    risk of unauthorized access. Without standardized annotations, it is difficult to enforce policies, track resources for
    auditing purposes, or apply automated security checks. Attackers may exploit these misconfigurations to gain unauthorized
    insights into the cluster configuration or access sensitive workloads.
  description: This control checks that all Kubernetes resources have standardized annotations as defined by organizational
    policy. Proper annotations help automate policy enforcement, streamline resource management, and enhance auditing capabilities.
    Standardized annotations can include security-related information, such as compliance tags or environment indicators,
    which are crucial for maintaining security controls and ensuring compliance with regulations such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/
  - https://kubernetes.io/docs/tasks/administer-cluster/enforce-policies/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-8-a_0424
- rule_id: k8s.resource.annotations.for_inventory_metadata
  service: resource
  resource: annotations
  requirement: For Inventory Metadata
  scope: resource.annotations.for_inventory_metadata
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Kubernetes Resource Annotations Include Inventory Metadata
  rationale: Without proper inventory metadata annotations, tracking and managing Kubernetes resources becomes challenging,
    increasing the risk of configuration drift, unauthorized modifications, and compliance violations. Attackers can exploit
    poorly documented resources to establish persistence or escalate privileges within the cluster.
  description: This rule checks that all Kubernetes resources are annotated with standardized inventory metadata. Proper annotations
    facilitate better asset management, streamline audits, and enhance traceability. A well-annotated resource includes metadata
    that identifies ownership, purpose, and compliance requirements, which collectively help in maintaining a secure and compliant
    Kubernetes environment by enabling better monitoring and incident response.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#annotating-resources
  - https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-8_0048
- rule_id: k8s.resource.annotations.maintained
  service: resource
  resource: annotations
  requirement: Maintained
  scope: resource.annotations.maintained
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Consistent and Secure Resource Annotations
  rationale: If resource annotations are not consistently maintained, it can lead to misconfigurations that attackers might
    exploit to gain unauthorized access or disrupt services. Annotations can be used to convey security policies or operational
    metadata, and mismanaged annotations could result in non-compliance with security baselines, making the cluster vulnerable
    to attacks such as privilege escalation or data exfiltration.
  description: This control checks that all Kubernetes resource annotations are maintained according to a predefined security
    policy. It ensures that annotations, which may include important security metadata, are correctly configured and not left
    in a default or insecure state. Properly configured annotations help in enforcing security policies, operational automation,
    and auditing, thereby reducing the risk of security breaches and ensuring compliance with security standards such as the
    CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - nist_800_53_rev5_multi_cloud_CM-8-b_0425
- rule_id: k8s.scheduler.bind.address_check
  service: scheduler
  resource: bind
  requirement: Address Check
  scope: scheduler.bind.address_check
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Secure Scheduler Bind Address Configuration
  rationale: Misconfiguring the scheduler bind address can expose the Kubernetes scheduler to unauthorized network access,
    leading to potential denial-of-service attacks or unauthorized command execution. By limiting the bind address to localhost
    or a secure internal network, you reduce the attack surface and protect against external threats.
  description: This rule checks that the Kubernetes scheduler is configured to bind only to a secure address, such as 127.0.0.1
    or an internal IP range, instead of binding to all network interfaces (0.0.0.0). A secure configuration prevents unauthorized
    external access to the scheduler, which is critical for maintaining cluster integrity and security. Proper configuration
    ensures compliance with the CIS Kubernetes Benchmark and reduces risk by limiting potential entry points for attackers.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-scheduler-access
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-traffic-to-control-plane
  - https://kubernetes.io/docs/concepts/architecture/control-plane-node/#kube-scheduler
  compliance:
  - cis_kubernetes_kubernetes_1.4.2_0060
- rule_id: k8s.scheduler.conf.file_permissions_check
  service: scheduler
  resource: conf
  requirement: File Permissions Check
  scope: scheduler.conf.file_permissions_check
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Secure File Permissions for Scheduler Configuration
  rationale: Improper file permissions on the Kubernetes scheduler configuration file can lead to unauthorized access and
    manipulation by malicious users. This could allow attackers to alter scheduler behavior, potentially bypassing resource
    allocation constraints or disrupting service availability. Ensuring correct file permissions mitigates risks such as privilege
    escalation and unauthorized configuration changes.
  description: This rule checks that the Kubernetes scheduler configuration file has secure permissions set, typically allowing
    read and write access only to the root user. A secure configuration involves setting file permissions to '644' or stricter,
    preventing unauthorized users from modifying or viewing sensitive scheduler settings. Ensuring proper permissions helps
    protect the integrity and confidentiality of the scheduler configuration, aligning with CIS Kubernetes Benchmark recommendations.
  references:
  - https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/setup/best-practices/certificates/
  compliance:
  - cis_kubernetes_kubernetes_1.1.15_0015
- rule_id: k8s.secret.access.control_configured
  service: secret
  resource: access
  requirement: Control Configured
  scope: secret.access.control_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure RBAC Limits Access to Secrets
  rationale: Improperly configured access controls for Kubernetes secrets can lead to unauthorized access, resulting in potential
    data breaches. Attackers could exploit weak access controls to retrieve sensitive information such as database credentials
    or API keys, which can be used to further penetrate the infrastructure.
  description: This rule checks that Role-Based Access Control (RBAC) is properly configured to restrict access to Kubernetes
    secrets only to authorized users and service accounts. A secure configuration involves defining roles with the least privilege
    necessary and binding them to specific users or service accounts. This prevents unauthorized access to sensitive data
    stored in secrets and helps maintain the confidentiality and integrity of your applications.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - nist_800_53_rev5_multi_cloud_IA-5-g_0578
- rule_id: k8s.secret.backup.and_recovery_enabled
  service: secret
  resource: backup
  requirement: And Recovery Enabled
  scope: secret.backup.and_recovery_enabled
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: low
  title: Ensure Backup and Recovery of Kubernetes Secrets
  rationale: Without proper backup and recovery mechanisms for Kubernetes secrets, there is a risk of data loss and disruption
    in case of accidental deletion, corruption, or malicious activity. Attackers could exploit the absence of secret backups
    to escalate privileges or access sensitive information, compromising the entire cluster's security posture.
  description: This control checks that Kubernetes secrets have a robust backup and recovery process in place. A good configuration
    involves automated, regular backups of secrets stored securely and independently from the cluster. Additionally, recovery
    procedures should be tested to ensure that secrets can be quickly restored to maintain application availability and integrity.
    This helps prevent unauthorized access and data loss, ensuring compliance with security standards.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  compliance:
  - soc2_multi_cloud_a1_2_0025
- rule_id: k8s.secret.backup.policy_configured
  service: secret
  resource: backup
  requirement: Policy Configured
  scope: secret.backup.policy_configured
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Backup Policies for Kubernetes Secrets
  rationale: Without properly configured backup policies, sensitive data stored in Kubernetes secrets is at risk of loss or
    unauthorized access. Attackers can exploit misconfigurations to access sensitive information or disrupt services, potentially
    leading to data breaches or compliance violations.
  description: This rule checks that backup policies for Kubernetes secrets are configured according to security best practices.
    It ensures that secrets are regularly backed up in a secure manner, with measures in place to prevent unauthorized access
    during backup operations. A well-configured policy includes encryption, access controls, and regular validation of backup
    integrity. This reduces the risk of data loss, unauthorized data access, and aids in disaster recovery processes, thereby
    enhancing the overall security posture of the Kubernetes environment.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CP-9_0059
  - pci_dss_v4_multi_cloud_11.3.1.2_0167
  - pci_dss_v4_multi_cloud_3.1.2_0033
  - pci_dss_v4_multi_cloud_3.3.1.2_0035
- rule_id: k8s.secret.certificate.expiration_monitoring_enabled
  service: secret
  resource: certificate
  requirement: Expiration Monitoring Enabled
  scope: secret.certificate.expiration_monitoring_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Certificate Expiration Monitoring for Kubernetes Secrets
  rationale: Without expiration monitoring, expired certificates can remain in use, leading to potential unauthorized access
    and man-in-the-middle attacks. Malicious actors can exploit expired certificates to intercept or alter data, compromising
    confidentiality and integrity. This is crucial for maintaining trust boundaries and ensuring service availability.
  description: This rule checks that expiration monitoring is enabled for certificates stored in Kubernetes Secrets. A good
    configuration involves automated alerts for upcoming expirations and timely certificate renewal processes. By ensuring
    that all certificates are monitored for expiration, it helps prevent disruptions in service continuity, avoids potential
    security breaches due to expired certificates, and supports compliance with security standards such as the CIS Kubernetes
    Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/tls/certificate-rotation/
  - https://kubernetes.io/docs/tasks/administer-cluster/certificates/
  compliance:
  - rbi_bank_multi_cloud_17.1_0011
  - rbi_bank_multi_cloud_7.1_0023
- rule_id: k8s.secret.certificate.renewal_process_defined
  service: secret
  resource: certificate
  requirement: Renewal Process Defined
  scope: secret.certificate.renewal_process_defined
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Certificate Secret Renewal Processes are Defined and Automated
  rationale: If the renewal process for certificate secrets is undefined or not automated, expired certificates can lead to
    service disruptions and security vulnerabilities, such as man-in-the-middle attacks or unauthorized access. An automated
    renewal process ensures certificates are updated before expiration, maintaining secure communications.
  description: This control checks for the presence of a defined and automated renewal process for Kubernetes certificate
    secrets. A good configuration involves using tools such as cert-manager to automate the renewal of TLS certificates used
    within Kubernetes. By ensuring that certificate secrets are automatically renewed, we mitigate the risk of service downtime
    due to expired certificates and reduce the likelihood of security breaches arising from outdated cryptographic materials.
  references:
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://cert-manager.io/docs/usage/certificate/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  compliance:
  - rbi_bank_multi_cloud_17.1_0011
  - rbi_bank_multi_cloud_7.1_0023
- rule_id: k8s.secret.certificate.revocation_list_configured
  service: secret
  resource: certificate
  requirement: Revocation List Configured
  scope: secret.certificate.revocation_list_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Certificate Revocation Lists are Configured for Kubernetes Secrets
  rationale: Without a properly configured Certificate Revocation List (CRL), Kubernetes secrets using certificates may rely
    on invalid or compromised credentials, leading to unauthorized access and potential data breaches. Attackers could exploit
    this gap to impersonate services or intercept sensitive data, undermining the cluster's security and integrity.
  description: This rule verifies that all certificates managed as Kubernetes secrets have an associated and up-to-date Certificate
    Revocation List (CRL). A correctly configured CRL ensures that any compromised or expired certificates are effectively
    revoked, preventing their misuse. This validation involves checking CRL distribution points and ensuring they are accessible
    and current. Proper CRL configuration helps maintain the confidentiality and integrity of communication within the cluster,
    supporting compliance with standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/certificates/
  compliance:
  - rbi_bank_multi_cloud_17.1_0011
  - rbi_bank_multi_cloud_7.1_0023
- rule_id: k8s.secret.configmap.sensitive_data_prohibited_enforced
  service: secret
  resource: configmap
  requirement: Sensitive Data Prohibited Enforced
  scope: secret.configmap.sensitive_data_prohibited_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Prevent Sensitive Data Storage in ConfigMaps
  rationale: Storing sensitive data in ConfigMaps poses a security risk as they lack encryption and access controls inherent
    to secrets, making them susceptible to unauthorized access and data leakage. Attackers exploiting misconfigured ConfigMaps
    can gain access to sensitive information, leading to data breaches and potential compliance violations.
  description: This control checks that ConfigMaps do not contain sensitive data by scanning for patterns or keywords indicative
    of sensitive information, such as passwords or API keys. A correctly configured environment will use Kubernetes Secrets
    for managing sensitive data, which provides built-in encryption at rest and more granular access controls. Implementing
    this control enhances the security posture by reducing the risk of sensitive data exposure and aligns with best practices
    for secure configuration management.
  references:
  - https://kubernetes.io/docs/concepts/configuration/configmap/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.secret.database.credentials_encrypted
  service: secret
  resource: database
  requirement: Credentials Encrypted
  scope: secret.database.credentials_encrypted
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Ensure Encryption of Database Credentials in Kubernetes Secrets
  rationale: Encrypting database credentials stored in Kubernetes Secrets is crucial to protect sensitive information from
    unauthorized access and data breaches. Without encryption, attackers who gain access to the etcd datastore can easily
    retrieve plaintext credentials, leading to potential compromise of database systems. This control mitigates risks associated
    with data exfiltration and helps in adhering to data protection regulations.
  description: This rule checks that all database credentials stored in Kubernetes Secrets are encrypted using strong encryption
    methods before being stored in the etcd datastore. Proper encryption of secrets ensures that sensitive information is
    protected at rest and significantly reduces the attack surface. By implementing this control, organizations can prevent
    unauthorized access to sensitive database information and comply with security standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  compliance:
  - rbi_bank_multi_cloud_8.2_0025
- rule_id: k8s.secret.encryption.at_rest_enabled
  service: secret
  resource: encryption
  requirement: At Rest Enabled
  scope: secret.encryption.at_rest_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Encryption at Rest for Kubernetes Secrets
  rationale: If Kubernetes secrets are not encrypted at rest, they are vulnerable to unauthorized access by anyone with file
    system access to the etcd database. This can lead to exposure of sensitive data, such as passwords and API keys, which
    could be exploited by attackers to gain unauthorized access to cluster resources.
  description: This rule checks that encryption at rest is enabled for Kubernetes secrets by verifying that the etcd database
    is configured with an encryption provider. A proper configuration involves setting up an appropriate encryption provider
    and key management system. Ensuring that secrets are encrypted at rest reduces the risk of data breaches by protecting
    sensitive information stored in the etcd database from being accessed in plaintext.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#data-encryption-at-rest
  - https://kubernetes.io/docs/concepts/configuration/secret/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CM-9_0049
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-12_0135
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-13_0136
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-28_0144
  - fedramp_moderate_multi_cloud_CM-6_0116
  - fedramp_moderate_multi_cloud_CM-6_1_0117
  - fedramp_moderate_multi_cloud_CM-6_2_0118
  - fedramp_moderate_multi_cloud_CP-9_0160
  - fedramp_moderate_multi_cloud_CP-9_1_0161
  - fedramp_moderate_multi_cloud_CP-9_2_0162
  - fedramp_moderate_multi_cloud_CP-9_3_0163
  - fedramp_moderate_multi_cloud_CP-9_5_0164
  - fedramp_moderate_multi_cloud_CP-9_8_0165
  - fedramp_moderate_multi_cloud_SC-12_0346
  - fedramp_moderate_multi_cloud_SC-12_1_0347
  - fedramp_moderate_multi_cloud_SC-13_0348
  - fedramp_moderate_multi_cloud_SC-28_0357
  - fedramp_moderate_multi_cloud_SC-28_1_0358
  - gdpr_multi_cloud_Article_25_Data_protection_by_design_and_by_defaul_0001
  - gdpr_multi_cloud_Article_32_Security_of_processing_0003
  - hipaa_multi_cloud_164_308_a_1_ii_b_0002
  - hipaa_multi_cloud_164_312_c_1_0027
  - hipaa_multi_cloud_164_312_c_2_0028
  - hipaa_multi_cloud_164_312_e_2_i_0031
  - hipaa_multi_cloud_164_312_e_2_ii_0032
  - iso27001_2022_multi_cloud_A.10.1_0001
  - iso27001_2022_multi_cloud_A.8.24_0080
  - iso27001_2022_multi_cloud_A.8.27_0083
  - iso27001_2022_multi_cloud_A.8.3_0085
  - iso27001_2022_multi_cloud_A.8.30_0086
  - iso27001_2022_multi_cloud_A.8.31_0087
  - iso27001_2022_multi_cloud_A.8.32_0088
  - iso27001_2022_multi_cloud_A.8.33_0089
  - iso27001_2022_multi_cloud_A.8.34_0090
  - iso27001_2022_multi_cloud_A.8.9_0096
  - nist_800_171_r2_multi_cloud_3_5_10_3.5.10_Store_and_transmit_only_cryptograph_0042
  - nist_800_53_rev5_multi_cloud_CM-6-a_0397
  - nist_800_53_rev5_multi_cloud_CP-9-d_0512
  - nist_800_53_rev5_multi_cloud_IA-5-g_0578
  - nist_800_53_rev5_multi_cloud_PM-11-b_0867
  - nist_800_53_rev5_multi_cloud_SC-13-a_1248
  - pci_dss_v4_multi_cloud_3.1.1_0032
  - pci_dss_v4_multi_cloud_3.3.1.1_0034
  - pci_dss_v4_multi_cloud_3.4.1_0039
  - pci_dss_v4_multi_cloud_3.4.2_0040
  - pci_dss_v4_multi_cloud_3.5.1.2_0042
  - pci_dss_v4_multi_cloud_8.2.1_0101
  - rbi_bank_multi_cloud_11.1_0004
  - rbi_bank_multi_cloud_18.1_0012
  - rbi_bank_multi_cloud_3.3_0016
  - rbi_bank_multi_cloud_8.1_0024
  - rbi_nbfc_multi_cloud_2.9_0013
  - rbi_nbfc_multi_cloud_6.1_0029
  - soc2_multi_cloud_cc_3_1_0003
  - soc2_multi_cloud_cc_6_1_0009
  - soc2_multi_cloud_cc_a_1_1_0021
  - soc2_multi_cloud_cc_c_1_1_0022
- rule_id: k8s.secret.external.manager_configured
  service: secret
  resource: external
  requirement: Manager Configured
  scope: secret.external.manager_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure External Secrets Manager is Configured Properly
  rationale: Improperly configured external secret managers can lead to unauthorized access and potential exfiltration of
    sensitive data. Attackers could exploit misconfigurations to gain access to credentials, API keys, or other sensitive
    information stored in secrets, leading to potential breaches or data leaks.
  description: This rule checks that an external secret manager is properly configured to manage Kubernetes secrets. A correct
    configuration ensures that secrets are securely stored and accessed, thereby preventing unauthorized access and potential
    data leaks. Proper configuration involves setting up access controls, ensuring encryption is enabled, and verifying that
    only authorized entities have access to the secret manager. This practice not only reduces security risks but also helps
    in adhering to compliance requirements such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-12_0135
  - pci_dss_v4_multi_cloud_3.6.1.4_0048
- rule_id: k8s.secret.git.repo_url_no_sensitive_credentials
  service: secret
  resource: git
  requirement: Repo Url No Sensitive Credentials
  scope: secret.git.repo_url_no_sensitive_credentials
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Prevent Sensitive Credentials in Git Repository URLs
  rationale: Including sensitive credentials like usernames, passwords, or tokens directly in Git repository URLs within Kubernetes
    secrets poses a significant security risk. Attackers who gain access to these URLs can easily access your repositories,
    potentially leading to code leaks, data breaches, or unauthorized code modifications. This vulnerability can be exploited
    through misconfigured access controls or by intercepting traffic, especially if not encrypted.
  description: This control checks Kubernetes secrets for Git repository URLs to ensure they do not contain sensitive credentials.
    A secure configuration involves using SSH keys or OAuth tokens managed outside of the repository URL to authenticate access
    to Git repositories. This practice mitigates the risk of credential exposure, reduces the attack surface for unauthorized
    access, and aligns with best practices for secret management and regulatory compliance.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  compliance:
  - pci_dss_v4_multi_cloud_6.3.2_0080
- rule_id: k8s.secret.key.rotation_policy_defined
  service: secret
  resource: key
  requirement: Rotation Policy Defined
  scope: secret.key.rotation_policy_defined
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Define and Enforce Secret Key Rotation Policies
  rationale: If secret keys are not rotated regularly, they can become vulnerable to compromise through prolonged exposure,
    increasing the risk of unauthorized access. Attackers might exploit stale keys to maintain persistent access to sensitive
    data or services, leading to potential data breaches and compliance violations.
  description: This rule checks whether Kubernetes secret keys have clearly defined rotation policies. It ensures that secret
    keys are rotated within a specified timeframe, minimizing the risk of key compromise. Proper configuration involves setting
    up automatic rotation mechanisms via tools like external secrets or custom Kubernetes controllers. This not only reduces
    the window of opportunity for attackers but also helps organizations meet compliance requirements and follow industry
    best practices.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - fedramp_moderate_multi_cloud_SC-12_0346
  - fedramp_moderate_multi_cloud_SC-12_1_0347
- rule_id: k8s.secret.management.policies_documented
  service: secret
  resource: management
  requirement: Policies Documented
  scope: secret.management.policies_documented
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Documentation of Secret Management Policies
  rationale: Undocumented secret management policies can lead to misconfigurations and unauthorized access, exposing sensitive
    data and increasing the risk of insider threats and data leaks. Attackers could exploit these weaknesses to access confidential
    information stored in secrets, potentially compromising the entire cluster.
  description: This rule verifies that there are documented policies for managing Kubernetes secrets. Proper documentation
    includes guidelines on creation, storage, rotation, and access control of secrets. A well-documented policy ensures consistent
    handling of secrets, reduces the risk of human error, and facilitates adherence to compliance standards by providing a
    clear framework for securing sensitive information.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - iso27001_2022_multi_cloud_A.5.1_0005
  - iso27001_2022_multi_cloud_A.5.10_0006
  - iso27001_2022_multi_cloud_A.5.11_0007
  - iso27001_2022_multi_cloud_A.5.12_0008
  - iso27001_2022_multi_cloud_A.5.13_0009
  - iso27001_2022_multi_cloud_A.5.14_0010
  - iso27001_2022_multi_cloud_A.5.16_0012
  - iso27001_2022_multi_cloud_A.5.17_0013
- rule_id: k8s.secret.minimize.key_storage_locations
  service: secret
  resource: minimize
  requirement: Key Storage Locations
  scope: secret.minimize.key_storage_locations
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Limit Kubernetes Secret Storage to Secure Locations
  rationale: Storing Kubernetes secrets in multiple or insecure locations increases the risk of unauthorized access, data
    leaks, and potential compromise of sensitive information. Attackers may exploit misconfigured storage to access secrets,
    leading to unauthorized actions within the cluster.
  description: This control ensures that Kubernetes secrets are only stored in secure, designated locations such as encrypted
    etcd or an external secret management solution. The configuration should enforce that secrets are not stored in plaintext
    files or logs. By validating secret storage, organizations can reduce the attack surface and enhance compliance with security
    frameworks, preventing unauthorized access and maintaining data integrity.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - pci_dss_v4_multi_cloud_3.6.1.4_0048
- rule_id: k8s.secret.networkpolicy.dlp_policies_enforced
  service: secret
  resource: networkpolicy
  requirement: Dlp Policies Enforced
  scope: secret.networkpolicy.dlp_policies_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Data Loss Prevention Policies on NetworkPolicies for Secrets
  rationale: Without proper Data Loss Prevention (DLP) policies, sensitive information stored in Kubernetes Secrets can be
    exfiltrated through network communications. Attackers could exploit misconfigured NetworkPolicies to access and steal
    confidential data, leading to potential data breaches and non-compliance with regulations such as GDPR or HIPAA.
  description: This control checks that NetworkPolicies are configured to enforce DLP policies specifically for Kubernetes
    secrets. It ensures that only authorized applications can communicate with pods handling sensitive data, reducing the
    risk of data leakage. A compliant configuration involves defining NetworkPolicies that restrict ingress and egress traffic
    to only those entities that require access, thereby minimizing the attack surface.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.secret.networkpolicy.gdpr_compliance_enabled
  service: secret
  resource: networkpolicy
  requirement: Gdpr Compliance Enabled
  scope: secret.networkpolicy.gdpr_compliance_enabled
  domain: vulnerability_management
  subcategory: vulnerability_scanning
  severity: low
  title: Ensure NetworkPolicy Restricts Access to Secrets
  rationale: If NetworkPolicies do not restrict access to Kubernetes Secrets, sensitive data can be exposed to unauthorized
    pods. This misconfiguration can lead to data breaches and non-compliance with GDPR by allowing unregulated access to personal
    data stored in Secrets. Attackers may exploit overly permissive network policies to intercept or manipulate sensitive
    information.
  description: This control checks that NetworkPolicies are configured to explicitly restrict pod access to Kubernetes Secrets.
    A properly configured NetworkPolicy should limit traffic to only those pods that need access, thereby reducing the attack
    surface. By ensuring that only authorized pods can communicate with the kube-system namespace or any namespaces holding
    Secrets, it helps in minimizing the risk of data leakage and supports compliance with GDPR regulations.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  compliance: []
- rule_id: k8s.secret.networkpolicy.transfer_controls_enabled
  service: secret
  resource: networkpolicy
  requirement: Transfer Controls Enabled
  scope: secret.networkpolicy.transfer_controls_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce NetworkPolicies for Secret Transfer Control
  rationale: If NetworkPolicies are not enforced, unauthorized entities may access or intercept sensitive data, such as Kubernetes
    Secrets, during network communication. This could lead to data breaches, privilege escalation, and compliance violations.
    Attackers could exploit open network policies to perform man-in-the-middle attacks or exfiltrate secrets, compromising
    the cluster's security.
  description: This rule checks whether NetworkPolicies are implemented to control the transfer of Kubernetes Secrets between
    pods. A secure configuration ensures that only designated pods can access secrets, reducing the attack surface by restricting
    network communication paths. NetworkPolicies should specify ingress and egress rules that limit access to secrets to only
    those pods that require them. This helps prevent unauthorized access and data leakage, ensuring adherence to security
    best practices and regulatory standards.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.secret.no.sensitive_data_in_env
  service: secret
  resource: 'no'
  requirement: Sensitive Data In Env
  scope: secret.no.sensitive_data_in_env
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Prevent Sensitive Data Exposure in Environment Variables
  rationale: Storing sensitive data in environment variables increases the risk of accidental exposure through logs, process
    listings, or unauthorized access. Misconfiguration can lead to data breaches where attackers exploit vulnerable components
    or gain unauthorized access to pods, exposing sensitive information.
  description: This control checks that sensitive data, such as credentials or API keys, are not stored in environment variables
    within Kubernetes pods. Instead, it ensures that Kubernetes Secrets are used for managing sensitive information. Proper
    usage of Secrets reduces the risk of inadvertent exposure and provides a secure mechanism for accessing sensitive data.
    This is achieved by validating pod configurations to ensure that secrets are mounted as files or accessed through appropriate
    APIs, rather than being directly injected as environment variables, which can be exposed more easily.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - pci_dss_v4_multi_cloud_6.3.2_0080
- rule_id: k8s.secret.not.in_env_variables
  service: secret
  resource: not
  requirement: In Env Variables
  scope: secret.not.in_env_variables
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Avoid Storing Secrets in Environment Variables
  rationale: Storing secrets in environment variables can expose sensitive information if a container is compromised. Attackers
    who gain access to the environment variables of a container can obtain these secrets, potentially leading to unauthorized
    access to other services or resources.
  description: This rule checks that Kubernetes secrets are not injected into containers via environment variables. Instead,
    it should validate that secrets are mounted as volumes or accessed through Kubernetes' native secret management capabilities.
    Properly managing secrets this way minimizes the exposure of sensitive data, reduces the attack surface, and aligns with
    security best practices like those outlined in the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#configuring-environment-variables
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  compliance:
  - rbi_nbfc_multi_cloud_2.9_0013
  - rbi_nbfc_multi_cloud_6.1_0029
  - soc2_multi_cloud_cc_c_1_1_0022
- rule_id: k8s.secret.pod.labels_enforced_enabled
  service: secret
  resource: pod
  requirement: Labels Enforced Enabled
  scope: secret.pod.labels_enforced_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Labeling on Secret-Handling Pods
  rationale: Without enforced labeling on pods handling secrets, security teams cannot efficiently track and manage access,
    leading to unauthorized access and potential data breaches. Attackers can exploit misconfigured or unlabeled pods to gain
    access to sensitive information, bypass auditing, and escalate privileges.
  description: This rule checks if all pods managing Kubernetes Secrets have mandatory labels applied. Enforced labeling ensures
    that these pods are consistently categorized and managed according to security policies. Proper labeling enhances visibility,
    facilitates effective monitoring and auditing, and helps in implementing fine-grained access controls. It ensures compliance
    with best practices outlined in security benchmarks, reducing the risk of unauthorized access to sensitive data.
  references:
  - https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/
  compliance: []
- rule_id: k8s.secret.rotation.policy_configured
  service: secret
  resource: rotation
  requirement: Policy Configured
  scope: secret.rotation.policy_configured
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Kubernetes Secret Rotation Policies
  rationale: Without a secret rotation policy, sensitive data such as passwords, tokens, and keys can become stale and susceptible
    to exposure through prolonged use. Attackers might exploit long-lived secrets to gain unauthorized access, leading to
    potential data breaches and system compromises. Implementing a rotation policy ensures secrets are regularly updated,
    minimizing the risk of exploitation from leaked or compromised credentials.
  description: This control checks whether Kubernetes clusters have a secret rotation policy in place, ensuring that secrets
    are periodically updated in line with security best practices. A well-configured rotation policy defines the frequency
    of updates and the mechanisms for replacing secrets, thereby reducing the window of opportunity for attackers to exploit
    long-lived secrets. Proper implementation of these policies aligns with compliance requirements and enhances the overall
    security posture by ensuring that only up-to-date and secure credentials are in use.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-12_0135
  - rbi_bank_multi_cloud_11.1_0004
  - soc2_multi_cloud_cc_c_1_2_0023
- rule_id: k8s.secret.secret.access_monitoring_enabled
  service: secret
  resource: secret
  requirement: Access Monitoring Enabled
  scope: secret.secret.access_monitoring_enabled
  domain: identity_and_access_management
  subcategory: access_control
  severity: medium
  title: Enable Audit Logging for Secret Access
  rationale: Without audit logging of secret access, unauthorized access attempts or data exfiltration activities remain undetected,
    increasing the risk of data breaches. Attackers can exploit this to gain access to sensitive data, move laterally within
    the cluster, or escalate privileges without leaving traceable evidence.
  description: This rule checks that audit logging is enabled for accessing Kubernetes secrets. Properly configured audit
    logs will record all access attempts to secrets, providing an audit trail that can be analyzed to detect unauthorized
    access or malicious activities. Ensuring audit logs are enabled helps in incident response and forensic analysis by providing
    visibility into who accessed what, when, and from where. A good configuration includes enabling the Kubernetes audit logging
    feature and properly setting up log storage and retention policies.
  references:
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  compliance: []
- rule_id: k8s.secret.secret.auto_rotation_configured
  service: secret
  resource: secret
  requirement: Auto Rotation Configured
  scope: secret.secret.auto_rotation_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Automated Secret Rotation for Enhanced Security
  rationale: Without automated secret rotation, secrets may become stale and vulnerable to leaks or unauthorized access. Attackers
    could exploit long-lived secrets to gain persistent access to sensitive resources. Regular rotation ensures secrets are
    refreshed, reducing the attack surface and mitigating the risk of credential compromise.
  description: This rule checks if Kubernetes secrets have automated rotation configured, ensuring they are regularly updated.
    Proper configuration involves setting up mechanisms like external secret management tools that support automatic rotation
    and integration with Kubernetes, such as Vault or AWS Secrets Manager. Automated rotation minimizes the risk of using
    outdated credentials and helps prevent unauthorized access by ensuring secrets are periodically regenerated and replaced
    in applications.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/
  compliance: []
- rule_id: k8s.secret.secret.certificate_revocation_list_configured
  service: secret
  resource: secret
  requirement: Certificate Revocation List Configured
  scope: secret.secret.certificate_revocation_list_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure Certificate Revocation List (CRL) is Configured for Secrets
  rationale: Without a properly configured Certificate Revocation List (CRL), Kubernetes cannot verify if a certificate used
    in secret management has been revoked. This opens up the risk of unauthorized access and man-in-the-middle attacks if
    compromised certificates are not invalidated promptly.
  description: This control checks that the Certificate Revocation List (CRL) is properly configured for Kubernetes secrets.
    A valid CRL configuration ensures that any revoked certificates are recognized and rejected by the system, preventing
    their use in securing communication and data access. Proper CRL configuration helps mitigate risks such as unauthorized
    access due to the use of compromised certificates, thus enhancing the overall security of encryption in transit.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.secret.secret.encryption_at_rest_enabled_enforced
  service: secret
  resource: secret
  requirement: Encryption At Rest Enabled Enforced
  scope: secret.secret.encryption_at_rest_enabled_enforced
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Enforce Encryption at Rest for Kubernetes Secrets
  rationale: Without encryption at rest, secrets stored in etcd are vulnerable to unauthorized access or exfiltration. Attackers
    gaining access to the etcd datastore or a backup can potentially access sensitive information like passwords, tokens,
    and certificates, leading to privilege escalation and data breaches.
  description: This control checks that Kubernetes Secrets are stored encrypted at rest in etcd by verifying the encryption
    configuration in the kube-apiserver. A proper configuration involves setting up an encryption provider configuration file
    and specifying it in the kube-apiserver startup parameters. This ensures that sensitive data is protected from unauthorized
    access by encrypting it before storage, thus safeguarding against data leakage and complying with security best practices.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  compliance: []
- rule_id: k8s.secret.secret.etcd_encryption_enabled_enforced
  service: secret
  resource: secret
  requirement: Etcd Encryption Enabled Enforced
  scope: secret.secret.etcd_encryption_enabled_enforced
  domain: data_protection
  subcategory: encryption
  severity: high
  title: Enforce Etcd Encryption for Kubernetes Secrets
  rationale: Without etcd encryption, sensitive data stored as Kubernetes secrets can be exposed if unauthorized access to
    the etcd database occurs. Attackers gaining access to unencrypted etcd data can exploit this to retrieve sensitive information
    such as API keys, passwords, and other confidential data, leading to potential security breaches and data leaks.
  description: This check ensures that encryption at rest is enabled for etcd, which stores all cluster data, including Kubernetes
    secrets. A properly configured encryption mechanism for etcd involves setting up an encryption configuration file that
    specifies which resources should be encrypted. This is crucial for protecting the confidentiality and integrity of sensitive
    data stored within the Kubernetes cluster, mitigating risks of unauthorized data access, and adhering to security best
    practices.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
  - https://kubernetes.io/docs/setup/best-practices/enforcing-policies/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  compliance: []
- rule_id: k8s.secret.secret.external_secret_operator_enabled
  service: secret
  resource: secret
  requirement: External Secret Operator Enabled
  scope: secret.secret.external_secret_operator_enabled
  domain: data_protection
  subcategory: secrets_management
  severity: high
  title: Ensure External Secrets Operator is Enabled for Secret Management
  rationale: If the External Secrets Operator is not enabled, Kubernetes Secrets may rely solely on in-cluster storage, which
    is susceptible to configuration drift and potential unauthorized access. External Secrets Operator helps manage secrets
    securely by integrating with external secret management systems such as AWS Secrets Manager or HashiCorp Vault. This reduces
    the risk of exposing sensitive information through misconfigurations or unauthorized access.
  description: This control checks that the External Secrets Operator is installed and properly configured in the Kubernetes
    cluster. It ensures that secrets are managed using external secret management solutions, which provide robust access controls
    and auditing capabilities. A proper configuration involves verifying that the External Secrets Operator is deployed, configured
    to interact with the designated external secret stores, and that it has appropriate permissions to fetch and update secrets.
    This setup enhances security by minimizing the risk of secrets exposure due to Kubernetes API server vulnerabilities or
    misconfigurations.
  references:
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.secret.secret.git_repo_url_no_sensitive_credentials_configured
  service: secret
  resource: secret
  requirement: Git Repo Url No Sensitive Credentials Configured
  scope: secret.secret.git_repo_url_no_sensitive_credentials_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Secrets Do Not Contain Git Repository Credentials
  rationale: Storing sensitive credentials such as Git repository access tokens in Kubernetes Secrets without implementing
    access controls can lead to unauthorized access. If attackers gain access to these credentials, they can manipulate or
    extract sensitive data from repositories, potentially injecting malicious code or exfiltrating proprietary information.
  description: This check ensures that Kubernetes Secrets do not contain sensitive Git repository credentials, such as personal
    access tokens or SSH keys, without appropriate controls. A good configuration involves using environment variables or
    secure CI/CD mechanisms to handle credentials dynamically, combined with RBAC to restrict access. This prevents unauthorized
    access to Git repositories and complies with security best practices by minimizing the risk of credential exposure.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/concepts/security/overview/#protecting-cluster-components-from-compromise
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  compliance: []
- rule_id: k8s.secret.secret.hardcoded_secrets_scanned_enforced
  service: secret
  resource: secret
  requirement: Hardcoded Secrets Scanned Enforced
  scope: secret.secret.hardcoded_secrets_scanned_enforced
  domain: data_protection
  subcategory: secrets_management
  severity: high
  title: Enforce Scanning for Hardcoded Secrets in Kubernetes Secrets
  rationale: Hardcoded secrets in Kubernetes can lead to unauthorized access if not properly managed. Attackers who gain access
    to these secrets can exploit them to access sensitive resources or escalate privileges. By enforcing scanning for hardcoded
    secrets, you minimize the risk of exposing credentials and sensitive data, protecting your cluster from potential breaches.
  description: This control checks for the presence of hardcoded secrets within Kubernetes Secrets objects. It ensures that
    there is a mechanism in place to scan these secrets for any hardcoded credentials or sensitive information before deployment.
    Proper configuration involves integrating secret scanning tools that automatically detect and alert on the presence of
    hardcoded secrets, thereby reducing the risk of credential leakage and ensuring sensitive data is not exposed in plain
    text.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-config-file/
  compliance: []
- rule_id: k8s.secret.secret.minimize_key_storage_locations_configured
  service: secret
  resource: secret
  requirement: Minimize Key Storage Locations Configured
  scope: secret.secret.minimize_key_storage_locations_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Limit Secret Key Distribution Across Nodes
  rationale: Minimizing the distribution of secret keys across the cluster reduces the attack surface and mitigates the risk
    of secret exposure. If secret keys are stored in multiple locations, it increases the likelihood of unauthorized access
    through compromised nodes or misconfigured permissions, potentially leading to data breaches and privilege escalation.
  description: This control checks that Kubernetes secrets are configured to limit their storage locations to only the necessary
    nodes. A well-configured secret should be accessible only to the specific pods that require it, using mechanisms like
    projected volumes or environment variables. By restricting the distribution of secrets, clusters can prevent unauthorized
    access and reduce the risk of secrets being compromised due to node breaches. Proper configuration involves using tools
    like Secret Management solutions and ensuring secrets are not exposed in pod specs or logs.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/concepts/security/overview/#protecting-cluster-components
  compliance: []
- rule_id: k8s.secret.secret.no_sensitive_data_in_env_configured
  service: secret
  resource: secret
  requirement: No Sensitive Data In Env Configured
  scope: secret.secret.no_sensitive_data_in_env_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Avoid Storing Sensitive Data in Environment Variables
  rationale: Storing sensitive data such as passwords or API keys in environment variables poses a significant security risk
    because environment variables can be exposed via logs, debugging processes, or by unauthorized access to the environment.
    Attack vectors include pod compromise, where attackers can access environment variables, potentially leading to unauthorized
    access to services and data breaches.
  description: This rule checks that sensitive data is not stored in environment variables within Kubernetes pods. Best practices
    recommend using Kubernetes Secrets to manage sensitive information securely. Environment variables should not contain
    sensitive information to prevent exposure through logs or debugging tools. A good configuration uses Secrets mounted as
    volumes or accessed through the Kubernetes API, ensuring sensitive data is encrypted at rest and transmitted securely.
    This approach mitigates the risk of unauthorized data exposure and enhances overall cluster security.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  compliance: []
- rule_id: k8s.secret.secret.secret_rbac_configured_enforced
  service: secret
  resource: secret
  requirement: Secret Rbac Configured Enforced
  scope: secret.secret.secret_rbac_configured_enforced
  domain: identity_and_access_management
  subcategory: authorization
  severity: high
  title: Enforce RBAC for Kubernetes Secrets Access
  rationale: Misconfigured Role-Based Access Control (RBAC) for Kubernetes secrets can lead to unauthorized access, allowing
    attackers to exfiltrate sensitive data such as database credentials or API keys. Without proper RBAC, any user or service
    account could potentially access, modify, or delete secrets, increasing the risk of privilege escalation and lateral movement
    within the cluster.
  description: This control verifies that RBAC policies are correctly applied to restrict access to Kubernetes secrets. It
    ensures that only authorized users and service accounts have the necessary permissions to access specific secrets. A secure
    configuration involves creating roles with the least privilege necessary, and binding these roles to specific users or
    service accounts. This minimizes the attack surface and helps prevent data breaches by ensuring that secret data is not
    exposed to unauthorized entities.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance: []
- rule_id: k8s.secret.secret.secure_injection_enabled
  service: secret
  resource: secret
  requirement: Secure Injection Enabled
  scope: secret.secret.secure_injection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable Secret Injection via Environment Variables
  rationale: Failure to securely inject secrets into containers can expose sensitive data to unauthorized access. Attackers
    could exploit improperly managed secrets to escalate privileges, access sensitive information, or compromise the Kubernetes
    cluster's integrity.
  description: This control verifies that Kubernetes secrets are injected into containers using environment variables in a
    secure manner. It checks for best practices such as using Kubernetes secrets instead of hardcoding sensitive information
    within application code or configuration files. Secure injection via environment variables ensures that secrets are not
    exposed unnecessarily and are only accessible to authorized containers, reducing the risk of data breaches and facilitating
    compliance with security standards.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-kubectl/
  compliance: []
- rule_id: k8s.secret.secret.tls_certificates_expiration_configured
  service: secret
  resource: secret
  requirement: Tls Certificates Expiration Configured
  scope: secret.secret.tls_certificates_expiration_configured
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure TLS Certificates in Kubernetes Secrets Have Expiration Dates
  rationale: TLS certificates without expiration dates can lead to the use of outdated and potentially compromised encryption,
    making it easier for attackers to perform man-in-the-middle attacks or decrypt sensitive data. Expiration dates ensure
    that certificates are regularly updated, maintaining the integrity and confidentiality of data in transit and protecting
    against unauthorized access.
  description: This rule checks that all TLS certificates stored within Kubernetes Secrets have properly configured expiration
    dates. A valid configuration requires that each TLS certificate includes a specific expiration date, ensuring that the
    certificate is periodically reviewed and renewed. By enforcing expiration dates, Kubernetes clusters can mitigate risks
    associated with outdated cryptographic credentials, thereby reducing the potential for data breaches and ensuring compliance
    with security standards like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/tasks/administer-cluster/certificates/
  compliance: []
- rule_id: k8s.secret.secret.transmission_tls_enabled
  service: secret
  resource: secret
  requirement: Transmission Tls Enabled
  scope: secret.secret.transmission_tls_enabled
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enable TLS for Secret Transmission
  rationale: Without TLS encryption, secrets transmitted over the network are susceptible to interception and unauthorized
    access, leading to potential data breaches and exposure of sensitive information. Attackers can exploit unencrypted traffic
    to perform man-in-the-middle attacks and gain access to confidential data.
  description: This rule checks that all Kubernetes secrets are transmitted over the network using TLS encryption. Proper
    configuration ensures that data in transit is encrypted, protecting it from unauthorized interception. A secure setup
    requires that all communication channels involving Kubernetes secrets use TLS to provide confidentiality and integrity
    of data, thereby reducing the risk of exposure to eavesdropping and man-in-the-middle attacks.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  compliance:
  - nist_800_171_r2_multi_cloud_3_5_10_3.5.10_Store_and_transmit_only_cryptograph_0042
- rule_id: k8s.secret.secret.unused_secrets_cleaned_enabled
  service: secret
  resource: secret
  requirement: Unused Secrets Cleaned Enabled
  scope: secret.secret.unused_secrets_cleaned_enabled
  domain: data_protection
  subcategory: secrets_management
  severity: high
  title: Ensure Removal of Unused Kubernetes Secrets
  rationale: Failure to clean up unused Kubernetes secrets can lead to unnecessary exposure of sensitive data. Unused secrets
    may become a target for attackers who exploit misconfigured access controls, potentially leading to unauthorized access
    and data breaches.
  description: This rule checks for the presence of unused secrets within a Kubernetes cluster and ensures they are removed
    in a timely manner. A proper cleanup strategy involves routinely identifying secrets that are no longer associated with
    any running pods or services and deleting them to minimize the attack surface. This helps prevent attackers from exploiting
    stale secrets and enhances overall cluster security by maintaining only necessary sensitive information.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance: []
- rule_id: k8s.secret.tls.certificates_expiration_check
  service: secret
  resource: tls
  requirement: Certificates Expiration Check
  scope: secret.tls.certificates_expiration_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure TLS Secret Certificates Have Valid Expiration Dates
  rationale: Expired TLS certificates can lead to unauthorized data interception and man-in-the-middle attacks, as they may
    allow attackers to exploit expired certificates to impersonate services. Regularly checking and updating certificate expiration
    dates mitigates the risk of using outdated certificates that compromise data integrity and confidentiality.
  description: This control checks that TLS secrets in Kubernetes have certificates with valid, non-expired expiration dates.
    It ensures that each certificate under Kubernetes TLS secrets is renewed before expiration, adhering to security best
    practices. Properly configured expiration dates help maintain secure encrypted communication channels, preventing potential
    data breaches and ensuring compliance with security standards like PCI DSS and the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/certificates/
  compliance:
  - pci_dss_v4_multi_cloud_4.1.4_0060
- rule_id: k8s.secret.transmission.tls_enabled
  service: secret
  resource: transmission
  requirement: Tls Enabled
  scope: secret.transmission.tls_enabled
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce TLS for Kubernetes Secret Transmission
  rationale: Without TLS, Kubernetes secrets transmitted over the network are vulnerable to interception and man-in-the-middle
    attacks, potentially exposing sensitive data such as credentials and API keys. Enabling TLS ensures that communication
    is encrypted, protecting against eavesdropping and unauthorized data access.
  description: This rule checks that all communications involving Kubernetes secrets are secured with TLS encryption. Proper
    configuration requires that all API server and etcd communications are encrypted and certificates are correctly set up.
    Ensuring TLS is enabled helps maintain data confidentiality and integrity in transit, reducing the risk of data breaches
    and aligning with security best practices.
  references:
  - https://kubernetes.io/docs/concepts/configuration/secret/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/#protecting-cluster-components
  compliance: []
- rule_id: k8s.service.mesh.dos_protection_enabled
  service: service
  resource: mesh
  requirement: Dos Protection Enabled
  scope: service.mesh.dos_protection_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enable DoS Protection in Service Mesh
  rationale: Without DoS protection, the service mesh is vulnerable to Denial-of-Service attacks, which can overwhelm services,
    degrade performance, and lead to service outages. Attackers can exploit these vulnerabilities to disrupt service availability,
    impacting both application performance and user experience.
  description: This control verifies that DoS protection measures are enabled within the Kubernetes service mesh. It checks
    for configurations such as rate limiting and circuit breaking that are essential to mitigate the risk of resource exhaustion
    due to excessive requests. A properly configured service mesh will limit the impact of DoS attacks, ensuring that services
    remain available and performant even under attack. The configuration should follow best practices, such as those outlined
    in the CIS Kubernetes Benchmark, to ensure comprehensive protection.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
  - https://kubernetes.io/docs/concepts/policy/resource-quotas/
  - https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-5-a_1191
- rule_id: k8s.service.mesh.intrusion_detection_enabled
  service: service
  resource: mesh
  requirement: Intrusion Detection Enabled
  scope: service.mesh.intrusion_detection_enabled
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Enable Intrusion Detection in Kubernetes Service Mesh
  rationale: Without intrusion detection enabled in the service mesh, Kubernetes clusters are vulnerable to undetected attacks
    such as lateral movement, data exfiltration, and unauthorized access. Attackers could exploit these vulnerabilities to
    compromise cluster integrity and confidentiality, leading to significant security breaches and compliance violations.
  description: This control checks if intrusion detection systems (IDS) are enabled and properly configured within the Kubernetes
    service mesh. An effective IDS monitors network traffic for suspicious activities and potential threats, providing alerts
    and logs that help in identifying and mitigating attacks. Proper IDS configuration should align with best practices, such
    as monitoring ingress and egress traffic, and integrating with alerting mechanisms. This enhances security by detecting
    unauthorized access attempts and preventing potential intrusions, thereby maintaining compliance with industry standards
    like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/tasks/security/defend-cluster/audit/
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  compliance:
  - nist_800_53_rev5_multi_cloud_IR-4-a_0625
- rule_id: k8s.service.mesh.mtls_enabled
  service: service
  resource: mesh
  requirement: Mtls Enabled
  scope: service.mesh.mtls_enabled
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce mTLS in Kubernetes Service Mesh for Secure Communication
  rationale: Without mutual TLS (mTLS), data transmitted between services in a Kubernetes service mesh can be intercepted
    or manipulated by malicious actors. This misconfiguration exposes services to man-in-the-middle attacks, unauthorized
    access, and data breaches, undermining the confidentiality and integrity of sensitive information and communications.
  description: This rule checks that mutual TLS (mTLS) is enabled within the Kubernetes service mesh, ensuring that all communications
    between services are authenticated and encrypted. Proper mTLS configuration requires both client and server to authenticate
    each other using certificates, thus securing data in transit and mitigating the risk of eavesdropping or unauthorized
    data access. This control is crucial for compliance with security standards and industry best practices, contributing
    to a robust security posture by ensuring encrypted service-to-service communication.
  references:
  - https://istio.io/latest/docs/tasks/security/authentication/mtls/
  - https://istio.io/latest/docs/concepts/security/
  - https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-external-load-balancers
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_CA-9_0040
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-8_0133
  - fedramp_moderate_multi_cloud_SC-8_0343
  - fedramp_moderate_multi_cloud_SC-8_1_0344
  - hipaa_multi_cloud_164_312_e_1_0030
  - hipaa_multi_cloud_164_312_e_2_ii_0032
  - iso27001_2022_multi_cloud_A.8.24_0080
  - nist_800_171_r2_multi_cloud_3_11_3_3.11.3_Remediate_vulnerabilities_in_accord_0002
  - nist_800_171_r2_multi_cloud_3_13_15_3.13.15_Protect_the_authenticity_of_commu_0006
  - nist_800_171_r2_multi_cloud_3_1_13_3.1.13_Employ_cryptographic_mechanisms_to_0022
  - nist_800_171_r2_multi_cloud_3_1_1_3.1.1_Limit_system_access_to_authorized_use_0020
  - pci_dss_v4_multi_cloud_4.1.2_0059
  - pci_dss_v4_multi_cloud_4.1.4_0060
  - pci_dss_v4_multi_cloud_9.4.1.2_0132
- rule_id: k8s.service.mesh.rate_limiting_configured
  service: service
  resource: mesh
  requirement: Rate Limiting Configured
  scope: service.mesh.rate_limiting_configured
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Implement Rate Limiting in Kubernetes Service Mesh
  rationale: Without rate limiting, a Kubernetes service mesh is vulnerable to denial-of-service (DoS) attacks, where excessive
    requests can overwhelm services, leading to degradation or complete service outage. Rate limiting helps mitigate these
    risks by controlling the number of requests within a time period, thus reducing potential abuse and ensuring consistent
    service performance.
  description: This control checks that rate limiting is configured in the Kubernetes service mesh. A correctly implemented
    rate limiting policy defines thresholds for requests, which helps prevent resource exhaustion and service disruptions.
    Proper configuration involves setting request limits based on service capacity and expected load, thereby enhancing resilience
    against DoS attacks and aiding in traffic management. It ensures services are not overwhelmed and maintain availability
    under varying loads.
  references:
  - https://istio.io/latest/docs/tasks/policy-enforcement/rate-limit/
  - https://kubernetes.io/docs/tasks/administer-cluster/enforce-network-policies/
  - https://kubernetes.io/docs/concepts/services-networking/service/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-5-b_1192
- rule_id: k8s.service.mesh.threat_detection_integrated
  service: service
  resource: mesh
  requirement: Threat Detection Integrated
  scope: service.mesh.threat_detection_integrated
  domain: logging_and_monitoring
  subcategory: threat_detection
  severity: medium
  title: Integrate Threat Detection in Kubernetes Service Mesh
  rationale: Without integrated threat detection, malicious activities within the service mesh may go unnoticed, leading to
    potential data breaches and unauthorized access. Attack vectors such as lateral movement and man-in-the-middle attacks
    can exploit weaknesses in the service mesh traffic if not monitored effectively.
  description: This rule checks that a Kubernetes service mesh has threat detection capabilities integrated, such as anomaly
    detection and intrusion detection systems (IDS). Proper integration involves monitoring service-to-service communications
    for suspicious activities and anomalies, leveraging tools like Istio or Linkerd with built-in security features. This
    setup helps identify and mitigate threats early, ensuring compliance with security standards and reducing the risk of
    security incidents.
  references:
  - https://istio.io/latest/docs/ops/integrations/prometheus/
  - https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
  - https://istio.io/latest/docs/tasks/security/
  compliance:
  - iso27001_2022_multi_cloud_A.5.7_0039
  - nist_800_53_rev5_multi_cloud_RA-10-a_1029
- rule_id: k8s.service.mesh.tls_certificate_expiration_check
  service: service
  resource: mesh
  requirement: Tls Certificate Expiration Check
  scope: service.mesh.tls_certificate_expiration_check
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Ensure TLS Certificate Expiration is Monitored in Service Mesh
  rationale: TLS certificates are critical for encrypting data in transit within a Kubernetes service mesh. If certificates
    expire without being renewed, encrypted communications can be disrupted or fall back to insecure defaults, leading to
    exposure of sensitive data. Attackers can exploit expired certificates to perform man-in-the-middle attacks or impersonate
    services within the mesh.
  description: This control checks that all TLS certificates in use by the Kubernetes service mesh have a monitoring mechanism
    to track expiration dates and trigger renewals before expiry. A well-configured setup includes automated alerts and renewal
    processes to ensure continuous encryption of traffic within the mesh. This prevents potential disruptions, mitigates man-in-the-middle
    attacks, and ensures compliance with security standards such as PCI DSS and CIS Benchmarks.
  references:
  - https://kubernetes.io/docs/tasks/security/overview/
  - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  compliance:
  - pci_dss_v4_multi_cloud_3.6.1.1_0045
- rule_id: k8s.service.mesh.traffic_control_enabled
  service: service
  resource: mesh
  requirement: Traffic Control Enabled
  scope: service.mesh.traffic_control_enabled
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Enforce Traffic Management in Service Mesh
  rationale: Without proper traffic control in a service mesh, services may become vulnerable to unauthorized access, denial-of-service
    attacks, and data breaches. Traffic management ensures that only legitimate traffic is routed, reducing the attack surface
    and supporting the enforcement of security policies across microservices.
  description: This rule checks that traffic management is enabled within the Kubernetes service mesh. Proper configuration
    includes setting up ingress and egress control, rate limiting, and circuit breaking. These controls help prevent unauthorized
    access, manage traffic loads, and mitigate potential denial-of-service attacks. Ensuring traffic control is enabled and
    properly configured is crucial to maintaining network integrity and regulatory compliance.
  references:
  - https://istio.io/latest/docs/concepts/traffic-management/
  - https://kubernetes.io/docs/tasks/administer-cluster/traffic-control/
  - https://kubernetes.io/docs/concepts/services-networking/service/
  compliance:
  - fedramp_moderate_multi_cloud_SC-5_0331
- rule_id: k8s.service.no.external_ip
  service: service
  resource: 'no'
  requirement: External Ip
  scope: service.no.external_ip
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Prohibit External IPs on Kubernetes Services
  rationale: Allowing external IPs on Kubernetes services can expose internal services to the internet, increasing the risk
    of unauthorized access and potential data breaches. Attackers could exploit open external IPs to bypass network controls
    and launch attacks directly against the cluster's services.
  description: This control checks that Kubernetes services do not have external IPs assigned, which could unintentionally
    expose them to the internet. Best practices dictate that external traffic should be routed through controlled ingress
    points like LoadBalancers or Ingress controllers, which provide additional layers of security such as authentication,
    SSL termination, and traffic filtering. By ensuring no external IPs are assigned directly to services, this rule helps
    to maintain a secure boundary and prevent unauthorized access.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
  - https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#exposing-services
  - https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/
  compliance:
  - nist_800_53_rev5_multi_cloud_SC-25_1277
- rule_id: k8s.service.service.mesh_tls_enforced
  service: service
  resource: service
  requirement: Mesh Tls Enforced
  scope: service.service.mesh_tls_enforced
  domain: data_protection
  subcategory: encryption_in_transit
  severity: high
  title: Enforce Mutual TLS in Kubernetes Service Mesh
  rationale: Without mutual TLS (mTLS), data transmitted between services within the mesh can be intercepted or altered by
    attackers, leading to potential data breaches and unauthorized access. This misconfiguration can expose sensitive data
    to man-in-the-middle attacks and compromise the integrity and confidentiality of inter-service communication.
  description: This control ensures that mutual TLS is enforced within the Kubernetes service mesh, verifying that all communication
    between services is encrypted and authenticated. It checks for the presence of mTLS configurations in the service mesh,
    such as Istio or Linkerd, ensuring they are set to require encryption and mutual authentication. Enforcing mTLS helps
    in preventing eavesdropping and tampering of data in transit, thereby securing the communication channel and maintaining
    the integrity and confidentiality of data exchanged between services.
  references:
  - https://istio.io/latest/docs/tasks/security/authentication/mtls-migration/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://linkerd.io/2.10/features/automatic-mtls/
  compliance: []
- rule_id: k8s.service.service.no_external_ip_configured
  service: service
  resource: service
  requirement: No External Ip Configured
  scope: service.service.no_external_ip_configured
  domain: network_security
  subcategory: network_configuration
  severity: medium
  title: Disable External IPs for Kubernetes Services
  rationale: Exposing services with external IPs can inadvertently make them accessible from outside the cluster, increasing
    the risk of unauthorized access and potential attacks such as DDoS. Attackers could exploit misconfigured services to
    gain entry into the cluster, leading to data breaches or service disruptions.
  description: This control checks that no external IPs are configured for Kubernetes services. A service with an external
    IP allows traffic from outside the cluster to access it directly, which could bypass network policies and other security
    controls. Ensuring that services do not have external IPs unless absolutely necessary reduces the attack surface and aligns
    with security best practices. Properly configured services should use internal cluster IPs or be accessible through controlled
    ingress points, such as ingress controllers or load balancers with specific firewall rules.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
  - https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/
  - https://kubernetes.io/docs/concepts/services-networking/#headless-services
  compliance: []
- rule_id: k8s.service.type.loadbalancer_restricted
  service: service
  resource: type
  requirement: Loadbalancer Restricted
  scope: service.type.loadbalancer_restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict External LoadBalancer Usage in Services
  rationale: Improper configuration of LoadBalancer services can expose internal resources to the public internet, increasing
    the risk of unauthorized access and potential attacks such as DDoS or data exfiltration. Limiting LoadBalancer usage to
    only necessary services helps reduce the attack surface and protects sensitive data.
  description: This rule checks that Kubernetes services using the LoadBalancer type are appropriately configured to limit
    exposure to external networks. A proper configuration includes setting up network policies, using internal LoadBalancers
    where possible, and ensuring that only essential services have LoadBalancer access. This reduces the risk of inadvertent
    exposure and aligns with security best practices to prevent unauthorized access.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  - https://kubernetes.io/docs/concepts/security/overview/
  compliance:
  - canada_pbmm_moderate_multi_cloud_CCCS_AC-17_0013
  - canada_pbmm_moderate_multi_cloud_CCCS_SC-7_0132
  - fedramp_moderate_multi_cloud_AC-17_0034
  - fedramp_moderate_multi_cloud_AC-17_1_0035
  - fedramp_moderate_multi_cloud_AC-17_2_0036
  - fedramp_moderate_multi_cloud_AC-17_3_0037
  - fedramp_moderate_multi_cloud_AC-17_4_0038
  - fedramp_moderate_multi_cloud_SC-7_0332
  - fedramp_moderate_multi_cloud_SC-7_3_0333
  - fedramp_moderate_multi_cloud_SC-7_4_0334
  - fedramp_moderate_multi_cloud_SC-7_5_0335
  - fedramp_moderate_multi_cloud_SC-7_7_0336
  - fedramp_moderate_multi_cloud_SC-7_8_0337
  - fedramp_moderate_multi_cloud_SC-7_10_0338
  - fedramp_moderate_multi_cloud_SC-7_12_0339
  - fedramp_moderate_multi_cloud_SC-7_18_0340
  - fedramp_moderate_multi_cloud_SC-7_20_0341
  - fedramp_moderate_multi_cloud_SC-7_21_0342
  - iso27001_2022_multi_cloud_A.8.2_0075
  - iso27001_2022_multi_cloud_A.8.20_0076
  - iso27001_2022_multi_cloud_A.8.21_0077
  - nist_800_171_r2_multi_cloud_3_13_1_3.13.1_Monitor_control_and_protect_commu_0004
  - nist_800_171_r2_multi_cloud_3_13_11_3.13.11_Employ_FIPS-validated_cryptograph_0005
  - nist_800_171_r2_multi_cloud_3_13_16_3.13.16_Protect_the_confidentiality_of_CU_0007
  - nist_800_171_r2_multi_cloud_3_13_2_3.13.2_Employ_architectural_designs_softw_0008
  - nist_800_171_r2_multi_cloud_3_13_3_3.13.3_Separate_user_functionality_from_sy_0009
  - nist_800_171_r2_multi_cloud_3_13_4_3.13.4_Prevent_unauthorized_and_unintended_0010
  - nist_800_171_r2_multi_cloud_3_13_5_3.13.5_Implement_subnetworks_for_publicly_0011
  - nist_800_171_r2_multi_cloud_3_13_6_3.13.6_Deny_network_communications_traffic_0012
  - nist_800_171_r2_multi_cloud_3_13_8_3.13.8_Implement_cryptographic_mechanisms_0013
  - nist_800_171_r2_multi_cloud_3_1_14_3.1.14_Route_remote_access_via_managed_acc_0023
  - nist_800_171_r2_multi_cloud_3_1_3_3.1.3_Control_the_flow_of_CUI_in_accordance_0026
  - nist_800_171_r2_multi_cloud_3_3_1_3.3.1_Create_and_retain_system_audit_logs_a_0031
  - nist_800_53_rev5_multi_cloud_AC-17-b_0152
  - nist_800_53_rev5_multi_cloud_SC-7-a_1224
  - nist_800_53_rev5_multi_cloud_SC-7-b_1225
  - nist_800_53_rev5_multi_cloud_SC-7-c_1226
  - pci_dss_v4_multi_cloud_1.1.1_0001
  - pci_dss_v4_multi_cloud_1.1.2_0002
  - pci_dss_v4_multi_cloud_1.2.3_0004
  - pci_dss_v4_multi_cloud_1.2.6_0006
  - pci_dss_v4_multi_cloud_1.3.2_0010
  - pci_dss_v4_multi_cloud_1.3.3_0011
  - pci_dss_v4_multi_cloud_2.2.3_0022
  - pci_dss_v4_multi_cloud_7.2.1_0091
  - pci_dss_v4_multi_cloud_7.2.3_0093
  - pci_dss_v4_multi_cloud_7.2.5_0094
  - pci_dss_v4_multi_cloud_7.2.5.1_0095
  - pci_dss_v4_multi_cloud_8.3.9_0111
  - pci_dss_v4_multi_cloud_8.3.11_0112
  - pci_dss_v4_multi_cloud_9.2.2_0123
  - pci_dss_v4_multi_cloud_9.4.2_0133
  - pci_dss_v4_multi_cloud_11.1.1_0162
  - pci_dss_v4_multi_cloud_11.1.2_0163
  - pci_dss_v4_multi_cloud_11.2.1_0164
  - pci_dss_v4_multi_cloud_11.2.2_0165
  - pci_dss_v4_multi_cloud_11.3.2_0168
  - pci_dss_v4_multi_cloud_11.3.2.1_0169
  - pci_dss_v4_multi_cloud_12.3.1_0181
  - pci_dss_v4_multi_cloud_12.3.3_0182
  - pci_dss_v4_multi_cloud_12.3.4_0183
  - pci_dss_v4_multi_cloud_12.6.1_0188
  - pci_dss_v4_multi_cloud_12.6.2_0189
  - pci_dss_v4_multi_cloud_12.6.3_0190
  - pci_dss_v4_multi_cloud_12.6.3.1_0191
  - soc2_multi_cloud_cc_6_1_0009
  - soc2_multi_cloud_cc_6_6_0012
- rule_id: k8s.service.type.not_nodeport
  service: service
  resource: type
  requirement: Not Nodeport
  scope: service.type.not_nodeport
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict Use of NodePort Services
  rationale: Using NodePort services can expose internal applications to the internet, making them susceptible to unauthorized
    access and potential attacks such as DDoS or port scanning. NodePort services open a high-numbered port on every node
    in the cluster, creating multiple entry points for attackers if not properly secured.
  description: This control checks that Kubernetes services are not configured with the NodePort type. A NodePort service
    exposes a service on a static port on each node's IP, which can lead to inadvertent exposure of services to the external
    network. Ensuring services are not of type NodePort reduces the attack surface by limiting external access. Instead, use
    LoadBalancer or ClusterIP service types to manage external access through controlled ingress points.
  references:
  - https://kubernetes.io/docs/concepts/services-networking/service/#nodeport
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance:
  - iso27001_2022_multi_cloud_A.13.1_0004
  - iso27001_2022_multi_cloud_A.8.1_0064
  - iso27001_2022_multi_cloud_A.8.10_0065
  - iso27001_2022_multi_cloud_A.8.11_0066
  - iso27001_2022_multi_cloud_A.8.12_0067
  - iso27001_2022_multi_cloud_A.8.13_0068
  - iso27001_2022_multi_cloud_A.8.14_0069
  - iso27001_2022_multi_cloud_A.8.15_0070
  - iso27001_2022_multi_cloud_A.8.17_0072
  - iso27001_2022_multi_cloud_A.8.18_0073
  - iso27001_2022_multi_cloud_A.8.19_0074
  - nist_800_171_r2_multi_cloud_3_4_7_3.4.7_Restrict_disable_or_prevent_the_use_0040
  - pci_dss_v4_multi_cloud_1.2.1_0003
  - pci_dss_v4_multi_cloud_1.2.5_0005
  - pci_dss_v4_multi_cloud_1.2.7_0007
  - pci_dss_v4_multi_cloud_1.2.8_0008
  - pci_dss_v4_multi_cloud_1.3.1_0009
  - pci_dss_v4_multi_cloud_2.2.2_0021
  - pci_dss_v4_multi_cloud_2.2.6_0025
  - pci_dss_v4_multi_cloud_2.2.7_0026
  - pci_dss_v4_multi_cloud_7.2.2_0092
  - pci_dss_v4_multi_cloud_9.2.4_0125
  - pci_dss_v4_multi_cloud_9.3.2_0128
  - pci_dss_v4_multi_cloud_9.4.1.1_0131
  - pci_dss_v4_multi_cloud_11.3.1_0166
  - pci_dss_v4_multi_cloud_12.1.2_0178
  - pci_dss_v4_multi_cloud_12.1.3_0179
  - pci_dss_v4_multi_cloud_12.1.4_0180
  - pci_dss_v4_multi_cloud_12.5.1_0186
  - pci_dss_v4_multi_cloud_12.5.3_0187
  - pci_dss_v4_multi_cloud_12.7.1_0192
  - pci_dss_v4_multi_cloud_12.8.1_0193
  - pci_dss_v4_multi_cloud_12.8.2_0194
  - pci_dss_v4_multi_cloud_12.8.3_0195
  - pci_dss_v4_multi_cloud_12.8.4_0196
  - pci_dss_v4_multi_cloud_12.8.5_0197
  - pci_dss_v4_multi_cloud_12.10.3_0200
  - pci_dss_v4_multi_cloud_12.10.4_0201
  - pci_dss_v4_multi_cloud_12.10.4.1_0202
  - pci_dss_v4_multi_cloud_12.10.5_0203
  - pci_dss_v4_multi_cloud_12.10.6_0204
  - pci_dss_v4_multi_cloud_12.10.7_0205
  - rbi_bank_multi_cloud_8.2_0025
- rule_id: k8s.software.patch.management_enabled
  service: software
  resource: patch
  requirement: Management Enabled
  scope: software.patch.management_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Automatic Kubernetes Patch Management is Enabled
  rationale: Failure to enable automatic patch management can expose Kubernetes clusters to known vulnerabilities, as unpatched
    software is a common target for attackers. Without timely application of patches, critical security updates may not be
    implemented, leaving the system susceptible to exploitation through well-documented attack vectors such as privilege escalation
    and remote code execution.
  description: This control verifies that Kubernetes clusters have automatic patch management enabled, ensuring that security
    patches and updates are applied promptly. Proper configuration involves enabling automated processes that check for and
    apply patches to Kubernetes components, such as the kube-apiserver, kube-scheduler, and kube-controller-manager. This
    practice helps minimize the attack surface by ensuring that known vulnerabilities are mitigated swiftly, reducing the
    risk of unauthorized access and compromise.
  references:
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/
  - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/
  - https://kubernetes.io/releases/version-skew-policy/
  compliance:
  - pci_dss_v4_multi_cloud_6.2.1_0075
- rule_id: k8s.storage.backup.enabled
  service: storage
  resource: backup
  requirement: Enabled
  scope: storage.backup.enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure Persistent Volume Backups Are Enabled and Configured
  rationale: Without properly configured backups, data stored in Kubernetes persistent volumes is at risk of loss due to accidental
    deletion, corruption, or attacks such as ransomware. Backups are critical for data recovery and continuity, protecting
    against data breaches and ensuring system resilience.
  description: This rule checks that Kubernetes persistent volumes have backup solutions enabled and correctly configured
    according to best practices. It ensures that data is regularly backed up and securely stored, reducing the likelihood
    of data loss and facilitating rapid recovery. A good configuration includes automated backup schedules, secure storage
    of backups, and regular restoration tests. This enhances data integrity and availability, and helps meet compliance requirements
    by protecting sensitive data.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://kubernetes.io/docs/tasks/administer-cluster/disaster-recovery/
  - https://kubernetes.io/docs/concepts/architecture/cloud-controller/
  compliance: []
- rule_id: k8s.storage.volume.backup_enabled
  service: storage
  resource: volume
  requirement: Backup Enabled
  scope: storage.volume.backup_enabled
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: medium
  title: Ensure Persistent Volume Backups Are Configured
  rationale: Without regular backups, persistent volumes in Kubernetes are vulnerable to data loss due to incidents such as
    hardware failures, accidental deletions, or ransomware attacks. Lack of backups can lead to significant business disruptions,
    loss of critical data, and inability to recover quickly from failures, impacting both operational continuity and regulatory
    compliance.
  description: This rule checks whether persistent volumes in Kubernetes clusters have an associated backup process configured.
    It validates configurations against best practices for backup frequency, retention policies, and secure storage of backups.
    Ensuring backups are enabled helps in mitigating risks associated with data loss, supports disaster recovery plans, and
    aids in maintaining compliance with industry standards such as the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://kubernetes.io/docs/tasks/administer-cluster/cluster-backup/
  - https://kubernetes.io/docs/tasks/administer-cluster/disaster-recovery/
  compliance:
  - fedramp_moderate_multi_cloud_CP-6_0146
  - fedramp_moderate_multi_cloud_CP-6_1_0147
  - fedramp_moderate_multi_cloud_CP-6_2_0148
  - fedramp_moderate_multi_cloud_CP-6_3_0149
  - nist_800_53_rev5_multi_cloud_CP-6-a_0485
  - nist_800_53_rev5_multi_cloud_CP-9-a_0509
- rule_id: k8s.storage.volume.backup_policy_defined
  service: storage
  resource: volume
  requirement: Backup Policy Defined
  scope: storage.volume.backup_policy_defined
  domain: workload_security
  subcategory: admission_control
  severity: medium
  title: Enforce Backup Policies for Persistent Volumes
  rationale: Without a defined backup policy for Persistent Volumes, critical data may be lost due to accidental deletion,
    corruption, or malicious attacks, including ransomware. A lack of backups increases the risk of prolonged downtime and
    data breaches, as attackers could exploit this to cause intentional data loss or demand ransom.
  description: This control checks that all Kubernetes Persistent Volumes have an associated backup policy in place. A proper
    backup policy ensures that data is regularly copied and can be restored in case of failure or data loss. It should include
    details such as backup frequency, retention periods, and recovery procedures. Implementing and validating these policies
    helps mitigate risks associated with data loss, supports disaster recovery efforts, and aligns with security best practices.
  references:
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  - https://kubernetes.io/docs/tasks/administer-cluster/disaster-recovery/
  - https://kubernetes.io/docs/concepts/storage/volume-snapshots/
  compliance: []
- rule_id: k8s.storage.volume.snapshot_enabled
  service: storage
  resource: volume
  requirement: Snapshot Enabled
  scope: storage.volume.snapshot_enabled
  domain: business_continuity
  subcategory: backup_and_recovery
  severity: low
  title: Ensure Persistent Volume Snapshots are Enabled and Properly Configured
  rationale: Without enabled and properly configured snapshots, persistent data on volumes may be lost in the event of accidental
    deletions or malicious attacks, such as ransomware or data corruption. Snapshots provide a mechanism for data recovery,
    allowing for restoration to a previous state, thereby protecting against data integrity issues and ensuring business continuity.
  description: This rule checks whether Kubernetes Persistent Volumes have snapshot features enabled and configured according
    to best practices. A properly configured snapshot policy ensures regular, consistent backups and secure storage of snapshots.
    This helps in recovering from data loss events and mitigates the risk of data corruption or loss due to malicious activities.
    A good configuration includes setting up scheduled snapshots, maintaining a history of snapshots, and ensuring snapshots
    are stored securely with access controls.
  references:
  - https://kubernetes.io/docs/concepts/storage/volume-snapshots/
  - https://kubernetes.io/docs/tasks/administer-cluster/csi-snapshot/
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  compliance:
  - fedramp_moderate_multi_cloud_CP-6_0146
  - fedramp_moderate_multi_cloud_CP-6_1_0147
  - fedramp_moderate_multi_cloud_CP-6_2_0148
  - fedramp_moderate_multi_cloud_CP-6_3_0149
  - nist_800_53_rev5_multi_cloud_CP-6-a_0485
  - rbi_nbfc_multi_cloud_3.4_0020
  - rbi_nbfc_multi_cloud_7.2_0034
  - rbi_nbfc_multi_cloud_7.3_0035
- rule_id: k8s.workload.cronjob.execution_constraints_enforced
  service: workload
  resource: cronjob
  requirement: Execution Constraints Enforced
  scope: workload.cronjob.execution_constraints_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Resource Limits on CronJobs
  rationale: Without resource limits, CronJobs can consume excessive CPU or memory, leading to resource exhaustion and potential
    denial of service. This misconfiguration can be exploited by attackers to impact the availability of the cluster.
  description: This rule verifies that all CronJobs have defined resource requests and limits for CPU and memory. Properly
    enforcing these constraints ensures that CronJobs do not monopolize resources, potentially disrupting other workloads.
    It also helps prevent potential denial-of-service attacks by limiting the resources an attacker can consume if they gain
    control over a CronJob.
  references:
  - https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/
  compliance: []
- rule_id: k8s.workload.daemonset.daemonset_host_access_controlled_enforced
  service: workload
  resource: daemonset
  requirement: Daemonset Host Access Controlled Enforced
  scope: workload.daemonset.daemonset_host_access_controlled_enforced
  domain: identity_and_access_management
  subcategory: access_control
  severity: high
  title: Restrict Host Network Access for DaemonSets
  rationale: If host network access is misconfigured in DaemonSets, it can lead to potential security risks such as unauthorized
    access to sensitive host resources or network interfaces. Attackers could exploit this access to perform lateral movement,
    eavesdrop on network traffic, or execute further attacks on cluster nodes. It is crucial to ensure that only DaemonSets
    that absolutely require host network access are granted such permissions to minimize the attack surface.
  description: This rule checks that Kubernetes DaemonSets do not unnecessarily have host network access enabled. A secure
    configuration ensures that the 'hostNetwork' field in the DaemonSet specification is set to 'false' unless explicitly
    required for the workload to function correctly. By enforcing this control, organizations can significantly reduce the
    risk of unauthorized network access, limit potential attack vectors, and adhere to security best practices outlined in
    the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces-and-networking
  - https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
  compliance: []
- rule_id: k8s.workload.daemonset.host_pid_restricted
  service: workload
  resource: daemonset
  requirement: Host Pid Restricted
  scope: workload.daemonset.host_pid_restricted
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Restrict HostPID Namespace Access in DaemonSets
  rationale: Allowing DaemonSets to use the host's PID namespace can lead to potential privilege escalation attacks, where
    a compromised container gains the ability to view and interact with processes on the host. This misconfiguration may expose
    sensitive data and processes, making the cluster more vulnerable to attacks.
  description: This rule checks that DaemonSets do not have the 'hostPID' field set to 'true', ensuring that containers within
    the DaemonSet do not share the host's PID namespace. Proper configuration prevents containers from being able to view
    or interact with host-level processes, reducing the attack surface and improving the overall security posture of the Kubernetes
    cluster.
  references:
  - https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  - https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
  compliance: []
- rule_id: k8s.workload.deployment.resource_limits_enforced
  service: workload
  resource: deployment
  requirement: Resource Limits Enforced
  scope: workload.deployment.resource_limits_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Resource Limits on Deployments
  rationale: Without resource limits, a single pod can monopolize cluster resources, leading to denial-of-service conditions
    by exhausting CPU and memory. Resource limits also protect against accidental or malicious overconsumption of resources,
    which can degrade the performance and stability of other workloads and the cluster as a whole.
  description: This control checks that all Kubernetes Deployment configurations specify resource limits for CPU and memory.
    Proper resource limit configurations ensure that each pod is only able to consume a predetermined amount of resources,
    preventing excessive resource usage that could lead to performance degradation or denial-of-service conditions. By enforcing
    these limits, clusters maintain stability and ensure fair resource distribution among workloads, thereby enhancing security
    posture and operational reliability.
  references:
  - https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/
  - https://kubernetes.io/docs/tasks/administer-cluster/resourcequota/
  compliance: []
- rule_id: k8s.workload.job.completion_policies_enforced_enabled
  service: workload
  resource: job
  requirement: Completion Policies Enforced Enabled
  scope: workload.job.completion_policies_enforced_enabled
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Enforce Job Completion Policies in Kubernetes Workloads
  rationale: Without enforced job completion policies, Kubernetes jobs may run indefinitely or restart more times than necessary,
    consuming resources and potentially leading to denial-of-service conditions. Misconfigured policies could also allow unauthorized
    users to exploit jobs for resource exhaustion attacks.
  description: This control checks that Kubernetes jobs have completion policies properly configured, such as 'completions',
    'parallelism', and 'activeDeadlineSeconds'. A well-configured job ensures that resources are utilized efficiently and
    prevents jobs from running indefinitely, which helps mitigate risks associated with resource exhaustion and potential
    service disruptions. Enforcing these policies also aligns with security best practices and helps comply with standards
    like the CIS Kubernetes Benchmark.
  references:
  - https://kubernetes.io/docs/concepts/workloads/controllers/job/
  - https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/
  - https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  compliance: []
- rule_id: k8s.workload.serviceaccount.service_mesh_cert_auth_enforced
  service: workload
  resource: serviceaccount
  requirement: Service Mesh Cert Auth Enforced
  scope: workload.serviceaccount.service_mesh_cert_auth_enforced
  domain: identity_and_access_management
  subcategory: authentication
  severity: high
  title: Enforce Service Mesh Certificate Authentication for Service Accounts
  rationale: If service mesh certificate authentication is not enforced, workloads may authenticate without proper security
    measures, increasing the risk of impersonation attacks. Attackers could potentially exploit this to gain unauthorized
    access to services and sensitive data within the cluster.
  description: This control checks whether Kubernetes workload service accounts have service mesh certificate authentication
    enforced. A proper configuration ensures that only authenticated and authorized workloads communicate within the service
    mesh, reducing the risk of unauthorized access and data breaches. Enforcing this policy helps maintain secure communication
    channels and aligns with security best practices for identity and access management within a Kubernetes cluster.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/concepts/security/overview/
  - https://istio.io/latest/docs/concepts/security/
  compliance: []
- rule_id: k8s.workload.statefulset.persistence_secured_enforced
  service: workload
  resource: statefulset
  requirement: Persistence Secured Enforced
  scope: workload.statefulset.persistence_secured_enforced
  domain: infrastructure_security
  subcategory: configuration_management
  severity: low
  title: Ensure StatefulSet PVCs Are Properly Secured
  rationale: Improperly secured persistent volumes in StatefulSets can lead to unauthorized data access or data loss. Attackers
    may exploit misconfigurations to gain access to sensitive data or disrupt application functionality by manipulating stored
    data.
  description: This control checks that Persistent Volume Claims (PVCs) associated with StatefulSets are configured with appropriate
    access modes and storage class policies to ensure data integrity and confidentiality. It validates that PVCs are not given
    excessive permissions, such as unrestricted ReadWriteMany access, unless absolutely necessary. Properly configured PVCs
    mitigate the risk of unauthorized access and potential data breaches.
  references:
  - https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/
  - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
  - https://kubernetes.io/docs/concepts/storage/storage-classes/
  compliance: []
